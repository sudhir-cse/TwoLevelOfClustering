Generalizing the Notion of Support


Michael Steinbach
Dept. of Comp. Sci. & Eng.
University of Minnesota
steinbac@cs.umn.edu
Pang-Ning Tan
Dept. of Comp. Sci. & Eng.
Michigan State University
ptan@cse.msu.edu
Hui Xiong, Vipin Kumar
Dept. of Comp. Sci. & Eng.
University of Minnesota
{huix,kumar}@cs.umn.edu




ABSTRACT
The goal of this paper is to show that generalizing the notion
of support can be useful in extending association analysis to
non-traditional types of patterns and non-binary data. To
that end, we describe a framework for generalizing support
that is based on the simple, but useful observation that sup-
port can be viewed as the composition of two functions: a
function that evaluates the strength or presence of a pattern
in each object (transaction) and a function that summarizes
these evaluations with a single number. A key goal of any
framework is to allow people to more easily express, explore,
and communicate ideas, and hence, we illustrate how our
support framework can be used to describe support for a
variety of commonly used association patterns, such as fre-
quent itemsets, general Boolean patterns, and error-tolerant
itemsets. We also present two examples of the practical use-
fulness of generalized support. One example shows the use-
fulness of support functions for continuous data. Another
example shows how the hyperclique pattern--an association
pattern originally defined for binary data--can be extended
to continuous data by generalizing a support function.

Categories and Subject Descriptors: H.2.8 [Database
Management]: Database Applications - Data Mining

General Terms: Algorithms, Theory

Keywords: association analysis, support, hyperclique

1. INTRODUCTION
For binary transaction data, the support of a set of binary
attributes (items) X is the number of objects (transactions)
for which all the attributes of X have a value of 1. While
simple, this notion of support is central to the definition of
frequent and maximal itemsets, association rules, sequential
patterns, and other ideas in the area of data mining known
as association analysis [1, 2, 6, 7, 14]. Nonetheless, few ef-
forts to extend association analysis to handle non-traditional
types of patterns and non-binary data do so by modifying
the notion of support, and those efforts that do have been
specific to the work at hand. Thus, an overall framework
for understanding and extending support is still lacking.


Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22Г25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
The goal of this paper is to provide such a framework and
show its usefulness. Towards that end, this paper makes the
following contributions:
We introduce a framework for support based on
a view of support as the composition of two func-
tions: a pattern evaluation function that evaluates
the strength or presence of a pattern in each ob-
ject (transaction) and a summarization function that
summarizes these evaluations with a single number.
Since a key goal of any framework is to allow people to
more easily express, explore, and communicate ideas, we il-
lustrate how our framework can be used to describe support
for a variety of association patterns. This includes support
for traditional frequent itemsets, as well as support for as-
sociation patterns such as those based on general Boolean
formulas [3, 10] and error-tolerant itemsets (ETIs) [13].
We extend traditional support measures to data sets
with continuous attributes. Traditional support mea-
sures were designed for binary data, and although a con-
tinuous attribute can be mapped to binary attributes, this
technique has some well known limitations, e.g., information
is lost. We illustrate this fact and the usefulness of support
functions for continuous data through an example based on
Min-Apriori [5]. Also, because the anti-monotone property
of support is important for the efficient generation of asso-
ciation patterns, we investigate the conditions under which
support measures for continuous data possess this property.
We show how an association pattern defined for bi-
nary data, the hyperclique pattern [12], can be ex-
tended to continuous data by using a generalized
notion of support.
The key step is to choose pattern
evaluation and summarization functions to construct a ver-
sion of support that preserves both the anti-monotone and
high affinity properties of the hyperclique pattern. The high
affinity property guarantees that the attributes in a hyper-
clique are pairwise similar to one another at some minimum
level, e.g., have a pairwise cosine similarity of 0.5.
2. TRADITIONAL SUPPORT
In this section, we review the definitions of support-based
concepts used in traditional transaction analysis. An overview
of the notation used in this and later sections is provided in
Table 1. Also, throughout this document, the terms `row,'
`transaction,' and `object' are used interchangeably, as are
the terms `column,' `item,' `variable,' and `attribute.'
Given binary transaction data, the support of a set of
binary attributes (items) X is the number of objects (trans-
actions) for which all the attributes of X have a value of 1.
More formally, for a binary data matrix D, the support of an



689
Research Track Poster

Table 1: Summary of Notation
Notation
Description
D
Data matrix of M rows and N
columns
T = {t1, t2, и и и , tM}
Set
of
objects
(transactions,
rows) of D
I = {i1, i2, и и и , iN}
Set of attributes (items,
vari-
ables, columns) of D
t
An object (transaction, row) or
its index
i, j, k
An attribute (item, variable, col-
umn) or its index
i, j
An attribute (item, variable, col-
umn) considered as a vector
X, Y
A set of attributes (items)

itemset X  I is given by (X) = |{t  T : D(t, i) = 1, i 
X} where |{и}| denotes the number of elements that belong
to a given set. An itemset is frequent if (X) > minsup,
where minsup is a specified minimum support threshold.
An association rule, X  Y , describes a relationship be-
tween two itemsets X and Y such that the items of Y occur
in a transaction whenever the items of X occur. We measure
the strength of such a relationship by the support of an as-
sociation rule, (X  Y ) = (X  Y ), which is the number
of transactions in which the relationship holds, and by the
confidence of an association rule, conf(X  Y ) =
(XY )
(X)
=
(XY )
(X)
, which is the fraction of transactions containing the
items of X that also contain the items of Y .
An important property of support is the anti-monotone
property: If X and Y are two itemsets where X  Y , then
(Y )  (X).
The downward closure or anti-monotone
property [14] of standard support can be used to efficiently
find frequent itemsets and is the foundation of the well-
known Apriori algorithm [1]. If a new support measure also
possesses the anti-monotone property, then we may also be
able to find its associated patterns efficiently, and thus, in
what follows, we shall often focus on this issue.
3. A GENERAL SUPPORT FRAMEWORK
3.1 Basics
In the next three subsections, we describe the three con-
cepts that are fundamental to our support framework: pat-
tern evaluation (eval) functions, summarization (norm) func-
tions, and the support functions that can be created from
eval and norm functions. We then show how this support
framework can be used to express support for frequent item-
sets, general Boolean patterns, and Error Tolerant Itemsets
(ETIs) [13].
3.1.1
Pattern Evaluation Functions
The evaluation of the strength of a pattern can take var-
ious forms. Most commonly, and this is the case for tra-
ditional association analysis, the pattern is either present,
i.e., the pattern strength is 1, or it is absent, i.e., the pattern
strength is 0. An example of such a pattern is the elemen-
twise `and' as defined in Table 2. In other situations, such
as continuous or count data, a binary evaluation of pattern
strength may not be as interesting. For example, suppose
that we are interested in sets of values that are relatively
homogenous within an object. Then, for non-binary data,
the range of the attribute values might be a useful mea-
sure of pattern strength--one that gives a wider variation in
strength than 0 and 1. This might be useful for count data,
such as that in Table 4, which shows the number of times
that a term occurs in a document. However, we may want
to combine both of the preceding approaches, by measur-
ing the strength of the pattern using a continuous measure,
such as the range, but then evaluating whether this measure
meets a specified condition, such as whether the range of the
values is less than a specified threshold.
Thus, an evaluation function, eval, is a function that takes
a set of of attributes X  I as an argument, and returns
a pattern evaluation vector, v, whose ith component is the
strength of the target pattern in the ith object. More for-
mally, we can write
v
=
eval(X),
or
(1)
v(t)
=
eval(t, X), t  T
(2)

If there are several sets of attributes under consideration,
e.g., X and Y , then we will distinguish between their pat-
tern evaluation vectors by using subscripts, e.g., vX and vY .
Notice that an eval function may be applied either to a sin-
gle object, in which case, it returns a single value, or to a
set of objects, in which case, it returns a vector of values.
Various eval functions are shown in Table 2.
Table 2: eval functions. X = {i1, i2, иии , ik}  I.
eval
function
Definition
1

eval(t, X) = D(t, i1)  . . .  D(t, ik)
2
eval
(t, X) = D(t, i1)  . . .  D(t, ik)
3
min
evalmin(t, X) = min1
jk
{D(t,ij)}
4
max
evalmax(t, X) = max1
jk
{D(t,ij)}
5
range
evalrange(t, X) = evalmax(t, X) - evalmin(t, X)

6
ETI
evaleti, (t, X) =
iX
D(t,i)
|X|
 1 -

Table 3: norm functions.
M is the length of the
vector, k is a parameter, and w is a vector of weights.
norm function
Definition

1
Lk
||v||k =
k
M
t=1
|v(t)|k

2
L1
||v||1 =
M
t=1
|v(t)|

3
L2
||v||2 =
M
t=1
|v(t)|2

3
L22
||v||22 =
M
t=1
|v(t)|2
4
weighted sum
normw(v, w) =
M
t=1
w(t)v(t)
5
sum (norm
)
norm
(v) =
M
t=1
v(t)
6
avg (normavg)
normavg(v) =
1
M
M
t=1
v(t)
7
weighted avg
normwavg(v, w) = normw(v, w),
(normwavg)
where
M
t=1
w(t) = 1

8
weighted Lk
||v||k,w =
k
M
t=1
w(t)|v(t)|k

3.1.2
Summarization Functions
It is useful to summarize the pattern evaluation vector v
by a single number, e.g., by using a vector norm [4]. The
most common vector norm is the Lk norm which is defined
in Table 3, along with two of its most useful specific versions,
the L1 and L2 norms. We also use the squared L2 norm, L22,
which is the sum of the squares of the components of v. We
use the notation Lk or normLk to refer to these functions.
We can also consider norm functions which are weighted
sums, where the weights are associated with objects. We
identify the following special cases: the weights sum to 1 (the
weighted average norm, normwavg); the weights are equal
and sum to 1 (the average norm, normavg); (the weights
are all 1 (the sum norm, norm
). It is also possible to
define the weighted Lk norm. For completeness, these norm
functions are also shown in Table 3, but for simplicity, we
restrict our discussion to the L1, L2, and L22 norms.
3.1.3
Generalized Support Functions
The support of a pattern among a set of attributes X is a
function, (X), that is the composition of a pattern evalu-
ation function, eval, and a summarization function, norm,



690
Research Track Poster

which summarizes these evaluations with a single number.

(X) = (norm  eval)(X) = norm(eval(X))
(3)

Given a support function, the goal is to use it to find sets
of attributes that meet some support criterion. If, our sup-
port function has the anti-monotone property, as is typically
the case, then we proceed by setting a minimum support
threshold minsup and using an algorithm such as Apriori.
The result is a collection of strong pattern sets,1 i.e., a col-
lection of sets of attributes that have support greater than
minsup.

3.2 Example: Standard Support
We present different choices of eval and norm that re-
produce the standard definition of support for binary data.
Consider the following three support functions from Table
2: the logical and of the attribute values, eval, the prod-
uct of the values, eval
, and the minimum of the values,
evalmin, and let X = {i1, i2, иии , ik} be an itemset (set of
binary attributes). For a specific binary transaction, any of
these pattern evaluation functions will produce a 1 exactly
when all the attributes of X have attribute values of 1; if
any attribute value is 0, then these functions return a 0.
If we use any of these three eval functions to produce the
pattern evaluation vector, v, then the L1, and L22 norms--see
Table 3--will yield a value that is the count of the number
of transactions that have all the items of X.
We adopt the following notation to refer to the different
types of support functions that we have created:

eval,
norm
= norm  eval
(4)

For example, the support function that is based on the
evalmin and normL22 functions is written as follows:

min,
L2
2
= normL22  evalmin = ||evalmin||22
(5)


3.3 Example: Boolean Support Functions
A Boolean support function, b,
L1
, is any support func-
tion that uses a Boolean pattern evaluation function evalb
and the L1 norm. (A Boolean pattern evaluation function
returns either 0 or 1.) An example of a Boolean support
function is the traditional support of an itemset X, which
is equivalent to measuring the size of the set of transactions
for which a conjunction of of the items (binary attributes)
in X is true. This approach can be generalized--see for ex-
ample [3, 10]--to more general Boolean formulas that use
the logical connectives  (and),  (or), and г (not). Even
more generally, we can consider a Boolean pattern evalua-
tion function such as evalrange<constant [9], where the eval
function is not a Boolean formula and where the data may
not be binary. To illustrate, consider the data of Table 4. Set
constant = 3 and let X = {term1, term2, term3}. Then the
pattern evaluation vector is given by v = evalrange<
3
(X) =
(1, 0, 0, 0, 0, 1, 1, 0, 0, 0), and thus, range<constant,
L1
= 3,
i.e., only 3 documents of Table 4 support the pattern. While
range<constant,
L1
has the anti-monotone property, in gen-
eral, Boolean support functions may not be either monotone
or anti-monotone.

1
The name frequent itemset is not appropriate in the general
case.
3.4 Example: Error Tolerant Itemsets
Error tolerant itemsets [13] relax the requirement that
every transaction supporting the itemset must contain every
item. Instead, it is enough that each transaction contain
most of the items in the specified itemset. The definition of
a strong ETI given below is taken from [13], but modified
to make the notation and terminology consistent with that
of this paper. For example, we might specify a strong ETI
by requiring that each supporting transaction have at least
4 of the 5 specified items ( = 0.2), and that at least 2% of
the transactions ( = 0.02) support the strong ETI.
Definition 1. Strong Error Tolerant Itemset
A strong ETI consists of a set of items X  I, such that
there exists a subset of transactions R  T consisting of at
least   M transactions and, for each t  R, the fraction of
items in X which are present in t is at least 1 - . M is the
number of transactions,  is the minimum support expressed
as fraction of M, and
is the fraction of items that can be
missing in a transaction.
Given a parameter , we can define a Boolean evaluation
function evaleti, to detect a strong ETI pattern:

evaleti, (t, X) =
iX
D(t,i)
|X|
 1 -
(6)

This eval function, together with the L1 norm, can be
used to define a support function for strong ETIs.

(eti,
), L1
(X)
=
(normL1  evaleti, )(X)
(7)
=
||evaleti, (X)||1
(8)

4. SUPPORT FOR CONTINUOUS DATA
The traditional approach to dealing with continuous data
in association analysis is to convert each continuous attribute
into a set of binary attributes. This is typically a two step
process. First the continuous attribute is discretized, i.e.,
we find a set of thresholds that can be used to convert the
attribute into a categorical variable. Then, each value of the
categorical variable is mapped to a binary variable. How-
ever, converting continuous data to binary transaction data
loses information with respect to both the magnitude of the
data and the ordering between values. The motivation for
considering continuous support measures is to allow asso-
ciation analysis for continuous data without such a loss of
information.
4.1 Example: Min-Apriori
We begin our investigation of continuous support mea-
sures with an example based on the Min-Apriori algorithm
[5] and the data of Table 4. Min-Apriori corresponds to the
use of the support function min,
L1
. However, Min-Apriori
first normalizes the data in each column by dividing each col-
umn entry by the sum of the column entries. The normalized
data is shown in Table 5. One reason for using normalization
is to make sure that the resulting support value is a number
between 0 and 1. Another, perhaps more important reason
is to ensure that all data is on the same scale so that sets of
items that vary in the same way have similar support val-
ues. For example, suppose we have three items i1, i2, and i3,
and that i2 = 2i3, while i3 = 3i1. Without normalization,
min,
L1
({i1, i2}) is not equal to min,
L1
({i2, i3}).
Thus,
normalization is often desirable in many domains, e.g., text
documents.
However, a side-effect of normalization is that individual
items can no longer be pruned using a support threshold



691
Research Track Poster

since all items have a support of 1. In Section 6.3, we discuss
normalization in the context of the hyperclique pattern.
The computation of the support of the set of attributes
X = {term3, term4} is shown in Table 6, where the first two
columns show the normalized values for term3 and term4,
while the third column shows the minimum of these two val-
ues for each row (object), i.e., column 3 is the pattern eval-
uation vector v = evalmin({term3, term4}). The support of
{term3,term4} is computed by taking the sum of column
3. Notice that term3 and term4 have individual supports of
1, as do all individual terms after normalization. The sup-
port of {term3, term4} is 0.33, which indicates a moderate
relationship. By contrast, the support of {term2, term4} is
0 since these two terms do not co-occur in any document.
An alternative would be to convert the original data to
a binary matrix2 and then compute support. If we express
support as a fraction, this yields a support of 0.1 for {term3,
term4}. The reason for the discrepancy between the two
versions of support is that these two terms do not co-occur
much, but both have about a third of their weight in the last
document. As an example of a case, where both versions
of support are close, the traditional support for {term1,
term2} is 0.5, which is similar to the value of 0.53 computed
using normalized data and min,
L1
.

Table 4: Table of document-term frequencies.
term1
term2
term3
term4
term5
term6
doc1
9
8
8
0
0
0
doc2
5
0
0
1
13
10
doc3
8
3
0
0
1
4
doc4
4
0
0
0
4
10
doc5
0
9
0
0
5
10
doc6
7
5
0
0
11
0
doc7
11
11
12
0
0
0
doc8
9
1
0
0
0
9
doc9
9
0
0
10
0
0
doc10
4
0
10
7
0
0

Table 5: Table of document-term frequencies nor-
malized to have an L1 norm of 1.
term1
term2
term3
term4
term5
term6
doc1
0.14
0.22
0.27
0.00
0.00
0.00
doc2
0.08
0.00
0.00
0.06
0.38
0.23
doc3
0.12
0.08
0.00
0.00
0.03
0.09
doc4
0.06
0.00
0.00
0.00
0.12
0.23
doc5
0.00
0.24
0.00
0.00
0.15
0.23
doc6
0.11
0.14
0.00
0.00
0.32
0.00
doc7
0.17
0.30
0.40
0.00
0.00
0.00
doc8
0.14
0.03
0.00
0.00
0.00
0.21
doc9
0.14
0.00
0.00
0.56
0.00
0.00
doc10
0.06
0.00
0.33
0.39
0.00
0.00

Table 6: Computation of support for the set of at-
tributes containing term3 and term4.
Document/Term
term3
term4
min(term3, term4)
doc1
0.27
0.00
0.00
doc2
0.00
0.06
0.00
doc3
0.00
0.00
0.00
doc4
0.00
0.00
0.00
doc5
0.00
0.00
0.00
doc6
0.00
0.00
0.00
doc7
0.40
0.00
0.00
doc8
0.00
0.00
0.00
doc9
0.00
0.56
0.00
doc10
0.33
0.39
0.33
Support
1.00
1.00
0.33

2
We convert entries to a 1 only if they are greater than 0.
4.2 Preserving the Anti-Monotone Property of
Support Measures for Continuous Data
The situation with respect to the anti-monotone prop-
erty of support depends on the norm and eval functions,
as well as the data. We start by defining the concept of an
anti-monotone eval function and the conditions under which
selected norm functions are monotonic. We then prove a
general theorem that relates the anti-monotone property of
an eval function and the monotonicity of a norm function
to the anti-monotone nature of a support function based
on them. (This is important, of course, because an anti-
monotone support function can yield efficient algorithms for
discovering support based patterns.) Using these results and
the anti-monotone property of evalmin and eval
, we can
then show that support functions based on eval
or evalmin
and the Lk and L22 norms, also have the anti-monotone prop-
erty for continuous data. We will also use this result later.
Simply put, an eval function is anti-monotone if its values
is guaranteed to be non-increasing as the number of items
increases. More formally, we have the following definition:
Property 4.1. Anti-monotone Property for Pattern
Evaluation Functions
A pattern evaluation function, eval, is anti-monotone if,
for any two sets of attributes X and Y
where X  Y ,
eval(t, Y )  eval(t, X), t  T .

Before proving the main theorem of this section, we need
a lemma about norm functions.
Lemma 4.1. For any two vectors u and v of length M, if
|u(t)|  |v(t)|,t 1  t  M, then norm(u)  norm(v) for
the Lk and L22 norms.
Proof. The Lk and L22 norms (and their weighted ver-
sions with non-negative weights) are monotonic functions of
the absolute values of the components of u and v.

The following key theorem connects the anti-monotone
property of an eval function with the anti-monotone prop-
erty of a support function based on it.

Theorem 4.1. Let eval be an anti-monotone, non-negative
pattern evaluation function.
Then the support functions,
eval,
Lk
and eval,
L2
2
, have the anti-monotone property.

Proof. We assume that X and Y are sets of attributes,
X = {i1, . . . , ik} and Y = X  {ik
+1
}, where ik
+1
/
 X.
Let eval(X) = vX and eval(Y ) = vY . Since eval is anti-
monotone, vY (t)  vX(t). Because eval is non-negative,
vX and vY are as well, and Lemma 4.1 can then be applied
to yield norm(vY )  norm(vX) for the Lk and L22 norms.
Therefore, eval,
Lk
and eval,
L2
2
have the anti-monotone
property for non-negative data.

The eval functions, evalmin and eval
, have the anti-
monotone property, i.e., evalmin is anti-monotone for non-
negative data and eval
is anti-monotone for non-negative
data between 0 and 1. (These proofs are straightforward
and are omitted to save space.) Thus, we can prove the fol-
lowing two theorems about the anti-monotone property of
support functions based on these two eval functions.

Theorem 4.2. For non-negative data, support functions,
min,
Lk
and min,
L2
2
, have the anti-monotone property.

Proof. This follows directly from the anti-monotone prop-
erty of evalmin and Theorem 4.1.



692
Research Track Poster

Theorem 4.3. For non-negative data between 0 and 1,
i.e., 0  D(t, i)  1, t  T , i  I, the support functions,

, Lk
and 
, L2
2
, have the anti-monotone property.

Proof. This follows directly from the anti-monotone prop-
erty of eval
and Theorem 4.1

5. THE HYPERCLIQUE PATTERN
A hyperclique pattern [12] is a frequent itemset with the
additional requirement that every item in the itemset implies
the presence of the remaining items with a minimum level
of confidence known as the h-confidence (or all-confidence
[8]). More formally we have the following definition:
Definition 2. Hyperclique A set of attributes, X  I,
forms a hyperclique with a particular level of h-confidence,
where h-confidence is defined as

hconf(X)
=
min
iX
{conf({i}  {X - {i}})}
(9)

=
(X)/ max
iX
{(i)}
(10)


5.1 Properties of h-confidence
The following properites of h-confidence are proved in
[12]. h-confidence is in the interval [0, 1] and has the anti-
monotone property. The cross support property, which is
useful for efficiently finding hypercliques, states that the
only possible attributes that can be in a hyperclique with an
attribute i for a given level of h-confidence hc are those at-
tributes whose support falls in the interval [hc(i), (i)/hc].
This feature of the hyperclique pattern implies that attributes
that are too different in terms of their support cannot belong
to the same hyperclique pattern. Finally, hypercliques also
have the high affinity property, i.e., items in a hyperclique
with a high h-confidence have a high pairwise similarity.

5.2 H-Confidence As Support
In Section 6, we will show that we can extend the hy-
perclique pattern to continuous data. However, even before
that, we can show an important relationship between hy-
percliques in binary transaction data and the support func-
tion min,
L2
2
(X). In particular, since min,
L2
2
(X) is equiv-
alent to standard support for binary data, we can substi-
tute min,
L2
2
(X) for the standard support function (X) in
Equation 10. If we normalize all attributes to have an L2
norm of 1, then min,
L2
2
({i}) = 1 for all items i, and by
Equation 10, hconf(X) = min,
L2
2
(X). Hence, our support
framework provides a simple interpretation of h-confidence
as support for normalized data.
To illustrate this point, we provide an example. In tables 7
and table 8 we show, respectively, some sample data and the
same data after it has been normalized to have an L2 norm
of 1. Let X be the itemset consisting of all five items. Then,
from Table 7, we see that the (standard) support of X is 3,
while the maximum support of any item is 5. Thus, the h-
confidence of X is 3/5 = 0.6. Using Table 8, we can compute
min,
L2
2
(X) by taking the min of each row, squaring it, and
then summing, i.e., min,
L2
2
(X) = 3  (0.447)2 = 0.6.

6. EXTENDING THE HYPERCLIQUE PAT-
TERN TO CONTINUOUS ATTRIBUTES
In this section, we extend the hyperclique pattern to con-
tinuous data by using the min,
L2
2
support function. It is
straightforward to show that all the properties of h-confidence
Table 7:
Example to illustrate h-confidence as
support--original data.
Transaction/Item
1
2
3
4
5
1
1
1
1
1
1
2
1
1
1
1
1
3
1
1
1
1
1
4
1
0
1
1
1
5
1
0
0
0
1

Table 8:
Example to illustrate h-confidence as
support--normalized data.
Trans/Item
1
2
3
4
5
min
1
0.447
0.577
0.500
0.500
0.447
0.447
2
0.447
0.577
0.500
0.500
0.447
0.447
3
0.447
0.577
0.500
0.500
0.447
0.447
4
0.447
0
0.500
0.500
0.447
0
5
0.447
0
0
0
0.447
0

for binary data that were discussed in section 5.1, also hold
for continuous data. However, because of space limitations,
we only prove results for the high-affinity property of hyper-
cliques with normalized data. Further details are in [11].
6.1 The High-Affinity Property
As with binary data, the high-affinity property for hyper-
cliques with continuous data guarantees that the attributes
are pairwise similar to one another at some minimum level.
Specifically, a lower bound for the minimum pairwise cosine
similarity is given by the h-confidence of the hyperclique.
We formally prove this in Theorem 6.1.
In the following, i and j are attributes i and j interpreted
as vectors and they have an L2 norm of 1.
Theorem 6.1. Cosine high-affinity property. Assume
that the data is non-negative and all attributes have an L2
norm of 1. Let X be a set of attributes with an h-confidence
of hc. Then, for any two attributes of X, i and j, cos(i, j) 
hc, where cos(i, j) is the cosine similarity between i and j.

Proof.

cos(i, j)
=
i и j
 ||v||22, where v = evalmin(X)
=
min,
L2
2
(X)

=
hconf(X)
=
hc

Line 2 follows from line 1 because i and j are elementwise
 v for i  X or j  X. Line 3 follows from the definition
of min,
L2
2
. Line 4 follows from line 3 because hconf(X) =
min,
L2
2
(X) when attributes have an L2 norm of 1.

6.2 An Example
To illustrate the high-affinity property for continuous hy-
percliques, we use an example based on the data of Table 4.
The computation of the support of the set of attributes X =
{term1, term2, term3} is shown in Table 9, where the first
three columns are normalized versions of term1, term2, and
term3 from Table 4. (Here, we use the L2 norm, not the L1
norm as in the Min-Apriori example.) The fourth column
shows the minimum of these three attributes for a particular
row (object). The support of the three terms is computed
by taking the sum of squares of column 4, and that value,
0.38, is also the h-confidence. This is indeed a lower bound
for the pairwise cosine similarity, since the lowest pairwise
similarity of these items is 0.6.



693
Research Track Poster

Table 9: Computation of support for the set of at-
tributes containing term1, term2, and term3.
Doc/Term
term1
term2
term3
min(1, 2, 3)
1
0.39
0.46
0.46
0.39
2
0.22
0
0
0
3
0.35
0.17
0
0
4
0.17
0
0
0
5
0
0.52
0
0
6
0.30
0.29
0
0
7
0.48
0.63
0.68
0.48
8
0.39
0.06
0
0
9
0.39
0
0
0
10
0.17
0
0.57
0
Support
1.0
1.0
1.0
0.38

6.3 Normalization
Normalization is not required for extending the hyper-
clique pattern to continuous data--see [11].
However, as
with Min-Apriori, normalization adjusts for attributes with
different measurement scales and produces a support value
that is between 0 and 1. On the negative side, after normal-
ization, all single items have a support of 1 and thus, cannot
be pruned by using a support threshold or the cross support
property.
To more fully understand the pluses and minuses of nor-
malization, we consider two additional facts. First, contin-
uous attributes can have widely different support and still
be very similar to one another.
This is not true for bi-
nary attributes.3 Second, for continuous hypercliques, the
cross support property still dictates that two attributes with
widely different levels of support cannot be together in a
hyperclique with high h-confidence--see [11]. Thus, contin-
uous attributes, which are highly similar, but which have
widely different support, can only appear in hypercliques
with low h-confidence. However, many attributes in such
low h-confidence hypercliques will not be very similar to
one another.
To summarize, without normalization, we can effectively
find continuous hypercliques with highly similar attributes
only if they have similar support. This is exactly the same
as with binary attributes. However, to effectively find highly
similar continuous attributes with widely differing support,
normalization is necessary.
7. RELATED WORK
To save space, we limit our discussion of prior work to
that already present in the body of the paper and refer the
reader to our technical report [11] for more details.
8. CONCLUSIONS AND FUTURE WORK
We have described a framework for generalizing the no-
tion of support and have shown that this framework can
be used to express support for several existing association
patterns: frequent itemsets, general Boolean patterns, and
error tolerant itemsets. We also showed how this framework
can be used to extend binary association patterns, e.g., the
hyperclique pattern, to continuous data.
There are many possibilities for future work. On the prac-
tical side, we plan to explore applications of the continuous
hyperclique pattern. On the theoretical side, we plan to in-
vestigate new types of support for non-binary data and non-
traditional association patterns, and to explore how confi-

3
It is straightforward to show that for two binary at-
tributes i and j, with ({i})  ({j}), that cos(i, j) 
({i})/({j}), where  is standard support.
dence should be extended for non-standard support mea-
sures. Preliminary work in both areas is presented in [11].
Finally, a key benefit of a framework is that it allows re-
searchers to more easily express, explore, and communicate
ideas. We hope that our framework will prove useful and
will motivate additional research in this area.
9. ACKNOWLEDGMENTS
This work was partially supported by NASA grant #
NCC 2 1231, by DOE/LLNL grant W-7045-ENG-48, by
NSF grant IIS-0308264, and by the Army High Performance
Computing Research Center under the auspices of the De-
partment of the Army, Army Research Laboratory coopera-
tive agreement number DAAD19-01-2-0014. The content of
this work does not necessarily reflect the position or policy
of the government and no official endorsement should be in-
ferred. Access to computing facilities was provided by the
AHPCRC and Minnesota Supercomputing Institute.

10. REFERENCES
[1] R. Agrawal and R. Srikant. Fast algorithms for mining
association rules. In VLDB 94, pages 487Г499, 1994.
[2] R. Agrawal and R. Srikant. Mining sequential
patterns. In ICDE 95, pages 3Г14, 1995.
[3] P. Bollmann-Sdorra, A. Hafez, and V. V. Raghavan. A
theoretical framework for association mining based on
the boolean retrieval model. In DaWaK 2001,
Munich, Germany, pages 21Г30, 2001.
[4] J. W. Demmel. Applied Numerical Linear Algebra.
SIAM, January 1997.
[5] E.-H. Han, G. Karypis, and V. Kumar. Tr# 97-068:
Min-apriori: An algorithm for finding association rules
in data with continuous attributes. Technical report,
Department of Computer Science, University of
Minnesota, Minneapolis, MN, 1997.
[6] J. Han and M. Kamber. Data Mining: Concepts and
Techniques. Morgan Kaufmann Publishers, 2000.
[7] J. Hipp, U. Gеuntzer, and G. Nakhaeizadeh.
Algorithms for association rule mining Г a general
survey and comparison. SIGKDD Explorations,
2(1):58Г64, July 2000.
[8] E. R. Omiecinski. Alternative interest measures for
mining associations in databases. IEEE TKDE,
15(1):57Г69, January/February 2003.
[9] J. Pei and J. Han. Can we push more constraints into
frequent pattern mining? In KDD 2000, pages
350Г354, 2000.
[10] R. Srikant, Q. Vu, and R. Agrawal. Mining association
rules with item constraints. In KDD 97, pages 67Г73,
1997.
[11] M. Steinbach, P.-N. Tan, H. Xiong, and V. Kumar.
Tr# 2004-114: : Extending the notion of support.
Technical report, Army High Performance Computing
Research Center, April 2004.
[12] H. Xiong, P. Tan, and V. Kumar. Mining strong
affinity association patterns in data sets with skewed
support distribution. In ICDM 2003, pages 387Г394,
2003.
[13] C. Yang, U. M. Fayyad, and P. S. Bradley. Efficient
discovery of error-tolerant frequent itemsets in high
dimensions. In KDD 2001, pages 194Г203, 2001.
[14] M. J. Zaki and M. Ogihara. Theoretical foundations of
association rules. In DMKD 98, pages 7:1Г7:8, 1998.



694
Research Track Poster

