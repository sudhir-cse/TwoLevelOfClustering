Efficiently Mining Frequent Trees in a Forest


Mohammed J. Zaki *
Computer Science Department, Rensselaer PolytechnicInstitute,TroyNY 12180

zaki @cs.rpi.edu, http://www.cs.rpi.edu/~zaki




ABSTRACT
Mining frequent trees is very useful in domains like bioinfor-
matics, web mining, mining semi-structured data, and so on.
We formulate the problem of mining (embedded) subtrees
in a forest of rooted, labeled, and ordered trees. We present
TREEMINER, a novel algorithm to discover all frequent sub-
trees in a forest, using a new data structure called scope-list.
We contrast TREEMINER with a pattern matching tree min-
ing algorithm (PATTERNMATCHER).
We conduct detailed
experiments to test the performance and scalability of these
methods.
We find that TREEMINER outperforms the pat-
tern matching approach by a factor of 4 to 20, and has good
scaleup properties.
We also present an application of tree
mining to analyze real web logs for usage patterns.


1.
INTRODUCTION
Frequent Structure Mining (FSM) refers to an important
class of exploratory mining tasks, namely those dealing with
extracting patterns in massive databases representing com-
plex interactions between entities.
FSM not only encom-
passes mining techniques like associations [3] and sequences [4],
but it also generalizes to more complex patterns like frequent
trees and graphs [12, 14]. Such patterns typically arise in
applications like bioinformatics, web mining, mining semi-
structured documents, and so on. As one increases the com-
plexity of the structures to be discovered, one e~tracts more
informative patterns; we are specifically interested in mining
tree-like patterns.
As a motivating example for tree mining, consider the web
usage mining [17] problem. Given a database of web access
logs at a popular site, one can perform several mining tasks.
The simplest is to ignore all link information from the logs,
and to mine only the frequent sets of pages accessed by users.
The next step can be to form for each user the sequence
of links they followed, and to mine the most frequent user
access paths. It is also possible to look at the entire forward
accesses of a user, and to mine the most frequently accessed
subtrees at that site.

*This work was supported in part by NSF CAREER Award
IIS-0092978, and NSF Next Generation Software Program
grant EIA-0103708.




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the firstpage. To copy otherwise, to
republish, to post on serversor to redistributeto lists, requires prior specific
permission and/or a fee.
SIGKDD '02 Edmonton, Alberta, Canada
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00.
In recent years, XML has become a popular way of storing
many data sets because the semi-structured nature of XML
allows the modeling of a wide variety of databases as XML
documents. XML data thus forms an important data min-
ing domain, and it is valuable to develop techniques that
can extract patterns from such data. Tree structured XML
documents are the most widely occurring in real applica-
tions. Given a set of such XML documents, one would like
to discover the commonly occurring subtrees that appear in
the collection.
Tree patterns also arise in Bioinformatics. For example, re-
searchers have collected vast amounts of RNA structures,
which are essentially trees.
To get information about a
newly sequenced RNA, they compare it with known RNA
structures, looking for common topological patterns, which
provide important clues to the function of the RNA [19].
In this paper we introduce TREEMINER, an efficient algo-
rithm for the problem of mining frequent subtrees in a for-
est (the database).
The key contributions of our work are
as follows: 1) We introduce the problem of mining embedded
subtrees in a collection of rooted, ordered, and labeled trees.
2) We use the notion of a scope for a node in a tree. We show
how any tree can be represented as a list of its node scopes,
in a novel vertical format called scope-list. 3) We develop a
framework for non-redundant candidate subtree generation,
i.e., we propose a systematic search of the possibly frequent
subtrees, such that no pattern is generated more than once.
4) We show how one can efficiently compute the frequency
of a candidate tree by joining the scope-lists of its subtrees.
5) Our formulation allows one to discover all subtrees in a
forest, as well as all subtrees in a single large tree. Further-
more, simple modifications also allow us to mine unlabeled
subtrees, unordered subtrees and also frequent sub-forests
(i.e., disconnected subtrees).
We contrast TREEMINER with a base tree mining algorithm
based on pattern matching, PATTERNMATCHER.
Our ex-
periments on several synthetic and one real dataset show
that TREEMINER outperforms PATTERNMATCHER by a fac-
tor of 4 to 20. Both algorithms exhibit linear scaleup with
increasing number of trees in the database. We also present
an application study of tree mining in web usage mining.
The input data is in the form of XML documents that rep-
resent user-session extracted from raw web logs. We show
that the mined tree patterns indeed do capture more inter-
esting relationships than frequent sets or sequences.


2.
PROBLEM STATEMENT
A tree is an acyclic connected graph, and a forest is an acyclic
graph. A forest is thus a collection of trees, where each tree
is a connected component of the forest. A rooted tree is a
tree in which one of the vertices is distinguished from others,
and called the root. We refer to a vertex of a rooted tree




71

as a node of the tree. An ordered tree is a rooted tree in
which the children of each node are ordered, i.e., if a node
has k children, then we can designate them as the first child,
second child, and so on up to the kth child. A labeled tree is
a tree where each node of the tree is associated with a label.
In this paper, all trees we consider are ordered, labeled,
and rooted trees.
We choose to focus on labeled rooted
trees, since those are the types of datasets that are most
common in a data mining setting, i.e., datasets represent
relationships between items or attributes that are named,
and there is a top root element (e.g., the main web page on
a site). In fact, if we treat each node as having the same
label, we can mine all ordered, unlabeled subtrees as well!

Ancestors and Descendants
Consider a node x in a rooted
tree T with root r. Any node y on the unique path from
r to x is called an ancestor of x, and is denoted as y <_l x,
where I is the length of the path from y to x.
Ify is an
ancestor of x, then x is a descendant of y. (Every node is
both an ancestor and descendant of itself). If y _<1 x (i.e., y
is an immediate ancestor), then y is called the parent of x,
and x the child of y. We say that nodes x and y are siblings
if they have the same parent, and we say they are embedded
siblings if they have some common ancestor.

Node
Numbers
and Labels We denote a tree as T =
(N, B), where N is the set of labeled nodes, and B the set of
branches. The size of T, denoted ITh is the number of nodes
in T.
Each node has a well-defined number, i, according
to its position in a depth-first (or pre-order) traversal of
the tree.
We use the notation nl to refer to the ith node
according to the numbering scheme (i -- 0... ITI - 1). The
label (also referred to as an item) of each node is taken from a
set of labels L = {0, 1, 2, 3, ..., m-
1}, and we allow different
nodes to have the same label, i.e., the label of node number
i is given by a function, I : N --~ L, which maps n~ to some
label l(n~) = y E L. Each node in T is thus identified by its
number and its label. Each branch, b = (n~,ny) E B, is an
ordered pair of nodes, where n~ is the parent of ny.

Subtrees
We say that a tree S = (N~, B~) is an embed-
ded subtree of T = (N,B), denoted as S ~ T, provided i)
N~ C_ N, ii) b = (n~,nu) E B~ if and only ifn u _<l n~, i.e.,
n= is an ancestor of nu in T.
In other words, we require
that a branch appears in S if and only if the two vertices
are on the same path from the root to a leaf in T. If S ~ T,
we also say that T contains S. A (sub)tree of size k is also
called a k-(sub)tree. Note that in the traditional definition
of an induced subtree, for each branch b = (n~, nu) E B,, n~
must be a parent of nu in T. Embedded subtrees are thus
a generalization of induced subtrees; they allow not only
direct parent-child branches, but also ancestor-descendant
branches.
As such embedded subtrees are able to extract
patterns "hidden" (or embedded) deep within large trees
which might be missed by the traditional definition.
Hence-
forth, a reference to subtree should be taken to mean an
embedded subtree, unless indicated otherwise.
By defini-
tion, a subtree must be connected. A disconnected pattern
is a sub-forest of T. Our main focus is on mining subtrees,
although a simple modification of our enumeration scheme
also produces sub-forests.

Scope Let T(nt) refer to the subtree rooted at node nt, and
let nr be the right-most leaf node in T(nl).
The scope of
node nz is given as the interval [l,r], i.e., the lower bound
is the position (l) of node nt, and the upper bound is the
position (r) of node n,.. The concept of scope will play an
important part in counting subtree frequency.
Tree Mining Problem
Let D denote a database of trees
(i.e., a forest), and let subtree S ~ T for some T E D. Each
occurrence of S can be identified by its match label, which
is given as the set of matching positions (in T) for nodes in
S. More forraally, let {tl, t2,..., tn} be the nodes in T, with
ITI = n, anti let {sl, s2 .... ,s~} be the nodes in S, with
IS I = m. Then S has a match label {tq, t,~,.., tim },if and
only if: 1) l(sk) = l(t~) for all k = 1,...m,
and 2) branch
b(sj,sk) in S iff tij is an ancestor of ti~ in T. Condition 1)
indicates that all node labels in S have a match in T, while
2) indicates that the tree topology of the matching nodes
in T is the same as S.
A match label is unique for each
occurrence of S in T.
Let ST(S) denote the number of occurrences of the subtree
S in a tree T. Let dT(S) = 1 if ST(S) > 0 and dT(S) = 0
if 6T(S) = O. The support of a subtree S in the database is
defined as a(S) = ~T6D dT(S), i.e., the number of trees in
D that contain at least one occurrence of S. The weighted
support of S is defined as a~,(S) = ~T6D 6T(S), i.e., total
number of occurrences of S over all trees in D. Typically,
support is given as a percentage of the total number of trees
in D. A subtree S is frequent if its support is more than or
equal to a user-specified minimum support (minsup) value.
We denote by Fk the set of all frequent subtrees of size k.
Given a user specified minsup value our goal is to efficiently
enumerate all frequent subtrees in D. In some domains one
might be interested in using weighted support, instead of
support.
Both of them are supported by our mining ap-
proach, but we focus mainly on support.

T (a tree in D)
nO.s ffi[0. 61~


n l , s f [ L ~
C~
n6, sffi[6.61


n2. s- [ ~ ,
~
ns..- [5.51


n3. ,-[3. 31(.~)
~
B4. · ffi [4. 4]


T's String Encoding: 0 1 3 1 -1 2 -1 -I 2 -1 -1 2 -I

Sl
$2
$3




su~rrt=l
su
rt=l
*o~.h~.'ppo.-,
weigh~.ppert = 2
not a sub,tee; a ;ub-forest
match labels ffi{134, 135}
match label ffi(03456}
string = I I -I 2 -1
suing = 0 1-I 2 -I 2 -I 2 -I

Figure
1: An
Example
Tree with Subtrees

EXAMPLE 1. Consider Figure 1, which shows an example
tree T with node labels drawn from the set L = {0, 1, 2, 3}.
The figure shows for each node, its label (circled), its number
according to depth-first numbering, and its scope. For ex-
ample, the root occurs at position n = O, its label l(no) = O,
and since the right-most leaf under the root occurs at posi-
tion 6, the scope of the root is s = [0, 6]. Tree Sl is a sub,tee
of T; it has a support of 1, but its weighted support is 2,
since node n2 in $1 occurs at positions 4 and 5 in T, both
of which support $1, i.e., there are two match labels for Sl,
namely 134 and 135 (we omit set notation for convenience).
82 is also a valid sub,tee. $3 is not a (sub)tree since it is
disconnected; it is a sub-forest. ·


3.
GENERATING CANDIDATE TREES
There are two mains steps for enumerating frequent sub-
trees in D. First, we need a systematic way of generating
candidate su.btrees whose frequency is to be computed. The




72

candidate set should be non-redundant, i.e., each subtree
should be generated as most once. Second, we need efficient
ways of counting the number of occurrences of each candi-
date in the database D, and to determine which candidates
pass the minsup threshold. The latter step is data structure
dependent, and will be treated later. Here we are concerned
with the problem of non-redundant pattern generation. We
describe below our tree representation and candidate gener-
ation procedure.

Representing
Trees as Strings Standard ways of repre-
senting a labeled tree are via an adjacency matrix or adja-
cency list. For a tree with n nodes and m branches (note:
m = n - 1 for trees), adjacency matrix representation re-
quires n+fn
= n(f+ 1) space (f is the maximum fanout; n
term is for storing labels and fn term for storing adjacency
information), while adjacency lists require 2n + 2m = 4n- 2
space (2n term is for storing labels and header pointers for
adjacency lists, 2m is for storing label and next pointer per
list node).
Since f can possibly be large, we expect adja-
cency lists to be more space-efficient. If we directly store a
labeled tree node as a (label, child pointer, sibling pointer)
triplet, we would require 3n space.
For efficient subtree counting and manipulation we adopt a
string representation of a tree. We use the following proce-
dure to generate the string encoding, denoted T, of a tree T.
Initially we set T = 0. We then perform a depth-first pre-
order search starting at the root, adding the current node's
label x to T. Whenever we backtrack from a child to its par-
ent we add a unique symbol -1 to the string (we assume that
-1 !i~ L). This format allows us to conveniently represent
trees with arbitrary number of children for each node. Since
each branch must be traversed in both forward and back-
ward direction, the space usage to store a tree as a string is
exactly 2m + 1 = 2n - 1. Thus our string encoding is more
space-efficient than other representations.
Moreover, it is
simpler to manipulate strings rather than adjacency lists or
trees for pattern counting. We use the notation l(T) to refer
to the label sequence of T, which consists of the node labels
of T in depth-first ordering (without backtrack symbol -1),
i.e., label sequence ignores tree topology.

EXAMPLE2. In Figure 1, we show the string encodings
for the tree T as well as each of its subtrees. For example,
subtree $1 is encoded by the string 1 1 -1 2 -1.
That is,
we start at the root of $1 and add 1 to the string. The next
node in preorder traversal is labeled 1, which is added to
the encoding.
We then backtrack to the root (adding -1)
and follow down to the next node, adding 2 to the encoding.
Finally we backtrack to the root adding -1 to the string.
Note that the label sequence of $1 is given as 112. ·

3.1
Candidate Subtree Generation
We use the anti-monotone property of frequent patterns for
efficient candidate generation, namely that the frequency of
a super-pattern is less than or equal to the frequency of
a sub-pattern.
Thus, we consider only a known frequent
pattern for extension. Past experience also suggests that an
extension by a single item at a time is likely to be more
efficient. Thus we use information from frequent k-subtrees
to generate candidate (k + 1)-subtrees.

Equivalence Classes We say that two k-subtrees X, Y are
in the same prefix equivalence class iff they share a common
prefix up to the (k - 1)th node. Formally, let X, y be the
string encodings of two trees, and let function p(,~', i) return
the prefix up to the ith node. X, Y are in the same class
iff p(X, k - 1) = p(y, k - 1). Thus any two members of an
equivalence class differ only in the position of the last node.
ClassPrefix

0
nl
".,)......

:x:
·,,
.....,'


".;,,...
~x
,....,'



III
',,....*
~uivalenc©Class
I
PrefixString:342 -11
[

I
ElementList:(label.attachedtoposition)

(x,0).
heO ton0:i-m=rrt-i-1 x-if
(x,
ton:r 345-, 1[-,
(x.3)//attachedton3:L342-1 lJx-1-1-lJ



Figure 2: Prefix Equivalence Class

EXAMPLE 3. Consider Figure 2, which shows a class tem-
plate for subtrees of size 5 with the same prefix subtree P of
size 4, with string encoding P = 3 4 2 -1 1. Here x denotes
an arbitrary label from L. The valid positions where the last
node with label x may be attached to the prefix are no,n1
and na, since in each of these cases the subtree obtained by
adding x to P has the same prefix. Note that a node at-
tached to position n2 cannot be a valid member of class P,
since it would yield a different prefix, given as 3 4 2 x.
The figure also shows the actual format we use to store an
equivalence class; it consists of the class prefix string, and
a list of elements.
Each element is given as a (x,p) pair,
where x is the label of the last node, and p specifies the
depth-first position of the node in P to which x is attached.
For example (x, 1) refers to the case where x is attached to
node nl at position 1.
The figure shows the encoding of
the subtrees corresponding to each class element. Note how
each of them shares the same prefix up to the (k- 1)th node.
These subtrees are shown only for illustration purposes; we
only store the element list in a class. ·

Let P be prefix subtree of size k - 1; we use the notation
[P]k-1 to refer to its class (we omit the subscript when there
is no ambiguity). If (x, i) is an element of the class, we write
it as (x, i) E [P]. Each (x, i) pair corresponds to a subtree of
size k, sharing P as the prefix, with the last node labeled x,
attached to node ni in P. We use the notation Px to refer
to the new prefix subtree formed by adding (x,i) to P.

LEMMA 1. Let P be a class prefix subtree and let nr be
the right-most leaf node in P, whose scope is given as [r, r].
Let (x, i) E [P]. Then the set of valid node positions in P to
which x can be attached is given by {i : ni has scope [i,r]},
where ni is the ith node in P.

This lemma states that a valid element x may be attached
to only those nodes that lie on the path from the root to the
right-most leaf n~ in P. It is easy to see that if x is attached
to any other position the resulting prefix would be different,
since x would then be before n,. in depth-first numbering.

Candidate
Generation
Given an equivalence class of k-
subtrees, how do we obtain candidate (k+l)-subtrees? First,
we assume (without loss of generality) that the elements,
(x,p), in each class are kept sorted by node label as the
primary key and position as the secondary key.
Given a
sorted element list, the candidate generation procedure we
describe below outputs a new class list that respects that
order, without explicit sorting. The main idea is to consider
each ordered pair of elements in the class for extension, in-
cluding self extension. There can be up to two candidates
from each pair of elements to be joined. The next theorem
formalizes this notion.

THEOREM 1
(CLASSEXTENSION). Let P be a prefix class
with encoding 7~, and let (x, i) and (y,j) denote any two el-
ements in the class.
Let P~ denote the class representing




73

............................................
!
Equlvtden~ CIm
Prefix: I 2
Elementt Lilt: (33.1)(4.0)




L
Pt~t~ : I 2 3
Element LIiu (3.1) (3,2)
(4.0~
Prefix: 12 -I 4
Elen~at List: (4,O) (4. I )




Figure 3: Candidate Generation

extensions of element (x, i).
Define a join operator @ on
the two elements, denoted (x,i)@(y,j), as follows:
case I-
(i = j):
a) If7~ ¢ O, add (y,j) and (y,j + 1) to class [Px].
b) If7 ~ = O, add (y,j + 1) to [P~].
case II - (i > j): add (y,j) to class [Px].
case III-
(i < j): no new candidate is possible in this case.

Then all possible (k + 1)-subtrees with the prefix P of size
k - 1 will be enumerated by applying the join operator to
each ordered pair of elements (x, i) and (y, j).
PROOF: Omitted due to lack of space. ·

EXAMPLE 4. Consider Figure 3, showing the prefix class
~o = (1 2), which contains 2 elements, (3, 1) and (4,0). The
first step is to perform a self join (3, 1)@(3, 1). By case I
a) this produces candidate elements (3, 1) and (3, 2) for the
new class 'Pa = (1 2 3). That is, a self join on (3, 1) produces
two possible candidate subtrees, one where the last node is a
sibling of(3,1) and another where it is a child of(3, 1). The
left-most two subtrees in the figure illustrate these cases.
When we join (3,1)@(4, O) case II applies, i.e., the second
element is joined to some ancestor of the first one, thus i > j.
The only possible candidate element is (4, 0), since 4 remains
attached to node no even after the join (see the third sub(tee
in the left hand class in Figure 3). We thus add (4, 0) to class
[Pal.
We now move to the class on the right with prefix
"P4 = (1 2 - 1 4). When we try to join (4, 0)@(3, 1), case
III applies, and no new candidate is generated. Actually, if
we do merge these two subtrees, we obtain the new subtree
1 2 3 - 1 - 1 4, which has a different prefix, and was
already added to the class [Pal. Finally we perform a sclf-
join (4, 0)@(4, 0) adding elements (4, 0) and (4, 1) to the class
[P4] shown on the right hand side. ·

Case I b) applies only when we join single items to pro-
duce candidate 2-subtrees, i.e., we are given a prefix class
[0] = {(xi,-1), i = 1,..., m}, where each xi is a label, and
-1 indicates that it is not attached to any node.
If we
join (xl,-1)@(xj,-1), since we want only (connected) 2-
subtrees, we insert the element (xj, 0) to the class of xi.
This corresponds to the case where xj is a child of xl. If we
want to generate sub-forests as well, all we have to do is to
insert (xj, -1) in the class of xi. In this case x# would be a
sibling of xi, but since they are not connected, they would
be roots of two trees in a sub-forest. If we allow such class
elements then one can show that the class extension theorem
would produce all possible candidate sub-forests. However,
in this paper we will focus only on subtrees.

COROLLARY 1
(AUTOMATIC ORDERING). Let [P]k-1 be
an prefix class with elements sorted according to the total or-
dering < given as /ollows: (x,i) < (y,j) i/and only if x < y
or (x = y and: i < j).
Then the class extension method
generates candidate classes [P]k with sorted elements.

COROLLARY 2
(CORRECTNESS).The class extension -
method correctly generates all possible candidate subtrees,
and each candidate is generated at most once.


4.
TREEMINER ALGORITHM
TREEMINER performs depth-firstsearch (DFS) for frequent
subtrees, using a novel tree representationcalled scope-list
for fastsupport counting, as discussedbelow.

DatabaseDof3Trees

T~TO




T~TI
TztT2


nl'[12llI'

nl,[Z2]l.~)?



~,1531



.7.rl,/]
Din H~zoml Foml: (ilL,~eacoding)
(T0, 12-I 34-I -I)
(TI, 212-14-I-12-13-I)
("1"2,132-I 512-I 34-I -I -I-I)


DinVet(in]Fret(illsqz)

I
2
3
4
5
0,(0,3] 0,ll,lll 0,J2,3]10,t3,3l[ 2,[3,7]l
~'[~'3]I ~'t0'511i'[S'5]I t'ls'3]I-----"
12,t0,711U~2]I 2,[Ul I W,7] I
I 2,[4,~ I ~,[4,4]12,[~71[----'




Figure 4: Scope-Lists

Scope-List l:tepresentation Let X be a k-subtree of a tree
T. Let Xk refer to the last node of X. We use the notation
£(X) to refer to the scope-list of X. Each element of the
scope-list is a triple (t, m, s), where t is a tree id (tid) in
which X occurs, m is a match label of the (k - 1) length
prefix of X, and s is the scope of the last item xk. Recall
that the prefix match label gives the positions of nodes in
T that match the prefix.
Since a given prefix can occur
multiple times in a tree, X can be associated with multiple
match labels as well as multiple scopes. The initial scope-
lists are created for single items (i.e., labels) i that occur in
a tree T. Since a single item has an empty prefix, we don't
have to store the prefix match label m for single items. We
will show later how to compute pattern frequency via joins
on scope-lists.

EXAMPLE 5. Figure 4 shows a database of 3 trees, along
with the horizontal format for each tree, and the vertical
scope-lists format for each item. Consider item 1; since it
occurs at noch~ position 0 with scope [0, 3] in tree To, we
add (0, [0, 3]) to its scope list. Item 1 also occurs in T1 at
position nl with scope [1,3], so we add (1,[1,3]) to /~(1).
Finally, 1 occurs with scope [0, 7] and [4, 7] in tree T2, so
we add (2, [0, 7]) and (2, [4, 7]) to its scope-list. In a similar
manner, the scope lists for other items are created.

4.1
Frequent Subtree Enumeration
Figure 5 shows the high level structure of TREEMINER. The
main steps include the computation of the frequent items
and 2-subtrees, and the enumeration of all other frequent
subtrees via DFS search within each class [P]i E F2. We
will now describe each step in some more detail.

Computing
F/ and F2: TREEMINER
assumes that the
initialdatabase is in the horizontalstringencoded format.




74

TREEMINER (D, m.insup):
F1 = { frequent 1-subtrees };
F2 = { classes [P]i of frequent 2-subtrees };
for all [P]I · E do Enumerate-Frequent-Subtrees([P]l);

ENUMERATE-FREQUENT-SUBTREES([P])
:
for each element (x, i) · [P] do
[P,] = O;
for each element (y, j) · [P] do
R = {(x, ~)®(y, j)};
L(R) = {L(x) n® Z(y)};
if for any R · R, /:/is frequent then
[P~] = [P~] U {R};
Enumerate-Frequent-Subtrees ([P~]);

Figure 5: TREEMINER Algorithm

To compute F1, for each item i · T, the string encoding
of tree T, we increment i's count in a 1D array. This step
also computes other database statistics such as the number
of trees, maximum number of labels, and so on. All labels
in FF1 belong to the class with empty prefix, given as [P]0 =
[0] = {(i,-1), i · F1}, and the position -1 indicates that i
is not attached to any node. Total time for this step is O(n)
per tree, where n = ]T].
By Theorem 1 each candidate class [P]I = [i] (with i · F1)
consists of'elements of the form (j, 0), where j > i. For effi-
cient F2 counting we compute the supports of each candidate
by using a 2D integer array of size F~ x F1, where cnt[i]~']
gives the count of candidate subtree with encoding (i j -1).
Total time for this step is O(n 2) per tree. While computing
F2 we also create the vertical scope-list representation for
each frequent item i · El.

Computing
Fk(k > 3): Figure 5 shows the pseudo-code for
the depth-first search for frequent subtrees (ENUMERATE-
FREQUENT-SUBTREES). The input to the procedure is a set
of elements of a class [P], along with their scope-lists. Fre-
quent subtrees are generated by joining the scope-lists of
all pairs of elements (including self-joins).
Before joining
the scope-lists a pruning step can be inserted to ensure that
subtrees of the resulting tree are frequent.
If this is true,
then we can go ahead with the scope-list join, otherwise we
can avoid the join. For convenience, we use the set R to de-
note the up to 2 possible candidate subtrees that may result
from (x, i)®(y,j), according to the class extension theorem,
and we use E(R) to denote their respective scope-lists. The
subtrees found to be frequent at the current level form the
elements of classes for the next level. This recursive pro-
cess is repeated until all frequent subtrees have been enu-
merated.
If [P] has n elements, the total cost is given as
O(ln2), where l is the cost of a scope-list join (given later).
In terms of memory management it is easy to see that we
need memory to store classes along a path in DFS search.
At the very least we need to store intermediate scope-lists
for two classes, i.e., the current class [P], and a new candi-
date class [Px]. Thus the memory footprint of TREEMINER
is not much.

4.2
Scope-List Joins (£(x) n® Z(y))
Scope-list join for any two subtrees in a class [P] is based
on interval algebra on their scope lists. Let s~ = [l~, u~] be
a scope for node x, and Sy = [ly,uy] a scope for y. We say
that s~ is strictly less than s~, denoted s~ < sy, if and only
if u~ < ly, i.e., the interval s~ has no overlap with s~, and
it occurs before sy. We say that s~ contains su, denoted
s~ D sy, if and only if l~ < lu and u~ _> uy, i.e., the interval
su is a proper subset of s=. The use of scopes allows us to
compute in constant time whether y is a descendant of x
or y is a embedded sibling of x. Recall from the candidate
extension theorem 1 that when we join elements (x, i)®(y, j)
there can be at most two possible outcomes, i.e., we either
add (y, j + 1) or (y, j) to the class [P~].

In-Scope Test The first candidate (y,j + 1) is added to
[P~] only when i = j, and thus refers to the candidate sub-
tree with y as a child of node x. In other words, (y,j + 1)
represents the subtree with encoding (P~ y). To check if this
subtree occurs in an input tree T with tid t, we search if there
exists triples (ty,sy,my)
· £(y) and (t~,s~,rn~) · E(x),
such that:
1) tu = t~ = t, i.e., the triples both occur in the same tree,
with tid t.
2) m~ = m~ = m, i.e., x and y are both extensions of the
same prefix occurrence, with match label m.
3) sy C s~, i.e., y lies within the scope of x.
If the three conditions are satisfied, we have found an in-
stance where y is a descendant of x in some input tree T.
We next extend the match label m~ of the old prefix P, to
get the match label for the new prefix P~ (given as my ul~),
and add the triple (ty,sy, {my tA/~}) to the scope-list of
(y,j + 1) in [P~]. We refer to this case as an in-scope test.

Out-Scope Test The second candidate (y, j) represents the
ease when y is a embedded sibling of x, i.e., both x and y
are descendants of some node at position j in the prefix P,
and the scope of x is strictly less than the scope of y. The
element (y,j), when added to [Px] represents the pattern
(Px -1
... -1 y) with the number of -l's depending on
path length from j to x. To check if (y,j) occurs in some
tree T with tid t, we need to check if there exists triples
(ty, sy, my) · £(y) and (t~, s~, mx) · £(x), such that:
1) ty = tx = t, i.e., the triples both occur in the same tree,
with tid t.
2) my = m~ = m, i.e., x and y are both extensions of the
same prefix occurrence, with match label m.
3) s~ < sy, i.e., x comes before y in depth-first ordering,
and their scopes do not overlap.
If these conditions are satisfied, we add the triple (ty, sy, {my U
/~}) to the scope-list of (y,j) in [Pal. We refer to this case
as an out-scope test. Note that if we just check whether sx
and sy are disjoint (with identical tids and prefix match la-
bels), i.e., either sx < su or s~ > sy, then the support can
be counted for unordered subtrees!
Each application of in-scope or out-scope test takes O(1)
time. Let a and b be the distinct (t, m) pairs in £(x,i) and
£(y,j), respectively. Let c~ denote the average number of
scopes with a match label. Then the time to perform scope-
list joins is given as O(ofl (a + b)), which reduces to O(a + b)
if a is a small constant.


F.lemms =(I,-I),(2,-I),(3,-I),(4,-1)

@
@
®
@
o,lo,3] o,[1, Ij [ o,[~3j I o,K3,3]
1,fl,31
],fO, hj
l, ih,5]J 1,[3,3j
] 2,[0,7] ] 1,[2,2] ] 2,[1,2] [ 2,[7,7]
I~[4,7]I I,[4,41[Zl6,71
~---1 z[z2j ~----~


ff~m Ekmtzts
C5,-I):5
Pnflx=I
ERrors=C2~,(4,0)

22
0,0,11,II 0,0,[3,31 1
1,1,[2,2l 1,1,[3,31 ::
lo,1121 lo,17,71
::
2,0, 5,5] 2,4,[7,7] !

12,4,[5,511
i i

tc~a~km~e
(1'0): I I -I
0,0):13-1

Figure 6: Scope-list joins: minsup=
I~x=12
F~.aats--(4.~




,................... J
Id~qem~lmeae
(2.0):t2-12
(2,1):t22-1-t
14.B:124-1-1

100%




75

EXAMPLE 6. Figure 6 shows an example of how scope-list
joins work, using the database D from Figure 4, with min-
sup= i00%, i.e., we want to mine subtrees that occur in all
3 trees in D. The initial class with empty prefix consists of
four frequent items (1,2,3, and 4), with their scope-lists. All
pairs of elements are considered for extension, including self-
join. Consider the extensions from item I, which produces
the new class [1] with two frequent subtrees: (1 2 - 1) and
(1 4 - 1). The infrequent subtrees are listed at the bottom
of the class.
While computing the new scope-list for the subtree (1 2 - 1)
from L(1) N®/:(2), we have to perform only in-scope tests,
since we want to find those occurrences of 2 that are within
some scope of 1 (i.e., under a subtree rooted at I). Let sl
denote a scope for item i. For tree To we find that s2 =
[1, 1] C
81 =
[0, 3]. Thus we add the triple (0, 0, [1, 1]) to the
new scope list. In like manner, we test the other occurrences
of 2 under 1 in trees T1 and T2. Note that for T2 there are
three instances of the candidate pattern: s2 = [2, 2] C Sl=
[0, 7], s2 = [5, 51 C s~ = [0, 71, and s2 = [5, 51 C s~ = [4, 7].
lf a new scope-list occurs in at least minsup tids, the pattern
is considered frequent.
Consider the result of extending class [1]. The only frequent
pattern is (1 2 - 1 4 - 1), whose scope-list is obtained from
E(2, O) n® E(4, 0), by applications of out-scope test.
We
need to test for disjoint scopes, with s2 < s4, which have
the same match/abel. For example we find that s2 = [1, 1]
and s4 = [3, 3] satisfy these condition.
Thus we add the
triple (0, 01, [1, 1]) to £:(4, 0) in class [1 2]. Notice that the
new prefix match label (01) is obtained by adding to the old
prefix match label (0), the position where 2 occurs (1). The
final scope list for the new candidate has 3 distinct rids, and
is thus frequent.
There are no more frequent patterns at
minsup= 100%. ·


Reducing
Space Requirements
Generally speaking the
most important elements of the in-scope and out-scope tests
is to make sure that sy C s~ and s~ < su, respectively.
Whenever the test is true we add (t,sy, {my U l~}) to the
candidate's scope-list. However, the match labels are only
useful for resolving the prefix context when an item occurs
more than once in a tree. Using thisobservation itispossible
to reduce the space requirements for the scope-lists. We
add l~ to the match label my if and only ifx occurs more
than once in a subtree with tid t. Thus, ifmost items occur
only once in the same tree,thisoptimization drasticallycuts
down the match label size,since the only match labels kept
referto items with more than one occurrence. In the special
case that all items in a tree are distinct,the match label is
always empty, and each element of a scope-list reduces to a
(rid, scope) pair.

EXAMPLE 7. Consider the scope-list of (4, 0) in class [12]
in Figure 6. Since 4 occurs only once in To and T1 we can
omit the match label from the first two entries altogether,
i.e., the triple (0, 01, [3, 3]) becomes a pair (0, [3, 3]), and the
triple (1,12, [3,3]) become (1, [3,3]). ·


Opportunistic Candidate Pruning We mentioned above
that before generating a candidate k-subtree, S, we perform
a pruning test to check ifits (k - 1)-subtrees are frequent.
While thisiseasilydone in a BFS pattern search method like
PATTERNMATCHER(see
next section), in a DFS
search we
may not have allthe information availablefor pruning, since
some classesat level(k-l) would not have been counted yet.
TREEMINER
uses an opportunistic pruning scheme whereby
it firstdetermines if a (k - 1)-subtree would already have
been counted. ]If it had been counted but is not found in
F~-i, we can safely prune S. How do we know if a sub-
tree was counted? For this we need to impose an ordering
on the candidate generation, so that we can efficiently per-
form the subtree pruning test. Fortunately, our candidate
extension method has the automatic ordering property (see
Corollary 1). Thus we know the exact order in which pat-
terns will be enumerated. To apply pruning test for a can-
didate S, we generate each subtree X, and test if X < S
according to the candidate ordering property. If yes, we can
apply the pruning test; if not, we test the next subtree. If
S is not pruned, we perform scope-list join to get its exact
frequency.


5.
PATTERNMATCHER
ALGORITHM
PATTERNMATCHER serves as a base pattern matching algo-
rithm to compare TREEMINER against. PATTERNMATCHER
employs a breadth-first iterative search for frequent sub-
trees. Its high-h~vel structure, as shown in Figure 7, is simi-
lar to Apriori [3]. However, there are significant differences
in how we count the number of subtree matches against
an input tree T. For instance, we make use of equivalence
classes throughout, and we use a prefix-tree data structure
to index them, as opposed to hash-trees.
The details of
pattern matching are also completely different. PATTERN-
MATCHER assumes that each tree T in D is stored in its
string encoding (horizontal) format (see Figure 4). F1 and
F2 are computed as in TREEMINER. Due to lack of space we
describe only the main features of PATTERNMATCHER; see
[22] for details.

PATTERNMATCHER (D, minsup):
1. F1 = { frequent 1-subtrees };
2. F2 = { classes of frequent 2-subtrees };
3. for (k=3;Fk-l#O;k=k+l)
do
4.
C~ -- { classes [P]k-1 of candidate k-subtrees };
5.
for all[ trees T in D do
6.
Increment count of all S ~ T, S E [P]k-1
7.
Ck = { classes of frequent k-subtrees };
8.
Fk ----( hash table of frequent subtrees in Ck};
9. Set of all frequent subtrees -- Us Fk;

Figure 7: PATTERNMATCHER Algorithm


Pattern Pruning Before adding each candidate k-subtree
to a class in Ck we make sure that all its (k - 1)-subtrees
are also frequent. To efficientlyperform this step, during
creation of F~-I (line8), we add each individual frequent
subtree into a hash table. Thus it takes O(1) time to check
each subtree of a candidate, and since there can be k sub-
trees of length k - 1, it takes O(k) time to perform the
pruning check for each candidate.

Prefix Tree Data
Structure Once a new candidate set
has been generated, for each tree in D we need to efficiently
find matching candidates. We use a prefix tree data struc-
ture to index the candidates (C~) to facilitatefast support
counting. Furthermore, instead of adding individual sub-
trees to the prefix tree, we index an entire class using the
classprefix. Thus ifthe prefixdoes not match the input tree
T, then none of the classelements would match either. This
allows us to rapidly focus on the candidates that are likely
to be contained in T. Let [P] be a class in Ck. An internal
node of the prefix tree at depth d refersto the dth node in
P's label sequence. An internal node at depth d points to a
leafnode or an internal node at depth d + 1. A leafnode of
the prefixtree consistsof a listof classeswith the same label
sequence, thus a leafcan contain multiple classes. For ex-




75

ample, classes with prefix encodings (1 2 -1 4 3), (1 2 4 3),
(1 2 4 -1 -1 3), etc., all have the same label sequence 1243,
and thus belong to the same leaf.
Storing equivalence classes in the prefix tree as opposed
to individual patterns results in considerable efficiency im-
provements while pattern matching.
For a tree T, we can
ignore all classes [P]k-1 where P 1~T. Only when the prefix
has a match in T do we look at individual elements. Sup-
port counting consists of three main steps: 1) to find a leaf
containing classes that may potentially match T, 2) to check
if a given class prefix P exactly matches T, and 3) to check
which elements of [P] are contained in T.

Finding Potential Matching Leafs Let l(T) be the label
sequence for a tree T in the database. To locate matching
leafs, we traverse the prefix tree from the root, following
child pointers based on the different items in liT), until we
reach a leaf. This identifies classes whose prefixes have the
same label sequence as a subsequence of lit ). This process
focuses the search to some leafs of C~, but the subtree topol-
ogy for the leaf classes may be completely different. We now
have to perform an exact prefix match.
In the worst case
there may be !~) ~ n ~ subsequences of l(T) that lead to
different leafs, however, in practice it is much smaller, since
only a small fraction of the leafs match the label sequences,
especially as pattern length increases. The time to traverse
from the root to a leaf is O(klogm), where m is the average
number of distinct labels at an internal node. Total cost of
this step is thus O(kn k log m).

Prefix Matching Matching the prefix P of a class in a leaf
against the tree T is the main step in support counting. Let
X[i] denote the ith node of subtree X, and let X[i,... ,j]
denote the nodes from positions i to j, with j > i.
We
use a recursive routine to test prefix matching. At the rth
recursive call we maintain the invariant that all nodes in
P[0, 1.... , r] have been matched by nodes in T[io, il, ..., ir],
i.e., prefix node P[0] matches T[i0], Pill matches T[Q], and
so on, and finally P[r] matches T[ir]. Note that while nodes
in P are traversed consecutively, the matching nodes in T
can be far apart. We thus have to maintain a stack of node
scopes, consisting of the scope of all nodes from the root i0
to the current right-most leaf ir in T. If ir occurs at depth
d, then the scope stack has size d + 1.
Assume that we have matched all nodes up to the rth node
in P. If the next node Pit + 1] to be matched is the child
of P[r], we likewise search for P[r + 1] under the subtree
rooted at T[i,.].
If a match is found at position ir+l in T
we push i~+1 onto the scope stack. On the other hand, if
the next node P[r + 1] is outside the scope of P[r], and is
instead attached to position l (where 0 < l < r), then we
pop from the scope stack all nodes i~, where l < k < r, and
search for P[r + 1] under the subtree rooted at T[i~]. This
process is repeated until all nodes in P have been matched.
This step takes O(kn) time in the worst case. If each item
occurs once it takes O(k + n) time.

Element Matching If P -~ T, we search for a match in
T for each element ix, k) · [P], by searching for x starting
at the subtree T[ik-1].
(x, k) is either a descendant or em-
bedded sibling of P[k - 1]. Either check takes O(1) time. If
a match is found the support of the element ix,k) is incre-
mented by one. If we are interested in support iat least one
occurrence in T), the count is incremented only once per
tree, or else, if we are interested in weighted support In!!
occurrences in T), we continue the recursive process unt|]
all matches have been found.
6.
EXPERIMENTAL RESULTS
All experiments were performed on a 500MHz Pentium PC
with 512MB memory running RedHat Linux 6,0. Timings
are based on total wall-clock time, and include preprocessing
costs (such as creating scope-lists for TREEMINER).




1




2 r.
!
4
6
8
10--~--14
O0
5
10
15
Le~;~
Ler,~

Figure 8: Distribution
of Frequent Trees by Length

Synthetic Datasets We wrote a synthetic data generation
program mimicking website browsing behavior.
The pro-
gram first constructs a master website browsing tree, ~4),
based on parameters supplied by the user. These parame-
ters include the maximum fanout F of a node, the maximum
depth D of the tree, the total number of nodes M in the
tree, and the number of node lab0]s N. We allow multiple
nodes in the master tree to have tim same label. The master
tree is generated using the following recursive process. At
a given node in the tree 142, wc decide how many children
to generate. The number of children is sampled uniformly
at random from the range 0 to F.
Before processing chil-
dren nodes, we assign random probabilities to each branch,
including an option of backtracking to the node's parent.
The sum of all the probabilities for a given node is 1. The
probability associated with a branch b = ix,y), indicates
how likely is a visitor at x to follow the link to y. As long as
tree depth is less than or equal to maximum depth 19 this
process continues recursively.
Once the master tree has been created we create as many
subtrees of 142 as specified by the parameter T. To generate
a subtree we repeat the following recursive process starting
at the root: generate a random number between 0 and 1 to
decide which child to follow, or to backtrack.
If a branch
has already been visited, we select one of the other unvis-
ited branches, or backtrack. We used the following default
values for the parameters: the number of labels N =- 100,
the number of nodes in the master tree M = 10,000, the
maximum depth D = 10, the maximum fanout F = 10 and
total number of subtrees T = 100,000. We use three syn-
thetic datasets: D10 dataset had all default values, F5 had
all values set to default, except for fanout F = 5, and for
T1M we set T = 1,000, 000, with remaining default values.

CSLOGS
Dataset consists of web logs files collected over
1 month at the CS department.
The logs touched 13361
unique web pages within our department's web site. After
processing the raw logs we obtained 59691 user browsing
subtrees of the CS department website. The average string
encoding length for a user subtree was 23.3.
Figure 8 shows the distribution of the frequent subtrees by
length for the different datasets used in our experiments; all
of them exhibit a symmetric distribution.
For the lowest
minimum support used, the longest frequent subtree in F5
and TIM had 12 and 11 nodes, respectively. For cslogs and
D10 datasets the longest subtree had 18 and 19 nodes.

Performance Comparison Figure 9 shows the performance
of PATTERNMATCHER versus TREEMINER. On the real cslogs
dataset, we find that TREEMINER is about 2 times faster
than PATTERNMATCHER until support 0.5%. At 0.25% sup-




77

,~, 10000




IO0

~
lO

g"
1
)
I-
0.1




7OOO

6000



¢4000
E
i: 3OO0


10OO

O;
cslogs
.
.
.
.
i
PattemMatcher~
/




2.5
1 0.75 0.5
0.25
Minimum Support(%)
D10
1

1
i,

i:




-
10

~s

"
0.~
0.10.075
0

MinimumSupport(%)
T1M

Pa~rnMatcher+
'
' fl




0.5
0.10.0750.05
MinimumSup~Jort(%)
F5

PatternMatcher




0.5
0.10.075 O,OO
Minimum Support(%)

Figure 9: Performance Comparison
port TREEMINEFtoutperforms PATTERNMATCHERby more
than a factor of 20! The reason is that cslogs had a max-
imum pattern length of 7 at 0.5% support. The level-wise
pattern matching used in PATTERNMATCHERis able to eas-
ily handle such short patterns. However, at 0.25% support
the maximum pattern length suddenly jumped to 19, and
PATTERNMATCHERis unable to efficiently deal with such
long patterns. Exactly the same thing happens for D10 as
well.
For supports lower than 0.5% TREEMINER outper-
forms PATTERNMATCHERby a wide margin. At the low-
est support the difference is a factor of 15.
Both TiM
and F5 have relatively short frequent subtrees.
Here too
TREEMINER outperforms PATTERNMATCHER,but for the
lowest support shown, the difference is only a factor of 4.
These experiments clearly indicate the superiority of scope-
list based-method over the pattern matching method, espe-
cially as patterns become long.

010
minsup(0.05%)
1OOO0
140
.
.
.
.



^ 120




40
2o
o!
10 100 250
500
1000
Nu~r~tT~n l~'s)
Figure
xu: ~caleup
1000


100




1
PM-NoPruning
·
PM-Pruning --E~--.
TM-NoPruning -..-e.......
/ /




0.5
0.1

F.
ii21~um.~pport (%)
]gure lz: Frunlng


Scaleup Comparison Figure 10 shows how the algorithms
scale with increasing number of trees in the database D,
from 10,000 to 1 million trees. At a given level of support,
we find a linear increase in the running time with increasing
number of transactions for both algorithms, though TREEM-
INERcontinues to be 4 times faster than PATTERNMATCHER.

Effect of Pruning In Figure 11 we evaluated the effect of
candidate pruning on the performance of PATTERNMATCHER
and TREEMINER.
We find that PATTERNMATCHER (de-
noted PM in the graph) always benefits from pruning, since
the fewer the number of candidates, the lesser the cost of
support counting via pattern matching. On the other hand
TREEMINER (labeled TM in the graph) does not always ben-
efit from its opportunistic pruning scheme. While pruning
tends to benefit it at higher supports, for lower supports
its performance actually degrades by using candidate prun-
ing. TREEMINER with pruning at 0.1% support on D10 is
2 times slower than TREEMINER with no pruning. There
are two main reasons for this. First, to perform pruning, we
need to store Fk in a hash table, and we need to pay the
cost of generating the (k - 1) subtrees of each new k-pattern.
This adds significant overhead, especially for lower supports
when there are many frequent patterns. Second, the vertical
representation :is extremely efficient; it is actually faster to
perform scope-list joins than to perform pruning test.

minsup
No Pruning
Full Pruning
Opportunistic
1%
14595
2775
3505
0.5%
70250
10673
13736
0.1%
3555612
i
481234
536496

Figure 12: Full vs. Opportunistic Pruning

Table 12 shows the number of candidates generated on the
D10 dataset with no pruning, full pruning (in PATTERN-
MATCHER), and with opportunistic pruning (in TREEM-
INER). Both full pruning and opportunistic pruning are ex-
tremely effective in reducing the number of candidate pat-
terns, and opportunistic pruning is almost as good as full
pruning (within a factor of 1.3). Full pruning cuts down the
number of candidates by a factor of 5 to 7! Pruning is es-
sential thus for pattern matching methods, and may benefit
scope-list method in some cases (for high support).


7.
APPLICATION: WEB/XML MINING
To demonstrate the usefulness of mining complex patterns,
we present below a detailed application study on mining
usage patterns in web logs. Mining data that has been col-
lected from web server log files, is not only useful for study-
ing customer choices, but also helps to better organize web
pages. This is accomplished by knowing which web pages
are most frequently accessed by the web surfers.
We use LOGML [16], a publicly available XML application,
to describe log reports of web servers. LOGML provides a
XML vocabulary to structurally express the contents of the
log file information in a compact manner. LOGML docu-
ments have three parts: a web graph induced by the source-
target page pairs in the raw logs, a summary of statistics
(such as top hosts, domains, keywords, number of bytes ac-
cessed, etc.), and a list of user-sessions (subgraphs of the
web graph) extracted from the logs.
There are two inputs to our web mining system: 1) web site
to be analyzed, and 2) raw log files spanning many days, or
extended periods of time. The web site is used to populate
a web graph with the help of a web crawler. The raw logs
are processed by the LOGML generator and turned into a
LOGML document that contains all the information we need
to perform various mining tasks. We use the web graph to
obtain the page URLs and their node identifiers.
For enabling web mining we make use of user sessions within
the LOGML document. User sessions are expressed as sub-
graphs of the web graph, and contain complete history of
the user clicks. Each user session has a session id (IP or
host name), a list of edges (uedges) giving source and target
node pairs, and the time (utime) when a link is traversed.
An example user session is shown below:
<userSession name="pppO-69.ank2.isbank.net.tr"
...>
<uedge source="5938" target="16470" utime="7 :53 :46"/>
<uedge source="16470"
target="24754"
utime="7:56 :13"/>
<uedge source="16470"
target="24755"
utime="7:56:36"/>
<uedge source="24755"
target="47387"
utime="7:57:14"/>
<uedEe source="24755"
target="47397"
utime="7:57:28"/>
<uedEe source="16470" target="24756"
utime="7:58:30"/>




78

Itemset Mining To discover frequent sets of pages accessed
we ignore all link information and note down the unique
nodes visited in a user session. The user session above pro-
duces a user "transaction" containing the user name, and
the node set, as follows: (ppp0-69.ank2.isbank.net.tr,
5938
16470 24754 24755 47387 47397 24756).
After creating transactions for all user sessions we obtain a
database that is ready to be used for frequent set mining. We
applied an association mining algorithm to a real LOCML
document from CS web site (one day's logs). There were
200 user sessions with an average of 56 distinct nodes in each
session. An example frequent set found is shown below. The
pattern refers to a popular Turkish poetry site maintained
by one of our department members. The user appears to be
interested in the poet Akgun Akova.
Let
Path=http://www.cs.rpi.edu/~name/poetry
FREQUENCY-16, NODE IDS = 16395 38699 38700 38698 8938
Path/poems/akgun_akova/index.
html
Path/poems/akgun_akova/pict ure. html
Path/poems/akgun_akova/biyogral ±.html
Path/poems/akgun_akova/cont ents.html
Path/sair_listesi. him1
Sequence Mining If our task is to perform sequence min-
ing, we look for the longest forward links [6] in a user ses-
sion, and .generate a new sequence each time a back edge
is traversed.
We applied sequence mining to the LOGML
document from the CS web site. From the 200 user sessions,
we obtain 8208 maximal forward sequences, with an average
sequence size of 2.8. An example frequent sequence (shown
below) indicates in what sequence the user accessed some
of the pages related to Akgun Akova.
The starting page
sair_listesi
contains a list of poets.
Let Path=http ://www. cs. rpi. edu/~name/poetry
FREQUENCY - 20, NODE IDS =
5938 -> 16395 -> 38698
Path/sair_listesi. html ->
Path/poems/akgun_akova/index. him1 ->
Path/poems/akgun-akova/c ontents.html

Tree Mining For frequent tree mining, we can easily ex-
tract the forward edges from the user session (avoiding cy-
cles or multiple parents) to obtain the subtree corresponding
to each user. For our example user-session we get the tree:
(ppp0-69.ank2.isbank.net.tr, 5938 16470 24754 -1 24755 47387
-1 47397-1 -1 24756-1 -1)
We applied the TreeMiner algorithm to the CS logs. From
the 200 user sessions, we obtain 1009 subtrees (a single user
session can lead to multiple trees if there are multiple roots
in the user graph), with an average record length of 84.3
(including the back edges, -1). An example frequent subtree
found is shown below. Notice, how the subtree encompasses
all the partial information of the sequence and the unordered
information of the itemset relating to Akgun Akova. The
mined subtree is clearly more informative, highlighting the
usefulness of mining complex patterns.

Let Path=http ://www. cs. rpi.
edu/~name/poetry
Let Akova = Path/poems/akgun_akova
FP~QUENC7=59, NODES = 5938 16396 38699 -I 38698 -i 38700
Path/sair_listesi. html
f
Path/poems/akgun_akova/index.him1
/
l
\
Akova/picture.html Akova/contents.html Akova/biyografi.html

We also ran detailed experiments on logs files collected over
1 month at the CS department, that touched a total of
27343 web pages.
After processing the LOGML database
had 34838 user graphs.
We do not have space to shows
the results here (we refer the reader to [16] for details), but
these results lead to interesting observations that support
the mining of complex patterns from web logs. For exam-
ple, itemset mining discovers many long patterns. Sequence
mining takes longer time but the patterns are more useful,
since they contain path information. Tree mining, tough it
takes more time than sequence mining, produces very in-
formative patterns beyond those obtained from set and se-
quence mining.


8.
RELATED WORK
Tree mining, being an instance of frequent structure min-
ing, has obvious relation to association [3] and sequence [4]
mining. Frequent tree mining is also related to tree isomor-
phism [18] and tree pattern matching [8]. Given a pattern
tree P and a target tree T, with IP[ < ITh the subtree iso-
morphism problem is to decide whether P is isomorphic to
any subtree of T, i.e., there is a one-to-one mapping from P
to a subtree of T, that preserves the node adjacency rela-
tions. In tree pattern matching the pattern and target trees
are labeled and ordered. We say that P matches T at node
v if there exists a one-to-one mapping from nodes of P to
nodes of T such that: a) the root of P maps to v, b) if x
maps to y, then x and y have the same labels, and c) if x
maps to y and x is not a leaf, then the ith child of x maps
to the ith child of y. Both subtree isomorphism and pattern
matching deal with induced subtrees, while we mine embed-
ded subtrees. Further we are interested in enumerating all
common subtrees in a collection of trees. The tree inclusion
problem was studied in [13], i.e., given labeled trees P and T,
can P be obtained from T by deleting nodes? This problem
is equivalent to checking if P is embedded in T. The paper
presents a dynamic programming algorithm for solving or-
dered tree inclusion, which could potentially be substituted
for the pattern matching step in PATTERNMATCHER. How-
ever, PATTERNMATCHER utilizes prefix information for fast
subtree checking, and its three step pattern matching is very
efficient over a sequence of such operations.
There has been very little previous work in mining all fre-
quent subtrees. In a recent paper, Asai et al. [5] presented
FREQT, an apriori-like algorithm for mining labeled or-
dered trees; they independently proposed a candidate gen-
eration scheme similar to ours.
Wang and Liu [20] devel-
oped an algorithm to mine frequently occurring subtrees in
XML documents. Their algorithm is also reminiscent of the
level-wise Apriori [3] approach, and they mine induced sub-
trees only. A related problem of accurately estimating the
number of matches of a small node-labeled tree in a large
labeled tree, in the context of querying XML data, was pre-
sented in [7]. They compute a summary data structure, and
then give frequency estimates based on this summary, rather
than using the database for exact answers. In contrast we
are interested in exact frequency of subtrees. Furthermore,
their work deals with traditional (induced) subtrees, while
we mine embedded subtrees.
With the advent of XML as a data representation and ex-
change standard, there has been active work in indexing and
querying XML documents [15, 23, 2, 11], which are mainly
tree (or graph) structured.
To efficiently answer ancestor-
descendant queries various node numbering schemes similar
to ours have been proposed [15, 23, 1].
Other work has
looked at path query evaluation that uses local knowledge
within data graph based on path constraints [2] or graph
schemas [11]. The major difference between these works and
ours is that instead of answering user-specified queries based
on regular path expressions, we are interested in finding all
frequent tree patterns among the documents.
There has been recent work in mining frequent graph pat-
terns. The AGM algorithm [12] discovers induced (possibly
disconnected) subgraphs. The FSM algorithm [14] improves
upon AGM, and mines only the connected subgraphs. Both
methods follow an Apriori-style level-wise approach. If one




79

were to use ACM or FSM for tree mining one would dis-
cover only induced subtrees.
In contrast we discover em-
bedded subtrees.
Also there are important differences in
graph mining and tree mining. Our trees are rooted, and
thus have a unique ordering of the nodes based on depth-
first traversal. In contrast graphs do not have a root, and
allow cycles. For mining graphs the methods above first ap-
ply an expensive canonization step to transform graphs into
a uniform representation. This step is unnecessary for tree
mining.
Graph mining algorithms are likely to be overly
general (thus not efficient) for tree mining. Our approach
utilizes the tree structure for efficient enumeration.
The work by Dehaspe et al [10] describes a level-wise Induc-
tive Logic Programming based technique to mine frequent
substructures (subgraphs) describing the carcinogenesis of
chemical compounds.
They reported that mining beyond
6 predicates was unfeasible due to the complexity of the
subgraph patterns. The SUBDUE system [9] also discovers
graph patterns using the Minimum Description Length prin-
ciple. An approach termed Graph-Based Induction (GBI)
was proposed in [21], which uses beam search for mining sub-
graphs. However, both SUBDUE and GBI may miss some
significant patterns, since they perform a heuristic search.
We perform a complete (but not exhaustive) search, which
guarantees that all patterns are found. In contrast to these
approaches, we are interested in developing efficient algo-
rithms for tree patterns.


9.
CONCLUSIONS
In this paper we introduced the notion of mining embed-
ded subtrees in a (forest) database of trees.
Among our
novel contributions is the procedure for systematic candi-
date subtree generation, i.e., no subtree is generated more
than once. We utilized a string encoding of the tree that
is space-efficient to store the horizontal dataset, and we use
the notion of a node's scope to develop a novel vertical rep-
resentation of a tree called scope-lists. Our formalization of
the problem is flexible enough to handle several variations.
For instance, if we assume the label on each node to be
the same, our approach mines all unlabeled trees. A simple
change in the candidate tree extension procedure allows us
to discover sub-forests (disconnected patterns). Our formu-
lation can find frequent trees in a forest of many trees or all
the frequent subtrees in a single large tree. Finally, it is rel-
atively easy to extend our techniques to find unordered trees
(by modifying the out-scope test) or to use the traditional
definition of a subtree. To summarize, this paper proposes a
framework for tree mining which can easily encompass most
variants of the problem that may arise in different domains.
We introduced a novel algorithm, TREEMINER, for tree min-
ing.
TREEMINER uses depth-first search; it also uses the
novel scope-list vertical representation of trees to quickly
compute the candidate tree frequencies via scope-list joins
based on interval algebra.
We compared its performance
against a base algorithm, PATTERNMATCHER.
Experiments
on real and synthetic data confirmed that TREEMINER out-
performs PATTERNMATCHER from a factor of 4 to 20, and
scales linearly in the number of trees in the forest. We stud-
led an application of TREEMINER in web usage mining.
For future work we plan to extend our tree mining frame-
work to incorporate user-specified constraints.
Given that
tree mining, though able to extract informative patterns, is
an expensive task, performing general unconstrained mining
can be too expensive and is also likely to produce many pat-
terns that may not be relevant to a give user. Incorporating
constraints is one way to focus the search and to allow in-
teractivity. We also plan to develop efficient algorithms to
mine maximal frequent subtrees from dense datasets which
may have very large subtrees.
Finally, we plan to apply
our tree mining techniques to other compelling applications,
such as finding common tree patterns in RNA structures
within bioinformatics, as well as the extraction of structure
from XML documents and their use in classification, clus-
tering, and so on.


10.
REFERENCES
[1] S. Abitebou], H. Kaplan, and T. Milo. Compact labeling
schemes for ancestor queries. In ACM Symp. on DiscreSe
Algorithms, January 2001.
[2] S. Abiteboul and V. Vianu. Regular path expressions with
constraints. In ACM Int'l Conf. on Principles of Database
Systems, May 1997.
[3] R. Agrawal et al. Fast discovery of association rules. In
U. Fayyad and et al, editors, Advances in Knowledge
Discovery and Data Mining, pages 307-328. AAAI Press,
Menlo Park, CA, 1996.
[4] R. Agrawal and R. Srikant. Mining sequential patterns. In
11th Intl. Conf. on Data Engg., 1995.
[5] T. Asal, K. Abe, S. Kawasoe, H. Arimura, H. Satamoto,
and S. Arikawa. Efficient substructure discovery from large
semi-structured data. In Pnd SIAM Int'l Conference on
Data Mining, April 2002.
[6] M.S. Chen, J.S. Park, and P.S. Yu. Data mining for path
traversal patterns in a web environment. In International
Conference on Distributed Computing Systems, 1996.
[7] Z. Chen et al. Counting twig matches in a tree. In 17th
Intl. Conf. on Data Engineering, 2001.
[8] R. Cole, R. Hariharan, and P. Indyk. Tree pattern matching
and subset matching in deterministic o(n log3 n)-time. In
lOth Symposium on Discrete Algorithms, 1999.
[9] D. Cook and L. Holder. Substructure discovery using
minimal description length and background knowledge.
Journal of Artificial Intelligence Research, 1:231-255, 1994.
[10] L. Dehaspe, H. Toivonen, and R. King. Finding frequent
substructures in chemical compounds. In $th Intl. Conf.
Knowledge Discovery and Data Mining, August 1998.
[11] M. Fernandez and D. Suciu. Optimizing regular path
expressions using graph schemas. In IEEE Int'l Conf. on
Data Engineering, February 1998.
[12] A. Inokuchi, T. Washio, and H. Motoda. An apriori-based
algorithm for mining frequent substructures from graph
data. In ~th European Conference on Principles of
Knowledge Discovery and Data Mining, September 2000.
[13] P. Kilpelalnen and H. Mannila. Ordered and unordered tree
inclusion. SIAM .1. of Computing, 24(2):340-356, 1995.
[14] M. Kuramochi and G. Karypis. Frequent subgraph
discovery. In 1st IEEE Int'l Conf. on Data Mining,
November 2001.
[15] Q. Li and B. Moon. Indexing and querying XML data for
regular path expressions. In 27th Int'l Conf. on Very Large
Data Bases, 2001.
[16] J. Punin, M. Krishnamoorthy, and M. Zaki. LOGML: Log
markup language for web usage mining. In ACM SIGKDD
Workshop on Mining Log Data Across All Customer
TouchPoints, August 2001.
[17] R. Cooley, B. Mobasher, and J. Srivastava. Web Mining:
Information and Pattern Discovery on the World Wide
Web. In 8th IEEE Intl. Conf. on Tools with AI, 1997.
[18] R. Shamir and D. Tsur. Faster subtree isomorphism.
Journal of .Algorithms, 33:267-280, 1999.
[19] B. Shapiro and K. Zhang. Comparing multiple rna
secondary strutures using tree comparisons. Computer
Applications in Biosciences, 6(4):309-318, 1990.
[20] K. Wang and H. Liu. Discovering typical structures of
documents: A road map approach. In ACM SIGIR
Conference on Information Retrieval, 1998.
[21] K. Yoshida and H. Motoda. CLIP: Concept learning from
inference patterns. Artificial Intelligence, 75(1):63-92, 1995.
[22] M. J. Zaki. Efficiently mining trees in a forest. Tech.
Report 01-7, CS Dept., RPI, July 2001.
[23] C. Zhang et al. On supporting containment queries in
relational database managment systems. In ACM Int'l
Conf. on Management of Data, May 2001.




80

