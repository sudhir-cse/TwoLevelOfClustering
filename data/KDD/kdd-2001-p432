REVI-MINER, a KDD-Environment for Deviation Detection
and Analysis of Warranty and Goodwill Cost Statements in
Automotive Industry
E. Hotz, U. Grimmer, W. Heuser and G.
Nakhaeizadeh
DairnlerChryslerAG
Research&Technology
P.O. Box2360, 89013 UIm
49-0731-505-2860
{edgar.hotz, udo.grimmer, wolfgang.heuser,
rheza.nakhaeizadeh}@daimlerchrysler.com

ABSTRACT
REVI-MINER is a KDD-environment which supports the
detection and analysis of deviations in warranty and goodwill cost
statements. The system was developed within the framework of a
cooperation between DaimlerChrysler Research & Technology
and Global Service and Parts (GSP) and is based upon the
CRISP-DM methodology as a widely accepted process model for
the
solution
of Data Mining problems.
Also, we
have
implemented different approaches based on Machine l.earning
and statistics which can be utilized for data cleaning in the
preprocessing phase. The Data Mining models applied have been
developed by using a statistical deviation detection approach. The
tool supports controllers in their task of auditing the authorized
repair shops. In this paper we describe the development phases
which have led to REVI-MINER.


General Terms
Management, Measurement, Economics.


Keywords
Data Mining, deviation detection, data cleaning.


1. PROBLEM DESCRIPTION AND
BUSINESS NEEDS
The complex controlling procedure described in the abstract is
driven by numerous business factors and business needs:

·
The increasing complexity of the product structure with
a total of 6 vehicle business divisions (passenger cars, trucks,
transporters, buses, cross country vehicles, Unimog)
about 150 vehicle series with a variety of body and engine
types.
more than twenty production plants.
M. Wieczorek

DaimlerChryslerAG
GlobalServiceand Parts(GSP)
70546 Stuttgart
49-0711-17-91048

michael.wieczorek@daimlerchrysler.com


·
Different warranty and goodwill policies for each of the
different sales markets and repair areas.

So far DaimlerChrysler has deployed a single standard auditing
system, which is quite inflexible and, moreover, has the following
shortcomings:

·
The report generated by the system is a complicated
hardcopy table which can only be processed with difficulty
as this is done manually.
·
This system is not tailor-made for its designated use, making
it time-consuming and prone to expose fraudulent activities
only accidentally, if at all.

An ideal auditing system - one which is easy to use from the
perspective of the user - however, does not just need to overcome
the above shortcomings, it should also include the following
features:

·
Periodic auditing of
repair shops within shortened time
intervals.
·
Fast detection of anomalies in the warranty cost statements,
trend analysis, and the pinpointing of which repair shop is
largely responsible for these trends.
·
Avoidance of false alarms by indicating fraudulent activities
that reallyjustify the controlling of the repair shops.
·
Selection from a wide range of parameters when initiating an
audit report.
·
Visualization of the results.

Owing to the fact that a huge amount of stored warranty cost data
is available, we have examined the practicabilityand efficiency of
a KDD-based approach in an elementary plausibility study,
together with the users coneerned. We also investigated whether
Machine Learning technology would help us to improve data
quality in the data preprocessing phase. The results of the
plausibility study proved satisfactory, prompting us to realize our
approach.



Permission to make digital or hard copies of all or part of this work lbr
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the lull citation on the Iirst page, To copy
otherwise, to republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
KDD 01 San Francisco CA USA
Copyright ACM 2001 1-58113-391-x/01/08...$5.00
2. A DATA CLEANING APPROACH FOR
THE DATA SOURCES OF REVI-MINER

2.1 Data Sources for REVI-MINER
The available stored data on warranty and goodwill costs is part of
the QUIS (QUality Information System) database, which can be
considered as a kind of data warehouse containing information on
the vehicles produced and any repairs required (see also [1]). The




432

underlying normalized database model manages more than fifty
tables with over 400 fields. QUIS allows several different views
of each separate vehicle repair in order to meet the divergent
information requirements of data analysts for commercial and
technical applications. One field of these records is reserved for
the so-called dealer number (repair shop number), which allows
an exact assignment of costs and repairs to the respective repair
facilities.
The administration of repair facilities is part of the GSP after-sales
system VEGA (Vereinfachte und Effiziente Garantieabwicklung;
in English: simplified and efficient transaction of warranty and
goodwill claims). A description of the process environment for
both systems is given in Figure 1.


~Ipr°ducti°nplant

technical data

<2
damage parts
repair shops



I



~
warranty
']general
parts
and good-
[[vehicle
testing
·
will claims
~data
·
data

A
commercial data

<2
,
k.12.222J
warranty claims
data
claim processing



Figure 1. Data collection for QUIS and VEGA

As Figure 1 shows, data collection begins in the repair shops. On
the one hand, if the repair is covered by a warranty or goodwill
agreement, the shop claims a refund of the warranty costs. All
claim-related data is stored and processed within VEGA. Then, if
the claim is accepted, the warranty cost data and the replaced part
data is transferred to QUIS. On the other hand, some of the
replaced parts are sent to the production plants to analyze the
causes of the damage or failure. The analysis results are then
transferred to QUIS together with general vehicle data (vehicle ID
number, date of production, engine type etc.).

An efficient way to select and update the needed raw data
periodically is to establish a kind of data mart managed by REVI-
MINER, thus allowing users to execute standardized queries on
the tables and fields of QUIS and VEGA, as needed. Figure 2
shows the most important fields the content of which is the input
for REVI-MINER.
query I: general vehicle data
·
VIN (vehicle ID number)
·
date of production
·
engine type
·
sales continent
·
sales country
new vehicle series
new engine types for
existing vehicle series
query 2: data on repairs
·
VIN (vehicle ID number)
·
date of production
·
date of first registration
·
date of repair
·
date of credit note
·
dealer number (shop)
=:~repair area
·
total cost
·
cost of materials
·
unit cost
*
incidentals



query 1: data on repair facilities

repair shop (dealership) number
address
repair authorization for the different
vehicle business divisions
affiliation to special repair shop
subgroups
I>
branch offices
~,
trade partners
representatives


Figure 2. Queries executed on QUIS and VEGA for the
extraction of raw data

2.2 Data Cleaning Approaches
The transaction of warranty and goodwill claims has undergone a
substantial change during the last years. Before the introduction of
VEGA, many of the claims were checked manually for plausibility
and validity of their content at the Sales and Services department
of former Daimler-Benz AG. In order to achieve a high speed
transaction of the claims by VEGA, the sales market organizations
collect and check the warranty and goodwill claims of their
respective repair area and transfer them to VEGA where the
control for validity and correctness of the data has been reduced
to a minimum. This can cause serious data quality problems
within QUIS. As QUIS delivers the raw data for systems like
REVI-MINER or WAPS (WArranty Prediction System), we
decided to develop and implement a data cleaning functionality
for this central database.

To check the quality of data, we have developed three initial
approaches which can be used for data cleaning. The selection of
these three approaches resulted
from both business
user
requirements and system constraints (Oracle/Unix).

·
Exploratory approach: Stored (historic) data is described
by descriptive statistics. The descriptions are compared to
values known from the documentation or other sources. One
example is the number of different values for the attribute
ACHS_BR (from the QUIS table V3QRACHS): where 71
different values are stored, whereas only 20 values are
contained in the corresponding reference field ACBR_BR
(table V3QRACBR). Absolute frequencies H(v) of each of
the 71 values v show that H(v) =1, for a large number of
values not covered by the 20 reference values. These values
are random errors with high probability.




433

Development of a
statistical
prototype for outlier
detection: The basic idea behind this approach is to build
models for time series data assuming statistical normal
distribution. Once models have been built, the correspon-
ding values for new data are derived, and tests statistics are
being applied. If statistically significant deviations are
discovered, suspicious data sets are tagged, and warning
messages are generated (see Figure 3).

b"tep I - Model generation from historic data




I;!!i ;! ilZ!i i:i;!iiiii!ii]

i
i

· . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
b"tep II - Model application for devlatlolt detection
|

Q
i:i:ii!ii~ii:i!iii}:iiii}ii}ii:~i}!!:~ii:!;!ii!;:!i~
;!~i:!!:i~iii!iii!~:i':i[i!!iii{,~?:~!!!i!':i::i!i!


Figure 3. Process for model generation and deviation detection

Example findings from this prototype are, for example,
irregular (that is below, rasp. above configurable action
limits) numbers of warranty claims on a daily or weekly basis
for each repair shop (see Figure 4).


Deviation detection
Claims from garage 212, Period: 01.01.1999-30.06.1999


Weekends (Saturday +
Sunday)
16./17.1.1999
claims:
58
expected:
31.23
test statistic: 0.0056096



Weekda~ (Monday,Lhrough friday}
05.-09.04.1999
claims:
782
expected:
1146.72
test statistic: 0.0046876


Figure 4. Sample deviations found by the prototype

The application of this program
can contribute
to an
automated early detection of irregularities for any numeric-
valued attribute. Aggregation levels like day, week or month
only need to be specified by the user once. If desired, the
normal distribution models can easily be updated whenever
new data arrives. This prototype currently runs on fiat files
only, that is the data has to be extracted from the database
and preprocessed to some extent before it can be analyzed.

Application of GritBot (see also [2]): The third approach
applies the commercial tool GritBot to each one of the QUIS
database tables as well as to several joined tables (views).
Although quite sealable, GritBot can only process flat files.
GritBot performs several rule generating runs, each time
considering another attribute out of a subset of n (default
n=5) attributes as target attribute. Following, each data set
that violates a rule is assigned a significance value indicating
how likely the anomalous value could occur by chance rather
than by error. For any analysis, GritBot needs two files: one
containing all the data and a names file containing a short
description of the names and formats of the data file. Both
files need to be prepared manually. GritBot can handle
nominal, numeric, and date values. Figure 5 contains part of
an example output for the analysis of the QUIS table
V3QRACHS.


G~Bot [Rekmse1.02]
MonJul 3 13:30:522000
Optiona:Application "ACHSI'
Read5219164 cases(I 4 attributes) from ACHS1.data


case 1324925: [0.000]
ACHS..EDAT= 19~9/,72/,:.~ (55911 cases,mean 1996/03/31, 100.00%>= 1995/09/01
ACH$_BRVin [311, 919, 305, 306, 061,018, 400, 313, 470, 054, _., 075] |480|
ACH$_PDAT> 199,5/09/I I and <= 1997/10/29 [1995/11/15]
caee3773780: {0.000]
AOH3 EPO$ffi 2 (118966 caaea, 100.00%" l')
ACH$_BRV" 510
case2746691: [0.000]
ACH$_BR= ~;'~!; (32459 cases, 100.00%"730')
ACI~..BRV" 018
AOH$_EPO$= I
ACHS..BKB= D


Figure 5. Sample GritBot output

For case 1324925 the value for the field ACHS_EDAT
might be erroneous, whereas for case 2746691 the value of
the field ACHS BR seems to be incorrect. Care should be
taken when interpreting the results: there is no evidence that
the value for the top listed attribute is wrong - it might as
well be the case that one of the attribute values listed below
is erroneous. A more detailed description of the tool,
including syntactic and semantic instructions can be found at
the company's web page www.rulequest.com. As the tool
generates large output listings (some 270,000 lines of output
for all single QUIS tables), additional postprocessing steps
need to be applied to aggregate the results. Currently, this is
done semi-antomaticaily for all the QUIS tables and views
using several scripts.

Results from all three approaches (exploratory statistics, statistical
prototype and GritBot) are to be aggregated and collected over
time in database tables. From these values, corresponding data
quality figures will be derived and monitored. As all three
approaches yielded promising and useful results, we plan to refine
them during the following months.


3. THE APPLIED KDD-PROCESS
The concept of REVI-MINER is based upon the CRISP-DM I
methodology and covers the main steps

·
Business understanding
·
Data understanding
·
Data preparation
·
Modeling
·
Evaluation
,
Deployment.
The activities that we have performed in each phase are described
in Figure 6. Phase 1 of Business Understandinghas been partly
described in section 1. An excellent understanding of the project


I CRISP-DM: CRoss Industry Standard Process for Data Mining
(see http://www.crisp-dm.org)




434

objectives and business requirements defined by the project
partners is a crucial prerequisite for the Data Understanding
phase (see also section 2). All activities related to Data Cleaning
are realized in this phase. Also, the identification of the required
tables and fields within the data sources QUIS and VEGA results
in the design of queries upon these databases to update the data
mart (raw data collection) for REVI-MINER.

Data Understanding,
Data Cleaning (sec. 2)


Business
Understanding
(section 1)

QtaS
I
I VEGA




Deployment
-~

4,
VISUAL BASIC
prototype
4,
lultlate and control the
Data Mining phases
·Data Understanding
*Data Preparation
·Modeling
4,
Realize enhancements of
REVI-MINER resulting
~._
from the evaluation pha~,j




f

4,
T
Evaluation

Does the modelmeet
a// business
objectives?
Is therea needto
adaptthe implemeated
model?
÷
~ata
Preparation ~

Step 1:panmlacrs for a
repo~
· vehicle aggregation
· damage code
aggregation
· choice of deviation
analysiscriteria
· number oftop damage
codes to be examined
· repair shop
aggregation

Step2: Data processing
· determine top damage
codes
· calculate selected
~critetia
J
+

4,
Modeling (section 4)

Statisticalanalysisof the criteria
j
·
compare shop criteria to
the criteria of repair shop
~k
clusters
·
braid~-criteria
·
perform extremevalue
analysis ofcriteria
J
Figure 6. Data Mining process implemented for REVI-MINER

Two of the parameters that have to be chosen by the user during
the Data Preparation phase will now be described shortly:

(1) Vehicle aggregation refers to how many vehicles are used to
calculate the deviation analysis criteria. Level 1 comprises
the vehicles of all divisions, level 2 the vehicles of one
division, level 3 the vehicles of one class and level 4 the
vehicles of one vehicle serie.
(2) Each repair case is encoded with a so-called damage code.
This can be a 2-digit, 5-digit or 7-digit number. As we have
about five thousand different damage codes on the 5-digit
aggregation level, it would not be very useful to calculate the
deviation analysis criteria for all of them. REVI-MINER can
identify the most expensive and most frequent damage codes
thus creating a list of top damage codes, for which the
analysis will be executed. The user just has to determine the
number of codes for the top list.


4. STATISTICS-BASED DEVIATION
ANALYSIS CRITERIA
Discussions with the users showed that the criteria needed to
identify and analyze deviations in warranty and goodwill data
should cover the main cost types (damage types)
·
Total cost (total number of repairs).
·
Labor cost (number of working hours).
·
Cost for repair material (number of repairs where material
was needed).
·
Cost for replacement of aggregate parts, e.g. gear unit, air
conditioner unit, engine (number of repairs with deployment
of aggregate parts).
·
Incidental cost (e.g. for transportation of a vehicle to a repair
shop site)
All criteria regarding cost and damage types has to be calculated
for each damage code on the chosen level of damage code
aggregation (2-digit, 5-digit or 7-digit damage code) for each
repair shop. The calculated criteria during the data preparation
step are stored in a separate database table for each repair shop.
Each top damage code is represented by a table record (see Tablel
as an example).

Table 1. Calculated criteria for one repair shop

de
nl
suml
avgl devl
n2
sum,2 Avg2 dev2

XXXXX
19414 4474462
23C
0,13 18639 546720
29
0,17

82302
119
22097
186 -0,04
119
1736
15
0,23

:92075
122
23948
196
0,49
122
3926
32
0,34

74311
116
14113
122 0,24
115
2045
18
0,21

32206
45
8877
197
0,73
45
1283
29
0,44

72401
941
10882
116 -0,15
94
1511
16 -0,04

78138
63
3379
54
0,17
63
537i
9
0,13

73001
113
12300
109
0,63
113
1976
17
0,36

73021
151
16188
107
0,12
151
2397
16
0,03

The table fields contain the following information:

·
dc: damage code
·
nl: total number of repair cases
·
suml: sum of total costs for all repair eases
·
avgl: average of total costs for each repair case
·
devl: deviation of repair shop avgl to avgl of the respective
repair shop cluster
n2: number of repair cases with labor costs
sum2: sum of labor costs for all repair cases
avg2: average of labor costs for each repair case
dev2: deviation of repair shop avg2 to the avg2 of the
respective repair shop cluster




435

The number of table fields depends on the choice of cost and
damage types during step I of the data preparation phase (see
Figure 6). The nature of the criteria created allows different
analysis approaches for the detection of suspicious repair shops to
be implemented. We will now, in short, describe two of the seven
applied approaches.


4.1 Deviation Analysis for a Single Repair
Shop by Comparison to Predef'med Repair
Shop Clusters
Repair shop clusters can be built by assigning facilities according
to selected criteria, for example:

~"
Turnover from all repairs over a specified period of time.
Affiliation of the repair shop to given repair shop subgroups,
as there are branch offices, trade partners, representatives,
general representatives.

Comparison of repair shop average total costs (labor, material,
units and systems, incidentals)
per damage code with the
respective values of the repair shop cluster delivers first hints of
irregularities
in warranty and goodwill transactions. We can
measure absolute and relative deviations of repair shop averages
from averages of the related cluster and weigh the absolute
deviation with the number of underlying repair cases.

xi,j : average costs of repair shop i for damage codej

Xk,l,j :average costs of repair shop cluster with turnover group

k and subgroup 1for damage codej
ni, j :number of cases with damage codej for repair shop i

xi'j
- 1: relative deviation of average cost for damage codej
~k,t,j
@i,j - Xk,l,j )" hi,j: weighted absolute deviation of average

costs for damage codej


Weighted absolute deviation of averages
between repair shop and repair shop cluster
for top damage codes
7000
6000
~:~?~o~i~%i~
~-~
~ ~;-~
5000
~;-[ti,~ ~ ~

3000
....
"




Figure 7. Weighted absolute deviation of averages between one
shop and the respective shop cluster
4.2 Meta-criteria for All Repair Facilities or
Repair Shop Subgroups
We can add up the weighted absolute deviations defined in
section 4.1 for all top damage codes, thus arriving at a new meta-
criteria for each shop. Graphical presentation of these figures
shows conspicuous differences
between the facilities.
The
controller will have to judge whether there are reasonable
explanations for these differences. If this should not be the case,
the facilities of interest will have to be more closely investigated
separately using the method set out in section 4.1.

m :number of top damage codes

m

E
(~i,j - ~k,l,j )" ni,j:sum of weighted absolute deviations
j=l
over all m top damage codes



Sum of weighted absolute deviations of
average costs between repair shop and
repair shop cluster for top damage codes
1200000
...............
4 nnnnnn
~!i'ii~i':i~!i;i~,
~i!~';~:~;"


000nnn
...........................
.............

4aannn

~ v ~ v
~
~
:~
!
~
~
,~;~ ~
~
:z~i~.~,~i~ 1




Figure 8. Sum of weighted absolute deviations for repair
facilities


Let
/~n(X):= 1 E
I(xi <-x)
be
the
sample
distribution
i<_n
function (dO for a series of univariate data x1..... Xn, e.g. the

mean values xi,j
for damage code j of repair shop i. Then we

can create the sample (If for each top damage code j,j
= 1..... m
and construct a weighted average for each repair shop. The weight
for each top damage code is the quotient out of total costs for
damage codej at repair shop i and total costs for repair shop i:

n :
number of repair shops
m :
number of top damage codes

F j:
sample
(dr)
damage
jdistribution function
for
code

V repair shops i,i = 1..... n

Yi : total costs for repair shop i

V top damagecodes j, j = 1,...,m

nj : sample size for damage code j

Yi,j : total costs for damage code j at repair

shop i




436

End V
=,,j(*,,J):
n.



nj
k=l




......... ::¢,weighted average
Yi
j=l

End V

This weighted average gives clear evidence as to which facilities
could be candidates for a thorough audit on site or for further data
analysis activities that would expose the nature of the underlying
problem.

Weighted distribution classification number
for 1012 repair shops


0,8
0,6
0,4
0,2
0




Figure 9. Weighted averages for repair facilities
5. DEPLOYMENT OF REVI-MINER
The Data Mining tool REVI-MINER supports controlling in
detecting
and
preventing
fraudulent
activities
within
the
DaimlerChrysler AG repair shop organization. Its functionality
covers the essential phases of a Data Mining process and provides
a user interface with easily manageable menus based on VISUAL
BASIC forms. REVI-MINER provides the means for a fast,
efficient, and meaningful analysis of the warranty and goodwill
data for repair facilities, thus giving controllers in the auditing
department GSP/SMW early warning of possibly fraudulent
activities.

REVI-MINER allows to the controllers flexible and detailed
preparatory work before visiting a repair shop site thus saving
plenty of time that can be used for the on-site audit.


6. Acknowledgements
We would like to express our deep gratitude to the colleagues of
the department GSP/SMW within the Direction Global Service
and Parts (GSP) of DaimlerChrysler in Stuttgart for the generous
and trustful cooperation during the development of REVI-
MINER.


7. REFERENCES
[1] Hotz, E., Nakhaeizadeh, G., Petzsche, B. and Spiegelberger,
H. "WAPS, a Data Mining Support Environment for the
Planning of Warranty and Goodwill Costs in the Automobile
Industry". Proceedings of the Fifth ACM SIGKDD
International Conference on Knowledge Discovery and Data
Mining, 1999, pp. 417-419.

[2] Quinlan, R., GritBot - An informal tutorial,
http://www.rulequest.com/gritbot- unix.htmI, 2000.




437

