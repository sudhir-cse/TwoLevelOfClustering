Empirical Bayes Screening for Multi-Item Associations
William DuMouchel and Daryl Pregibon
AT&T Labs---Research
180 Park Avenue
Florham Park, NJ 07932, USA
{dumouchel, daryl}@research.att.com

ABSTRACT
This paper considers the franlework of the so-called "market
basket problem", in which a database of transactions is mined for
the occurrence of unusually frequent item sets.
h~ our case,
"unusually frequent" involves estimates of the frequency of each
item set divided by a baseline frequency computed as if items
occurred independently.
The focus is on obtaining reliable
estimates of this measure of interestingness for all item sets,
even item sets with relativelylow frequencies. For example, in a
medical database of patient histories, unusual item sets including
the item "patient death" (or other serious adverse event) might
hopefully be flagged with as few as 5 or 10 occurrences of"the
item set, it being unacceptable to require that item sets occur in
as many as 0.1% of millions of patient reports before the data
mining algorithm detects a signal. Similar considerations apply
in fraud detection applications.
Thus we abandon the
requirement that interesting item sets must contain a relatively
large fixed minimal support, and adopt a criterion based on the
results of fitting an empirical Bayes model to the item set counts.
The model allows us to define a 95% Bayesian lower confidence
limit for the "interestingness" measure of every item set,
whereupon the item sets can be ranked according to their
empirical Bayes confidence limits. For item sets of size J > 2,
we also distinguish between muhi-item associations that can be
explained by the observed J(J-l)12 pairwise associations, and
item sets that are significantlymore frequent than their pairwise
associations would suggest. Such item sets can uncover complex
or synergistic mechanisms generating multi-item associations.
This methodology has been applied within the U.S. Food and
Drug Administration(FDA) to databases of adverse drug reaction
reports and within AT&T to customer international calling
histories.
We also present graphical techniques for exploring
and understandingthe modelingresults.


Categories and Subject Descriptors
H,2.8 [Database Management] Database Applications - data
mining, statistical databases.


General Terms
Statistical Models, Data Mining, Knowledge Discovery.



Permission to make digital or hard copies of all or part of this work lbr
personal or classroom use is ffanted without fee provided that copies
are not made or disu'ibuted for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, to republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
KDD 01 San Francisco CA USA
Copyright ACM 2001 1-58113-391-x/01/08...$5.00
Keywords
Association rules, empirical Bayes methods, garmna-Poisson
model, market basket problem, shrinkage estimation.

1. THE MARKET BASKET PROBLEM
A common data mining task is the search for associations of
items in a database of transactions. Each transaction, such as a
list of items purchased, or a list of diagnoses and medications in
a patient medical report, defines a subset of all possible items in
the union of all transactions. If there are K total items, we define
for each transaction, binary random variables X1..... XKthat are
1 if the corresponding item is included in the transaction, and 0
otherwise, ff the total number of transactions in the database is
N, then the data being modeled could be represented as an N x K
matrix X = Xik,i = 1..... N; k = I..... K. To add more realism to
the statistical model, we do not assume that the distribution of
each row of X is identical.
Rather, we assume that the
transactions are stratified, for example by store location in the
case of supermarket scanner data, or by patient age, sex, etc. in
the ease of medical data. Time period of the transaction could
also be a stratification variable. The purpose of stratification is
to avoid finding spurious associations between two items merely
because they both tend to be more frequent in the same strata.
We assume that associations due to stratification variables (so
called between strata associations) are not of interest, and instead
require the methodology to identify items that are associated
within strata. For example, certain upscale store locations might
have greater sales volume of high-priced items, but we may only
wish to say such items have interesting associations if they tend
to be purchased together more frequently than their per-store
marginal frequencies indicate, since an overall association would
show up even if the items were purchased independentlywithin
each store.

As the preceding paragraph indicates, we are departing from the
majority of published treatments of the market basket problem by
going beyond the enumeration of the support (proportion of
transactions) of frequent item sets. No matter how frequentlyan
item set occurs, we assume that it is of little interest to the
analyst if its frequencyis about the same as would be predicted if
the members of the item set occur independently within each
stratum.
This comparison frequency, based on within-strata
independence, will be called the baseline frequency of the
particular item set, and our measure of interestingness is the
ratio of actual to baseline frequency.
Two earlier papers that
focus on interestingness measures as deviations from expected
frequencies predicted by independence for the market basket
problem are Silverstein, Brin and Motwandi [14] and Aggarwal
and Yn [1]. Considering the former paper first, [14] rightly
criticizes the more common support-confidence framework for
association rules and suggests using a chi-squared test statistic




67

for the hypothesis of complete independence of items in an item
set as the basis for comparing the degree of dependence among
different item sets.
They develop a theory for using this test
statistic that relies on the property of upward closure, namely
that the computed chi-squared statistic testing independence for a
set of items (binary variables in a database) can only increase if
you increase the number of items (size or dimensionalityof the
item set). Unfortunately, the authors mistakenly state that the
degrees of freedom of the chi-squared statistic is always just 1,
for all values of J, the item set size.
ha fact, the degrees of
freedom of their test statistic is 2J - J - 1. Thus, although the
test statistic will increase as more variables are included in the
computations, the statistical significance or "p-value" will not be
monotone and may increase or decrease, thereby invalidatingthe
supposed theoretical properties of their proposed procedures and
computational methods.
In addition, as demonstrated in
DuMouchel [8] and discussed in Section 3 below, relying solely
on a statistical significance test statistic for measuring
dependence has the built-in flaw of being overly sensitive to
sample size.
The latter paper [1] also criticizes the standard
support-confidence framework for item sets, pointing out how it
is susceptible to identifyingspurious associations. These authors
propose a measure they call collective strength.
If I is an item
set, let v(/) be the violation rate of the item set in the database,
which is the fraction of transactions that contain some, but not
'all, of the members of 1. Then, if E[] refers to expectation under
the assumption of independence of items, collective strength is
defined as

C(/) = (1 - v(1)) E[v(/)] / v(/) (1 - E[v(/)])

For example, for item sets of size 3, the notion of collective
strength pools the proportions of the two triples
(0,0,0) and
(1,1,1), and contrasts their proportion with v(/), the pooled
proportions of the other six possible triples. Then the ratio (1-
v(/))/v(/) is divided by the correspondingratio of expected values
under independence. Aggarwal and Yu [1] show that collective
strength avoids the undesirable spuriousness to which the
support-confidence framework is susceptible, yet has at least as
nice computational properties, which allow the eil]cient
enumeration of what they define as "strongly collective item
sets." However, in our opinion, this is achieved at a considerable
loss of interpretability and practical value as a measure of
association. For example, let us continue the example of an item
set of size 3, and suppose the three items each occur in 10% of
transactions.
Under the
independence assumption, the
probabilities of observing (0,0,0) and (1,1,1) are .93 = .729 and
.13 = .001, respectively, leading to E[1 - v(/)] = .729 + .001 = .73
and E[v(l)] = .27. Note how the probabilityof observing (0,0,0)
dominates this calculation, compared to that of (1,1,1), even
though it is the simultaneous presence (not absence) of the three
items that is almost always of more interest in applications.
Suppose that the observed probabilities of (0,0,0) and (1,1,1) are
.75 and .02, in which case the triple of items is present 20 times
more than expected from independence. But this sense of a 20:1
increase is lost in the computation of collective strength, where
C(/) = (.77)(.27) / (.23)(.73) = 1.24 is close to 1, the value of
C(/) under independence. This undesirable property of collective
strength will persist whenever the individual items are not very
frequent (e.g., when most items occur in less than one-third of
transactions) and we believe this describes the majority of
market basket problems.

Both the cited papers, as well as [6], refer to the simple ratio of
observed to baseline frequencies of occurrence as the interest of
an item set, which we denote R. The primary criticisms of R as a
measure of dependence are

1. Since R is bounded above by 1/P(A)P(B) .... if all the items
A, B.... in an item set are quite probable, the range of R
can be restricted to seeminglysmall values.

2. Since R is symmetric in the items A, B, ..., it does not
indicate predictive power or suggest causation.

3. Computing all item sets with greater than a specified value
of R will be costly computationally, since a great many item
sets with small support can have large values of R.

We believe that the first criticism is mitigated by the fact that the
vast majority of market basket problems deal with items whose
probabilities are not large.
Data mining is, by definition, a
search for low probability events of high value. We view the
lack of directionality of R as more an advantage than a
disadvantage.
Once an item set is identified as unusually
frequent, domain knowledge is required to identify mechanisms
and potential causality among the items in the item set,
something far beyond the presumption of a statistical algorithm.
The third criticism above identifies both a computational and a
statistical issue. We do not focus on computational issues here,
which are already well treated in the association rule literature,
but rather we focus on the statistical issues that arise for item
sets with small support.

A novel element of our approach to identifying interesting item
sets is the ability to detect interesting associations even if they
have relatively small support. To do so, we must explicitly allow
for noise or spurious association due to small samples. Consider
an item set observed in 200 transactions, even though the
baseline (independence model) frequency is just 2. How should
this 200:2 ratio of actual to baseline be compared to other item
sets having the same ratio, such as 20:0.2, or even 2:0.02? Our
answer involves an empirical Bayes model, described in detail in
[8], that produces a posterior distribution fbr the true ratio of
actual to baseline expected frequencies, assuming that the
observed counts of every item set have separate Poisson
distributions. Our conservative estimate of the ratio of actual to
baseline is the lower 95% Bayesian confidence limit for this
ratio, which we denote ~,.os. When observed counts are large, ~..0s
will be almost identical to the naYveobserved ratio, namely R =
100 in the 200:2 example. However, ~,.0swill be rather less than
100 for the case of 20:0.2, and will be much smaller, perhaps
near 1, in the 2:0.02 example. This tendency of empirical Bayes
estimates to reduce the observed effect when sample sizes are
small gives rise to the label shrinkage estimate.

To summarize, we introduce three improvements to the existing
market basket methodology.
First, we allow stratification
adjustments to eliminate spurious association due to non-
interesting stratification variables. Second, we focus on the ratio
of item set frequencies to baseline frequencies computed via an
independence assumption, rather than merely the support of each
item set. And third, we replace the observed ratios of actual to
baseline frequencies by empirical Bayes shrinkage estimates in




68

order to smooth away the noise due to small samples. A fourth
innovation, applicable only to item sets of size greater than 2,
will be described in Section 4.

2. BASELINE AND RELATIVE RATES
Let Ns be the number of transactions in stratum s, let Nsj be the
number of transactions in stratum s that include item j, let Nsjkbe
the number of transactions in stratum s that include both item j
and item k, and so forth, where s = I..... S, the number of strata,
and l<j<k_<K .
Define P.q = NsffN.~ as the proportion of
transactions in stratum s that include item j. The baseline
frequencies for item sets of" size 1, 2, 3, and 4 are, respectively,
ej, ejk,e~kt,and ejk~, where

ej = Es N~P~j: Es N~j
ej~= ~s NsPsjPsk

ejkt= Es N,Psps~P~.t
eyktm= Zs NsP~jP,kP,tPsm

Analogous definitions are used for baseline frequencies of larger
item sets. This method of computing separate baseline frequency
estimates for each stratum and then adding across strata became
standard in the biostatistics literature after the article by Mantel
and Haenzel [12].
The method provides protection against
Simpson's
paradox
[15]
whereby
associations
that
hold
separately within each stratum can disappear or even be reversed
when data are pooled across strata. DuMouchel [8, Table 5 and
related discussion] provides an example of how failure to stratify
by age could lead to an incorrect assessment that polio vaccine is
associated with sudden infant death syndrome (SIDS).
Since
polio vaccine is overwhelmingly given to infants, and of course
only infants develop SIDS, a database including suspected
adverse drug reactions for patients of all ages will show an
excess of SIDS cases from polio vaccine recipients, although
many well-regarded clinical trials of vaccine recipients show no
such relationship.

We use lower case ns for total across-stratum counts: nj = Zs Ns.i

= ej, njk = Z~JV~j~,etc. Then, for all item sets defined by (j<k),

(j<k<l), (]<k<l<m), etc., the raw relative rates are defined as

Rjk = njk!eik,
Rjkt = njJej~,
Rjto,= njkMej~m.

In domains tbr which only very large counts (e.g., n > 500) are of
interest, then sorting and otherwise examining item sets
according to their values of R may provide a sufficient analysis.
It is recommended that separate examinations be conducted for
item sets of size 2, of size 3, etc., because in general the
independence baseline frequencies tend to fit the raw ns more
poorly for larger item sets than for smaller item sets. That is, the
distribution of values of R will usually be diftbrent for pairs than
for triples or quadruples or higher-way item sets.

3. EMPIRICAL BAYES ESTIMATATION
The empirical Bayes approach smoothes the values of R to allow
more reliable estimates for smaller sample sizes. The empirical
Bayes process is repeated separately for each size item set. The
process for pairs is the same as for triples, quadruples, etc., so
for simplicity we will drop the subscripts in our notation and
assume that we have a large set of values (n, e) where each
observed count n is a draw from a Poisson distribution with its
own unknown mean ,u--&e. Our goal is to evaluate the posterior
distribution of each value of ~. = IMe = E[R].
This statistical
framework, where there are a great many similar estimation
problems that can be fomaulated within a common model, is a
natural application of empirical Bayes methodology, sometimes
also called hierarchical modeling.
See the books [7] or [13] for
extensive expositions of the methodology. DuMouchel et al [9]
and [8] introduced the application of empirical Bayes estimation
to data mining of large sparse frequency tables in the domains of
natural language processing and screening of adverse drug
reaction reports, respectively. The latter paper is accompanied
by the discussion of two FDA scientists, Drs. Ana Szarfman and
Robert O'Neill, who describe the adoption and successful
application of the methodology at the FDA.
Those two papers
considered the analysis of large sparse two-dimensional tables,
whereas in this paper we extend the methodology to J
dimensions, where J is the item set size being considered. This
Section provides an overview of the rationale and properties of
the empirical Bayes method applied to the market basket
problem, while a more formal development is deferred to the
Appendix.

Empirical Bayes estimation enhances the simple use of the
separate values of"R = n/e as the estimates for each separate 9, by
adding the prior information that the many values of 9, are
connected in that they can be treated as having arisen from a
common super-population of ~,-values. The method assumes that
the set of unknown ~,s are distributed according to the continuous
parametric density function of a specific form rt(~,l 0). The
parameter vector 0 is estimated from the data, the feature that
distinguishes empirical Bayes methods from full Bayes methods.
The estimation of 0 involves fitting the distribution of all the
values of R, and once 0 has been estimated, the prior distribution
of every ~, is assumed to be ~z(~.l d ). Since this allows us to
improve over the simple use of each R as estimates of the
corresponding Ls, we say that the values of R "borrow strength"
from each other to improve every estimate. To summarize, the
empirical Bayes algorithm involves the following series of
assumptions and steps:

1. A collection of pairs (n, e), where each
n > n*, the
minimum support.

2. Assume that conditional on known ~,, each n is a Poisson
random variable with mean ~,e.

3. Assume that tbr some fixed 0, the ~,s are distributed
according to a family of prior distributions, 7t(~,l0).
As
discussed in the Appendix, we use a mixture of two gamma
distributions as a convenient five-parameter fan:fly.

4. Compute the unconditional distribution of each n as f(n) =

S Poi(n [).e)~(A I0)d2, where Poi( ) is the Poisson density

function.

5. Use the product of all expressions fin) in Step 4 to compute

the maximum likelihood estimate, t~.

6. For each (n, e) use Bayes rule to compute the posterior

distribution of X as Poi(n[Ae)~r(2 l0) I fin).

7. For each ~,, use the results of Step 6 to obtain the posterior
mean of log0,) and exponentiate it to obtain A, the
empirical Bayes geometric mean of X. Also obtain ~,.o.s,the




69

5~hpercentile of the posterior distribution of ~,, for use as a
Bayesian lower confidence limit.

Although the above procedure may seem complicated, as
discussed in the Appendix, the amount of computation involved
is small compared to the computation required just to tabulate
the item set frequencies n. The resulting estimates A and lower
confidence limits )~.05are much more reliable than R when n and
e are not large, and they will be very close to R when n and e are
large.
[8, Tables 4 and 6] compared three methods of ranking
collections of pairs (n. e) (in the adverse drug reaction domain):
based on the value of R, based on the value of A, and based on
the significance level of the hypothesis test that n ~ Poisson(e).
The highest-ranking pairs according to R had n = 1 or 2 but tiny
e, such as (n = 1, e = .0003, R = 3300, A = 2.4). The highest-
ranking pairs according to statistical significance had huge n, but
not very large values of R, such as (n = 7500, e = 500, R = 15, A
= 15).
The highest-ranking pairs according to the empirical
Bayes geometric mean were a compromise between these
extremes, such as (n = 300, e = .2, R = 1500, A = 1200).
Ranking item sets according to their significance levels of an
hypothesis test of independence tends to give too much weight to
highest frequency item sets that may have only moderately high
interest measures )~. We believe that in most domains, (n = 300,
e = 0.2) is more "interesting" than is (n = 7500, e = 500).

Sorting item sets of each size by the largest values of A or ~..05is
one recommended way to prioritize the examination of" the
modeling results. This assumes that the ratio parameter ~, is of
primary interest, and in our experience that assumption is
reasonable when the main objective is to uncover causal
relationships among the items.
On the other hand, in many
business applications the discovery value of an association may
be roughly proportional to the number of times that the
corresponding item set occurs in excess of that predicted by
independence.
In such a situation, we recommend sorting the
item sets by the variable EXCESS, where

EXCESS = e× ()~.05- 1).
(1)

EXCESS is interpreted as a conservative (95% lower confidence
limit) estimate of the number of times that the item set occurred
in excess of the level of pure noise.

4. MULTI-ITEM ASSOCIATION VERSUS
PAIRWISE ASSOCIATION
For pairs of items, the interpretation of association, or deviation
from independence, is straightforward, since there is only one
degree of freedom for dependence and the single correlation
between the two items is a familiar concept. For larger item sets
the situation is more complicated.
Contributing to the mutual
dependence of"J items, there are 2J - J - 1 degrees of freedom,
which can be partitioned into J(J - 1)/2 pairwise correlations and
the remaining 2J - J(J + 1)/2 - 1 degrees of freedom for higher-
order association. For triples, there are 3 pairwise associations
and 1 degree of freedom for 3-factor interaction. For quadruples,
there are 6 pairwise associations and 5 degrees of freedom for 3-
and 4-factor interactions.
When an analyst finds that the
frequency of a triple or quadruple is much greater than
independence predicts, what actually has been found? Suppose a
triple ABC is unusually frequent. Is that just because AB and/or
AC and/or BC are unusually frequent, or is there something
special about the triple that all three occur frequently in
transactions?
If the former, then the mining of triples hasn't
really found anything that couldn't have been deduced from the
results of examining pairs. If the latter, further investigation may
uncover an important new insight into the domain. For example,
in the medical domain, suppose in a database of patient adverse
drug reactions, A and B are two drugs, and C is the occurrence of
kidney failure. In case 1, A and B may act independently upon
the kidney, but since A mad B are sometimes prescribed together
there may be many occurrences of ABC. In case 2, A and B may
have no effect on the kidney if taken alone, but when taken
together a drug interaction occurs that often leads to kidney
failure.
It would be valuable to be able to recognize case 2
situations automatically. In general, it would be quite valuable
to automatically pick out the multi-item associations that cannot
be explained by the pairwise associations in the item set. After
all, there may be tens of thousands of supposedly interesting
triples and quadruples to investigate after performing a market
basket analysis, and it would be nice to know which of them can
be discarded as "nothing new" once you have completed the
analysis of pairs.

We will address this problem using the standard statistical theory
of log-linear models as treated in textbooks like [4] or [5].
Consider the 2J frequency table determined by the joint
distribution of any J>2 columns of X. The counts in the table are
denoted mj,A...jj, where each jl ..... jj is either 0 or 1. The J

variables are said to obey the "all-two-factor" model if the
expected frequencies of the array of counts are of the form

E[ mj,A...j~ ] =
,~J,~,A
I,./,~:,J~,.J,:~,J~A
,.i,_,j,
(2)
~u1 "2 "~J
~12
t'13
~23
""~J-i,J

The model parameters are a, bl,..., bj, cl2..... cj-i,J. Thus, for the
item set defined by these J items, the all-two-factor model
predicts that E[n~...0] = a transactions contain none of the J
items, and that E[mal...j] = abl...bj cl2...cJ.i.j
transactions
contain the complete item set.
Log-linear model theory states
that the all-two-factor model is the canonical model for
determining a joint distribution from all the two-way marginal
distributions without adding any more complex information. For
example, the sufficient statistics for fitting the all-two-factor
model are the Jx(J-1)/2 2x2 tables of counts from the two-way
marginals. The fitted values (2) are easily computed using the
iterative proportional fitting algorithm [4], which is guaranteed to
converge. For any item set of size J > 2, whose frequency has
been computed as n = m~t...~ > n*, the frequencies of smaller
included item sets will be already counted and available to
supply the statistics necessary to fit the all-two-factor model and
therefore its value of E[mll...l].
This leads to the following
definitions for each item set of size greater than 2:

e/tuar = E[m11...~] = predicted count of all-two-factor model
based on all two-way distributions
(3)

EXCESS2 = Axe -eAtt2r
(4)

EXCESS2
is an estimate of the number of transactions
containing the item set over and above those that can be
explained by the pairwise associations of the items in the item
set.
Although EXCESS2
could be negative, we hesitate
interpreting such item sets, focusing mainly on large values of




70

EXCESS2 as possibly indicatingcomplex relationships involving
more than pairwise association among the items of the item set.

Note that in computing (3), rather than use the raw (and possibly
unstable) two-way counts to fit the all-two-factor model, we use
the shrinkage estimates of two-way counts based on substituting
IXjk= A1~xei~for the raw two-way count ny~,for every pair of items
(j, k) contained in the item set. The four elements of the 2x2
table used are thus p,j~,n~- bryn,n~- kt~,and N- n~- n~+ Ix~.

5. EXAMPLE: INTERNATIONAL
CALLING BEHAVIOR
In this section we apply the ideas to an example drawn from
telecommunications. We do not pursue a complete analysis of
these data. Rather we highlight those aspects of the analysis that
differentiates our work from that reported in the literature.

In support of security operations, AT&T maintains a calling
signature for all accounts that make international calls. This
signature contains features of calling behavior such as estimated
calling rate (e.g., calls/week), call time (e.g., time-of-day and
day-of-week), call duration, and call terminations (e.g., countries
called) that serve as a baseline for detecting anomalous activity.
We focus on the call terminations component of the signature
that consists of a list of the top-5 recently and frequently called
countries for each account. This list evolves through time using a
probabilisticbumping algorithm, so that while the list itself has a
fixed length, the labels and associated frequencies of those labels
change as calls are made. The complete item set consists of the
228 countries where AT&T terminates international calls. We
generate "market baskets" by taking monthly snapshots of the
top- 5 country list for each account for three consecutive months.
Note that on average, our accounts call only two different
countries per month so that for most accounts there are fewer
than 5 countries (items) in their monthly "market basket".

There are about 7 million accounts that regularly make
international calls. These accounts are stratified along several
dimensions:

- account type: business/residence/casual(unknown)

- line type: multi-line/single-line

- usage category: low/medium/high

These account descriptors define 18 strata corresponding to the
unique combinations of their levels. The number of accounts in
each stratum varies from a low of a few hundred to a high of 2
million. It is desirable to stratify the data since the
communication behavior is known to be different across these
variables. In searching for interesting country combinations, we
would not want to highlight country combinations that were
interesting solely due to the individual countries' popularity in
the same strata.

Our data set consists of over 20 million transactions (7M/month
x 3 months). We chose the minimum support size of n* = 4,
and by construction, we only examined item sets up to size J =
5. Each of the three monthly analyses required approximately52
minutes of CPU time on a Pentium 750 MHz processor. Almost
all of this time was spent generating the collection of item sets
satisfying the minimum support requirement (including about
16,000 pairs, 120,000 triples, 150,000 4-tuples and 29,000 5-
tuples). About 3 of the 52 CPU minutes were devoted to the
statistical computations, namely computing the maximum
likelihood estimates of 0 and the statistics associated with the
(empirical
Bayes)
posterior
distributions, including the
interestingnessmeasures A, ~..05,EXCESS, and EXCESS2.

Minimum support size. A common rule of thumb for specifying
the minimum support size is to use 0.1% of the number of
transactions. Since each month has approximately 7 million
transactions, this would limit the analysis to item sets with
frequencies of 7,000 or greater. Applying this rule to item sets
of size one (i.e., single countries) would eliminate over half
(120 of 228) of the countries from the analysis. For applications
in fraud detection and targeted marketing, this is dearly far too
large.




Figure 1. Shading of Country pairs based on raw counts n.




'e




Figure 2. Shading of Country pairs based on empirical
Bayes lower limit Z.o5

Figure 1 displays the support for item sets of size 2 (i.e., country
pairs). The layout of the countries along the rows and columns
roughly corresponds to geography. It was determined by a
cluster analysis using an inter-countrydistance matrix based on
an average A over the three months. We employ a gray scale to
denote countrypairs satisfyingminimumsupport at four levels:

white: n < 10
light gray: 10 < n < 100

dark gray: 100 < n < 1000
black: 1000 < n




71

Only 1,633 of the 25,878 (=228*227/2) possible country pairs
have support n >1000. The situation gets more extreme as the
size of the item sets inareases. Lowering the minimum support
requirement to n*--4 leads to 66% of the country pairs being
eligible for the subsequent analysis.

Shrinkage. Our proposed analysis focuses on estimates of the
ratios of observed to baseline cell frequencies.
Figure 2
displays the conservative estimate ~..05averaged over the three
successive months. The gray scale corresponds to increasing
~,.05, with lightest gray corresponding to 2< X.05< 4 and the
darkest to L.05> 16. White locations have either n < 4 or ~..05<
2. Just over 8% of the country pairs are shaded; this means that
in these cells, our lower confidence limit has at least twice as
many accounts calling this pair of countries than we would
expect
under
the
assumption
that
countries
are
called
independently of each other. An interesting aspect of Figure 2
is that ~,.05is greatest for country pairs in the same part of the
world -- this is intuitivelyreasonable.




gabon.mrtna.sengl(11 )
gabon.ivcst.mali(13)
ivcst.mali.togo(14)
gnbsu.niue.stome(17)
cpvde.gnbsu.prtgl(18)
amrma.samoa.tonga(21 )
fiji.samoa.tonga(23)
djbti.kenya.somla (25)
kyrgz, russa, uzbek(28)
benin.ghana.togo(33)
ivcst.libra.sileo(38)
ghana.ivcst.togo(39)
albna.mnaco,serba(45) !
ghana.guina.sileo(55)
armna.azrbj.russa(63)
marsh.mcrsa.palau(73)
ertra.ethpa.sudan(89)
fiji.newzd.tonga(123)
gmbia.guina.sileo(1 72)
amrma.newzd.samoa(250)
dmnca.grnda.mntrt(746)
1000
2000
5000
.... 1
I
I
I

L
M
L
M
L
M
L
M
L
M
L
M
L
M
L
M --~---R ....
Estimates
of Lamda

10000
30000
100000
300000
1000000
I
I
I
I
I
I
I
I
I

R-



R'

Ft




L--M~4a ....
L M------R------
L M ~----la-----
L--M-FI-----
L--M--R----.
L M--R----
-L-MR---
LM
LM ---R---
L-IVI-R--.
LM
LM-R-.
. . . .
p~ . . . .

. . . .
F! . . . .




--la--
Ft-




I
I
I
I
I
I
I
I
I
I
I
I
I
1000
2000
5000
10000
30000
100000
300000
1000000

M: Post.Geom.Mean
L: Lower.05.Limit
---R---: Observed R with 99.9% Classical Conf. Int.

Figure 3. Shrinkage for some month 2 triples having very large ~..osand a wide range of n (in parentheses)

Figure 3 displays a comparison of the raw value of R = n/e with
the empirical Bayes estimates ~..05and A for some of the month
2 triples that have very large ~,.05. The triple of countries and
the observed n of the triple are listed on the vertical axis. For
each triple, the graph shows a classical 99.9% confidence
interval for ~., based on the assumption that n is distributed
Poisson(~.e), as well as the two shrinkage estimators.
The
triples are ordered in increasing n from top to bottom of the
graph, making it easy to see that for small n the shrinkage is
more drastic than for larger n. One exception to this rule is the
triple "marsh.mcrsa.palau" having (n = 73, e = 0.0002), which
exhibits drastic shrinkage even though n is fairly large.
This
happens because the Bayesian estimate tends to shrink large R
when e is very small, even ifn is moderately large.

One way to ascertain the significance of individual measures
when looking at a large collection is to use a quantile-quantile
plot. This technique is standard in the statistics community as it
allows one to visually calibrate a collection of measures that are
all drawn from a common distribution. The intuition is that any
sample drawn from a common distribution will have a smallest
and a largest value -- the question is "How big is big?". The
quantile-quantile plot attempts to answer this
question by
displaying the quantiles (e.g., ordered values) of the observed
measure against the quantiles of the assumed distribution of the
measure. If the assumed distribution is correct, apart from
random variation, the configuration of points should follow a
straight line. Systematic departures from linearity indicate that
the assumed distribution is not correct -- common anomalies
being systematic curvature or outliers at the extremes.

Figure 4 is an example of a "standard normal quantile" plot, so-
called because the assumed distribution is a normal (i.e.,
Gaussian) distribution with mean zero and unit standard
deviation. In it we display the ordered values of the standardized
change in the A measure (also referred to as "EBGM") from
month one to month two.
(Because our methods yield a
posterior variance for every estimated log ~,, we can compute
approximately standard normal test statistics tbr comparing the
ratios of corresponding ~,s from one month to the next.) Mild




72

systematic curvature suggests that the distribution of this
measure is not Gaussian. The more relevant feature for this
discussion however is the suggestion of a few outliers (labeled
in the figure). We can provide a partial interpretation for the
two extreme country pairs (St Kitts, Vanuatu) and (Niue,
Vanuatu). All three countries are islands that host "adult
entertainment" telecommunication services. The providers of
these services earn commissions from the state-owned phone
company for the traffic that they attract. In the case of (Niue,
Vanuatu) there appears to have been a drop-off in the relative
rate of accounts calling both of these countries in month two.
The opposite is true for (St Kitts, Vanuatu) where there appears
to be a surge in the relative rate of accounts that are callingboth
of these geographically dispersed islands. Indeed, callers are
often referred to other "hot lines" and callers unaware of
international dialing codes have little idea on where the calls
terminate -- until they receive their bill. This analysis provided
us with our first indication that the adult entertainmentbusiness
model is making a significantentry into the Caribbean area.




03
W
II



LU




"5
0
_=

8
stkit.vantu
o




o
cnada.stkit

o niue.vantu
I
I
I
I
I

-2
0
2
4

Quantiles
of a Standard Normal


Figure 4. QQ plot for test statistics comparing each pair's A
from month 1 to month 2.

Multi-item versus pairwise association. The interpretation of
"interesting" large item sets can be confusing since it is often
unclear whether the item set is interesting because it contains
all the items, or if it is interesting because it consists of
interesting subsets of items. Our models allow these alternatives
to be compared by fitting all-two-factor log-linear models and
comparing the fits to the shrinkage estimates.

Figure 5 displays the different measures of excess frequency of
4-tuples that result depending on whether or not we adjust for
pairwise association. The figure displays the excess frequency
for item sets of size four depending on whether comparison is to
a complete independence model (abscissa = EXCESS) or the
all-two-factor model (ordinate = EXCESS2). Figure 5 includes
all 7,718 4-tuples in month 1 where both EXCESS and
EXCESS2 are nonnegative. Several of the points in Figure 5
have been identified with their country names and the observed
n. We comment on a few of them:

(Austria, Germany, Mexico, UK):This 4-tuple is unusual in
that the independence model suggests that this item set is
not so interesting (EXCESS almost 0), but when the
pairwise
interactions
are
taken
into
account,
the
interestingness of the 4-tuple is increased (EXCESS2 >
600). Since some of the pairwise associations are negative,
the all-two-factor model predicts that this 4-tuple would
occur somewhat less than independencepredicts.

(Australia, Canada, India, UK): This 4-tuple has about
3600 more accounts calling them than independence
predicts, but when pairwise interactions are taken into
account, this 4-tuple has just about the frequency that we
would expect.

(China, Hong Kong, Mexico, Taiwan): The 1585 accounts
calling these four countries are far in excess of what either
the independence model or the all-two-factor
model
predicts.

6. DISCUSSION
We introduced three variations on the market basket problem
that are drawn from statistical considerations. First we
introduced stratification of transactions by features that are
known to correlate with item sets. Second we dramatically
reduced the recommended minimal support size for item sets by
introducing an empirical Bayes model that effectively takes into
account variation associated with small frequencies. And finally
we built on earlier work that considers interestingnessmeasures
that assess departures of observed frequencies from baseline
frequencies. For item sets of size two, the independence model
provides a natural baseline, but we argue that in larger item sets
interpretations are sometimes aided by comparisons with fitted
frequencies from other models, especially the log-linear model
specifying "alltwo-factor interactions. In every case, our methods
provide for a reliable ranking of item sets by "interestingness."
Given an interesting item set, our methods do not explicitly
choose a predictive association rule. We prefer to let a domain
expert make judgments about potential cause and effect or
directional association.
For example, our discovery that adult
entertainment lines had spread to St. Kitts in the Caribbean was
easy to verify once the outlier in Figure 4 was presented to
someone familiar with the situation in Vanuatu.

We deliberately did not focus on algorithmic issues. In our
experience the computing time to enumerate item sets and
compute interestingness measures is negligible compared to the
hours of analysis time spent perusing the voluminousoutput. We
have developed and continue to develop visualization techniques
to aid this phase of the problem. The figures in this paper are
examples of some of the static displays that we feel are valuable.
Yet the real power of visualizationlies in interactive approaches,
especially in high dimensions (i.e., large item sets). We are
currently exploring such techniques and hope to report on this
research at KDD2002.




73

0




0




U.




O~
(O
(0
0
0
x
UJ
0
O-




0
O-
cO




~0




0_




0
O-
CJ




O-
o china.hngng.mexco.taiwn: 1585

o enada.grmny.india.uk:4760

ocnada.elsdr.gtmla.mexco:947



oastra.grrnnybmexco.uk: 1009

0


0

0
0

o
o°
o
(9
o

°°~b
0
0
0

o~o~ Oo°
o
o


~
o
o
~ o ~ O
o
0
ochina.cnada.japan.taiwn:1964




cnada.pkstn.sarab.uk:3550 o




astla.cnada.india.uk:4812
o


I
I
I
I
0
1000
2000
3000

Excess Over Independence

Figure 5. Excess vs. Excess2 for all 4-tuples where both are nonnegative (observed n listed after ":").



7. REFERENCES
[1] Aggarwal CC, Yu PS (1998) A new framework for item set
generation. Proc. of ACM-PODS Symposium on Principles
of Database Systems, Seattle, WA, pp. 18-24.

[2] Agrawal R, Imilienski T, SwamiA (1993) Mining
association rules between sets of items in large databases.
Proc. ACM SIGMOD Intl. Conf. On Mgnt. of Data, pp.
207-216.

[3] Agrawal R, Srikant S (1994) Fast algorithms for mining
association rules. In Proc. 20th VLDB Conf Santiago, Chile.

[4] Agresti A (1990) Categorical Data Analysis. New York:
John Wiley.

[5] BishopYMM, Fienberg SE, Holland PW (1975) Discrete
Multivariate Analysis Cambridge, MA: MIT Press.

[6] Brin S, Motwani R, Ullman JD, Tsur S (1997) Dynamic
item set counting and implication rules for market basket
data. Proc. ACM SIGMOD 1997 Intl. Conf. on Mgnt. of
Data, pp. 255-264.

[7] BryckA, Randenbush S (1992) Hierarchical Linear
Models. Newbury Park, CA: Sage Publications.

[8] DuMouchel W (1999) Bayesian data mining in large
frequency tables, with an application to the FDA
Spontaneous Reporting System (with discussion), The
American Statistician, 53:177-202.
[9] DuMouchel W, Friedman C, Hripcsak G, Johnson S,
Clayton P (1996) Two applications of statistical modeling to
natm'allanguage processing. AI and Statistics V, ch. 39,
edited by D. Fisher and H. Lenz,Springer-Verlag.

[10] DuMouchel W, Volinsky C, Johnson T, Cortes C, Pregibon
D (1999) Squashing flat files flatter, Proc. KDD 1999,
ACM Press, San Diego, CA, p. 6-15.

[11] Johnson N, Kotz S (1969) Discrete Distributions. Houghton
Mifflin, now distributed by New York: John Wiley.

[12] Mantel N, Haenszel W (1959) Statistical aspects of the
analysis of data fromretrospective studies of disease. J.
Natl. Cancer Inst. 22" 719-748.

[13] O'Hagan A (1994) Kendall's Advanced Theory of Statistics,
vol. 2, Bayesian Inference. New York: Halstead Press (John
Wiley).

[14] Silverstein C, Brin S, Motwani R (1998) Beyondmarket
baskets: generalizing association rules to dependence rules.
Data Mining and Knowledge Discovery 2: 39-68.

[15] Simpson EH (1951) The interpretation of interaction in
contingencytables. J. Royal ,Statistical Soc., B13:238-241.

8. APPENDIX: EMPIRICAL BAYES
SHRINKAGE ESTIMATES
This development follows that of DuMouchel [8]. Rather than
treat the separate values of ~. as unrelated constants, assume that
each ~, is a random variable drawn from a common prior




74

distribution.
This distribution is assumed be a mixture of two
gamma
distributions.
The
density function
of a gamma

distribution, having mean = cff13and variance = ct/132, is

gO~; co, 13) = 13a~ct-1 e-13~./ F(a)

The prior probability density of ~. is assumed to be

n(~,; Ct1, 131'cc2' 132,P) = P g(X; COl,~1) +

(1 -p) g(~.; tz2, 132)
(A1)

Therefore the prior mean of X under this mixture model is
p~1/131 + (1 -p) t~2/132, and its prior variance is p(1 -p)(t~l/131

- ty.2/132)2 + ptztl/1312 + (1 - p) ~2/1322. The exact choice of prior

distribution for ~, is not so important as that it have several free
parameters so that the distribution of ~, can be fit using observed
data.
The
density
in
(1)
has
five
free
parameters:
0 = (ct1, 131'ct2, 132, P)"
The distributions for pairs, triples,

quadruples, etc. have separate values of 0. The family of gamma
distributions are often used to model populations of Poisson rates
because of the conjugate relationship between the Poisson and
gamma
distributions
([7][11][13]).
The
calculations
are
simplified in two respects: first, the marginal distribution of each
n is a mixture of negative binomial distributions; and second, the
posterior distribution of each 2 is a mixture of two gamma
distributions with modified parameters.
Assuming that 0 and e
are known, then the distribution of n is

Prob(n)= pf(n;cq,131, e) + (1-p)f(n;ct2,132, e),
(A2)

f(n; ct, 13,e) = (1 + 13/e)-n(1 + e/13)-ct r(a + n) [ r(o0 n!

Let Qn be the posterior probability that ~, came from the first
component of the mixture, given n.
From Bayes rule, the
formula for Qn is

an =P f(n; al, 131'e) / [/9f(n; c~1, 131,e) +
(1 -p) f(n; (22, 132,e)]
(A3)

The posterior distribution of ~,, after observing n, can be
represented as

~, [ n ~ rc()~; 0t1 + n, 131 + e, 0~2 + n, 132 + e, Qn)
(A4)

where g( ) is given by (AI).
Using well-known properties of
gamma distributions, the posterior expectations of ~, and log(~,)
are given by

E[X I n] = Qn (al + n)/(131 + e) +

(1 - Qn) (a2 + n)/(132 + e)
(A5)

E[log(),) I n] = Qn [~(al + n) - 1og(131 + e)] +
(1 - an) lilt(Or2+ n) - log(132 + e)]
(A6)

where
u)(x)
is
the
digamma
function,
the
derivative of
log[F(x)].The posterior geometric mean (denoted EBGM in [8]
and denoted A in this paper), of ~. is then the exponential of
(A6):

A = exp(Qn [~(t~1 + n) - 1og(131 + e)] +

(1 - Qn) [xg(ct2 + n) - log(132 + e)])
(AV)
This provides a "best" point estimate of each ratio ~,, in the sense
of minimizing squared error loss in log(k). On the other hand, if
a conservative estimate of ~ is desired, in which the risk of
overestimating every ~, is constrained to be just 5%, then we can
compute the 5~ percentile of the posterior distribution of each X,
denoted ~,.0s, as the solution to the equation:


0.05= J0 ~r(;4;aq+n'pl+e'cr2+n'flz+e'Q,)dA
(A8)

where n( ) is given by (AI).
The integral in (A8) is easily
computed
using
standard
computer
approximations
of the
incomplete gamma function, while the solution ~..0srequires an
iterative technique such as Newton's method.

If e and n are both large, then A and ~..05will both approach R =
n/e, as can be seen from (A6), noting that for large arguments,
~t(x) ~ log(x), and log(or + n) - 1og(13+ e) will approach log(n/e).
However, when e or n are not large, then the effect of using A is
to "shrink" R toward smaller values, and the shrinkage is even
greater for ~,.0s. This is exactly the desired effect when sampling
variation makes the true degree of association between the items
in the item set uncertain.

To evaluate (AV) or (A8), estimates of 0 = (o~1, 131,cz2, 132,P) are

obtained by considering the marginal distribution of the set of all
n for a given item set size, which is given in (A2). The negative
binomial 'distributions f( ) in (A2) are derived as a mixture of
Poisson distributions, where the Poisson means have a gamma
distribution [i 1, p. 125].
The likelihood function for 0 is the
product of mixtures of two negative binomial densities:

L(0) = H{p f(n; ~l, 131,e) + (1 -p) f(n; c¢2, 132,e)}
(A9)

The maximum likelihood estimate of 0 is the vector that
maximizes (A9), where the product in (A9) is over all possible
item sets of the given size. The maximization involves an
iterative search in the five dimensional parameter space, where
each iteration involves computing log[L(0)] and its first- and
second-derivatives.
Since log[L(0)] would be the sum of an
astronomical number of terms if we included all possible item
sets (e.g., including item sets that were not observed in the
database and have n = 0), we use two modifications of (A9) to
allow the procedure to scale up to large values of K, the number
of unique items. First, we modify the likelihood to condition on
n > n*, where n* is the minimum support that we wish to
consider for item sets of the given size. This allows us to merely
tabulate and include in the likelihood item sets having n > n*,
but, to properly accommodate this, we must replace f(n; ~x, 13, e)
in (A9) by the conditional density of n, given that n _>n*, namely

n*-I
f.(n;ot, fl, e,n*) =f(n;cr, fl, e)l[1- ~'~f(m;c~,fl, e)]
(AI0)
m=0

Even choosing n* = 1 saves an enormous amount of computation
compared to enumerating all possible item sets and computing
the corresponding values of e and f().
As mentioned in the
previous
sections,
we
are focusing on
applications where
moderate values of n are of interest and so we take n* to be at
least 1 but as small as computational resources allow, ff n* is as
large as several hnndred, the noise in the counts for n >_n* will




75

typically be so little that the gain from computing A or ~,.05may
be small.

In order to further reduce the computational burden of the
iterative search to maximize L(0), we further reduce the number
of terms in (A9) by the technique of data squashing [10]. The
collection of all item set descriptors (n, e), where n > n*, is
replaced by a smaller set of M triples (ni, el, wi), i = l ..... M.
For each unique observed value of n, a set of values of (hi = n, ei,
wi) are chosen so that the distribution of the ei weighted by the
corresponding wi mimics the distribution of the original values of
e for that value of n. (A simple version of data squashing is to
partition the values of e into bins for each unique value of n and
replace the set of (n, e) in each bin by the single triple (hi = n, e~,
wi), where ei is the bin mean and wi is the bin count of the ith
bin.)
This reduces the expression tbr L(0) to just M terms,
namely:

M
,
:#
w.
L(8) = I-Ii=l[pf. (ni;aft, flt,ei,n ) + (1- p)f. (ni ;t~2,fl2.ei, n )] '

In our experience where M is a few thousand, the maximization
typically takes between 5 and 15 iterations from the starting
point 0o = (cxI = .1, [~1= .1, ix2= 10, [32= 10, p =p,,uu), where

p,,~ is the value from among (0.2, 0.5, 0.8) having the largest
value of L(00). [These starting values are used for item pairs, J =
2. Larger values of J tend to lead to more dispersed distributions
of ~,, so we divide the above starting values of the ¢¢s and 13sby
3J-2.]
This experience also indicates that the computational
effort of tabulating all the item sets having n > n*, which uses an
algorithm similar to the Apriori algorithm [2][3] involving one
pass over the transaction database for each item set size, is a
much greater computational burden than the empirical Bayes
calculations described in this Appendix and the log-linear model
calculations discussed
in Section 4.
Thus there is no
computation-related reason not to compute A, ~,.05,EXCESS and
EXCESS2 routinely.

Table 1 shows the estimates and standard errors of the hyper-
parameters 0 = (cq, [31,¢x2,152,p) for each month and for the item
set sizes 2-5.
Standard errors are based on the second
derivatives of the log likelihood function. It can be seen that the
parameter values are quite consistent across databases, but that
the values of ft and 6"5tend to decrease as the size of the item set
increases. This indicates that the distribution of ~,becomes more
and more dispersed as the dimension goes up, with more outlying
item sets having very large frequencies compared to the baseline
frequency based on the independence assumption.
This also
shows that the optimum amount of shrinkage varies depending
on item set size.

We have not performed extensive goodness of fit analyses to the
prior distribution family used here.
One approach is to fit
different strata separately and see if they lead to similar sets of
hyper-parameters.
If not, it is easy to combine results alter
separate fits to different subsets of the database or to different
subsets of item sets. Classical hypothesis tests for goodness of
fit to the observed distribution of (n, e) are possible but may not
be useful, since massive data sets are bound to yield small "p-
values" for rejecting almost any null hypothesis, even though the
computed shrinkage estimates perform well as interestingness
measures.


Table 1. Hyper-parameter estimates and standard errors for the three months for dimensions 2 through 5.
Month Dim Parameter Aiphal
Betal
p
Aipha2
Beta2
l-p

Oct
2


Nov
2


Dec
2


Oct


Nov


Dec


Oct


Nov


Dec


Oct


Nov


Dec
4

4

4
Estimate
0.
St Error
0.
Estimate
0.
St Error
0.
Estimate
0.
St Error
0.
1834
0.03935
01574
0.001915
1845
0.03962
01594
0.001933
1917
0.04174
01643
0.002039
0 2067
0 005411
0 2068
0 005448
0 2051
0 005536
2.843
4.275
0.7933
0.06065
0.1051
0.005411
2.857
4.219
0.7932
0.06153
0.1047
0.005448
2.792
4.109
0.7949
0.06052
0.1027
0.005536

Estimate
St Error
Estimate
St Error
Estimate
St Error
Estimate
St Error
Estimate
St Error
Estimate
St Error
0.001012
0.004768
0.000193
0.000111
0.001062
0.005027
0.0002021
0.000118
0.001147
0.005523
0.000218
0.0001328
0.000688
0.001037
0.000134
0.000026
0.0008258
0.001493
0.0001558
0.000036
0.0008782
0.001534
0.0001669
0.000040
0 1806
0 001951
0.1803
0.001978
0.1787
0.002002
0.1216
0.001316
0.127
0.001396
0.1218
0.001397
1 061
0 0135
1 059
0 0136
1 082
0 01381
0 02398
0 003049
0 03096
0 003939
0 02563
0 003259
2.075
0.8194
0.02612
0.001951
2.044
0.8197
0.02595
0.001978
2.076
0.8213
0.02616
0.002002
0.8088
0.8784
0.007799
0.001316
0.8202
0.873
0.008709
0.001396
0.7966
0.8782
0.007906
0 001397

Estimate
St Error
Estimate
St Error
Estimate
St Error
0.000258
0.000291
0.1164
0.000075
0.000026
0.002687
0.001313
0.000971
0.1307
0.0003172
0.000069
0.002942
0.00174
0.001474
0.1356
0.0004084
0.00~i061
0.003155
0 02204
0 002775
0 02604
0 003313
0 02851
0 003656
227
02236
309
02427
322
02539
0 8836
0 002687
0 8693
0 002942
0 8644
0 003155




76

