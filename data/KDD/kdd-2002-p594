Collusion in The U.S. Crop Insurance Program:
Applied Data Mining

Bertis B. Little, Walter L. Johnston, Jr., Ashley C. Lovell,
Roderick M. Rejesus, and Steve A. Steed

Center for Agribusiness Excellence
Tarleton State University
Stephenville, Texas
(254) 918-7676
cae@tarleton.edu


ABSTRACT
This paper quantitativelyanalyzes indicators of Agent
(policy seller), Adjuster (indemnityclaim adjuster), Producer
(policy purchaser/holder) indemnity behavior suggestive of
collusion in the United States Department of Agriculture
(USDA) Risk Management Agency (RMA) national crop
insurance program. According to guidance from the federal
law and using six indicator variables of indemnitybehavior,
those entities equal to or exceeding 150% of the county mean
(computed using a simplejackknife procedure) on all entity-
relevant indicators were flagged as "anomalous." Log linear
analysis was used to test (I) hierarchical node-node
arrangements and (2) a non-reeursivemodel of node
information sharing. Chi-square distributed deviance
statistic identified the optimal log linear model. The results
of the applied data mining technique used here suggest that
the non-recursivetriplet and Agent-producer doublet
collusionprobabilisticallyaccounts for the greatest
proportion of waste, fraud, and abuse in the federal crop
insurance program. Triplet and Agent-producer doublets
need detailed investigationfor possible collusion. Hence,
this data mining technique provided a high level of
confidence when 24 million records were quantitatively
analyzed for possible fraud, waste, or other abuse of the crop
insurance program administered by the USDA RMA, and
suspect entities reported to USDA. This data mining
technique can be applied where vast amounts of data are
available to detect patterns of collusion or conspiracy as may
be of interest to the criminaljustice or intelligenceagencies.

Categories and Subject Descriptors
1.5.4 [Pattern Recognition]: Applications

General Terms
Management, Measurement


Permissionto make digital or hard copies of all.or part of this work for
personalor classroomuse is grantedwithoutfee providedthat copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requiresprior specificpermissionand/ora fee.
SIGKDD '02, July 23-26,2002,Edmonton,Alberta,Canada.
Copyright 2002 ACM 1-58113-567-X/02/0007...$5.00.
Keywords:
Log linear models, non-recursive, triplets, doublets,
collusion, insurance fraud


1. INTRODUCTION
The U.S. Congress is concerned that the United States
Department of Agriculture (USDA) Risk Management
Agency (RMA) crop insuranceprograms are vulnerable to
waste, fraud, and abuse. Accordingly, the Agriculture Risk
Protection Act (ARPA) of 2000 was passed. In that
legislationit was stated that "data warehouse and data
mining technologies should be employed to improve
program compliance and integrity." The first action
mandated by ARPA, following construction of a data
warehouse, was to identify crop insurance Agents (sellers of
policies) and Adjusters (crop insurance indemnity claims
adjusters) whose business activities were anomalous. The
law further mandated that those to be considered anomalous
were > 150% of the county area mean losses. The ARPA of
2000, Subtitle B, Section 121, (if)(1) (A) & (B) directs RMA,
or its designees, to detect disparate performance of Agents
and Adjusters at the 150% of the mean for all loss claims in
the same area.

During the process of engineeringthe data warehouse and
conducting preliminary data mining, our research team was
briefed by USDA RMA Compliance investigators. These
investigators indicated that many of the cases they had
successfully investigated were configured as a classic
conspiracy. The investigators characterized the conspiracy
with a 'cart wheel' metaphor. The investigators'
observations subjectively indicated that crop insurance
conspiracies were founded on the principle of linked actions
from a central group of conspirators (cart wheel hub) through
a spiraling network of conspiracy operatives (spokes in the
wheel) relayed for action to many performing players (the
rim). RMA Compliance Investigators further observed that
Agents were probably the 'hub' of the cart wheel. Adjusters
were probably the' spokes' of the cart wheel, connecting
Agents and Producers in the conspiracy model of crop
insurance waste, fraud, and abuse. Finally, Producers were
posited to be the 'rim' of the cart wheel, completing the




594

common model of conspiracy in which collusion of few is
connected to actions of many by some intermediary.

Typically, investigationof conspiracy originates from a
qualitative analysis of individual instances. Such efforts are
time consuming and identifying instances of fraud rely
largely upon luck rather than objective analyses. In the
current analysis, data mining was applied to the USDA RMA
'national book of insurance business' for the year 2000 to
quantify conspiracy investigation. The working hypothesis
was the 'cart wheel metaphor' of linked anomalous business
transactions at the Agent, Adjuster, and Producer levels was
performed seeking evidence for linked behaviors suggestive
of collusion.


1.1 Data Mining And Insurance Fraud:
A Review
The use of data mining tools and techniques for insurance
fraud detection has been clearly documented in the literature.
Yeo [1], for example, documented its usefulness when he
described a U.S. health insurance company using data mining
techniques to compare an individualdoctor's claims against a
larger historical base of data. The analysis identified several
geographical areas where claims exceed the norm. The
insurance firm investigated and confirmed that a physician in
an area was submitting false bills. As a result, the doctor was
forced to pay restitution and fines. Data mining saved this
health insurance company as much as $4 million.

Grossman et al [2] have indicated that fraud detection is one
of the areas where data mining is considered a "successful"
tool. He et al [3]. showed how data mining through neural
networks could be used to detect medical fraud. Several
recent publications have documented the usefulness of data
mining techniques for fraud detection in health and
automobile insurance claims.[1,4-9] Most ofthese papers
mention the large amount of potential savings in human
investigative resources and indemnitypayments when using
data mining tools.

The literature indicates that there are two main data mining
techniques used in insurance fraud detection. A predominant
data mining technique used in the detection of fraudulent
claims are straightforward statistical techniques. Weisberg
and Derrig [9] have used linear regression models to analyze
the statistical significance of fraud indicators in bodily injury
claims in Massachusetts. Other investigators developed a
model and an expert system that can detect potentially
fraudulent claims based on a probit model using a number of
fraud indicators. [10,11]

The second main data mining technique used in insurance
fraud detection is clustering algorithms. Derrig and
Oztaszewski[12] used fuzzy set theories to classify and
identify characteristics of fraudulent claims. They found that
fuzzy techniques could efficiently classify claims based on
the suspected fraudulent content. Brockett, Xia, and Derrig
[13], on the other hand, apply neural networks and back-
propagation algorithms to cluster claims based on the degree
of fraud suspicion. They demonstrated that this data mining
technique performs better than the combinationof an
insurance adjuster's fraud assessment and an insurance
investigator's fraud assessment with respect to consistency
and reliability.

In summary, data mining techniques have been shown to be a
methodology capable of helping detect insurance fraud in
large databases. Thus, given its wide use in other insurance
markets, it has potential as a tool to detect fraud, waste, and
abuse in the U.S. crop insurance market where it has not
been widely used. The detection of fraud, waste, and abuse in
the U.S. crop insurance market is more important and
challenging today because of the enormous growth and
changes in the federal crop insurance program over the past
few years. Therefore, data mining techniques can be seen as
an efficient tool that can identify anomalous behavior of
farmers participating in the crop insuranceprogram, which
may be worthy of further human investigation. This paper
demonstrates how an applied data mining technique could be
used in analyzing anomalous behavior suggestive of
collusion in the crop insurance industry.


2. METHODOLOGY
2.1 Materials
A data warehouse was engineered and populated with RMA
data for the decade from reinsurance years 1991 to 2000
using the Teradata System by NCR. In this pilot
investigationwe analyzed data from the year 2000 only.
There were 157,180,000 acres insured under 1.002 million
producers' policies, sold by 13,434 crop insurance Agents
with $2.49 billion in indemnities adjusted by 3,842 adjusters.
Total liabilitywas $27.17 billion, and the total premium was
$2.26 billion. This represented the entire book of business
for the USDA Risk Management Agency in year 2000. CAT
(catastrophic) policy indemnitieswere excluded because they
would bias the analysis
2.2 Hypotheses
The null hypothesis that no linkages existed between Agent,
adjuster, and producer nodes was tested using a log linear
model.J14-16] The first three alternative hypotheses tested
classic conspiracy linkages with node doublets (Agent-
Adjuster-Producer, Agent-Producer-Adjuster, Adjuster-
Agent-Producer) linked. The Agent-Adjuster-Producer
represents the 'cart wheel' hypothesis discussed in the
Introduction.
In the fourth alternative, we tested the
hypothesis that all three nodes were linked to one another
non-recursively. [15]
2.3 Methods
Flags that indicated unusual behavior in crop insurance claim
premium, indemnity, and liabilitywere computed using six
derived measures indicating loss (Table 1). In accordance
with the ARPA, county means were computed using the
generalizedjackknife procedure. [17,18] The generalized
jackknife is used for the purpose of reducing bias in
estimating means and confidence intervals, and its reliability
is widely demonstrated. [19,20]

Agents, Adjusters, and Producers who were > 150% of the
county mean for a loss indicator were flagged on the derived
measures (flagged = 1, not flagged = 0). Only entities
(Agent, Adjuster, Producer) that were flagged on all indicator
variables were classified as anomalous (Table 2). Data




595

manipulationswere performed in SAS (SAS Institute, Cary,
NC). The statistical program R was used to perform the log
linear analysis. [21]

Log linear analysis is appropriate to this research problem
because it provides a flexible mechanism for the analysis of
Poisson distributed count data (i.e., number flagged vs.
number not flagged). The name log linear comes from the

Table 1. Derived Measures (Means Calculated by
Jackknife Method within County)

Derived Measure
Ratio 1 = $ Indemnity/ $ Premium



Ratio 2 = $ Indemnity/ $ Liability



Ratio 3 = # Loss Policies / Total # Sold Agent
Ratio 4 = # Loss Units / Total # Units Ins.
Ratio 5 = $ Adjusteri / $ County Indemnity
Ratio 6 = # Claims for Adjusteri /
Total County Claims
Level
Adjuster,
Agent,
Producer
Adjuster,
Agent,
Producer

Agent
Adjuster


Adjuster



use of a logarithmic transform to convert a multiplicative
model into a linear one. For example, the standard null
hypothesis in Pearson's g2test for a 2 dimensional
contingencytable tests for independence of row and column
classifications and is given as:

Ho: P(r l, c j)
=
P(r I) * P(c j).

Taking the log of H0 gives:

H0: log{P(r I, c j)} = log{P(r I)} + log{P(e j)}

Hence the label log linear.

The tnJe power of the technique is the ability to apply
standard linear modeling techniques such as continuous as
well as discrete covariates and the ability to specify a more
complex model using a combinationof discrete covariates,
continuous covariates, and interactions between covafiates.
The log linear model results presented in this paper were
performed using the glm0 function of R to solve the
Generalized Linear Model specified and the resulting
measure of model adequacy, or goodness of fit, is the
Deviance which is a -2 log likelihood measure and,
therefore, Z2 distributed. A detailed explanation of
Generalized Linear Models is published. [16,22] Pearson
correlation coefficient between entity pairings is the the
correlation based upon data for both variables grouped in
classes or categories (i.e., "flagged" vs. "not flagged")


3. RESULTS
Log linear analysis of anomalous entities (Agents, Adjusters,
and Producers) was employed to test various hypotheses of
paths of association, not causation. Table 3 (a) is the model
suggested by investigators who work in the crop insurance
program, Agent-Adjuster-Producer. It was a statistically
significantmodel for the observations. A model that is a
statistically'better fit is the Agent-Producer-Adjusterlinkage
(Table 3 (b)). However, of the three hierarchical models
possible in the classic conspiracy analysis, Adjuster-Agent-
Producer is the optimal statistical fit (Best Fit) for the data.
Thus, the associations shown are not precisely those
suggested by conspiracy investigators who work in the crop
insurance program, but do follow the classically described

Table 2. Frequencies of Agents, Adjusters, and Producers
with Positive Indemnities



Entity
Anomalous
Not Anomalous
Total
Producers
33,630
143,245
176,875
Agents
4,203
9,231
13,434
Adjusters 1,128
2,716
3,844

Doublets
Agent-ProdLucer
7,971
174,259
182,230
Agent-Adjuster
1,067
29,840
30,907
Adjuster-producer
2,636
196,492
199,128

Triplet
Adjuster-Agent-Producer
1,135
199,713
200,848

Total number of policies = 1,002,409; Percent with
Indemnity = 26.8% (268,158/1,002,409)


conspiracy model. The path Adjuster-Agent-Producerwas
the log linear best fit for the hierarchical association model.

The best possible fit with the three entities, Adjuster-Agents-
Producers, was a non-recursivetriplet (Table 3 (d)). The
non-recursivetriplet indicates that there is not a true
hierarchy as in a classic conspiracy, but there is significant
sharing of informationamong all three entities. The
strongest relationship is between Producers and Agents in the
entire data set, whether or not they are anomalous (r = 0.20).
This indicates an R2of 4% from the correlation ofr = 0.20.
The best overall fit for the model was as shown in Table 3
(d). The second best fit was the Agent-Producer doublet.
The biserial correlation of all Agent-Producers was, of
course, at 0.20.

The best possible fit with the three entities, Adjuster-Agents-
Producers, was a non-recursivetriplet (Table 3 (d)). The
non-recursivetriplet indicates that there is not a true
hierarchy as in a classic conspiracy, but there is significant
sharing of information among all three entities. The
strongest relationship is between Producers and Agents in the
entire data set, whether or not they are anomalous (r = 0.20).
This indicates an R2of 4% from the correlation ofr = 0.20.
The best overall fit for the model was as shown in Table 3
(d). The second best fit was the Agent-Producer doublet.




596

The biserial correlation of all Agent-Producers was, of
course, at 0.20.

These findings are consistent with actual cases of collusion
that has been litigated.J23] In a particular case in the Texas
High Plains, the Agent was the central conspirator who
submitted inflated claims on behalf of the producers for
whom he wrote insurance. The producers involved
sometimes knew of the deception, sometimes they did not.

Table 3. Log Linear Analysis of Conspiracy Doublets
(Agent-Adjuster, Producer-Agent, Producer-Adjuster)
and Triplet

Model
Deviance
d....~
Null
484,957.7
7
Linked by an Intermediary
(a) Agent-Adjuster-Producer
7,501.9
2
(b) Agent-Producer-Adjuster
1,790.5
2
(c) Adjuster-Agent-Producer
1,230.1
2
Best Fit
All Linked-No Intermediaries
(d) Triplet
102.9
1
Best Fit
Doublets
(e) Adjuster-Producer
10,216.0
3
(0 Adjuster-Agent
9,655.5
3

(g) Agent - Producer
3,944.1
Best Fit

All results are significantat P < 0.0001. Statistic is Chi-
square distributed -2 Log Likelihood



These claims were then falsely verified by unscrupulous
adjusters who were paid by the collusive Agent. Therefore,
the pattern of collusion identified by our applied data mining
procedure is in line with actual fraud behavior that has been
observed in the past.

The geographic distributionof anomalous (a) Producer, (b)
Agent, and (e) Adjuster was also analyzed. This showed that
most flagged producers and Agents are located in the middle
of the country, while flagged adjusters are more
geographically dispersed. Similarly, geographic distribution
of flagged doublets revealed no obvious clustering of these
anomalous pairs, although majority of the flagged Agent-
Producer pairs are in the Midwest and the Great Plains
region. Finally, the flagged triplet (i.e., Adjuster-Agent-
Producer) indicated no apparent visual geographic clusters.
The absence of geographic clusters suggested that the
jackknife procedure had adequately normalized means
computed within counties.
4. DISCUSSION AND CONCLUSIONS
Linkage of the triplets and a specific doublet (Agent-
Producer) indicated need for detailed investigation for
possible collusion among two or more of the entities. This
provides a high level of confidence when screening cases for
investigation.

Table 4. Best Statistical Fit Models


Model
Rank
Deviance

Triplet (No intermediaries)
1
102.9
Adjuster-Agent-Producer (Hierarchical)
2
1,230.1
Agent-Producer (Doublet)
3
3,994.1



The next step in the screening process for investigation in
this specific application of data mining is to further analyze
the Indemnity-Liability(I-L) ratio for the linked triplets,
hierarchical and non-recursive, and the Agent-Producer
doublet as shown in Table 4. In this example of using log
linear models to establish links (possible collusion) between
nodes, the number of probable entities for investigationwas
reduced from more than 200,000 to 1,135. This is with the
highest level of confidence afforded under these statistical
models. It is recommended that I-L ratios which approach
1.0 will be prioritized for investigation.

These results were constrained by the federal mandate to
analyze and report on 150% of the county mean. Further
constrainingthe model by more tightly defining the outlier
(e.g., those at or above the 95thor 99th percentile) in the
models described in Table 4 would greatly enhance yield
during investigation.

In summary, the practical use of these results by the USDA is
investigationof entities that are flagged by the analysis in the
Triplet (Table 4). The analyticalresults presented here are
unique because they demonstrate that collusion or conspiracy
can be investigated through automated techniques (i.e., data
mining). [24-28] The results further indicate that nearly any
type of data may be analyzed for evidence of conspiracy or
collusion because the actual data analyzed was not metric,
and had no statistical assumptions regarding adherence to a
specific distributionor other parameters. These data were
simply frequency counts of events. These "events" could be
counts of e-mails, words, combinations of words, business
transactions, or an entire litany of things that normally
require human intervention for analysis. When events
number in the hundreds of thousands, millions, billions, or
even trillions, the log linear technique of linkage can
automate the detection of linkages that may point to
collusion, conspiracy, or terroristic plots.




597

Bibliography

[1] Yeo, D. "Better Insuring With Information." Canadian
Underwriter. 67(August 2000): 46-47.

[2] Grossman, R., S. Kasif, R. Moore, D. Rocke, and J.
Ullman. 1999. Data Mining Research: Opportunities
and
Challenge. A report of three NSF workshops on
Mining Large, Massive and Distributed Data. In:
http://www.ncdm.uic.edu/dmr-v8-4-52.htm.

[3] He, H., J. Wang, W. Graco. and S. Hawkins.
"Application of Neural Networks to Detection of Medical
Fraud." Exp. Sys. with Applic. 13(1996): 329-336.

[4] Johnson, R.A. "Digging the Dirt." Best's Rev.: Property/
Casualty-Insurance-Edition. 98(1997): 108-110.

[5] Williams, G.J. and Z. Huang. "Mining the Knowledge
Mine. The Hotspots Methodology for Mining Large
Real World Databases." In: Sattar, A. (ed.), Advanced
Topics in Artificial Intelligence, Springer Verlag:
Berlin, Germany. p. 340-348. 1997.

[6] Manchur, D. "Mining for Fraud." Canadian Insurance.
103(Sept. 1998): 24-27.

[7] Panko, R. "Getting a Jump on Crime." Best's
Rev.(Property/Casualty). 100(Oct. 1999): 73-75.

[8] Fox, B.R. "Technology: The New Weapon in the War on
Insurance Fraud." Defense Counsel Journal, 67(April
2000): 237-244.

[9] Weisberg, H.I. and R.A. Derrig. "Quantitative Methods
for Detecting Fraudulent Automobile Bodily Insurance
Claims." A1B Cost Containment/Fraud Filing, pp. 49-
82, 1993.

[10] Belhadji, D.B. and G. Dionne. "Development of an
Expert System for the Automatic Detection of
Automobile Insurance Fraud." Working Paper 97-06,
Risk Management Chair, HEC-Montreal, 1997.

[11] Belhadji, D.B., G. Dionne, and F. Tarkhani. "A Model
for Detection of Insurance Fraud." The Geneva Papers
on Risk and lnsurance. 25(October 2000): 517-538.


[12] Derrig, R.A. and K.M. Ostazewski. "Fuzzy Techniques
of Pattern Recognition in Risk and Claim
Classification." TheJ. ofRisk and Insurance. 62(Sept.
1995): 447-482.

[13] Brockett, P.L., X. Xia, and R.A. Derrig. "Using
Kohonen's Self-Organizing Feature Map to Uncover
Automobile Bodily Injury Claims Fraud." TheJ. ofRisk
andlnsurance. 65(June 1998): 245-274.
[141



[15]



[16]


[17]




[181



[191



[20]



[21]



[22]



[231




[24]



[25]



[26]


[27]


[28]
Fienberg S.E. The Analysis of Cross-Classified
Categorical Data.The MIT Press: Cambridge, Mass.,
1977.

Gilbert:G.N. Modelling Society: An Introduction to
Loglinear Analysis for Social Researchers. George
Allen and Unwin: London, UK, 1981.

McCullagh P., and Nelder J.A. Generalized Linear
Models. 2ndEd. Chapman and Hall, 1983.

Davison A.C., and Hinkley D.V. Bootstrap Methods
and Their Applications. Cambridge University Press:
Cambridge, 1997.

Gray H.L., and Schucany W.R. The Generalized
Jackknife Statistic. Marcel Dekker, Inc.: New York,
1972.

Schucany W.R., Gray H.L., and Owen D.B. On bias
reduction in estimation. Journal of the American
Statistical Association 66: 524-533, 1971.

Tukey J.W. Bias and confidence in not quit large
samples. Annals of Mathematical Statistics 29: 614,
1958.

Ihaka R., and Gentleman R. R: A language for data
analysis and graphics. Journal of Computational and
Graphical Statistics 5:299-314, 1996.

Nelder J.A., and Wedderburn R.W.M. Generalized
linear models. Journal of the Royal Statistical Society
A 135:370-384, 1972.

Rose L.J., and Freivogel W.H. "Fertile for Fraud
Farmers Routinely Collect Federal Insurance and
Disaster Payments Prone to Fail." St. Louis Post
Dispatch. January 15, 1995. pp. 0lB.

Bay S.D., and Pazzani M.J.: Detecting group
differences: Mining contrast set. Data Mining and
Knowledge Discovery 5:213-246, 2001.

Cabena P., Hadjinian P., Stadler R., Verhees J., and
Zanasi A. Discovering Data Mining. Prentice Hall:
Upper Saddle River, NJ, 1998.

Westphal C., and Blaxton T. Data Mining Solutions.
Wiley: New York, 1998.

Weiss S.M., and IdurkhvaN. Predictive Data Mining.
Morgan Kaurman Publishers: San Francisco, 1998.

Witten I.H., Frank E. Data Mining: Practical Machine
Learning Tools and Techniques with Java
Implementations. Acadrnic Press: San Diego, 2000.




598

