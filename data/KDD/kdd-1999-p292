Applying
General
Bayesian
Techniques
to Improve
TAN
Induction

Jestis Cerquides '
Ubilab
UBS AG
Bahnhofstrasse 45
P.O. Box, CH-8098 Ziirich
Jesus.Cerquides@ubs.com


Abstract
Free Augmented Naive Bayes (TAN) has shown to be com-
petitive
with state-of-the-art
machine learning
algorithms
[3]. However, the TAN induction
algorithm
that appears in
[3] can be improved in several ways. In this paper we identify
three shortcomings
in it and introduce
two ideas to over-
come those problems:
the multinomial
sampling
approach
to learning bayesian networks and local bayesian model av-
eraging.
These ideas approaches
are generic and can thus
be reused to improve other learning algorithms.
We empiri-
cally test the new algorithms,
and conclude that in most of
the cases they lead to an improvement
in accuracy
in the
classification
and in the quality
of the probabilities
given as
predictions.


1
Introduction

Tree Augmented
Naive Bayes (TAN)
has shown to
be competitive
with state-of-the-art
machine learning
algorithms.
In this paper we analyze the TAN induction
method
proposed in [3].
We have identified
three
shortcomings
where corrections
can lead to a more
coherent and accurate classifier:

The use of ad hoc softening of probabilities
is not
understandable
from a theoretical
point of view.

The algorithm selects a single model, ignoring model
uncertainty.

The algorithm tries to find the TAN that maximizes
likelihood,
while in order to have better classifica-
tion accuracy conditional
likelihood
should be max-
imized.

`The work was partially developed at the Institut de Investi-
gaci6 en Intel.ligBncia Artificial,
CSIC, Spain.


Permission to nuke digital or hard copies ofall or part ofthis work for
personal or classroomUSCis grantedwithout fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
olhcrwise, Lorepublish, to post on sewers or to redistribute to lists.
requires prior specific permission and/or a fee.
KDD-99 San Diego CA USA
Copyright ACM 1999 I-581 13-143-7/99/08...$5.00
In the following,
we propose solutions that try to over-
come these problems. We first introduce
bayesian net-
works and propose a new approach (the multinomial
sampling approach) to the problem of learning bayesian
networks in Section 2. In Section 3 we discuss TAN
into detail and use the multinomial
sampling
approach
to derive an algorithm for learning maximum likelihood
networks that uses a unique coherent probability
dis-
tribution
at every step and competes in accuracy with
the one proposed in [3]. In order to overcome problems
two and three, we consider bayesian model averaging
(BMA)
in 4.1; we introduce
local bayesian model aver-
aging (LBMA)
in 4.2 and we make the application
of
LBMA
to TAN induction
in 4.3. We empirically
eval-
uate the results of our improvements
in Section 5, to
finish up pointing
some conclusions in Section 6. A
longer version of this paper is available [2].


2
Learning
Bayesian
Networks

Let U = {XI,...
,X,}
be a set of discrete random
variables. A bayesian network is an annotated directed
acyclic graph that encodes a joint probability
distribu-
tion over U.
Formally
it is a pair B = (G, 0).
G is
a DAG whose vertices correspond to the random vari-
ables and whose edges represent direct dependencies be-
tween the variables.
0 represents the set of parame-
ters that quantify the network. It contains a parameter

`Silnzi
= PB (xi IIIzi ) for each possible value zi of Xi
and IIzi of IIx;,
where IIx;
denotes the set of parents
of Xi in G. A bayesian network B defines a unique joint
probability
distribution
over U given by




i=l
i=l

The problem of learning a bayesian network has been
informally
stated as:

Statement
1 (Friedman
et al., 1997)
Given
a
training
set D = {us, . . . ,UN} of instances of U find
the network B that best matches D.




292

For classification
purposes, the fact of trying to match
the data perfectly
usually causes overfitting.
That is
why, in order to use bayesian networks for classification
purposes we prefer the following informal statement:

Statement
2 Given a sample S = (~1,. . . ,UN}
of a
probability
distribution
P' find the network B that best
matches P*.

One of the measures used to learn bayesian networks is
the log likelihood:


(2)
i=l

This measure has the property that can be decomposed
according to the structure of B. Let Counto(X)
stand
for the number of observations in our sample that ful-
fill condition
X and FreqD(X)
= cOu*~(x).
VaZ(Xi)
is the set of possible states of the random variable Xi
and #VaZ(Xi)
the number of different possible states
Xi can be in.
If FreqD(xi,
II,;)
is strictly
positive and defined every-
where (i.e. we have at least one observation
for each
possible pair zi, II,;
in our dataset) it is easy to see
that LL(BID)
is maximized when




This results allow us to search separately the space of
network structures
and the space of network parame-
ters.

2.1
The
multinomial
sampling
approach
to
learning
bayesian
networks
The dataset D of Statement 1 can be seen as a sample of
a probability
distribution
P". We can assume that P*
is a multinomial
distribution
with a number of possible

states equal to:
States(P*)
=
fi #Val(Xi).
Each
i=l
instance in the dataset is equivalent to the observation
of a concrete state as an outcome of a multinomial
trial.
In order to learn a bayesian network we should take two
steps:

1. Approximate
the distribution
P* from the informa-
tion we have in the sample S by using multinomial
sampling methods. This generates P;.

2. Find the bayesian network which better fits P;.

We need a way to calculate
Pi.
We adhere to
the principle
of indiflerence,
which says that
if we
lack better
information,
we should assign an equal
probability
to each possible success. Our prior is then
a Dirichlet
distribution
with States(P*)
equiprobable
possible states. We still have to fix one more parameter,
the relevance we are giving
to the prior,
namely A.
Hence:


Pi(x1,...
,xn) =
Counts(xl,
. . .
`xrJ + stat&*)
N+X
(4)

3
Tree Augmented
Naive
Bayes
From our point of view TAN are the most coherent
and best performing
enhancement to Naive Bayes up
to now. To talk of the classification
problem we will
use the common notation of for distinguishing
between
the random variable we want to predict (the class, C)
and all the rest (the attributes,
Al,. . . , A,).

3.1
Learning
TAN
TAN are a restricted
family
of bayesian networks in
which the class variable has no parents and each at-
tribute
has the class variable and at most one other
attribute
as parents.
The interesting
property
of this
family is that we have an efficient procedure for identi-
fying the structure of the network with maximum likeli-
hood. In [3] we can find a procedure that builds a TAN
BT that maximizes LL(BTID)
and has time complex-
ity C(n"N).
The search for the maximum
likelihood
structure
is done using the result of Equation
3. Sur-
prisingly,
once the structure
is found, they empirically
noticed that softening the probabilities
instead of using
exactly the maximum
likelihood
estimates lead to an
improvement
in classification
accuracy [3]. They take:

%rL
= Q. Freqo(xlII,)
+ (1 - a) . Freqo(x)




The explanation
given there (summarizing,
that the
number
of observations
is not enough to estimate
reliably
the probabilities)
is not theoretically
sound,
because the results are not assymptotic.
A plausible
explanation
to this phenomenon
could be related to
the fact (usually disregarded) that the results only hold
when FreqD is strictly positive and defined everywhere.
From our point of view, the most likely explanation
comes from the fact that, by following
Statement 1 we
are focusing on fitting
the data, and not on predicting
future events. That is why we propose the usage of the
multinomial
sampling approach to provide a solution to
the problem.

3.2
Applying
the
multinomial
sampling
approach
to TAN
induction
In this case, instead of looking for maximum likelihood,
we would like to minimize the cross entropy or Kullback-
Leibler divergence [6] between the P< and the prob-
ability
distribution
generated by the TAN (following



293

the spirit of Statement 2). Cross entropy is minimized
when:




where IIx;
stands for the set of variables which are not
parents of Xi in the network. IIx; does not include Xi.
We define SC(Xi)
=
n
#VaZ(Xj).
SC(Xi)
is the
Xj
En,Yi
number of different states of the multinomial
for which
you have to add up the probability
in order to calculate
P;(xi,
II,;).
From this definition
and Equation
6 we
get:




In the case of TAN induction,
we have to set three
different kind of parameters:
Oa;lc,aj , eailc and 8,. In
these concrete three cases Equation 7 simplifies to:




eailc=
CountD(a;,c)+#"al(c)x#val(Ai)
Count&)+g&-j
8, =
countD(c)+#"&C)
N+X
63)



These probabilities
estimates are used consistently both
in the tree structure induction
and in the weight deter-
mination.
Empirical
results are given and explained in
Section 5.

4
Local
Bayesian
Model
Averaging
The second shortcoming
in the TAN induction
algo-
rithm
of [3] is that uncertainty
in model selection is
ignored. Bayesian modeEaveraging (BMA)
[5] provides
a coherent mechanism for accounting for uncertainty
in
modelling.

4.1
Bayesian
Model
Averaging
A coherent approach to solve the classification
problem
is calculating
the probability
of each class given the
data. If we assume that the data has been generated
from a model that is contained in a class of models M,
the probability
distribution
of the class given data, is:

P(W,S)
= c
P(CIM,~)P(MIS)
(9)
MEM

where P(CIM,I)
is the probability
distribution
of the
class when we know the model M that generated the
data, and the value of the attributes
for this instance
1. P(MIS)
is the probability
that M is the model that
generated the data given the sample S.
Equation 9 tells us not to use a single model to classify
the data, but instead to use all the models from the
class of models, weighting
each model prediction
by
the probability
of the model given the sample of data
we are analyzing.
Using Equation
9 to predict
is
known as Bayesian Model Averaging or BMA for short.
BMA produces optimally
accurate predictions
within
the chosen model family
from the probability
theory
point of view.
In order to use BMA in practice, we need
develop Equation
9. It can be expanded as:

WWMM)
P("`S)
= ,gM
P(SIM')P(M')


Here P(M)
is the prior probability
that M is the real
model and P(SIM)
is the probability
that model M
generates the data in S.
to further



(10)




4.2
Local
Bayesian
Model
Averaging
In practice, the usage of BMA presents some problems,
coming from:

l
The computational
cost of calculating
Equation
9.

l
The difficulty
in the specification of P(M),
the prior
distribution
over competing models.

In order to handle the first of these problems,
we
propose LBMA,
an heuristic approach to approximate
BMA.
The idea is similar
in spirit
to the Occam's
Window method described in [5, 71. To apply LBMA
we should have an heuristic h(M, S) such that

h(M,S)
fi: P(MIS)
(11)

In order to approximate
the summation in Equation
9,
and given that we have h(M, S), we define our set of
interesting
models M' as:

M' = {M E MIh(M,S)
> y}
(12)

y represents
a compromise
between the prediction
accuracy and its computational
cost. It should be big
enough to make #M'
<
#M , but small enough in
order for

P(MIS)
x P'(MIS)
=
xp(;;,);;;;M,)
(13)

M'EM'



P(CjI,S)
E
c
P(CIM,W'WIS)
(14)
MEM'




294

to be accurate approximations.
It is interesting
to note
that maximum
likelihood
prediction
is a concrete case
of LBMA where h(M, S) = P(S]M)
and y is implicitly
set in order for M' to contain only a model. Once we
have the resulting weighted set of models calculated, we
can use it to classify by calculating Equation 14 for each
class and choosing the one with higher probability.

4.3
Local
Bayesian
Model
Averaging
for
TAN
induction
For this concrete case, our class of models M is

M = {(G, O)jG E TANStructs,
0 E Params(
(15)

We perform a first reduction of M by using the results
in Equation
8 or the softened method proposed in [3]
depending on whether we decide to use the multinomial
sampling approach or the ad hoc adjustment
proposed
in [3]. In any case, we will only average over the struc-
tures, fixing the parameters by using the corresponding
equation in each case. Our heuristic over structures will
be given by the algorithm Construct-TAN,
just modify-
ing the step where a maximum spanning tree is induced,
to generate a set containing
the K maximum spanning
trees by using Gabow algorithm
[4]. In order to cal-
culate P'(M),
we set a prior over tree structures
that
assigns the same probability
to each possible tree struc-
ture (since they can be considered of a similar com-
plexity).
We also have to provide an implementation
for Predict,
that is, we have to know how to calculate
P(C = clM,u).
In a TAN:


h4(CIUi,l,...
,Ui,n)
=
Cp~~~t;;;,~~f;~~,n)
=
C'EC

fl
e"i,jlctu;,n(Aj)
(16)
=
j=l

c,Fc jfileui,jlc'~ui
~(a,)
'
3

where ui,n(Aj)
is adequately
set to the value of the
parent of Aj in the tree or nothing if Aj does not has
parents in it.
Applying
LBMA
to TAN induction
we are simulta-
neously providing
a solution for the second and third
shortcomings
in [3] noticed in the introduction.
It
is clear that
LBMA
is an approximation
to BMA,
and hence focuses on the second point, i.
e., taking
into account the uncertainty
in model selection.
The
third point was that what should be maximized
is the
conditional
likelihood
instead of the likelihood.
This
shortcoming
was already noticed in their paper, where
they stated that
it was an open question
whether
good heuristic
approaches can be found in order to
induce TAN models that maximize
conditional
likeli-
hood.
The way LBMA
weights the different
models
is exactly
multiplying
each model by its conditional
likelihood.
We are then trusting
more those models
with a higher conditional
likelihood.
The problem is far
from being solved, but we think this is a good first step.



5
Experimental
results

In order to use the algorithm
described in Section 4.3,
we need to set some parameters.
In our experimental
setting, we took:

Ic = min(lO, n)
x = 10
(17)


We tested three algorithms
over 11 datasets from the
Irvine
repository
[l] plus our own credit
screening
database. To discretize continuous attributes
we used
equal frequency
discretization
with
5 intervals.
For
each dataset and algorithm
we evaluated accuracy and
LogScore.
LogScore
gives an idea of how well the
classifier is estimating
probabilities.


LogScore(M,
S') = 5
- log(PM(ui))
i=l
(18)


For both evaluations we used 10 fold cross validation.
The error rates appear in Table 1, with the best method
for each dataset in cursive. LogScore's appear in Table
n The meaning of the column headers are:

TAN+MS+BMA,
is the method described in Sec-
tion 4.3.

STAN is the softened TAN algorithm
as described
in [3]

TAN+MS
is the maximum
likelihood
TAN induc-
tion using the multinomial
sampling approach




Table 1: Averages and standard deviations of error rates



5.1
Interpretation
of the results
We plot in Figure 1 the percentage of improvement
in
error rate between using TAN+MS+BMA
and STAN,
and in Figure 2 the improvement
in LogScore.
Both
figures favor TAN+MS+BMA.



295

Table 2: Averages and standard deviations of LogScore

Improvement
in error
rate
of
TAN+MS+BMA
over
STAN
(in
percent)




Figure 1: Comparison of the error rate of TAN+MS+BMA
and STAN


6
Conclusions

We have proposed solutions
for correcting
the three
shortcomings we noticed in TAN induction
as it is done
in [3]. We have introduced
the multinomial
sampling
approach, a new approach to learning bayesian networks
that provides a coherent way of estimating probabilities,
and have used it to develop a theoretically
coherent
maximum
likelihood
TAN induction
algorithm.
We
have introduced
local bayesian model averaging and
have used it to account for uncertainty
in the selection
of the model. By using LBMA,
we have weighted each
model by its conditional
likelihood,
instead of by its
likelihood,
thus providing
a partial solution to the fact
that conditional
likelihood
and not likelihood
is what
should be maximized.
We have provided empirical
evidence that shows that
in most of the cases the resulting new method provides
more accurate predictions
and probability
estimates.
Improvement
in
LogP
of TAN+MS+BMA
over
STAN
(in
percent)




Figure 2: Comparison of the LogScore of TAN+MS+BMA
and STAN


7
Acknowledgements
I would like to thank Maria Luisa Barja and Ramon
Lopez de Mantaras for carefully reviewing the prelim-
inary versions of this paper.
I would like to specially
thank Maria Luisa for accepting an additional
workload
in order to give me the time to write it.

References
PI

PI


131


141



[51




PI

171
C. Blake,
E. Keogh,
and C. J. Merz.
UC1 repository
of
machine learning
databases,
1998.

Jesus Cerquides.
Applying
General
Bayesian
Techniques
to
Improve
TAN
Induction.
Long
Version.
Technical
report,
Ubilab,
www.ubs.com/ubilab,
1999.

Nir Friedman,
Dan Geiger, and Moises Goldszmidt.
Bayesian
network
classifiers.
Machine
Learning,
29:131-163,
1997.

Harold
N. Gabow.
Two
algorithms
for generating
weighted
spanning
trees in order.
SIAM J. COMPUT.,
6(1):139-150,
March
1977.

Jennifer
A. Hoeting,
David
Madigan,
Adrian
E. Raftery,
and
Chris T. Volinsky.
Bayesian
model averaging.
Technical
Re-
port 9814, Department
of Statistics.
Colorado
State Univer-
sity, 1998.

S. Kullback
and R. A. Leibler.
On information
and sufficiency.
Annals
of Mathematical
Statistics, 22:76-86, 1951.

David
Madigan
and Adrian.
E. Raftery.
Model
selection
and accounting
for model
uncertainty
in graphical
models
using occam's window.
J. American Statistical Association,
&x9:1535-1549, 1994.




296

