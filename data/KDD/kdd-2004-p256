Rapid Detection of Significant Spatial Clusters


Daniel B. Neill
Department of Computer Science
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213

neill@cs.cmu.edu
Andrew W. Moore
Department of Computer Science
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213

awm@cs.cmu.edu


ABSTRACT
Given an N ×N grid of squares, where each square has a count ci
j
and an underlying population pi
j
, our goal is to find the rectangular
region with the highest density, and to calculate its significance by
randomization. An arbitrary density function D, dependent on a re-
gion's total count C and total population P, can be used. For exam-
ple, if each count represents the number of disease cases occurring
in that square, we can use Kulldorff's spatial scan statistic DK to
find the most significant spatial disease cluster. A naive approach
to finding the maximum density region requires O(N4) time, and
is generally computationally infeasible. We present a multiresolu-
tion algorithm which partitions the grid into overlapping regions
using a novel overlap-kd tree data structure, bounds the maximum
score of subregions contained in each region, and prunes regions
which cannot contain the maximum density region. For sufficiently
dense regions, this method finds the maximum density region in
O((N logN)2) time, in practice resulting in significant (20-2000x)
speedups on both real and simulated datasets.


Categories and Subject Descriptors
H.2.8 [Database Management]: Database Apps- Data Mining


General Terms
Algorithms


Keywords
Cluster detection, spatial scan statistics, biosurveillance


1. INTRODUCTION
One of the core goals of data mining is to discover patterns and
relationships in data. In many applications, however, it is impor-
tant not only to discover patterns, but to distinguish those patterns
that are significant from those that are likely to have occurred by
chance. This is particularly important in epidemiological appli-
cations, where a rise in the number of disease cases in a region
may or may not be indicative of an emerging epidemic. In order to




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
decide whether further investigation is necessary, epidemiologists
must know not only the location of a possible outbreak, but also
some measure of the likelihood that an outbreak is occurring in
that region. More generally, we are interested in spatial data min-
ing problems where the goal is detection of overdensities: spatial
regions with high scores according to some density measure. The
density measure can be as simple as the count (e.g. number of dis-
ease cases, or units of cough medication sold) in a given area, or can
adjust for quantities such as the underlying population. In addition
to discovering these high-density regions, we must perform statisti-
cal testing in order to determine whether the regions are significant.
As discussed above, a major application is in detecting clusters of
disease cases, for purposes ranging from detection of bioterrorism
(e.g. anthrax attacks) to identifying environmental risk factors for
diseases such as childhood leukemia [8, 11, 6]. [5] discusses many
other applications, including mining astronomical data (e.g. identi-
fying star clusters), military reconnaissance, and medical imaging.
We consider the case in which data has been aggregated to a uni-
form, two-dimensional grid. Let G be an N × N grid of squares,
where each square si
j
 G is associated with a count ci
j
and an
underlying population pi
j
. For example, a square's count may be
the number of disease cases in that geographical location in a given
time period, while its population may be the total number of people
"at-risk" for the disease. Our goal is to search over all rectangular
regions S  G, and find the region S with the highest density ac-
cording to a density measure D: S = argmaxS D(S). We use the
abbreviations mdr for the maximum density region S, and mrd for
the maximum region density D(S), throughout. We will also find
the statistical significance (p-value) of this region by randomization
testing, as described below.
The density D(S) of a region S can be an arbitrary function of
the total count of the region, C(S) = S ci
j
, and the total popula-
tion of the region, P(S) = S pi
j
. Thus we will often write D(C,P),
where C and P are the count and population of the region under
consideration. It is important to note that, while the term "den-
sity" is typically understood to mean the ratio of count to popula-
tion, we use the term in a much broader sense, to denote a class
of density functions D which includes the "standard" density func-
tion D1(C,P) =
C
P
. For our purposes, we assume that the density
function D satisfies the following three properties:

1. For a fixed population, density increases monotonically with
count:
D
C
(C,P)  0 for all (C,P).
2. For a fixed count, density decreases monotonically with pop-
ulation:
D
P
(C,P)  0 for all (C,P).

3. For a fixed ratio
C
P
, density increases monotonically with

population:
D
P
(C,P)+
C
P
D
C
(C,P)  0 for all (C,P).


256
Research Track Paper

The first two properties state that an overdensity is present when a
large count occurs in a small population. In the case of a uniform
population distribution, the population of a region is proportional
to its area, and thus an overdensity is present when a large count
occurs in a small area. The third property states, in essence, that
an overdensity is more significant when the underlying population
is large. This is true because smaller populations will have higher
variance in densities. However, we also allow D to remain constant
as population increases for a fixed ratio
C
P
, thus including the stan-
dard density function; we do not, however, allow functions where
D decreases in this case. In our discussion below, we will also make
one more assumption involving the second partials of D; this fourth
property is not strictly necessary but makes our computation eas-
ier (eliminating the need to check for local maxima of the density
function). A large class of functions satisfy all four properties, in-
cluding Kulldorff's spatial scan statistic, discussed in detail below.


1.1 Related work
This work builds on our previous work on detection of spatial
overdensities [7]. The primary difference is in the statement of the
problem: the goal of the present work is to detect the most signifi-
cant rectangular region, as opposed to the most significant square
region. This extension is extremely important in epidemiological
applications because disease clusters are often elongated: airborne
pathogens may be blown by wind, creating an ellipsoid "plume,"
and waterborne pathogens may be carried along the path of a river.
In each of these cases, the resulting clusters have high aspect ra-
tios, and a test for squares (or circles, as in Kulldorff's original scan
statistic) will have low power for detecting the overdensity. While
our discussion below focuses on finding "axis-aligned" rectangular
regions, it can be easily extended to find rectangular regions which
are not aligned with the coordinate axes. One simple method of
doing this is to examine multiple "rotations" of the data, mapping
each to a separate grid and computing the maximum density region
for each grid. In this case, we must also perform the same rotations
on each replica grid, and thus the complexity of the algorithm is
multiplied by the number of rotations. However, if we have infor-
mation about relevant conditions such as wind direction or the flow
of a river, we can use this information to align the coordinate axes,
reducing or avoiding the need to examine multiple rotations.
While this change from square to rectangular regions has little
effect on the underlying statistics, it creates a much more difficult
computational problem. While the maximum density square region
can be found naively in O(N3) time for an N × N grid, finding the
maximum density rectangular region requires O(N4), and thus is
computationally infeasible for even moderately sized grids. Our
solution is similar to our previous work in that we propose a mul-
tiresolution partitioning algorithm: we divide the grid into overlap-
ping regions, bound the maximum score of subregions contained in
each region, and prune regions which cannot contain the maximum
density region. Within that general framework, however, there are
major differences in our multiresolution data structure and the re-
sulting algorithm. Our current "center-based" approach (using a
novel "overlap-kd tree" data structure) allows us to achieve tighter
upper bounds on the score of a region, allowing much more prun-
ing to take place. As a result, our algorithm gives huge speedups
(as compared to the naive approach) without relying on approxima-
tion; the non-approximate version of our previous algorithm only
resulted in speedups when the maximum density region was suf-
ficiently dense, and actually performed slower than naive in some
cases. Our current algorithm is also more robust: while the pre-
vious algorithm was severely slowed by non-uniform populations,
these are shown to have little or no effect on the current method.
Various other methods for finding "dense clusters" have been
proposed in the data mining literature, including grid-based hierar-
chical methods such as CLIQUE [1], MAFIA [3], and STING [12].
Our work differs from these in three main ways: most importantly,
as discussed above, our goal is not only to find the highest scor-
ing cluster, but to determine the statistical significance of that clus-
ter (whether it is a true overdensity, or if it is likely to have oc-
curred by chance). Second, our method deals with non-uniform
underlying populations: this is particularly essential for real-world
epidemiological applications, in which an overdensity of disease
cases is more significant if the underlying population is large, and
is also important in many other applications where an "overdensity"
is defined relative to some other statistic (e.g. population, baseline
score, or covariate). Finally, our method is applicable to a wide
class of density measures D, where the other algorithms are specific
to the "standard" density measure D1(S) =
C(S)
P(S)
. The D1 measure
is the ratio of count per unit population; for example, in epidemi-
ology, maximizing D1 corresponds to finding the region with the
highest observed disease rate. However, this is not generally the
region we are interested in finding, since a region with a high dis-
ease rate and very small population (e.g. one person, who happens
to be sick) is not likely to be significant. In fact, the "region" with
the highest D1 will be the single square with highest
ci
j
pi
j
. This is
because D1 density is monotonic: if a region S with D1(S) = d is
partitioned into any set of disjoint subregions, at least one subre-
gion S will have D1(S )  d. Thus the other algorithms, rather
than maximizing D1, search for maximally sized regions with D1
greater than some threshold. There are two disadvantages to this
approach: one is the difficulty of measuring statistical significance
within this framework. The other is that the algorithms rely heav-
ily on the monotonicity of the D1 measure by first finding "dense"
1 × 1 squares, then merging adjacent squares in bottom-up fash-
ion. For a non-monotonic density measure such as Kulldorff's, it is
possible to have a large dense region where none of its subregions
are themselves dense, so bottom-up methods are not guaranteed to
find the correct region. Here, we will optimize with respect to arbi-
trary non-monotonic density measures, and thus require a different
approach from CLIQUE, MAFIA, or STING.

1.2 The spatial scan statistic
A non-monotonic density measure which is of great interest to
epidemiologists is Kulldorff's spatial scan statistic [4], which we
denote by DK. This statistic is in common use for finding signif-
icant spatial clusters of disease cases, which are often indicative
of an emerging outbreak. Kulldorff's statistic assumes that counts
ci
j
are generated by an inhomogeneous Poisson process with mean
qpi
j
, where q is the underlying "disease rate" (or expected value of
C
P
). We then calculate the log of the likelihood ratio of two possi-
bilities: that the disease rate q is higher in the region than outside
the region, and that the disease rate is identical inside and outside
the region. For a region with count C and population P, in a grid
with total count Ctot and population Ptot, we can calculate:

DK = Clog
C
P
+(Ctot -C)log
Ctot -C
Ptot -P
-Ctot log
Ctot
Ptot

if
C
P
>
Ctot
Ptot
, and DK = 0 otherwise. [4] proved that the spatial scan
statistic is individually most powerful for finding a single signifi-
cant region of elevated disease rate: for a fixed false positive rate,
and for a given set of regions tested, it is more likely to detect the
overdensity than any other test statistic. Again, we note that our al-
gorithm is general enough to use any density measure, and in some
cases we may wish to use measures other than Kulldorff's. For



257
Research Track Paper

instance, if we have some idea of the size of the maximum den-
sity region, we can use the Dr measure, Dr(S) =
C(S)
P(S)r
, 0 < r < 1,
with larger r corresponding to tests for smaller clusters. We have
also derived a variant of the Dr measure for normally distributed
counts, where the cumulative statistics of a region are not its raw
count and population, but a weighted average of squares' z-scores.
We are in the process of using this statistic to look for emerging
epidemics based on national sales of over-the-counter medications.

1.3 Randomization testing
Once we have found the maximum density region (mdr) of grid
G according to our density measure, we must still determine the
statistical significance of this region. Since the exact distribution
of the test statistic is only known in special cases (such as D1 den-
sity with a uniform underlying population), in general we must find
the region's p-value by randomization. To do so, we run a large
number R of random replications, where a replica has the same un-
derlying populations pi
j
as G, but assumes a uniform disease rate

qrep =
Ctot(G)
Ptot(G)
for all squares. For each replica G , we first generate
all counts ci
j
randomly from an inhomogeneous Poisson distribu-
tion with mean qreppi
j
, then compute the maximum region density
(mrd) of G and compare this to mrd(G). The number of replicas
G with mrd(G )  mrd(G), divided by the total number of repli-
cations R, gives us the p-value for our maximum density region.
If this p-value is less than .05, we can conclude that the discov-
ered region is statistically significant (unlikely to have occurred by
chance) and is thus a "spatial overdensity." If the test fails, we have
still discovered the maximum density region of G, but there is not
sufficient evidence that this is an overdensity.

1.4 The naive approach
The simplest method of finding the maximum density region is
to compute the density of all rectangular regions of sizes k1 × k2,
where kmin  k1,k2  kmax.1 Since there are a total of (N - k1 +
1)(N - k2 + 1) regions of each size k1 × k2, there are a total of
O(N4) regions to examine. We can compute the density of any
region S in O(1), by first finding the count C(S) and population
P(S), then applying our density measure D(C,P).2 This allows us
to compute the mdr of an N × N grid G in O(N4) time. However,
significance testing by randomization also requires us to find the
mrd for each replica G , and compare this to mrd(G). Since calcu-
lation of the mrd takes O(N4) time for each replica, the total com-
plexity is O(RN4), and R is typically large (we assume R = 1000).
As discussed in [7], several tricks may be used to speed up this
procedure for cases where there is no significant spatial overden-
sity, but these do not help in cases when an overdensity is found.
In general, the O(N4) complexity of the naive approach makes it
infeasible for even moderately sized grids: we estimate a runtime
of 45 days for a 256×256 grid on our test system, which is clearly
far too slow for real-time detection of disease outbreaks.
While one alternative would be to search for an approximate so-
lution using one of the variety of cluster detection algorithms in the
literature, we present an algorithm which is exact (always finds the
maximum density region) and yet is much faster than naive search.
The key intuition is that, since we only care about finding the max-
imum density region, we do not need to search over every single

1
We use kmin = 3 and kmax = N throughout.
2
An old trick makes it possible to compute the count and popu-
lation of any rectangular region in O(1): we first form a matrix
of the cumulative counts, then compute each region's count by
adding/subtracting at most four cumulative counts, and similarly
for populations.
rectangular region: in particular, we do not need to search a set of
regions if we can prove (based on other regions we have searched)
that none of them can be the mdr. As a simple example, if a given
region has a very low count, we may be able to conclude that no
subregion contained in that region can have a score higher than
the mrd, and thus we do not need to actually compute the score of
each subregion. These observations suggest a top-down, branch-
and-bound approach: we maintain the current maximum score of
the regions we have searched so far, calculate upper bounds on the
scores of subregions contained in a given region, and prune regions
which cannot contain the mdr. Similarly, when we are searching a
replica grid, we only care about whether the mrd of the replica is
higher than the mrd of the original grid. Thus we can use the mrd of
the original grid for pruning on the replicas, and can stop searching
a replica if we find a region with score higher than this mrd.


2. OVERLAP-MULTIRES PARTITIONING
Our top-down approach to cluster detection can be thought of
as a multiresolution search of the space under consideration: we
search first at coarse resolutions (large regions), then at succes-
sively finer resolutions (smaller regions) as necessary. This sug-
gests that a hierarchical, space-partitioning data structure such as
kd-trees [9], mrkd-trees [2], or quadtrees [10] may be useful in
speeding up our search. However, our desire for an exact solu-
tion makes it difficult to apply these data structures to our problem.
In a kd-tree, each spatial region is recursively partitioned into two
disjoint "child" regions, each of which can then be further subdi-
vided. The difficulty, however, is that many subregions of the par-
ent are not contained entirely in either child, but overlap partially
with each. Thus, in addition to recursively searching each child
for the mdr, we must also search over all of these "shared" regions
at each level of the tree.3 Since there are O(N4) shared regions
even at the top level of the tree (i.e. regions partially overlapping
both halves of grid G), an exhaustive search over all such regions
is too computationally expensive, and thus a different partitioning
approach is necessary.
An initial step toward our partitioning can be seen by consider-
ing two divisions of a rectangular spatial region S: first, into its left
and right halves (which we denote by S1 and S2), and second, into
its top and bottom halves (which we denote by S3 and S4). As-
suming that S has size k1 ×k2, this means that S1 and S2 have size
1
2
k1 ×k2, and S3 and S4 have size k1 ×
1
2
k2. Considering these four
(overlapping) halves, we can show that any subregion of S either a)
is contained entirely in (at least) one of S1 ...S4, or b) contains the
centroid of S. Thus one possibility would be to search S by exhaus-
tively searching all regions containing its centroid, then recursing
the search on its four "children" S1 ...S4. Again, there are O(N4)
"shared" regions at the top level of the tree (i.e. regions containing
the centroid of grid G), so an exhaustive search is infeasible.
Our solution, as in our previous work [7], is a partitioning ap-
proach in which adjacent regions partially overlap, a technique we
call "overlap-multiresolution partitioning," or "overlap-multires"
for short. Again we consider the division of S into its left, right,
top, and bottom "children." However, while in the discussion above
each child contained exactly half the area of S, now we let each
child contain more than half the area. We again assume that region
S has size k1 × k2, and we choose fractions f1, f2 >
1
2
. Then S1
and S2 have size f1k1 ×k2, and S3 and S4 have size k1 × f2k2. This
3
Note that an attempt to find the two "pieces" of the mdr, one
in each child, and then merge the two, fails because of the non-
monotonicity of the density measure: the mdr may have a higher
score than either of its two pieces!



258
Research Track Paper

S_3
S_1




S




S_C
some S_i, i = 1..4,
Any subregion of S:
S_2




S_4
either a) is contained in


or b) contains S_C.




Figure 1: Overlap-multires partitioning of region S


partitioning (for f1 = f2 =
3
4
) is illustrated in Figure 1. Note that
there is a region SC common to all four children; we call this region
the center of S. The size of SC is ((2 f1 - 1)k1 × (2 f2 - 1)k2), and
thus the center has non-zero area. When we partition region S in
this manner, it can be proved that any subregion of S either a) is
contained entirely in (at least) one of S1 ...S4, or b) contains the
center region SC. Figure 1 illustrates each of these possibilities.
Now we can search S by recursively searching S1 ...S4, then
searching all of the regions contained in S which contain the center
SC. Unfortunately, at the top level there are still O(N4) regions con-
tained in grid G which contain its center GC. However, since we
know that each such region contains the large region GC, we can
place very tight bounds on the score of these regions, often allow-
ing us to prune most or all of them. (We discuss how these bounds
are calculated in the following subsection.) Thus the basic outline
of our search procedure (ignoring pruning, for the moment) is:

overlap-search(S)
{
call base-case-search(S)
define child regions S_1..S_4, center S_C as above
call overlap-search(S_i) for i=1..4
for all S' such that S' is contained in S and contains S_C,
call base-case-search(S')
}


Now we consider how to select the fractions f1 and f2 for each
call of overlap-search, and characterize the resulting set  of re-
gions S on which overlap-search(S) is called. Regions S   are
called gridded regions, and regions S /
  are called outer regions.
For simplicity, we assume that the grid G is square, and that its
size N is a power of two. We begin the search by calling overlap-
search(G). Then for each recursive call to overlap-search(S), where
the size of S is k1 × k2, we set f1 =
3
4
if k1 = 2r for some integer
r, and f1 =
2
3
if k1 = 3×2r for some integer r. We define f2 iden-
tically in terms of k2, and then the child regions S1 ...S4 and the
center region SC are defined in terms of f1 and f2 as above. This
choice of f1 and f2 has the useful property that all gridded regions
have sizes 2r or 3×2r for some integer r. For instance, if the orig-
inal grid G has size 64×64, then the children of G will be of sizes
64 × 48 and 48 × 64, and the grandchildren of G will be of sizes
64×32, 48×48, and 32×64. This process can be repeated recur-
X
X
XX
X
X




Figure 2: The first two levels of the overlap-kd tree. Each node
represents a gridded region (denoted by a thick rectangle) of
the entire dataset (thin square and dots).



sively down to regions of size kmin × kmin, forming a structure that
we call an overlap-kd tree. The first two levels of the overlap-kd
tree are shown in Figure 2. Note that even though grid G has four
child regions, and each of its child regions has four children, G has
only ten (not 16) distinct grandchildren, several of which are the
child of multiple regions.
Our overlap-kd tree has several nice properties, which we present
here without proof. First, for every rectangular region S  G, either
S is a gridded region (contained in the overlap-kd tree), or there
exists a unique gridded region S such that S is an outer region
of S (i.e. S is contained in S , and contains the center region of
S ). This means that, if overlap-search is called exactly once for
each gridded region, and no pruning is done, then base-case-search
will be called exactly once for every rectangular region S  G. In
practice, we will prune many regions, so base-case-search will be
called at most once for every rectangular region, and every region
will be either searched or pruned. The second nice property of our
overlap-kd tree is that the total number of gridded regions || is
O((N logN)2) rather than O(N4). This implies that, if we are able
to prune (almost) all outer regions, we can find the mdr of an N ×N
grid in O((N logN)2) time. In fact, we may not even need to search
all gridded regions, so in many cases the search will be even faster.
Before we consider how to calculate score bounds and use them
for pruning, we must first deal with an essential issue in searching
overlap-kd trees. Since a child region may have multiple parents,
how do we ensure that each gridded region is examined only once,
rather than being called recursively by each parent? One simple
answer is to keep a hash table of the regions we have examined,
and only call overlap-search(S) if region S has not already been ex-
amined. The disadvantage of this approach is that it requires space
proportional to the number of gridded regions, O((N logN)2), and
spends a substantial amount of time doing hash queries and up-
dates. A more elegant solution is what we call lazy expansion:
rather than calling overlap-search(Si) on all four children of a re-
gion S, we selectively expand only certain children at each stage,
in such a way that there is exactly one path from the root of the
overlap-kd tree to any node of the tree. One such scheme is shown
in Figure 2: if the path between a parent and child is marked with an
X, lazy expansion does not make that recursive call. No extra space
is needed by this method; instead, a simple set of rules is used to
decide which children of a node to expand. A child is expanded if it
has no other parents, or if the parent node has the highest priority of
all the child's parents. We give parents with lower aspect ratios pri-
ority over parents with higher aspect ratios: for example, a 48×48
parent would have priority over a 64×32 parent if the two share a
48×32 child. This rule allows us to perform variants of the search


259
Research Track Paper

 
 ¡
¡
¢
¢£
£




¤
¤¥
¥




p_in
c_in
C_3(p_in)
C_2(p_in)C_1(p_in)




P_13


P_12
P_23




Figure 3: Maximizing count cin for a given population pin.
Count must be less than C1(pin), C2(pin), and C3(pin).


where regions with very high aspect ratios are not included; an ex-
treme case would be to only search for squares, as in our previous
work. Within an aspect ratio, we fix an arbitrary priority ordering.
Since we maintain the property that every node is accessible from
the root, the correctness of our algorithm is maintained: every grid-
ded region will be examined (if no pruning is done), and thus every
region S  G will be either searched or pruned.
2.1 Score bounds
We now consider which regions can be pruned (discarded with-
out searching) during our multiresolution search procedure. First,
given some region S, we must calculate an upper bound on the
scores D(S ) for regions S  S. More precisely, we are interested
in two upper bounds: a bound on the score of all subregions S  S,
and a bound on the score of the outer subregions of S (those regions
contained in S and containing its center SC). If the first bound is
less than or equal to the mrd, we can prune region S completely;
we do not need to search any (gridded or outer) subregion of S. If
only the second bound is less than or equal to the mrd, we do not
need to search the outer subregions of S, but we must recursively
call overlap-search on the gridded children of S. If both bounds are
greater than the mrd, we must both recursively call overlap-search
and search the outer regions.
Now we will explain the calculation of the second bound (on
subregions containing the center); the calculation of the first bound
(on all subregions) can be treated as a special case where the pop-
ulation, count, and area of the center are zero. We begin by assum-
ing as known various pieces of information about the subregions
of S; we discuss below how these are obtained. This information
includes: upper and lower bounds pmax, pmin on the population of
subregions S ; an upper bound dmax on the D1 density of S ; an up-
per bound dinc on the D1 density of S -SC; and a lower bound dmin
on the D1 density of S - S . We also know the count C and popu-
lation P of region S, and the count ccenter and population pcenter of
region SC. Let cin and pin be the count and population of S ; these
are presently unknown. To find an upper bound on D(S ), we must
calculate the values of cin and pin which maximize D(cin, pin), sub-
ject to the given constraints:

1.
cin-ccenter
pin-pcenter
 dinc
2.
cin
pin
 dmax

3.
C-cin
P-pin
 dmin
4. pmin  pin  pmax
This potentially difficult maximization problem could be solved by
convex programming, but is made much easier by the properties of
Population =


D_1 density =
p_center


c_center / p_center
S'




D_1 density = d_min
D_1 density = d_inc

D_1 density = d_max
Population = P_12 - p_center
Population = P_23-P_12
Population = P-P_23




Figure 4: Division of region S into layers of differing density.
In the typical case, subregion S includes all but the outer layer.


the density function D. Since
D
C
 0, we know that the maximum
value of D for a given pin occurs when cin is maximized subject to
the constraints. We solve the first three constraints for cin, giving
us cin = min(C1,C2,C3), where:

C1 = dincpin -(dincpcenter -ccenter) = dincpin -B1

C2 = dmaxpin


C3 = dminpin +(C -dminP) = dminpin +B3
In the typical case,4 we have dmin  dmax  dinc, B1 > 0, and B3 >
0: this means that cin = C1 for small pin, cin = C2 for moderate pin,
and cin = C3 for large pin, as illustrated in Figure 3. Thus we can
solve for the intersection points P12, P13, and P23, where Ci  Cj
for pin  Pij, and we use these quantities to find the maximum
allowable count cin for a given pin. Solving the equations, we find
that P12 =
B1
dinc-dmax
, P13 =
B1+B3
dinc-dmin
, and P23 =
B3
dmax-dmin
. In the
typical case,5 we have 0 < P12  P13  P23 < . In this case, we
use the values of P12 and P23, and the value P13 is not needed. Then
the count cin = ccenter + dinc(pin - pcenter) for pcenter  pin  P12,
cin = dmaxpin for P12  pin  P23, and cin = dmaxP23 + dmin(pin -
P23) for pin  P23. This is illustrated by Figure 4: the region S
is separated into four "layers" of differing densities. Starting from
the inside, we have the center (with a known population pcenter and
count ccenter), a layer of high D1 density dinc, a layer of moderate
D1 density dmax, and a layer of low D1 density dmin.
Now we can write cin as a function of pin, and thus the score D
becomes a function of the single variable pin. Where does the max-
imum of this function occur? Again we rely on properties of the
function D(C,P), and a case-by-case analysis is necessary. In the
typical case dinc > dmax >
ccenter
pcenter
, we know that the score increases
with population in the "high density" and "moderate density" lay-
ers. This follows from two properties of our density function:
D
C

0 and
D
P
+
C
P
D
C
 0. In the high density layer, the D1 density
of S
cin
pin
increases from
ccenter
pcenter
to dmax as we add more popula-
tion, so the score D is monotonically increasing with population. In
the moderate density layer, the D1 density of S stays constant (at
dmax) as population increases, so again D is monotonically increas-
ing. In the low density layer, D1 density of S decreases as popu-
lation increases: in this case, since count and population are both

4
We must also handle a variety of special cases where one or more
of these inequalities are violated, and some constraints may not be
relevant. We omit the details of this case-by-case analysis.
5
See previous note.



260
Research Track Paper

S_1
S_2




S_4S_3
S_C4
S_C1 S_C2

S_C3




Figure 5: Quartering of region S


increasing, the score may increase or decrease. We assume that the
score function D has no local maxima in the interval (P23,P), and
thus that the maximum occurs either at (cin, pin) = (dmaxP23,P23)
or at (cin, pin) = (C,P).6 We are only interested in finding sub-
regions with scores higher than the parent, so we can ignore the
latter case. Thus our upper bound on D(S ) is D(cin, pin), where
pin = P23 and cin = dmaxpin. The various special cases, where one
or more of the inequalities above are violated, are handled simi-
larly using the intersection points P12, P13, and P23 as necessary.
We also must adjust our value of pin if it violates the inequality
pmin  pin  pmax, adjusting cin accordingly given the density of
the layers being added or subtracted.
We now consider how the bounds on populations and D1 densi-
ties are obtained. The simplest method of doing so is to use global
values: first, we precompute the minimum and maximum popu-
lation and D1 density of all single squares sij in the grid. This
gives us usable (though very conservative) values for dmin, dmax,
and dinc. We can also use the minimum and maximum square pop-
ulations, together with the minimum and maximum area of a re-
gion, to obtain bounds pmin and pmax. Slightly less conservative
bounds can be obtained using our assumption of a minimum region
size kmin = 3. Any k1 × k2 region, where k1,k2  3, can be tiled
with rectangles of sizes 3  k1,k2  5. Thus we can precompute
the minimum and maximum D1 density and population per square
of all such rectangles in O(N2) time, and use these rather than the
single square bounds when allowable. For example, when bound-
ing maximum score of the outer regions of S, we can use the less
conservative bound for dmax; when bounding maximum score of
all subregions of S, we can also use the less conservative bound for
dinc.
These global bounds on populations and D1 densities are inex-
pensive to compute (we need only compute them once per grid),
but are very conservative estimates of the densities of squares in a
given region. We use these bounds in our algorithm as a sort of
"first pass" which prunes many regions but also leaves many un-
pruned. If a region survives this round of pruning, we compute
much tighter bounds on subregion scores in a "second pass," which
is also more computationally expensive. To do so, we obtain tighter
bounds on the population and D1 density of S using a novel tech-
nique we term quartering, then use these constraints to bound D(S )
as above.
Given a region S of size k1 ×k2, with a (non-zero) center region
SC, the quartering procedure calculates bounds on subregion popu-
lation and D1 density in O(k1k2) time. The first step of quartering

6
Formally, we assume the following constraint on the first and sec-
ond partials of D: D2PDCC +D2CDPP -2DCDPDCP  0. This is true
for a large class of functions, including Kulldorff's statistic. If this
constraint is violated, we must also calculate D(C,P) at each local
maximum, which is not difficult if the number of maxima is small
and each maximum is easy to calculate.
is to divide S into its four (non-overlapping) quadrants S1 ...S4, as
in Figure 5. We now consider each Si separately, together with the
quarter of the center (SCi) which overlaps that quadrant. For each
quadrant, we consider all rectangles Si with one corner at the cen-
troid of S, and one corner outside SCi (i.e. on one of the dots in
Figure 5). Note that there are O(k1k2) such rectangles, and thus
we can search over all of these regions Si in quadratic time. Our
search procedure is very simple: given a region Si, let pin, cin,
and Ain denote its population, count, and area; let pout, cout, and
Aout denote the population, count, and area of Si -Si; and let pdif ,
cdi
f
, and Adi
f
denote the population, count, and area of Si - SCi.
We then calculate the D1 density d and the average population per
square ps for each of Si, Si -Si, and Si -SCi: din =
cin
pin
, ps
,in
=
pin
Ain
,
and the other quantities (dout, ddi
f
, ps
,out
, ps
,di f
) are defined simi-
larly. We then set dmax equal to the maximum of all din, dinc equal
to the maximum of all ddi
f
, and dmin equal to the minimum of
all dout. Similarly, we take the minimum and maximum values of
ps
,in
, ps
,out
, and ps
,di f
; we can use these to calculate bounds pmin
and pmax once we are given the minimum and maximum area of
S . In essence, what we have done is bounded the D1 densities and
populations for the piece of region S contained in each quadrant.
Then since D1 density is monotonic, we know that the D1 density
of the entire region S is bounded by the maximum of the max-
densities and the minimum of the min-densities computed for all
regions Si. Population per square is also monotonic, so an identical
argument applies. Another way to think of this is that we are cal-
culating bounds on population and D1 density for all the irregular
(but rectangle-like) regions containing the center CS and consist-
ing of one rectangle in each quadrant, as drawn in Figure 5; then
these quantities are also bounds on the population and density of
all rectangles which contain CS.
We do not provide a formal proof here, but we note that the
bounds on population and density derived by quartering are ex-
act (i.e. no rectangle S  S, such that SC  S , can have density
or population outside these bounds) and that they are much tighter
than the global population and density bounds, allowing many more
regions to be pruned. However, as noted above, quartering is sig-
nificantly more computationally expensive than using the global
bounds, taking time quadratic in the size of region S, and thus
O(N2) for large regions. This is why we first use the global bounds
for pruning outer regions, and only use quartering on regions that
this initial pruning does not eliminate. We also note that quarter-
ing can be done in linear time in the special case where the parent
region S and its center SC have the same row size k1 or the same
column size k2; in this case we need only divide the region into two
halves, and each half can be searched linearly and the bounds com-
bined. We apply this linear-time "halving," as well as the standard
quadratic-time quartering, in our algorithm presented below.

2.2 The algorithm
We now possess all of the algorithmic and statistical tools needed
to present our algorithm in full. The basic structure is similar to
the top-down "overlap-search" routine presented above, with sev-
eral important differences. First, we use a best-first search (im-
plemented using a pair of priority queues q1 and q2) rather than a
recursive depth-first search. Our algorithm has two stages: in the
first stage we examine only gridded regions, and in the second stage
we search outer regions if necessary. In both stages, we prune re-
gions whenever possible, calculating increasingly tight bounds on
subregions' population and D1 density, and using these to calcu-
late upper bounds Dmax on D(S ) as above. The first stage of our
algorithm proceeds as follows, using the (loose) global bounds on
population and D1 density to calculate Dmax:



261
Research Track Paper

Add G to q_1.
While q_1 not empty:
Get region S with highest D(S) from q_1.
If D(S) > mrd, set mdr = S and mrd = D(S).
If D_max(S' in S) > mrd,
add gridded children of S to q_1 (using lazy expansion).
If D_max(S' in S containing S_C) > mrd, add S to q_2.


Thus, after the first stage of our algorithm, we have searched or
pruned all gridded regions (requiring at most O((N logN)2) time),
and the current mdr is the gridded region with highest D(S). q2
now contains the subset of gridded regions whose outer regions
have not yet been pruned, prioritized by their upper bounds Dmax.
The second stage of our algorithm proceeds as follows:

While q_2 not empty (and some S on q_2 has D_max(S) > mrd):
Get region S with highest D_max(S) from q_2.
Use quartering to calculate tighter pop and density bounds for S.
Recalculate D_max(S) using these bounds.
If D_max(S) > mrd, then search-outer-regions(S).


Now the only question left is how to perform the search-outer-
regions procedure. We first note that a rectangular region requires
four coordinates for specification: we use the row size k1, the col-
umn size k2, the minimum row x1, and the minimum column x2.
Then a naive search of the outer regions of S could be done using
four nested loops, stepping over each legal combination of these
four coordinates (i.e. such that the resulting region S is in S and
contains SC). We also use four nested loops (in the order k1, x1,
k2, x2), but take several more opportunities for pruning. Once we
have fixed k1(S ) and x1(S ), we can obtain a very tight bound on
Dmax(S ) by expanding the center region SC and contracting the
parent region S such that k1(S) = k1(SC) = k1(S ) and x1(S) =
x1(SC) = x1(S ). We then recalculate bounds on the D1 density
and population using the new S and SC (this can be done in linear
time using "halving," the special case of quartering, since the par-
ent and center have the same k1), and finally recompute Dmax for
the new parent and center. Only if Dmax is greater than the mrd
do we need to loop over k2 and x2 for that combination of k1 and
x1. Finally, once we have fixed k2, we can recompute Dmax again,
since we now precisely know the area of S , giving us much tighter
population bounds. Only if Dmax is greater than the mrd must we
search all x2 for that combination of k1, x1, and k2.
Thus the second stage of our algorithm can be seen as a series
of "screens" that an outer region must pass through if it is to be
searched. The first screen is whether the parent region is taken off
q2 and examined, the second screen is whether the parent region
passes the "quartering" test, the third screen is whether the new
parent region (formed after k1 and x1 are fixed) passes the "halving"
test, and the fourth screen is whether the new parent region passes
the "halving" test once the area of S is fixed. We now examine the
complexity of this procedure, given a large parent region (i.e. one
containing O(N4) outer regions S ). If the parent region does not
pass the first screen, we have spent only O(1) to search these O(N4)
regions; if the parent does not pass the second screen, we have spent
only the O(N2) time required by quartering. If the parent passes the
second screen, but none of the new parent regions pass through the
third and fourth screens, we have spent only O(N2) × O(N) (for
halving, given each k1 and x1) + O(N3) (for bounding, given each
k1, x1, and k2) = O(N3) time. Thus only if all four screens fail will
the algorithm have O(N4) complexity; typically well over 90% of
regions are eliminated at each screen, and thus we search only a
small fraction of possible regions.
3. A USEFUL APPROXIMATION
As opposed to our previous results [7], the algorithm presented
above gives large speedups as compared to the naive approach with-
out approximation: the algorithm is guaranteed to find the max-
imum density rectangular region. In some cases, however, very
rapid detection may be more important than guaranteed accuracy;
thus we present an approximate version of the algorithm which
finds the mdr 5-20x faster while maintaining over 90% accuracy.
As noted above, the "first pass" of our algorithm uses very con-
servative bounds on the D1 densities of S , S - SC, and S - S , de-
rived from the global minimum and maximum density values. Thus
one way to increase the speed of the algorithm is to use a closer
approximation of these densities as a bound. The disadvantage is
that if we use an estimate which is not guaranteed to bound the
densities, we may underestimate the score of a region, and hence
possibly prune away the mdr and find an incorrect region. Here we
consider an approximate lower bound on the D1 density of S - S :
using this bound instead of a guaranteed but much more conserva-
tive bound typically results in large speedups with minimal loss of
accuracy.
To derive tighter bounds on the maximum density of a subre-
gion S contained in a given region S, we consider the assumptions
being made by our statistical test. Kulldorff's statistic assumes,
both under the null hypothesis and the alternative hypothesis, that
at most one disease cluster Sdc exists, and that the disease rate q
is expected to be uniform outside Sdc (or uniform everywhere, if
no disease cluster exists). Thus, if Sdc is contained entirely in the
region under consideration S, we would expect that the maximum
density subregion S of S is Sdc, and that the disease rate of S-S is
equal to the disease rate outside S: E
C-cin
P-pin
=
Ctot-C
Ptot-P
= dout. As-
suming that the D1 density of S - S is equal to its expected value
dout, we can use the derivation above (using dout in place of dmin)
to find the maximum subregion score.
The problem with this approach is that we have not compensated
for the variance in densities. Our calculated value of dout is only
a lower bound for the D1 density of S - S in the most approxi-
mate probabilistic sense, in that we expect D1(S - S ) > dout half
the time. We can improve the accuracy of our probabilistic bound
by also considering the variance of
C-cin
P-pin
-
Ctot-C
Ptot-P
. Assuming that
all counts outside Sdc are generated by a inhomogeneous Poisson
distribution with parameter qpi
j
, we obtain: 2
C-cin
P-pin
-
Ctot-C
Ptot-P
=

2
Po(q(P-pin))
P-pin
-
Po(q(Ptot-P))
Ptot-P
=
q
P-pin
+
q
Ptot-P
=
q(Ptot-pin)
(P-pin)(Ptot-P)
.

Since the actual value of the parameter q is not known, we use a
conservative empirical estimate: q =
Ctot
Ptot-pin
. From this, we obtain


C-cin
P-pin
-
Ctot-C
Ptot-P
=
Ctot
(P-pin)(Ptot-P)
. Then we can compute the
maximum subregion density by using dout -b, for some constant
b, in place of dmin in the derivation above. One minor complication
is that, since  is dependent on pin, we must solve equations for P13
and P23 which are quadratic rather than linear; we omit the details
of this calculation.
By adjusting our approximation of dmin in this manner, we com-
pute a higher score D, reducing the likelihood that we will un-
derestimate the maximum subregion density and prune a region
that should not necessarily be pruned.7 Given a constant b, the
D1 density of S - S will be greater than dout - b with probabil-
ity Pr(Z < b), where Z is chosen randomly from the unit normal.
For b = 2, there is an 98% chance that we will correctly bound


7
This also increases the number of regions searched, and thus we
have a tradeoff between speed and accuracy.



262
Research Track Paper

D1(S - S ), giving a guaranteed correct upper bound for the max-
imum subregion score. In practice, the maximum score will be
lower than our approximate bound more often than this, since our
estimates for the other parameters are conservative. Thus, though
our algorithm is approximate, it is very likely to converge to the
globally optimal mdr.
One interesting feature of this approximation is that we expect
to underestimate the maximum subregion score if the disease clus-
ter Sdc is not contained entirely in S, since we are calculating dout
based on a region which includes this region of higher density. In
cases where there is only a single disease cluster, this is acceptable
(and desirable) since a region not containing Sdc does not need to
be expanded. In applications where multiple disease clusters are
present, however, there is a risk that the presence of one significant
disease cluster will cause the approximate algorithm to miss an-
other more significant cluster. This phenomenon is visible in our re-
sults below: in two of the three real-world datasets, the approximate
algorithms did not find the maximum density region, though they
did find another cluster that was also significant. In the cases where
only one disease cluster was present, as in our simulated trials, the
approximate algorithms achieved high accuracy. We present results
for the exact and approximate versions of the algorithm below.


4. RESULTS
We first describe results with artificially generated grids and then
real-world case data. An artificial grid is generated from a set of
parameters (N, k1, k2, µ, , q , q ). The grid generator first creates
an N × N grid, and randomly selects a k1 × k2 "test region." Then
the population of each square is chosen randomly from a normal
distribution with mean µ and standard deviation  (populations less
than zero are set to zero). Finally, the count of each square is chosen
randomly from a Poisson distribution with parameter qpi
j
, where
q = q inside the test region and q = q outside the test region.
For all our simulated tests, we used grid size N = 256, and a
background disease rate of q = .001. We tested for four different
combinations of test region parameters (k1 × k2, q ): (7 × 9, .01),
(11×5, .002), (4×3, .002), and (0×0, .001). These represent the
cases of an extremely dense disease cluster, large and small disease
clusters which are significant but not extremely dense, and no dis-
ease cluster respectively. We used three different population distri-
butions for testing: the "standard" distribution (µ = 104,  = 103),
and two types of "highly varying" populations. For the "city" distri-
bution, we randomly selected a 10×10 "city region": square pop-
ulations were generated with µ = 5 × 104 and  = 5 × 103 inside
the city, and µ = 104 and  = 103 outside the city. For the "high-"
distribution, we generated all square populations with µ = 104 and
 = 5 × 103. For each combination of test region parameters and
population distribution, run times were averaged over 20 random
trials, and an additional 90 trials (for a total of 110) were used to
test accuracy. A trial was counted "correct" if the algorithm either
found the test region Sdc, or another region S with D(S)  D(Sdc);
we emphasize that this is always the case for the exact version of
our algorithm, which always finds the maximum density region.8
We also recorded the average number of regions examined; for
our algorithm, this includes calculation of score bounds as well as
scores of individual regions. Separate results are presented for the
original grid and for each replica; for a large number of random
replications (R = 1000) the results per replica dominate, since total

8
In additional to our theoretical argument for correctness, we con-
firmed this empirically by running a large number of tests on
smaller grids, for all possible test region sizes. These results are
not given here, but we note that the algorithm found the correct
region in all cases.
Figure 6: Emergency Department dataset.
The left pic-
ture shows the "population" distribution and the right picture
shows the "counts." The winning region is shown as a rectangle.



run time is torig + R(trep) to search the original grid and perform
randomization testing. See Table 1 for results.
Our first observation was that the run time and number of re-
gions searched were not significantly affected by the underlying
population distribution; typically the three results differed by only
5-10%, and in many cases test regions were found faster for the
highly varying distributions than the standard distribution. Thus
Table 1, rather than presenting separate results for each population
distribution, presents the average performance over all three popu-
lation distributions for each test. This result demonstrates the ro-
bustness of the algorithm to highly non-uniform populations; this
is very different than our previous work [7], where the algorithm
was severely slowed by highly varying populations. The exact al-
gorithm achieved average speedups ranging from 35x (for no test
region), to 2300x (for an extremely dense test region) as compared
to the naive approach. We note that, for the case of no test re-
gion, it is typically not necessary to run more than 10-20 random-
izations before vconcluding that the discovered region is not sig-
nificant; thus our true average "worst-case" results will be closer to
the 95x speedup on small, significant (but not extremely dense) test
regions. Since the naive approach requires approximately 45 days
for a 256 × 256 grid with R = 1000, this suggests that our exact
algorithm can complete the same task in less than 12 hours. We
also tested two approximate variants of the algorithm, "approx-2"
and "approx-3," with adjustments for density variance b = 2 and
b = 3 respectively. These variants achieved up to 5000x speedups,
with over 1000x speedups whenever a test region was present, and
84-173x speedups in the "no test region" case, enabling us to find
the maximum density region and its significance in less than 1 hour.
While all variants of the algorithm achieved 100% accuracy on the
very dense test regions, the approximate versions missed some of
the less dense test regions. For the larger (11 × 5) test regions,
approx-2 and approx-3 achieved 98.5% and 99.7% accuracy re-
spectively (averaged over the 330 trials); for smaller (4 × 3) test
regions, these accuracies were reduced to 93.0% and 98.2% respec-
tively. In some cases, the guaranteed accuracy of the exact algo-
rithm may be more necessary than the additional speedups gained
by the approximate algorithm; in other cases, extremely fast results
are needed, and an approximation may be sufficient.
We now discuss the performance of the algorithm on various
real-world datasets. Our first test set was a database of anonymized
Emergency Department data collected from Western Pennsylvania
hospitals in the period 1999-2002. This dataset contained a total
of 630,000 records, each representing a single ED visit and giv-
ing the latitude and longitude of the patient's home location to the
nearest .005 degrees (
1
3
mile, a sufficiently low resolution to en-
sure anonymity). These locations were mapped to three grid sizes:



263
Research Track Paper

Table 1: Performance of algorithm, simulated datasets, N = 256
test
method
sec/orig
speedup
sec/rep
speedup
regions (orig)
regions (rep)
accuracy
all
naive
3864
x1
3864
x1
1.03B
1.03B
100%
7x9, 0.01
exact
5.47
x706
1.68
x2300
100K
1.20K
100%
approx-2
0.83
x4659
0.74
x5245
2.69K
16
100%
approx-3
0.86
x4475
0.76
x5091
2.76K
16
100%
11x5, 0.002
exact
21.72
x178
12.43
x311
1.03M
196K
100%
approx-2
1.39
x2780
0.77
x4992
87.2K
1.59K
98.5%
approx-3
1.72
x2246
0.74
x5207
134K
2.29K
99.7%
4x3, 0.002
exact
42.96
x90
40.57
x95
2.59M
1.87M
100%
approx-2
3.10
x1248
1.80
x2143
346K
94.7K
93.0%
approx-3
5.75
x672
3.20
x1209
738K
227K
98.2%
no region
exact
189.68
x20
110.25
x35
27.4M
12.7M
-
approx-2
36.67
x105
22.27
x173
5.75M
4.34M
-
approx-3
89.97
x44
45.90
x84
16.95M
9.65M
-




N = 128, 256, and 512. For each grid, we tested for spatial clus-
tering of "recent" disease cases: the "count" of a square was the
number of ED visits in that square in the last two months, and the
"population" of a square was the total number of ED visits in that
square. See Figure 6 for a picture of this dataset, including the
highest scoring region. For each of these grids, the exact and ap-
proximate versions of our algorithm found the same, statistically
significant region (p-value 0/1000) as the naive approach. The ma-
jor difference, of course, was in runtime and number of regions
searched (see Table 2). Our algorithms found the mdr of the orig-
inal grids 22-31x faster than the naive approach; however, much
faster performance was achieved when searching the replica grids.
The exact algorithm achieved speedups increasing from 450x to
4700x as grid size increased from 128 to 512; the approximate ver-
sions did even better, achieving 2300-24000x speedups.
Our second test set was a nationwide database of retail sales of
over-the-counter cough and cold medication. Sales figures were re-
ported by zip code; the data covered 5000 zip codes across the U.S.,
with highest coverage in the Northeast. In this case, our goal was
to see if the spatial distribution of sales on a given day (2/14/2004)
was significantly different than the spatial distribution of sales a
week before (2/7/2004), and to identify a significant cluster of in-
creased sales if one exists. Thus we used the sales on 2/7 as our un-
derlying population distribution, and the sales on 2/14 as our count
distribution. Slight modifications to Kulldorff's statistic were nec-
essary to deal with regions with zero population and nonzero count
(i.e. sales on 2/14 but not 2/7). We created four grids from this data,
two using all of the national data, and two using only data from
the Northeast (where a greater proportion of zip codes report sales
data). For both "national" and "regional" over-the-counter data,
we created grids of sizes N = 128 and N = 256, converting each
zip code's centroid to a latitude and longitude. For each of these
four grids, our exact algorithm found the same statistically signif-
icant region (p-value 0/1000) as the naive approach, and achieved
speedups of 96-132x on the 128 × 128 grids and 440-739x on the
256 × 256 grids. The approximate versions of the algorithm did
not find the correct region on these four grids, and thus we do not
include these in Table 2. We note, however, that they did find an-
other statistically significant region, though with a lower score than
the mdr; it is possible that the presence of this region caused the
algorithms to miss the most significant region, as discussed above.
Thus the exact version of our algorithm found the maximum
density region in all of our simulated and real-world trials, while
achieving speedups of at least 20x (and typically much larger) as
compared to the naive spatial scan. The approximate versions of
the algorithm achieved much larger speedups, though at the cost of
occasionally failing to find the correct region. This speedup is ex-
tremely important for the real-time detection of disease outbreaks:
if a system is able to detect an outbreak in minutes rather than days,
preventive measures or treatments can be administered earlier, pos-
sibly saving many lives. We believe that our algorithm will be use-
ful for rapid detection of significant spatial clusters in a variety of
other applications as well.


5. CONCLUSIONS AND FUTURE WORK
Thus we have presented a fast multiresolution partitioning algo-
rithm for detection of significant spatial overdensities, and demon-
strated that this method results in significant (20-2000x) speedups
on real and artificially generated datasets. We are currently ap-
plying this algorithm to national-level hospital and pharmacy data,
attempting to detect disease outbreaks based on statistically signifi-
cant changes in the spatial clustering of disease cases. Our eventual
goal is the automatic real-time detection of outbreaks, and applica-
tion of a fast partitioning method using the techniques presented
here may allow us to achieve this difficult goal.
Additionally, we are extending the algorithm (and the underlying
overlap-kd tree data structure) in various ways, making it usable for
a broader range of application domains. Most importantly, overlap-
kd trees can be extended to higher dimensions, as can the other
techniques (e.g. quartering) used in our multiresolution search. We
note, however, that various quantities (for example, number of chil-
dren of a node) grow exponentially with dimension, so overlap-kd
trees are probably not appropriate for very high dimensional data.
Nevertheless, we hope that our techniques will be useful for various
3-D (and higher dimensional) applications, including the discovery
of regions of significantly increased brain activity (corresponding
to given cognitive tasks) using fMRI data. As discussed above, we
also are actively engaged in deriving more powerful statistical tests
for overdensities (and the corresponding density functions) under a
variety of application-specific models (for example, normally dis-
tributed counts inferred from the time series of previous counts,
applied to the over-the-counter retail data). As long as the den-
sity function satisfies the simple conditions described above, our
algorithm can be used to rapidly find the maximum density region
according to this function. Finally, we are interested in extending
our search for overdensities to more general classes of multivariate
density functions, thus allowing the discovery of clusters which are
significant even after adjusting for multiple covariates.



264
Research Track Paper

Table 2: Performance of algorithm, real-world datasets
test
method
sec/orig
speedup
sec/rep
speedup
regions (orig)
regions (rep)
ED
naive
72
x1
68
x1
62.0M
62.0M
(N = 128)
exact
3
x24
0.15
x453
5.12M
15.9K
approx-2
3
x24
0.03
x2266
4.50M
510
approx-3
3
x24
0.03
x2266
4.55M
364
ED
naive
1207
x1
1185
x1
1.03B
1.03B
(N = 256)
exact
55
x22
1.2
x988
95.9M
74.7K
approx-2
41
x29
0.14
x8464
69.8M
2.58K
approx-3
42
x29
0.14
x8464
71.0M
2.10K
ED
naive
19146
x1
18921
x1
16.8B
16.8B
(N = 512)
exact
854
x22
4.0
x4730
1.51B
120K
approx-2
626
x31
0.8
x23651
1.10B
13.1K
approx-3
626
x31
0.8
x23651
1.12B
13.2K
national OTC
naive
71
x1
77
x1
62.0M
62.0M
(N = 128)
exact
2
x36
0.8
x96
682K
200K
national OTC
naive
1166
x1
1232
x1
1.03B
1.03B
(N = 256)
exact
14
x96
2.8
x440
3.24M
497K
regional OTC
naive
78
x1
79
x1
62.0M
62.0M
(N = 128)
exact
2
x39
0.6
x132
783K
101K
regional OTC
naive
1334
x1
1330
x1
1.03B
1.03B
(N = 256)
exact
13
x103
1.8
x739
3.10M
168K



6. ACKNOWLEDGEMENTS
This work was supported in part by NSF Grant IIS-0325581.


7. REFERENCES
[1] R. Agrawal, J. Gehrke, D. Gunopulus, and P. Raghavan.
Automatic subspace clustering of high dimensional data for
data mining applications. In Proc. ACM-SIGMOD Intl. Conf.
on Mgmt. of Data, pages 94­105, 1998.
[2] K. Deng and A. W. Moore. Multiresolution instance-based
learning. In Proc. 12th Intl. Joint Conf. on Artificial
Intelligence, pages 1233­1239, 1995.
[3] S. Goil, H. Nagesh, and A. Choudhary. MAFIA: efficient and
scalable subspace clustering for very large data sets.
Technical Report CPDC-TR-9906-010, Northwestern
University, 1999.
[4] M. Kulldorff. A spatial scan statistic. Communications in
Statistics: Theory and Methods, 26(6):1481­1496, 1997.
[5] M. Kulldorff. Spatial scan statistics: models, calculations,
and applications. In J. Glaz and N. Balakrishnan, editors,
Scan Statistics and Applications, pages 303­322. Birkhauser,
1999.
[6] M. Kulldorff and N. Nagarwalla. Spatial disease clusters:
detection and inference. Statistics in Medicine, 14:799­810,
1995.
[7] D. B. Neill and A. W. Moore. A fast multi-resolution method
for detection of significant spatial disease clusters. In
S. Thrun, L. Saul, and B. Sch¨olkopf, editors, Advances in
Neural Information Processing Systems 16. MIT Press, 2004.
[8] S. Openshaw, M. Charlton, A. Craft, and J. Birch.
Investigation of leukemia clusters by use of a geographical
analysis machine. Lancet, 1:272­3, 1988.
[9] F. Preparata and M. Shamos. Computational Geometry: An
Introduction. Springer-Verlag, New York, 1985.
[10] H. Samet. The Design and Analysis of Spatial Data
Structures. Addison-Wesley, Reading, MA, 1990.
[11] L. Waller, B. Turnbull, L. Clark, and P. Nasca. Spatial
analysis to detect disease clusters. In N. Lange, editor, Case
Studies in Biometry, pages 3­23. Wiley, 1994.
[12] W. Wang, J. Yang, and R. Muntz. STING: a statistical
information grid approach to spatial data mining. In Proc.
23rd Conference on Very Large Databases, pages 186­195,
1997.




265
Research Track Paper

