Mining
Features
for Sequence
Classification
Neal Leshl, Mohammed
J. Zaki2, Mitsunori
Ogihara3
leshQmerl.com, zaki@cs.rpi.edu, ogiharaQcs.rochester.edu

1 MERL - Mitsubishi Electric Research Laboratory, 201 Broadway, 8th Floor, Cambridge, MA 02139
' Computer Science Dept., Rensselaer Polytechnic Institute, Troy, NY 12180
3 Computer Science Dept., U. of Rochester, Rochester, NY 14627



Abstract
Classification algorithms are difficult to apply to sequential
examples becausethere is a vast number of potentially useful
features for describing each example. Past work on feature
selection has focused on searching the space of all subsets
of features, which is intractable for large feature sets. We
adapt sequencemining techniques to aEi as a preprocessor
to select features for standard classification algorithms such
as Naive Bayes and Winnow.
Our experiments on three
different datasets show that the features produced by our
algorithm improve classification accuracy by lo-50%,

1
Introduction
Some classification algorithms work well when there are
thousands of features for describing each example (e.g,
[Littlestone,
19881). In some domains, however, the
number of potentially
useful features is exponential
in
the size of the examples. Data mining algorithms
(e.g.,
[Zaki, 19981) have been used to search through billions
of rules, or patterns,
and select the most interesting
ones. In this paper, we adapt data mining techniques
to act as a preprocessor to construct a set of features
to use for classification.
In past work, the rules produced by data mining
algorithms
have been used to construct
classifiers
primarily
by ordering the rules into decision lists (e.g.
[Segal and Etzioni, 1994, Liu et al., 19981)or by merging
them into more general rules that occur in the training
data (e.f,
[Lee et @., 1998]~
In this paper, we
convert t e patterns discovered
y the mmmg algorithm
into a set of boolean features to feed into standard
classification
algorithms.
The classification
algorithms,
in turn,
assign weights to the features which allows
evidence from different rules to be combined in order
to classify a new example.
While
there has been a lot of work on feature
selection, it has mainly concentrated on non-sequential
domains.
In contrast,
we focus on sequence data in
which each example is represented as a sequence of
"events", where each event might be described by a set
of predicates. Examples of sequence data include text,
DNA sequences, web usage data, and execution traces.
In this
paper we combine
two powerful
mining
paradigms:
sequence mining,
which
can efficiently
search for patterns that are correlated with the target

pcmission
to make
digital
or
hardtopics of all or Partof this work fol
Personalor classroomUSCis grantedwithout feeprovided that copies
arenot lnadeor distributed forprofit or commercialadvantageand that
copieshearthis notice and the full citation on the lirst Page.`fo coPY
otllcl+se, torepublish,toposton serversor to rcdistributcto lists,
requiresprior specific permissionand/ora fee.
KDD-99 San Diego CA USA
CopyrightACM 1999l-581 13-143-7/99/08...%.00
classes, and classification,
which learns to weigh evi-
dence from different features to classify new examples.
We present FEATUREMINE,
a scalable disk-based fea-
ture mining algorithm.
We also specify criteria for se-
lecting good features, and present pruning
rules that
allow for more efficient feature minin
FEATUREMINE
integrates pruning constraints in the afgorithm itself, in-
stead of post-processing, enabling it to efficiently search
through large pattern spaces.

2
Data
mining
for features
We now formulate and present an al
minin
8. *
Let 3 be a set of distinct
eatures, each with
B
orithm for feature

some mte set of possible values. Let Z be the set of all
possible feature-value
pairs. A sequence is an ordered
list of subsets of Z. For example, if Z = {A, B, C.. .},
then an example sequence would be Al? -+ A + BC. A
sequence a! is denoted as ((~1 + CYZ+ ... -+ a,) where
each sequence element cri is a subset of 1. The length
of sequence (CQ + a2 + ... + a,) is n and its width
is the maximum size of any CY~for 1 5 i 5 n. We say
that cyis a subsequence of /3, denoted as (Y4 /I, if there
exists integers il < i2 < ... < i, such that aj C /3ij
for all CX~.For example, Al3 -+ C is a subsequence of
AB + A + BC.
Let C be a set of class labels.
An
example is a pair (a,~) where (Y= (~1+ ~2 + ... + cr,
is a sequence and c E C is a label. Each example has a
unique identifier
eid, and each (pi has a time-stamp
at
which it occurred. An example (cu,c) is said to contain
sequence p if p + 01.
Our input database 2) consists of a set of examples.
This means that the data we look at has multiple
sequences, each of which is composed of sets of items.
The frequency of sequence p in 2), denoted
Pd
p, V , is
the fraction of examples in V that contain
.
et
be
a sequence and c be a class label.
The confidence of
the rule p =+-c, denoted con
4
,0,c, D), is the conditional
probability
that c is the labe of an example in 2) given
that it contains sequence p. That is, conflp,c,rD)
=
fr(P, ?cffr(P,
v),
where 2), is the subset of examples
m D wit
class la el c. A sequence is said to be frequent
if its frequency is more than a user-specified nain-freq
threshold.
A rule is said to be strong if its confidence is
more than a user-specified min-confthreshold.
Our goal
is to mine for frequent and strong patterns.
Figure 1
shows a database of examples. There are 7 examples,
4 belonging to class cl, and 3 belonging to class ~2.
In general there can be more than two classes. We are
loo';`,"
. E*
for different minJreq in each class. For example,
is frequent for class ~2, it s not frequent for class
cl. The rule C + c2 has confidence 3/4 = 0.75, while
the rule C +- cl has confidence l/4 = 0.25.
A sequence classifier is a function
from sequences to
C. A classifier can be evaluated using standard metrics




342

New Boolean Features

EID
A
C
A->A
AB
A-9
B>A
8-B
C-z-A
AB-9
Class

1

2




6

7


gure 1: A) Original Database, B) New Database w
jolean Features
such as accuracy and coverage.
Finally, we describe how frequent sequences /?I, ....,&.
can be used as features for classification.
Recall that
the input to most standard classifiers is an example
represented
as vector
of feature-value
pairs.
We
represent a example sequence cy as a vector of feature-
value pairs by treatin
each sequence /?i as a boolean
feature that is true 1 ,& 3 cr. For example, suppose
2
the features are fi
= A + D, f2 = A -+ BC, and
f3 = CD.
The sequence AB + BD + BC woul~,",~
represented as (fl, true), (fi, true
that features can "skip'
steps:
, (fs, false).
tIie feature A -+ BC
holds in AB + BD -+ BC.

2.1
Selection
criteria
for mining
We now specify our selection
criteria
for selecting
features to use for classification.
Our objective is to
find sequences such that representing
examples with
these sequences will yield a highly accurate sequence
classifier.
However, we do not want to search over
the space of all subsets of features
[Caruana
and
Freitag,
1994]), but instead want to evaluate each
feature in isolation or by pair-wise comparison to other
candidate features. Certainly, the criteria for selecting
features might depend on the domain and the classifier
being used. We believe, however, that the following
domain-and-classifier-independent
heuristics are useful
for selecting sequences to serve as features:
1) Features should be frequent.
2) Features should be distinctive
of at least one class.
3) Feature sets should not contain redundant features.
The intuition
behind the first heuristic is simply that
rare features can, by definition,
only rarely be useful for
classifying examples. In our problem formulation,
this
heuristic translates into a requirement that all features
have some minimum frequency in the training set. Note
that since we use a different
min-freq for each class,
patterns that are rare in the entire database can still
be frequent for a specific class. We only ignore those
patterns which are rare for any class. The intuition
for
the second heuristic
is that features that are equally
likely in all classes do not help determine which class
an example belongs to.
Of course, a conjunction
of
multiple non-distinctive
features can be distinctive.
In
this case? our algorithm
prefers to use the distinctive
conjunctron as a feature rather than the non-distinctive
conjuncts.
We encode this heuristic by requiring that
each selected feature be significantly
correlated with at
least one class that it is frequent in.
The motivation
for our third heuristic is that if two
features are closely correlated with each other, then
either of them is as useful for classification
as both
are together.
We show below that we can reduce
the number of features and the time needed to mine
for features by pruning
redundant
rules.
In addition
to wanting to prune features which provide the same
information,
we also want to prune a feature if there
is another feature available that provides strictly
more
information.
Let M(f,V
be the set of examples in 2,
that contain feature f. IJ e say that feature fl subsumes
feature f2 with respect to predicting
class c in data
set 2) iff M(fz,D,)
C M(fl,Dz),)
and M(fl,D-,)
G
M(f2,2>,,).
Intuitively,
if fi subsumes f2 for class c
then fi is superior to f2 for predicting
c because fl
covers every example of c in the trainin
data that
fi
covers and fi
covers only a subset of the non-c
examples that f2 covers.
The third
heuristic
leads
to two pruning rules, in our feature mining algorithm
described below. The first pruning rule is that we do not
extend (i.e, specialize) any feature with 100% accuracy.
Let fi be a feature contained by examples of only one
class. Specializations
of fl may pass the frequency and
confidence tests in the definition of feature mining, but
will be subsumed by fi. The following lemma captures
this pruning rule:
Lemma
1: If fi 4 fj
and conf(fi,c,V)
= 1.0 then fi
subsumes fj with respect to class c.
Our next pruning rule concerns correlations between
individual
items.
Recall that the examples in 2) are
represented as a sequence of sets. We say that A u B in
examples 2, if B occurs in every set in every sequence in
V in which A occurs. The following lemma states that
if A + B then any feature containing
a set with both
A and B will be subsumed by one of its generalizations,
and thus we can prune it:
Lemma
2:
Let cy = (~1 -+ (~2 -+ ... -+ a,
where
A,BEaiforsomel<i<n.
IfA+B,thenawillbe
subsumed by CY~+ ...ai-1 + (CX~- B) + CX~+I...+ (-in.
Feature
mining:
We can now define the feature min-
ing task. The inputs to the FEATUREMINE algorithm
are a set of examples 2) and parameters min-freq,
max,,
and maxi.
The output is a non-redundant
set
of the frequent and distinctive
features of width max,
and length maxi.
Formally:
Given examples V and
parameters min-freq,
maxW, and maxi return feature
set F such that for every feature fi and every class
cj E C, if Zength(fi) 5 maxi and width(fi)
5 max,
and
fr(/3,Vcj)
> min-freq(cj)
and conf (p,cj,V)
is signifi-
cantly greater (via chi-squared test) than
F contains fi or contains a feature that
]V,
L
/IV] then
su sumes fi
with respect to class cj in data set 2).

2.2
Efficient
mining
of features
We now present the FEATUREMINE algorithm
which
levera es existing data mining techniques to efficieFn$
mine eatures from a set of training
examples.f
TUREMINE is based on the recently proposed SPADE



343

algorithm
[Zaki, 19981 for fast discovery of sequential
patterns.
SPADE
is a scalable and disk-based algo-
rithm that can handle millions
of example sequences
and thousands of items.
Consequently
FEATUREM-
INE shares these properties
as well.
To construct
FEATUREMINE,
we adapted the SPADE
algorithm
to
search databases of labeled examples. FEATUREMINE
mines the patterns predictive
of all the classes in the
database, simultaneously.
As opposed to previous ap-
proaches that first mine millions of patterns and then
apply pruning as a post-processing step, FEATUREMINE
integrates pruning techniques in the mining algorithm
itself.
This enables it to search a large space, where
previous methods would fail.
FEATUREMINE
uses the observation that the subse-
quence relation 5 defines a partial order on sequences.
If (Y+ p, we say that a is more general than ,0, or /I is
more specific than o.
The relation
5 is a monotone
specialization
relation
with respect to the frequency
fr(a,`D),
i.e., if ,0 is a frequent sequence, then all sub-
sequences a 5 0 are also frequent.
The algorithm
sys-
tematically
searches the sequence lattice spanned by the
subsequence relation, from general to specific sequences,
in a depth-first
manner.
r




Figure 2: SequenceLattice and Frequency Computation
Frequency
Computation:
FEATUREMINE
uses a
vertical
database layout, where we associate with each
item X in the sequence lattice its idlist, denoted C(X),
which is a list of all example IDS (eid) and event time
(time) pairs containing
the item.
Given the sequence
idlists, we can determine the support of any &se uence
by simply intersecting the idlists of any two of its
Ylc-
1)
length subsequences.
A check on the cardinality
of
the resulting idlist tells us whether the new sequence
is frequent
or not.
Figure 2 shows that the idlist
for A --+ B is obtained by intersectin
the lists of A
and B, i.e., L(A
+ B) = C(A) II LfB).
Similarly,
L(AB
+ B) = C(A +
B
maintain the class index
1
II L(B
+ B).
We also
tab e indicating
the classes for
each example. Using this table we are able to determine
the frequency of a sequence in all the classes at the same
time.
For example, A occurs in eids {1,2,3,4,5,6}.
However eids { 1,2,3,4}
have label cl and { 5,6} have
label cs. Thus the frequency of A is 4 for cl, and 2 for
ca. The class frequencies for each pattern are shown in
the frequency
table.
To use only a limited amount of main-memory
FEA-
TUREMINE
breaks up the sequence search space into
small, independent,
manageable chunks which can be
processed in memory. This is accomplished via suffix-
based partition.
We say that two Iclength sequences are
in the same equivalence class or partition
if they share
a common k - 1 length suffix. The partitions,
such as
{[A], [B], [Cl}, based on length 1 suffixes are called par-
ent partitions.
Each parent partition
is independent in
the sense that it has complete information
for gener-
ating all frequent sequences that share the same suffix.
For example, if a class [X] has the elements Y + X, and
.Z + X.
The possible frequent sequences at the next
step are Y + 2 -+ X, 2 + Y + X, and (YZ) + X.
No other item Q can lead to a frequent sequence with
the suffix X, unless (QX) or Q + X is also in [Xl.

FEATUREMINE(
min-freq(ci)):
P = { parent partitions, Pi}
for each parent partition
P; do EnumerateFeatures
ENUMERATEFEATURES(
for all elements Ai E S do
for all elements Aj E S, with j > i do
R = A; U Aj; C(R) = L(Ai)
n .L(Aj);
if RulePrune(R,
maxW, maal)
== FALSE and
frequency(R,
ci) 1 min-freq(ci)
for any c;
T=TU{R};7=FU{R};
EnumerateFeatures(
RULEPRUNE(R,
max,,
maxi):
if width(R)
> maxW or length(R)
> maxi return
TRUE;
if accuracy(R)
== 100% return
TRUE;
return
FALSE;
Figure 3: The FEATUREMINE Algorithm
Feature
Enumeration:
FEATUREMINE
processes
each parent partition
in a depth-first
manner, as shown
in the pseudo-code of Fi ure
af
3.
The input
to the
procedure is a partition,
ong with the idlist for each
of its elements.
Frequent sequences are generated by
intersecting
the idlists of all distinct pairs of sequences
in each partition
and checking the cardinality
of the
resulting
idlist
against
min-sup ci).
I
The sequences
found to be frequent for some c ass ci at the current
level form partitions
for the next level. This process is
repeated until we find all frequent sequences.
Integrated
Constraints:
FEATUREMINE
integrates
all pruning
constraints
into
the mining
algorithm
itself, instead of applying pruning as a post-processing
step. As we shall show, this allows FEATUREMINE
to
search very large spaces efficiently,
which would have
been infeasible otherwise.
The Rule-Prune
procedure
eliminates features based on our two pruning rules, and
also based on length and width constraints.
While the
first pruning rule has to be tested each time we extend
a sequence with a new item, there exists a very efficient
one-time method for applyin
f
the A cr) B rule.
The
idea is to first compute the requency of all 2 length
sequences. Then if P(B(A)
= fr(AB)
/
it
fr(A
= 1.0,
then A w B, and we can remove AB
rom t e suffix
partition
[B .
h
This guarantees that AB will never
appear toget er in any set of any sequence.

3
Empirical
evaluation
We now describe experiments to test whether the fea-
tures produced by our system improve the performance
of the Winnow
[Littlestone,
19881 and Naive Bayes
[Duda and Hart, 19731 classification
algorithms.
We
ran experiments on three datasets. In each case, we ex-
perimented with various settings for min-freq,
max,,
and maxi to generate reasonable results. We report the
values used, below.
Random
parity
problems:
We first describe a non-
sequential
problem
on which
standard
classification




344

algorithms perform very poorly.
The problem consists
of N parity problems of size M with L distracting,
or
irrelevant,
features.
For every 0 5 i 5 N and 0 5
j 5 M, there is a boolean feature Fi,j-
Additionally,
for 0 2 lc 5 L, there is an irrelevant,
boolean feature
Ik. To generate an instance, we randomly assign each
boolean feature true or false with 50150 probability.
An example instance for N = 3,M
= 2, and L =
2 is ( Fr,r=true,
Fi,s=false,
Fz,r=true,
Fz,s=true,
Fs,r=false,
Fs,2=false, Ir=true,ls=
false ). There are
N x M + L features, and 2NxM+L distinct instances.
We also choose N weights wr, ....WN which are used
to assign each instance one of two class labels (ON or
OFF) as follows. An instance is credited with weight Wi
iff the ith set of M features has an even parity.
That
is, the "score" of an instance is the sum of the wei
B
hts
Wi for which the number of true features in fi,r, ... i,M
is even. If an instance's score is greater than half the
sum of all the weights, Cy=, wi, then the instance is
assigned class label ON, otherwise it is assigned OFF.
Note that if M > 1, then no feature by itself is at
all indicative
of the class label ONor OFF, which is why
parity problems are so hard for most classifiers.
The
job of FEATUREMINE is essentially to figure out which
features should be grouped together. Example features
produced by FEATUREMINE are (fi,r=true,
fr,z=true),
and (fd,i=true,
fd,s=false).
We used a min-freq
of .02
to .05, maxi
= 1 and maxW = M.
Forest fire plans:
The FEATUREMINE algorithm was
originally
motivated
by the task of plan monitoring
in stochastic domains.
As an example domain, we
constructed
a simple forest-fire domain based loosely
on the Phoenix fire simulator
[Hart and Cohen, 19921.
We use a grid representation
of the terrain.
Each grid
cell can contain vegetation,
water, or a base. At the
beginning
of each simulation,
the fire is started at a
random location.
In each iteration
of the simulation,
the fire spreads stochastically.
The probability
of a
cell igniting
at time t is calculated based on the cell's
ve etation, the wind direction,
and how many of the
ccl s neighbors are burning at time t - 1. Additionally,7,
bulldozers are used to contain the fire before they reach
the bases. For each example terrain, we hand-designed a
plan for bulldozers to dig a fire line to stop the fire. The
bulldozer's speed varies from simulation
to simulation.
An example simulation
looks like:
(time0
Ignite
X3 Y7),
(time0
MoveTo
BDl
X3 Y4),
(time0
MoveTo
BD2
X7 Y4),
(time0
DigAt
BD2
X7 Y4),
.... (time6
Ignite
X4 YE),
(time6
Ignite
X3 Y8),
.... (time32
Ignite
X6
Yl),
(time32
Ignite
X6 YO), ....
We form a database of instances from a set of
simulations
as follows.
Because the idea is to predict
success or failure before the plan is finished, the instance
itself is a list of all events that happen by some time
Ic, which we vary in our experiments.
We label each
instance with SUCCESSif none of the locations
with
bases have been burned in the final state, or FAILURE
otherwise.
Thus, the job of the classifier is to predict
if the bulldozers will prevent the bases from burnin

B
iven a partial execution trace of the plan.
Examp e
B7
eatures produced by FEATUREMINE in this domain are
(MoveTo BDl X2) + (time6),
and (Ignite
X2) + (time8
MoveTo Y3) The first sequence holds if bulldozer BDl
moves to the second column before time 6. The second
holds if a fire i
then any bull f
nites anywhere in the second column and
ozer moves to third
row at time 8. Many
correlations used by our second pruning rule described
Table
1:
Classification
results
( W=Winnow,
B=Bayes,
WFM, BFM = Winnow,
Bayes with FEATUREMINE,
resp.)
I kkuerunent
II hvaluated
I belected
1
features
features
random,
N = 10, M = 4, L = 10
73693 I200
196
fire world,
time
=lO
64 766
553
spellmg,
there
vs. their
78i264
318
Table 2:
FEATUREMINE
Mining results
in section 2.2 arise in these data sets. For example,
Y8 -+ Ignite
arises in one of our test plans in which a
bulldozer never moves in the eighth column.
For fire data, there are 38 boolean features to describe
each event. Thus there are ((38 x 2)maZw )maZl possible
composite
features for describing
eat h sequence of
events.
In the experiments
reported here, we used a
min-freq
= .2, max,
= 3, and maxi = 3.
Context-sensitive
spelling
correction:
We also
tested our algorithm
on the task of correcting spelling
errors that result in valid words, such as substitut-
ing there for their ([Golding
and Roth, 19961). For
each test, we chose two commonly confused words and
searched for sentences in the l-million-word
Brown cor-
pus [Kucera and Francis, 19671containing either word.
We removed the target word and then represented each
word by the word itself, the part-of-speech tag in the
Brown corpus, and the position
relative to the target
word.
For example, the sentence "And then there is
politics"
is translated into (word=and
tag=cc pas=-2)
+ (word=then
tag=rb
pas=-1) + (word=is
tag=bez
pos=+l)
-+ (word=politics
tag=nn pos=+2).
Example
features
P
reduced by FEATUREMINE in-
clude (pos=+3)
+
word=the),
indicating
that the
word the occurs at least 3 words after the target word.
and (pas=-4) + (tag=nn
a noun occurs within
h
+ (pos=+l),
indicgting that
t ree words before the target
word.
These features
were significantly
\
for reasons not obvious to us)
corre ated with either there or their
in the training
set.
In the experiments reported here,
we used a min-freq
= .05, max,
= 3, and maxi = 2.

3.1
Results
For each test in the parity and fire domains, we mined
features from 1,000 examples, pruned features that did
not pass a chi-squared significance test (for correlation
to a class the feature was frequent in) in 2,000 examples,
and trained
the classifier on 5,000 examples.
Thus,
the entire training
process required
7,000 examples.
We then tested the resulting
classifier on 1,000 fresh
examples. The results in Tables 1 and 2 are averaged
over 25 trials
of the process (i.e., we retrained
and then
re-tested the classifier on fresh examples in each trial).
For the snelline: correction.
we trained on 80 nercent
of the eximple;
in the Brown corpus and tested on
the remaining
20 percent.
During training,
we mined
features from 500 sentences and trained the classifier on
all training
examples.
Table 1 shows that the features produced by FEA-
TUREMINE
improved
classification
performance.
We
compared using the feature
set produced
by FEA-




345

Table 3: Impact of pruning rules: results taken from one data set for each example.
TUREMINE with using only the primitive
features them-
selves, i.e. features of length 1.
Both Winnow
and
Naive Bayes performed much better with the features
produced by FEATUREMINE. In the parity experiments,
the mined features dramatically
improved the perfor-
mance of the classifiers and in the other experiments
the mined features improved the accuracy of the classi-
fiers by a significant amount, often more than 20%.
Table 2 shows the number of features evaluated and
the number returned,
for several problems.
For the
largest parity problem, FEATUREMINE evaluated more
than 7 million
features and selected only about 200.
There were in fact 100 million possible features (there
are 50 booleans features, giving rise to 100 feature-value
pairs; we searched to depth M = 4.) but most were
rejected implicitly
by the pruning rules.
Table 3 shows the impact of the A w B pruning
rule on mining time. The results are from one data set
from each domain, with slightly higher values for maxi
and maxv than in the above experiments.
The pruning
rule did not improve mining time in all cases, but made
a tremendous
difference in the fire world problems,
where the same event descriptors often appear together.
Without
A w B pruning,
the fire world problems are
essentially unsolvable because FEATUREMINE finds over
20 million frequent sequences.

4
Related
work
A great deal of work has been done on feature-subset
selection, motivated by the observation that classifiers
can perform worse with feature set T than with some
F'
c F (e.g., [Caruana and Freitag,
19941).
The
algorithms
explore the exponentially
large space of
all subsets of a given feature set.
In contrast,
we
explore exponentially
large sets of potential
features,
but evaluate each feature independently.
The feature-
subset approach
seems infeasible
for the problems
we consider, which contain hundreds of thousands to
millions of potential features.
[Golding and Roth, 19961 applied a Winnow-based
algorithm to context-sensitive
spelling correction.
They
use sets of 10,000 to 40,000 features and either use
all of these features or prune some based on the
classification
accuracy of the individual
features. They
obtain higher accuracy than we did. Their approach,
however, involves an ensemble of Winnows,
combined
by majority
weighting,
and they took more care in
choosing good parameters for this specific task.
Our
goal, here, is to demonstrate that the features produced
by FEATUREMINE improve classification
performance.
Data mining algorithms
have often been applied to
the task of classification.
[Liu et al., 19981build decision
lists out of patterns found by association mining. [Ali et
al., 19971and [Bayardo, 19971both combine association
rules to form classifiers. Our use of sequence mining is
a generalization
on association mining.
Our pruning
rules resemble ones used by [Segal and Etzioni,
19941,
which also employs data mining techniques to construct
decision lists. Previous work on using data mining for
classification
has focused on combining highly accurate
rules together.
By contrast, our algorithm
can weigh
evidence from many features which
each have low
accuracy in order to classify new examples.
[Liu and Setiono, 19981describes recent work on scal-
ing up feature-subset
selection.
They apply a proba-
bilistic Las Vegas Algorithm
to data sets with 16 to 22
features. One of the problems is a parity problem, much
like the one described above, which contains 20 features
(N=2,M=5,L=lO).
Their algorithms,
thus, search the
space of all 220 subsets of the available features.
For
comparison? we have applied our algorithms
to parity
problems with 50 features, which results in 100 feature-
value pairs. Our algorithm
then searches over the set
of all conjunctions
of up to max,
feature-value
pairs.
FEATUREMINE
can handle millions
of examples and
thousands of items, which makes it extremely scalable.
Our work is close in spirit to [Kudenko and Hirsh,
19981,which also constructs a set of sequential, boolean
features for use by classification
algorithms.
They
employ a heuristic
search algorithm,
called FGEN,
which incrementally
generalizes features to cover more
and more of the training
examples,
based on its
classification
performance on a hold-out set of training
data, whereas we perform
an exhaustive
search (to
some depth) and accept all features which meet our
selection
criteria.
Additionally,
we use a different
feature langua e and have tested our approaches on
different classi aers than they have.

References
[Ali et al., 19971 K. Ali, S. Manganaris,
and R. Srikant.
Partial
classification
using association
rules. In KDD97.
[Bayardo,
19971 R.J. Jr. Bayardo.
Brute-force
mining
of high-
confidence
classification
rules. In KDD97.
[Caruana
and Freitag,
19941 R. Caruana
and D. Freitag.
Greedy
attribute
selection.
In ICML94.
[Duda and Hart,
19731 R.O.
Duda
and
P.E.
Hart.
Pattern
Classification
and Scene Analysis.
Wiley.
[Golding
and Roth,
19961 A. Golding
and D. Roth.
Applying
winnow
to context-sensitive
spelling
correction.
In ICML96.
[Hart and Cohen, 19921 D.M
Hart
and P.R Cohen.
Predicting
and explaining
success and task
duration
in the
phoenix
planner.
In 1st Intl.
Conf. on AI Planning
Systems.
[Kucera
and Francis,, 19671 H. Kucera
and W.N. Francis.
Com-
putational
Analysrs
of Present-Day
American
English.
Brown
University
Press, Providence,
RI.
[Kudenko
and Hirsh,
19981 D. Kudenko
and H. Hirsh.
Feature
generation
for sequence categorization.
In AAAISB.
[Lee et al., 19981 W. Lee, S. Stolfo,
and K. Mok.
Mining
audit
data to build intrusion
detection
models.
In KDDSB.
[Littlestone,
19881 N. Littlestone.
Learning
quickly
when .irrel-
evant
attributes
abound:
A new linear-thresh0
d algorithm.
Machine
Learning,
2:285-318.
[Liu and Setiono,
19981 H. Liu and S. Setiono.
Some issues on
scalable
feature
selection.
In 4th World
Congress
of Expert
Systems:
Application
of Advanced Info. Technologies.
[Liu et al., 19981 B. Liu,. W.
Hsu,
and Y.
Ma.
Integrating
classification
and association
rule mining.
In KDD98.
[Sepl
and Etzioni,
19941 Richard
Segal
and
Oren
Etzioni.
earning decision hsts usmg homogeneous
rules. In AAA194.
[Zaki, 19981 M. J. Zaki.
Efficient
enumeration
of frequent
se-
quences. In 7th Zntl. Conf. Info. and Knowledge
Management.




346

