Multivariate Discretization of Continuous Variables for Set
Mining


Stephen D. Bay
Department of Information and Computer Science
University of California, Irvine
Irvine, CA 92697
sbay@ics.uci.edu


ABSTRACT
Many algorithms in data mining can be formulated as a set
mining problem where the goal is to nd conjunctions or
disjunctions of terms that meet user speci ed constraints.
Set mining techniques have been largely designed for cat-
egorical or discrete data where variables can only take on
a xed number of values. However, many data sets also
contain continuous variables and a common method ofdeal-
ing with these is to discretize them by breaking them into
ranges. Most discretization methods areunivariate and con-
sider only a single feature at a time sometimes in con-
junction with the class variable. We argue that this is a
sub-optimal approach for knowledge discovery as univari-
ate discretization can destroy hidden patterns in data. Dis-
cretization should consider the e ects on all variables in the
analysis and that tworegions Xand Y should only be in the
same cell afterdiscretization if theinstances in those regions
have similar multivariate distributions Fx  Fy across all
variables and combinations of variables. We present a bot-
tom up merging algorithm to discretize continuous variables
based on this rule. Our experiments indicate that the ap-
proach is feasible, that it does not destroy hidden patterns
and that it generates meaningful intervals.

Categories and Subject Descriptors
H.2.8 DatabaseManagemen t: Database Applications|
Data mining

1. INTRODUCTION
In set mining the goal is nd conjunctions or disjunctions
of terms that meet all user speci ed constraints. For ex-
ample, in Association Rule Mining 1 a common step is to
nd all itemsets that have support greater than a thresh-
old. Set mining is a fundamental operation of data mining.
In addition to association rule mining, many other large
classes of algorithms can be formulated as set mining such
as classi cation rules 14 where the goal is to nd sets of
attribute-value A-V pairs with high predictive power, or
contrast set mining 4, 5 where the goal is to nd sets that
represent large di erences in the probability distributions of
two or more groups.
There has been much work devoted to speeding up search
in set mining 6, 19 and there are many e cient algorithms
when all of the data is discrete or categorical. The problem
is that data is not always discrete and is typically a mix
of discrete and continuous variables. A central problem for
set mining and one that we address in this paper is How
should continuous values be handled?" The most common
approach is to discretize them into disjoint regions and then
apply the set mining algorithm.
Past work on discretization has usually been done in a clas-
si cation context where the goal is to maximize predictive
accuracy. For example, discretizing continuous attributes
for the naive Bayesian classi er can greatly improve accu-
racy over a normal approximation 8 . In Knowledge Dis-
covery we often analyze the data in an exploratory fashion
where the emphasis is not on predictive accuracy but rather
on nding previouslyunknownandinsightfulpatterns in the
data. Thus we feel that the criteria for choosing intervals
should be di erent from this predictive context as follows:
The discretizedintervals should not hide patterns. We
must carefully choose our intervals or we may miss
potential discoveries. For example, if the intervals are
too big we may miss important discoveries that occur
atasmallerresolution, butiftheintervals aretoosmall
we may not have enough data to infer patterns. We
refer to this as the resolution problem. Additionally,
if the intervals are determined by examining features
in isolation then with discretization we may destroy
interactions that occur between several features.
The intervals should be semantically meaningful. The
intervals we choose must make sense to a human ex-
pert. For example, when we are analyzing census data
we know that it is not appropriate to create inter-
vals such as salary 26K, 80K because people who
make 26K year are di erent qualitatively on many
variables such as education, occupation, industry, etc.
from people who make 80K year.
Inaddition, thereistheobviousrequirement thatthemethod
should be fast enough to handle large databases of interest.
We feel that one method of addressing these points is to
Permission to make digital or hard copies of part or all of this work or
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers, or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD 2000, Boston, MA USA
© ACM 2000 1-58113-233-6/00/08 ...$5.00




315

Figure 1: Noisy XOR




-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1




X1
X2
class 1
class 2




a XOR
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1




X1
X2
class 1
class 2




b XOR with multivari-
ate discretization
use multivariate as opposed to univariate discretization. In
multivariate discretization one considers how all the vari-
ables interact before deciding on discretized intervals. In
contrast, univariate approaches only consider a single vari-
able at a time sometimes in conjunction with the class.
Wepresentasimplemotivating example. Consider theprob-
lem of set mining on XOR data as in Figure 1a. Clearly one
should discretize the data as in Figure 1b which is the re-
sult of the method we are proposing in this paper. However
algorithms that do not consider more than one feature will
fail. For example, Fayyad and Irani's recursive minimum
entropy approach 9 will not realize that there is an inter-
action between X1, X2, and the class. It nds both X1 and
X2 irrelevant and ignores them.
Our approach tothis problem is to nely partition each con-
tinuous attribute into n basic regions and then to iteratively
mergeadjacentintervals only whentheinstances inthose in-
tervals have similar distributions. That is, given intervals X
and Y we merge them if Fx  Fy. We use a multivariate
test of di erences to check this. Merging allows us to deal
with theresolution problem andit automatically determines
the number of intervals. Our multivariate test means that
we will only merge cells with similar distributions so hidden
patterns are not destroyed and the regions are coherent.

2. PASTDISCRETIZATIONAPPROACHES
The literature on discretization is vast but most algorithms
are univariate in that they consider each feature indepen-
dently or only jointly with a class variable and do not con-
sider interactions with other features. For example, Fayyad
and Irani 9 recursively split an attribute to minimize the
class entropy. They use a minimum description length cri-
terion to determine when to stop. Dougherty, Kohavi and
Sahami 8 provide a good overview of many algorithms in
this category. Aswementioned previously, these approaches
can miss interactions of several variables.
Srikant and Agrawal 17 proposed an approach that would
avoid this limitation. They nely divide each attribute into
n basic intervals and then attempt to consider all possi-
ble combinations of consecutive basic intervals. However,
this creates two problems they refer to as ExecTime and
ManyRules. The ExecTime problem is that since there are
On2 combinations of intervals for each feature the com-
plexity will blow up" especially when we consider the in-
teractions with other features. The ManyRules problem is
also related to the number of combinations. If an interval
meets the minimum support requirement so does any range
containing the interval; e.g., if age 20,30 meets the mini-
mum support constraints then so will age 20,31 , age 20,40
and so on. This can result in a huge number of rules.
Miller and Yang 13 note that Srikant and Agrawal's solu-
tion may combine ranges that are not meaningful and can
result in unintuitive groupings. They present an alternative
approach based on clustering the data and then building
association rules treating the clusters as frequent itemsets.
3. MULTIVARIATE DIFFERENCE TESTS
A multivariate test of di erences takes as input instances
drawn from two probability distributions and determines if
the distributions are equivalent. In statistical terms the null
hypothesis H0 is that Fx =Fy and the alternate hypothesis
is that the two distributions are di erent Fx 6= Fy. In this
section, wereview past approaches and discuss whythey are
inappropriate for our application. We argue for a new test
based on recent work in contrast set mining 4, 5 .
Withasingledimension, wecanusetheKolmogorov-Smirnov
K-S two sample test or the Wald-Wolfowitz W-W runs
test 7 to check for di erences. These methods sort the ex-
amples and compute statistics based on the ranks of sorted
members in the list. For example, the K-S test looks at the
maximum absolute di erence in the cumulative distribution
functions. The Wald-Wolfowitz test uses the total number
of runs R, where a run is a set of consecutive instances with
identical labels. H0 is rejected if R is small.
The problem with these methods is that the notion of a
sorted list doesnot apply in multivariate data. Thusin their
basic form the K-S and W-W tests are not useful for our
problems. However, Friedman and Rafsky 10 generalized
the notion of asorted list by using aminimum spanning tree
MST. They use order information in the MST to calculate
multivariate generalizations of K-S and the W-W tests. For
theK-Svariant, theyuseaheight directedpreordertraversal
of the tree visit subtrees in ascending order of their height
to de ne a total order on nodes in the tree. For the W-W
test the multivariate generalization is to remove all edges in
the tree that have di erent labels for the de ning nodes and
let R be the number of disjoint subtrees.
Unfortunately, the MST based test has a number of disad-
vantages: First, the generation oftheMSTrequires pairwise
distance measures between all instances. In data mining,
variables can be both continuous and discrete thus devel-
oping a distance metric is not straightforward. Any MST
developed will be sensitive to the metric used. Second,
the MST is expensive to nd. Using Prim's algorithm it
is ON2 16 for N data instances. For our data sets N is
usually very large thus making the complexity prohibitive.
Finally, the above tests were designed to measure signi -
cance and have no meaningful interpretation as a measure
of the size of the di erences between the two distributions.
For example, one cannot relate changes in the test statistic
i.e. di erence in cumulative distribution function, distribu-
tion ofruns tomeaningful di erences in underlying analysis
variables such as age or occupation. Additionally, signif-
icance by itself is not su cient 2 because as N ! 1 all
di erences, nomatter howsmall, will showup as signi cant.
We propose using an alternate test of di erences based on
Contrast Set miners such as STUCCO 4, 5 . STUCCO at-



316

tempts to nd large di erences between two probability dis-
tributions based on observational data. For example, given
census data if we compare PhD and Bachelor's degree hold-
ers, STUCCO would return di erences between their distri-
butions such as: Poccupation
= sales
jPhD=2.7,while
Poccupation
= sales
j
Bachelor
 = 15.8.
The mining objectives of STUCCOcan be stated as follows:
Given two groups of instances G1 and G2, nd all conjunc-
tions of attribute value pairs C contrast sets such that:
PC jG16=PC jG2
1
jsupportCjG1,supportCjG2j
2
Support is a frequency measurement and is the percentage
of examples where C is true for the given group. Equation 1
is a signi cance criterion and ensures that the di erences
we nd could not be explained by uctuations in random
sampling. Wetestthisbyusing achi-square testwhichmust
reject independence of group membership and probability
of C. Equation 2 is a size criterion and estimates how big
the di erence is between two distributions. We require the
minimum di erence in support to be greater than .
STUCCO nds these contrast sets using search. It uses a
set enumeration tree 15 to organize the search and it uses
many of the techniques in 6, 19 such as dynamic ordering
of search operators, candidate groups and support bounds
in conjunction with pruning rules geared for nding support
di erences. STUCCOalso carefully controls error caused by
multiple hypothesis testing i.e., false positives.
We use STUCCOas a multivariate test of di erences as fol-
lows. If STUCCO nds any C that meets both constraints
then we say that Fx is substantially di erent from Fy, oth-
erwise we say that Fx is similar to Fy i.e. Fx  Fy.

4. MULTIVARIATE DISCRETIZATION
Given our test from the previous section, we now present
our algorithm for MultiVariate Discretization MVD:
1. Finely partition all continuous attributes into n basic
intervals with equal width or frequency discretization.
2. Select two adjacent intervals X and Y that have the
minimum combined support and do not have a known
discretization boundary between them as candidates
for merging.
3. If Fx  Fy then merge X and Y into a single in-
terval. Otherwise place a discretization boundary be-
tween them.
4. If there are no eligible intervals stop. Otherwise go to
2.
We test if Fx  Fy by using STUCCO where the instances
that fall in X and Y form the two groups whose distri-
butions we compare. STUCCO requires that we specify
which represents how big a di erence we are willing to
tolerate between two distributions. This allows us to con-
trol the merging process: small means more intervals and
large means fewer intervals. We set adaptively accord-
ing to the support of X and Y so that any di erence be-
tween the two cells must be larger than a xed percentage
of the entire dataset. For example, if we tolerate di er-
ences of size up to 1 of the entire distribution then we set
= 0:01=minfsupX;supYg.
4.1 Efficiency
For each continuous feature, MVD may call STUCCO up to
n,1 times where n is the number of basic intervals. Each
invocation of STUCCO may require an evaluation of an ex-
ponential number of candidates i.e. all combinations of
attribute-value pairsanduptojAjpassesthroughdatabase.
This begs the question of how we can implement MVD e -
ciently when it calls a potentially expensive mining routine.
We believe that it will run e ciently because of the follow-
ing reasons: First, although the worst case running time is
exponential, in practice STUCCO runs e ciently on many
datasets 5 . Second, the problems passed o to STUCCO
are often easier than that faced by the main mining pro-
gram. STUCCO only needs to consider the examples that
fall into the two ranges that are being tested for merging.
This can be only a small fraction of the data set. Third,
STUCCO only needs to nd a single di erence between the
groups and then it can exit. It does not need to nd all
di erences. Finally, calling STUCCO repeatedly will result
in many passes over the database. However the limiting
factor for mining algorithms is the exponential number of
candidates that need to be considered, not passes through
database e.g. 6 .

5. SENSITIVITY TO HIDDEN PATTERNS
We test the ability of MVD to properly discretize data with
hidden patternsinhigh dimensional databyde ning aprob-
lem called Parity R+I. This is a continuous version of the
parity problem where there areR continuous variables rang-
ing from -0.5,0.5 with a uniform distribution, one contin-
uous irrelevant variable also ranging from -0.5,0.5 , and a
boolean class variable. If an even number of the rst R
features are positive then the class variable is 1; 0 other-
wise. We then added 25 class noise and generated 10000
examples from this distribution.
Weused MVDwithequal frequencypartitioning 100exam-
ples per division on the Parity 5+I problem. This problem
is di cult because there is a hidden 6 dimensional relation-
ship and with our initial partitioning of features we have
only 10000 instances to be divided into 1006  2 possible
cells. We ran ve trials and Table 1 shows the cutpoints we
found for each feature MVD found at most 1 cutpoint per
feature. The true solution is 0,0,0,0,0,ignore . MVD did
very well at identifying the relationship between F1,:::,F5
and the class. Although it did not exactly reproduce the
desired cutpoints, it came reasonably close, and a set miner
should still be able to identify the parity relationship. MVD
failed only once out of the ve trials and it always managed
to identify the irrelevant variable. In contrast, univariate
discretizers will only be able to solve Parity 1+I problems.
Table 1: MVD Cutpoints for Parity 5+I
Feature
Trial
F1
F2
F3
F4
F5
F6
1
0.08
0.03 0.15 -0.08 0.01 ignore
2
-0.12 -0.10 -0.15 0.19 -0.02 ignore
3
-0.13 0.20 0.00
0.06 -0.10 ignore
4
-0.18 ignore ignore ignore ignore ignore
5
0.02
0.06 -0.15 0.16 0.08 ignore

6. EVALUATION
In this section, we show that our approach is e cient and
can be used in set mining without a large overhead on the



317

entire process. We also demonstrate by example that our
approach generates meaningful results.
For our experiments we compared MVD with Fayyad and
Irani's recursive minimum entropy approach with the MDL
stopping criteria. We refer to this as ME-MDL and we used
the MLC++ 12 implementation of this discretizer. Past
work has shown that ME-MDL is one of the best methods
for classi cation 8, 11 . We also compared our execution
times with Apriori to give an indication of how much time
discretization takes relative to the set-mining process. We
used C. Borgelt's implementation of Apriori in C.1
Weranexperiments on vedatabases which aresummarized
in Table 2. The rst four databases are publically avail-
able from the UCI KDD Archive 3 . The UCI Admissions
dataset represents all undergraduate student applications to
UCI for the years 1993-1999. The data contains variables
such as ethnicity, school e.g. Arts, Engineering, etc., if an
o er of admission was made, GPA, SAT, etc.
We ran all experiments on a Sun Ultra-5 with 128 MB of
RAM. We used the following parameter settings: The ba-
sic intervals were set with equal frequency partitioning with
100 instances per interval for Adult, SatImage, and Shuttle,
2000 per interval for UCI Admissions, and 10000 per in-
terval for Census-Income. We required di erences between
adjacent cells to be at least 1 of N. ME-MDL requires
a class variable and for Adult, Census-Income, SatImage,
and Shuttle we used the class variable that had been used
in previous analyses. For UCI Admissions we used Admit
= fyes,nog i.e., was the student admitted to UCI.
Table 2: Description of Data Sets
Data Set
 Features  Continuous  Examples
Adult
14
5
48812
Census-Income
41
7
199523
SatImage
37
36
6435
Shuttle
10
9
48480
UCI Admissions
19
8
123028

6.1 Execution Time
Table 3 shows the discretization time for MVD, ME-MDL
andthetimetakenbyAprioritoperformfrequentsetmining
on MVD's discretizations. MVD's time was generally com-
parable to ME-MDL. Both discretization processes usually
took longer than than Apriori but they were not excessively
slow. Census-Income was exceptionally di cult for Apriori
which ran out of memory and could not mine frequent item-
sets at 10 support. We tried mining with 30 support
but even at this level Apriori could not complete mining in
reasonable time and we stopped it after 10 CPU hours.
Table 3: Discretization Time in CPU seconds
Data Set
MVD ME-MDL Ap. 10 Ap. 5
Adult
65
541
44
104
Census-Income 11065
8142
-
-
SatImage
127
36
0
4
Shuttle
77
318
1
2
UCI Admissions
370
772
131
204

6.2 Qualitative Results
1
available from
http: fuzzy.cs.Uni-Magdeburg.de
borgelt
Webelieve thatourapproachofcombining rangesonly when
theyhavesimilar distributions willleadtosemantically mean-
ingful intervals. We demonstrate this by comparing the in-
tervals formed by MVD with ME-MDL.
For several variables MVD and ME-MDL obtained similar
discretizations. For example, Figures 2a and 2b show the
discretization boundaries for the age variable in the Adult
datasetsuperimposed ontopofit'shistogram. Theintervals
for both MVD and ME-MDL are narrow at younger age
ranges and wider at older ranges. This makes intuitive sense
as the data represents many employment related variables
and people's careers tend to change less as they age.
However,MVDandME-MDL di ered signi cantly on many
other variables. Figures 3a and 3b show the discretiza-
tions boundaries found for parental income on UCI Admis-
sions data. Note that we plotted the logarithm of the in-
come for visualization purposes only; we did not transform
the variables before discretization. The MVD cutpoints
occur at meaningful locations: f 17000, 30000, 51760,
75000g. Wecaneasily relatethesetoournotions ofpoverty
and wealth. The ME-MDL discretization is not meaningful.
Consider that it grouped everybody with parental income
between 36K and 200K together. Additionally, ME-MDL
had many cutpoints distinguishing applicants at the upper
range of parental income i.e. over 400K.
Another problem with using ME-MDL in a discovery con-
text is that it requires a class variable and the discretization
is sensitive to this. While being sensitive to the class vari-
able is probably good in a classi cation context, it is not for
discovery. Stable results are essential for domain users to
accept the discovered knowledge 18 . We discretized UCI
Admissions data with ME-MDL using sex and year as alter-
nate class variables and we found wildly di erent cutpoints.
Using sex produced income cutpoints at f 55K, 162K,
390Kgandusingyearproduced cutpointsatf 13K, 95Kg.
Figures 2c and 2d show the discretization cutpoints found
for Capital-Loss on Adult. In this case, most data analysts
would probably agree with MVD as opposed to ME-MDL.
WeranApriori using bothMVDandME-MDL'sboundaries
forcapital-loss usingMVD'sdiscretization forallothervari-
ables and found that the poor cutpoints chosen by ME-
MDL can hide important association rules. For example,
with MVD's cutpoint we were able to nd rules such as
capital-loss  155 ! salary 50K support 2.3, con -
dence 50.1. This rule states that declaring a capital-loss
is strongly predictive of having a high salary. The base rate
for salary 50K is 24 so the rule identi es a group with
twicethis probability. Wedid not nd any similar rules with
ME-MDL's discretization because it used too many parti-
tions making it di cult for Apriori to infer relationships.
IncomeandCapital-Loss areveryskewedvariables, however,
ME-MDL can also produce unintuitive boundary points for
morenormal data. Figures 3cand3dshowthediscretization
boundaries found for GPA on UCI Admissions.
These results suggest that perhaps ME-MDL is not appro-
priate for knowledge discovery when the goal is understand-
ing the data. ME-MDL can generate too many intervals
that are close together and are at odd locations. In MVD
our similarity test prevents this from happening because it



318

Figure 2: Discretization Cutpoints for Age and Capital-Loss on the Adult Census Data




10
20
30
40
50
60
70
80
90
0
500
1000
1500
2000
2500
3000




Age
#
Instances




a MVD
10
20
30
40
50
60
70
80
90
0
500
1000
1500
2000
2500
3000




Age
#
Instances




b ME-MDL
0
1000
2000
3000
4000
5000
0
1
2
3
4
5 x
10
4




Capital Loss
#
Instances




c MVD
0
1000
2000
3000
4000
5000
0
1
2
3
4
5 x
10
4




Capital Loss
#
Instances




dME-MDL
Figure 3: Discretization Cutpoints for Parental Income and GPA on UCI Admissions Data




3
3.5
4
4.5
5
5.5
6
0
5000
10000
15000




log(Parental Income)
#
Instances




a MVD
3
3.5
4
4.5
5
5.5
6
0
5000
10000
15000




log(Parental Income)
#
Instances




b ME-MDL
0
1
2
3
4
5
6
0
1000
2000
3000
4000
5000




GPA
#
Instances




c MVD
0
1
2
3
4
5
6
0
1000
2000
3000
4000
5000




GPA
#
Instances




d ME-MDL
requires thatadjacentintervals notonly bedi erent butalso
be di erent by a certain minimum size.

7. CONCLUSIONS
We presented a multivariate discretization algorithm that
nely partitions continuous variables and then merges adja-
cent intervals only if their instances have similar multivari-
ate distributions. Merging allows us to automatically deter-
mine an appropriate resolution to quantize the data. Our
multivariate test ensures that only similar distributions are
joined. Our experimental results on synthetic data indicate
that our algorithm can detect high dimensional interactions
between features and discretize the data appropriately. On
real data our algorithm ran in time comparable to a popu-
lar univariate recursive approach and produced sensible dis-
cretization cutpoints.

8. ACKNOWLEDGMENTS
This research was funded in part by the National Science
Foundation grant IRI-9713990.

9. REFERENCES
1 R. Agrawal, T. Imielinski, and A. Swami. Mining
associations between sets of items in massive
databases. In Proc. of the ACM SIGMOD Int. Conf.
on Management of Data, pages 207 216, 1993.
2 D. Bakan. The test of signi cance in psychological
research. Psychological Bulletin,666, 1966.
3 S. D. Bay. The UCI KDD archive. Irvine, CA:
University of California, Information and Computer
Science, 1999.
http: kdd.ics.uci.edu
.
4 S. D. Bay and M. J. Pazzani. Detecting change in
categorical data: Mining contrast sets. In Proc. of the
5th ACM SIGKDD Int. Conf. on Knowledge
Discovery and Data Mining, pages 302 306, 1999.
5 S. D. Bay and M. J. Pazzani. Detecting group
di erences: Mining contrast sets, under review.
6 R. J. Bayardo. E ciently mining long patterns from
databases. In Proc. of the ACM SIGMOD Conf. on
Management of Data, 1998.
7 W. J. Conover. Practical NonparametricStatistics.
John Wiley & Sons, rst edition, 1971.
8 J. Dougherty, R. Kohavi, and M. Sahami. Supervised
and unsupervised discretization of continuous features.
In Proc. 12th Int. Conf. on Machine Learning,1995.
9 U. M. Fayyad and K. B. Irani. Multi-interval
discretization of continuous-valued attributes for
classi cation learning. In Proc. of the 13th Int. Joint
Conf. on Arti cial Intelligence,1993.
10 J. H. Friedman and L. C. Rafsky. Multivariate
generalizations of the Wald-Wolfowitz and Smirnov
two-sample tests. Annals of Statistics,74, 1979.
11 R. Kohavi and M. Sahami. Error-based and
entropy-based discretization of continuous features. In
Proc. of the 2nd Int. Conf. on Knowledge Discovery
and Data Mining, pages 114 119, 1996.
12 R. Kohavi, D. Sommer eld, and J. Dougherty. Data
mining using MLC++, a machine learning library in
C++. Journal of Arti cial Intelligence Tools,
64:537 566, 1997.
13 R. J. Miller and Y. Yang. Association rules over
interval data. In Proc. of the 1997 ACM SIGMOD Int.
Conf. on Management of Data, 1997.
14 J. R. Quinlan. C4.5 programs for machine learning.
Morgan Kaufmann, 1993.
15 R. Rymon. Search through systematic set
enumeration. In Third Int. Conf. on Principles of
Knowledge Representationand Reasoning,1992.
16 R. Sedgewick. Algorithmsin C. Addison-Wesley, 1990.
17 R. Srikant and R. Agrawal. Mining quantitative
association rules in large relational tables. In Proc.
ACM SIGMOD Conf. on Management of Data, 1996.
18 P. Turney. Technical note: Bias and the quanti cation
of stability. Machine Learning, 20:23 33, 1995.
19 G. I. Webb. OPUS: An e cient admissible algorithm
for unordered search. Journal of Arti cial Intelligence
Research, 3:431 465, 1995.



319

