Evaluating Classifiers' Performance
In A Constrained Environment

Anna Olecka
olecka@rutcor.rutgers.edu

Fleet BostonFinancial
DatabaseMarketingDepartment
1075MainSt.
Waltham, MA 02451
RUTCOR
Rutgers University
640 BartholomewRoad
Piscataway,NJ 08854-8003


ABSTRACT
In this paper, we focus on methodology of finding a classifier
with a minimal cost in presence of additional performance
constraints. ROCCH analysis, where accuracy and cost are
intertwined in the solution space, was a revolutionary tool for
two-class problems. We propose an alternative formulation, as an
optimization problem, commonly used in Operations Research.
This approach extends the ROCCH analysis to allow for locating
optimal solutions while outside constraints are present. Similarly
to the ROCCH analysis, we combine cost and class distribution
while defining the objective function. Rather than focusing on
slopes of the edges in the convex hull of the solution space,
however, we treat cost as an objective function to be minimized
over the solution space, by selecting the best performing
classifier(s) (one or more vertex in the solution space). The Linear
Programming
framework
provides
a
theoretical
and
computational methodology for finding the vertex (classifier)
which minimizes the objective function.


1. INTRODUCTION
Consider a problem, where classifiers performance has to be
evaluated taking into account additional constraints related to
error rates. Such constraints often arise from implementation.
They could, for example, involve a limited workforce to resolve
cases of suspected fraud, a limited size of a direct mail campaign,
or restrictions in cost of incentives for responders. An application
example used throughout this paper involves an attrition model
for a bank. A bank plans a calling campaign to lower attrition rate
among its customers.


Naturally,
one of the implementation concerns is limited
availability of resources, such as phone representatives.

Traditionally, a model is selected first and summarized by a


Permission to make digital or hard copies of all or part of this work for
personal or classroomuse is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SIGKDD 02, July 23-26, 2002, Edmonton, Alberta, Canada.
Copyright 2002 ACM 1-58113-567-X/02/0007 $5.00.
modeler
into
several
performance
buckets.
Then,
the
implementation team will
eyeball
the thresholds of the
performance buckets
and pick the threshold that matches
constrained resources the closest. Such business practice often
results in a sub-optimal solution being selected. If the constraints
are known a-priori, they could be built into the system evaluating
classifiers
performance.
If they are not known
till the
implementation time, a system analogous to the ROCCH could be
built to select the best classifier at that time. To this end, we are
proposing an evaluation system that can deal with additional
constraints related to prediction errors. We will also show how to
apply such system in a business scenario described above.

Provost and Fawcett have shown in [3] that some specific metrics
frequently used in Machine Learning (eg, workforee constraints,
and the Neyman-Pearson decision criterion), are optimized by the
ROCCH method. Optimization approach proposed here extends
their results to any linear constraint. In fact, our approach can be
applied to an unlimited number of constraints, as long as they
remain linear.
Finding numerically intersection points of such
additional constraints with ROCCH can be computationally
tedious. A mathematical programming approach provides efficient
tools for finding optimal solutions without explicitly calculating
all the intersection points.


ROCCH analysis is a powerful and widely accepted tool for
visualizing and optimizing classification problems with two
classes. It plots performance of all classifiers under consideration
in a two-dimensional space, with false positive rate on one axis
and true positive rate on the other. It measures classifiers
performance for various costs of misclassification and under
various class distributions. It allows for visual representation of
classifiers and enables quick decisions in choosing the right
classifier for the given costs. The main advantage of this
methodology is its flexibility under varying conditions. One
drawback of this methodology is that it is somewhat rigid in
looking for an optimal classifier. It forms an optimal slope by
considering an optimal combination of class probabilities and
error costs, and then looks for one of the two scenarios. Either an
edge of the convex hull with a slope equal to the optimal one, or
for a vertex between two edges where a difference between the
edges slope and the optimal slope changes sign.

Similarly to the ROCCH, we construct the convex hull of all
classifiers in the error space. Convexity of the error space is




605

accomplished in the following steps. First we note that a convex
combination of two classifiers is also a viable classifier. Then, we
remove dominated classifiers.
Finally, we impose additional
performance constraints if any. Those additional constraints also
form a convex hull.
Intersection of the two regions, is a new
convex hull, the potential solution space. Finally, similarly to the
ROCCH, we treat a combination of costs and class probabilities as
an objective function to be minimized over the solution space.
Due to existence of additional constraints, however, iterating
slopes
of the
edges
of the
convex
hull
is
no
longer
computationally efficient because not all vertices of the convex
hull are known. The theory of linear programming provides
computational tools for finding the optimal solution(s) without
explicit knowledge of all vertices.


2. ROC CONVEX HULL
H
BRID CLASSIFIERS
Suppose we construct a series of k-1 classifiers by varying a
positive decision threshold from never , and gradually increasing
probability of decision Yes . By allowing more instances to be
classified as positive, each new threshold will increase the number
of correctly classified positive instances, but it may also increase
frequency of false positive classification. In the space (FP rate, TP
rate), each new classifier will be positioned up and to the right
from the previous one. Connecting each pair of points with a line
segment generates a convex set, where each vertex represents a
classifier (Figure1).

Table 1. Neural net model for bank attrition

Cutoff I
Threshold
Never
0.54
0.48
0.42
0.36
0.30
0.24
Always
FP rate
TP rate
p(Yln)
P(YlY)
0
0
0.06
0.25
0.14
0.43
0.23
0.58
0.33
0.70
0.42
0.82
0.53
0.90
1
1


Table 1 represents a set of classifiers obtained from a neural
networks model for a bank attrition problem. Figure 1 shows the
resulting convex set

ROCCurveforAttritionData

1

0.8



0.4

02

0

0
0.2
0.4
0.6
0.8
1

FPRate


Figure 1. ROC curve for the neural net attrition model

Figure 2 shows a set of classifiersobtained by varying thresholds
for two logisticregression models for the attritiondata. In this
representation, we can visually recognize dominated classifiers.
They are positioned inside the convex region, while potential
candidates for optimal solution are on the boundary. It is easy to
see [3], that under any cost structure, there is a classifier on the
boundary, which will outperform a dominated classifier.

ROC surve,s for two logisticregression models


0.9
0,8
0.7
"
·rP 0,8
,": 0.5- f
/
0.4]
..f

0.20,i;°V
,
;,
,
,
" ,,
,

0
O
0.2
0.4
"
0;6
0,8
1
FPRite
Loglafic2




Figure 2. Two competing logistic regression models

Figure 3 shows a hybrid classifier obtained by removing
dominated points. Any
point positioned inside the bounded
region, can be outperformed by some point on the boundary. The
boundary forms a hybrid classifier,or a setof potentialcandidates
for an optimal solution.

Hybdd dMsifler for two ioglstio regre~ion models "



O,g q
·
a


0;8 1
O.7




0.4 q
/,
/
0.3 -~
/,

0..2I
t
0.i //
0 ~
:
,
,,,,
=
·
,
,.
0.2
0.4
O;6
0.8
1
FPRite


Figure 3. ROCCH for the two logistic regression models


2.1 Remark on convexity
The hybrid classifier obtained from our two logisticregression
models formed a convex set in the (FP rate,TP rate)plane. In
general, thisdoesnt have to be the case. Figure 4 provides such
an example. Point B, where the region boundary transitionsfrom
the neural networks model to the logisticmodel. The boundary
dips below the line from A to C.
This can be
fixed by
creatinga new classifierB on the (A, C) line. For example, ifwe
want a point half way between A and C, the new classifieris
obtained by using classifiers A
and B randomly, each with
probability 0.5. Provost and Fawcett describe a similar scheme,
using random sampling to createa new classifier.

In general any convex combination of two classifiers becomes a
new classifier.
For any point X on a line segment created by
classifiers nl, ~,,
we can always construct a new classifier,
which would correspond to such point. We start by describing
such point as convex combination




606

X=arq+ (1-a)r~
where 0--< a_<l

Then, we randomly partition the population being classified into 2
groups in proportions a, 1- a and apply an appropriate model for
each group.


ROCCunmsfor LogisticandNeuralNelsmodels

1
C


o.s t
...%,,i


o.s .[
.:/
[
o.4 ~.
,/~r
"
X -'*-~'N'~ ~

0.3 -I
."


0,1
:
·
o/
/
:
o
0,2
0.4
o.6
o.e ....
1

FP
Rate



Figure 4. Boundary of ROC space for two classifiers is not
always convex


A new, convex boundary for the attrition problem is shown iN
Figure 5.




1
-

0.9-
O.8
0.7-
0,6~
0.5
~: 0,4
0.3
02
0.1
0
:
.
.
:.
:
.
.
.

#"
.
:..
:
.
I




0.2
0A "
0.6
·
0i8..
1.
~emo
"


Figure 5. Convexity is obtained by creating convex
combinations of existing classifiers


3. BASIC TERlVIINOLOG
The following terminology is used throughout the remaining
sections.

-
Two classes: positive (y) and negative (n) with probabilities
respectively p and 1-p
-
Classification decision: positive (Y) and negative (N)
-
FP, TP, FN, TN represent number of instances of each kind:
false positive, true positive, false negative and true negative
respectively
-
Rates of those instances are represented as follows:
FP rate = p(Y n)
TP_rate = pCYy)
FN-_rate = p (N y)
TN_rate = p(N n)
-
In the ROC space, the horizontal axis represents FP_rate, and
the vertical axis represents TP_rate. We will use x to denote
FP__rate and y to denote TP_rate
Unit cost of a false positive error = c(Y n)
-
Unit cost of a false negative error = c(N y)

In defining the cost, we need to start with the expected number of
errors and transform the resulting formula into the ROC space
terms, where the variables are error rates. The following scheme
visualizes interdependencies between terms.


Table 2. Two class classification scheme



Two class
classification scheme
/ Poo:.,,oo/




Let M be the total number of instances being classified. For given
p, x and y we can calculate the expected number of classified
cases of each kind as follows.

TP=Mp
y
FN=M p (l-y)
FP=M
(l-p) x
TN=M (l-p) (l-x)


4. COST FUNCTION
We now apply the cost function to the attrition problem and look
for a classifier on the boundaries of the convex hull, which will
minimize the total cost.


A false negative classification means that we won t recognize an
attriting customer and loose the account. The cost of loosing a
customer is tied to net income after taxes (NIAT) this customer
brings to the bank. In addition, both types of error generate labor
costs related preventive action. The line of business decided to
assign error costs as shown in Table3.


Table 3. Misclassification costs assignment


Error Cost
per un/t

False
False
i
positive
negative
c(YIn)
c(Nly)
[

$
30
$
2,575


Given cost of a false negative error c(N y), and a false positive
error c(Y n), the total expected cost

EC = c(Y n)
E(FP) + c(N y) E(FN)

Where Eft'N) is the expected number of false negatives and
E(FP) is the expected number of false positives.




607

We want to minimize the expected total cost in term of decision
variables x and y.
Minimize EC = c(Y n)
E(FP) + c(N y) E(FN)

=c(Y n)
(l-p)
M
x+c(N
y)
p M
(1
y)

=c(Y n)
(l-p)
M
x+c(N
y)
p
M-c(N
y) pMy

After subtracting the constant (not dependent on the decision
variables) term and dividing by M, this is equivalent to:


Minimize (EC - c(N y) p )/M =

=c(Y n) (l-p) x-c(N
y) p y

This is equivalent to maximizing the opposite function


Maximize
C =-c(Y
n) (l-p) x+c(N
y) py


C
in space (x,y) is a collection of parallel lines with slopes
depending on misclassification costs and the a-priori probability
of the positive class. In the attrition example we are analyzing,
p=3.75%. We are now ready to calculate slopes for the cost lines.


m= [c(Y n) (l-p)]/[c(N y) p] =

= 30 (1-0.0375) / (2575 0.0375) = 0.3

Intercepts of those lines vary with the position of each line on the
plane. The higher the position of the line on the plane in relation
to y, the higher will the value of C be. Since our objective is to
maximize C, we can start at the bottom of the convex hull and
move the line up, as far as possible. Each point of the region
touching the line in a given position has the same cost. This
defines iso-performinglines.

We want to find a point in the convex region that the line touches
in its highest position possible. Figure 6 shows the convex hull for
the attrition problem, and cost function moving up through the
region. The highest position of the cost line is obtained at vertex
A. The theory of Linear Programming shows, that for a convex
and bounded region, the objective fimction is maximized either at
one of the vertices, or on a line segment joining two vertices
Bazaara [1]. Thus, we can always pick one or more best
performing classifiers.




oH
..........
0.8
.........
"......
~.....
- ............

0.7
..,-
.,...---" ...... -"* ....
"

0,55 "
. ...............
"...... ""
"....
0.4
... - ..... ·.................




0
0.1
02
0.3
04
0.5
0.6
0.7
0,8
0.0
1

FPmte



Figure 6. Cost functions traversing the ROC convex hull
As in the ROCCH analysis, we have iso-performing lines, which
help visualize performance of classifiers under various cost
structures. In the Linear Programming setting, however, the
feasible region c~tnincorporate any additional constraints. We are
gaining flexibility and control.


5. ADDING ADDITIONAL CONSTRAINTS
In the attrition example, some implicit constraints are already
built into the ROC space. Error rates are non-negative and cannot
exceed 1. Those constraints bound the convex set and guarantee
existence of an optimal solution. In addition, a planned calling
campaign is limited by customer service resources availability. A
phone call scenario included approaching a customer to find out if
indeed their intention was to leave the bank, and to attempt to
entice them to stay. Naturally, there are limited resources bank
can devote to this undertaking. Traditionally, modelers would
select the best model, and any additional constraints would be
imposed at the implementation time, based on the pre-selected
model. But if a modeler is aware of the constraints, they can be
built into the system evaluating model s performance. The line of
business, for which these classifiers were developed, determined
that they could handle calling 20% of their customer base. This
resulted in the constraint


FP+TP<_ 0.2 M

In order to plot the constraint in the (x, y) error rate space, we
need to formulate it in terms of the decision variables x and y.


(l-p) x-p y _< 0.2

Geometrically, this inequality is represented by a half plane in the
(x,y) space. Figure 7 shows a case of the attrition problem with
an additional constraint. The new convex set is the intersection of
the original one with the half plane. The feasible region is now an
intersection of the previous convex set with the new one: the area
to the left (down) of the dotted line.


Attrition model with workforce capacityconstraints




i "1
fi

0
0,1
0:2
0.3
0,4
0.5
O.S
0.7
0.8
0~9
1

FP rate




Figure 7. The capacity constraint bounds the feasible region

Our optimal classifier is no longer feasible, since it is outside of
the new feasible region.

We need to slide the cost line back down to the feasible region.
It will touch the feasible region at the point where the hybrid
classifier intersects the constraint line. As noted earlier, this point




608

defines a new classifier. The new classifier is now optimal, as
shown in Figure 8.




1
0.9
0.8
0.7
0.8
0.5
0,4
0.3
0.2
0.t
0
0.00
Opl~ma)solution change
under cspacity connatrsint,~




"'" ~,fe~
~"~'~"
New opUmal

i
solution




0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
FPrate


Figure . New optimal solution

In the next section we show how to find the new vertex. Here, we
will point out an actual implementation of the new solution as
outlined by Fawcett and Provost in [3].


Assume that the intersection point divides the line segment
between A and B at ratio cx. To obtain the new classifier, we can
proceed as follows.


1.
With probability ~ use classifier A
2.
With probability 1- ~ use classifier B

If A and B were obtained from the same model by varying the
decision threshold, this process can be simplified by finding an
appropriate threshold between A and B.


6. LINEAR PROGRAMMING FORMULATION
A linear program is a constrained optimization problem, where
the objective function, as well as all constraints, are linear. We
need to select values for all decision variables so that all
constraints are satisfied and the objective function is minimized
(or maximized). In this case, we need to pick a point(s) in the
ROC space which will minimize the cost function (or maximize
the modified cost fimction). Decision variables are then the
coordinates (x,y) of points in the ROC space. Some of the
constraints arise from the classifiers performance.

In the ROC space, points under consideration need to be below
the boundary of the convex region. Additional constraints are
usually related to implementation and/or quality issues.

Finally, we have non-negativity constraints
and
bounding
constraints, since the ROC variables have to be between 0 and 1.
A canonical form of a linear (maximization) program takes the
following format.


Maximize
C --~
ej x j
J

Subjectto
E
a0xi --<bi
i
xj~O
i=l,
,k.


j=l,
,d
The theory of linear programming assures us, that if the set of
constraints forms a convex and bounded set (called the feasible
region), then an optimal solution is found on the boundaries of the
feasible region [I ]. In our - two dimensional - case, a solution can
be found at a vertex, or on an edge joining two vertices. Note that
the feasible set is created as a conjunction of several linear
inequalities. Not all vertices are known explicitly. A number of
computational techniques have been designed to find optimal
solutions without explicitly iterating over the vertices. More
details can be found in [1] and [2].

The theory of Linear Programming also aids an analysis of a
solution found, if we want to play some what if scenarios. At a
point of optimality, som,
constraints will be satisfied as
binding . That is, the left-hand-side will be equal to the right
hand side. Others will be satisfied as an inequality, leaving
slack, or room for improvement.

In a two-dimensional case, if two constraints are found binding at
an optimal solution, the classifier at the intersection of those
constraints is optimal. An analysis of slack at neighboring points
(called marginal analysis or sensitivity analysis), often provides
insight into alternative solutions and how close they are to
optimality. All commercially available optimization packages,
including a module that comes with Excel, will provide slack
information for all constraints. We are showing an example of
Excel sensitivity analysis report in the Appendix. In that example,
the
capacity
constraint,
which
was
based
on
workforce
availability, is binding. That means that any further improvement
of classifier s performance will require additional workforce
resources. Additional resources will provide slack on the capacity
constraint. This will allow the optimal solution to move along the
convex hull boundary in a direction improving the objective
function.

We will now formulate the problem of looking for optimal
classifier in the presence of additional constraints, as a linear
programming problem. We already have a formal representation
of the objective function.


Maximize
C (x, y) = - c(Y n) (l-p) x + c(N y) p y


Now we need to formulate the set of constraints related to the
classifiers and add a set of additional, bounding and non-
negativity constraints.

Given two classifiers Pi and Pj on the boundary, with error rates

(xj, ~) and (xi, Yi).Assume that xi --< xj .A line segment through
points P~ Pj has the slope
m=(yj
yi)/(xj
xO
and the equation
Y Yi= m ( x
xi)

The feasible region is positioned below the line, so it is
determined by a collection of inequalities


y
yi-<((yj
yl)/(xj
xi))(x
xi)

which can be rearranged as


-(Yj
yOx+(xj
xi) y ~
-xi(yj
YO+Y~(Xj x~)




609

The optimizationproblem can now be defined as follows.
Given classifiers Pi with error rates (xi, Yi),i = 1,2,
k,
such that xi -< xi+l

Maximize
C(x,y)=-c(Y
n) (l-p) x+c(N y) p y

Such that
a~iix+ a20y _<b0
fori=l,2,
kj=i+l
~l]x+~jY
~ ql
1=1,2,1o
0_<x_<l
0_<y_<l
where
alij =" (Yj
Yi), a2ij = (xj
xi)
bij = - xi (y]
Yi) + Yi (xj
xi)
and
r~ll, ~l and ql form additional constraints


6.1 Computational considerations

In the absence of additional constraints, all vertices are known a
priori. In such case, it is computationally efficient, to explicitly
calculate expected cost of each classifier and choose the one with
the smallest cost, rather than actually slide the lines over the
feasible region.

When additional constraints are introduced, the situation changes.
Finding new vertices, formed by the additional intersectionpoints,
can get cumbersome. Fortunately, the theory and practice of
mathematical programming provides efficient algorithms for
finding optimal solutions without a need to iterate over all
vertices. There are also efficient solvers on the market, which can
solve optimization problems. An add-on to Excel provides one
such solver. It is efficient for problems in small dimensions as the
ones described here, and it is available in almost any business
setting. Figure 9 shows the original attrition problem, without the
additional constraint, solved in Excel. Spreadsheet cell labeled
Cost contains the cost formula. Value of that cell is maximized,
by changing decision variables x and y, as long as all constraints
are satisfied. Point (0.64, 0.96) minimizes the objective function.

Maximizing Modified Cost Function
-c(Y I n)*(1-P) TM+ c(N I Y)*P*Y

Error Cost
per unit
Cost
Decision variables
; Ai(rltlOn

c(YJn)
c(Nly)
Rate p

~;
30
~2,575
0.0375

Data Points
Slopes

FP rate
TP rate
xl
~1
m
0
0
0.06
0.2,=
3.83
0.14
0.4~
2.31
0.23
0.5~
1.66
0.33
0.7(
1.35
0.42
0.8~
1.28
0.53
0.go
0.72
0.64
O.9E
0.55
0.75
0.9E
0.21
0.88
0.9~
0.08
1.00
1
0.04
Constraints


- mx + y
<=
- m'x1
+ y'


-IA8
<=
0~00
-0~51
<=
0.10
:
-0~09 ~
<=
0:19
0,10
<=
0~28
:
OA5
<=
0~20 ....
0:50
<=
0~52
o~61
<:
0:81
082
<=
'
0.82
0,91
<=
0.92
....i :;0~3
<=
;::0~06


Figure
. Excel optimi er solves the optimi ation problem
Slopes of line segments on the boundaries are calculated in
column 3. Note that at point (0.64, 0.96), slope changes from 0.55
to 0. 21. As noted earlier, slope of the cost line is 0.3. So point
(0.64, 0.96) would have been selected as optimal by the traditional
ROCCH analysis as well.

Figure 10 shows a new solution, after the additional capacity
constraintwas added. The new solution is (0.15, 0.44).


Maximizing Modified Cost Function
-c(Y I n)*(1-p)*x + c(N I Y)*P*Y

Error Cost
per unit

c(Yln)
c(Niy)
Rate p
I

$
30 J $ 2~576 0.0375

Data Points
Slopes

FPrate TP rate
xl
yl
m
0
0
0.06
0.25 3.83
0,14
0.43 2.31
0.23
0.58 1.66
0.33
0.70 1.35
0.42
0.82 1.28
0.53
0.90 0.72
0.64
0.96 0.55
0,75
0.98 0.21
0.88
0.99 0.08
1.00
1 0.04
Cost
Decision variables




Constraints


-
mx + y
<=
- m'x1 + yl


-0.14
<=
0,00
0,09
<=
0;10
0.19
<=
0,19
0,24
<=
0,26
0.25
<=
0.29
0,33
<=
0.52
0.36
<=
0i61
OA1
<=
0,82
0.43
<=
0.92
.....
0,'14
........
<=
o.os,
Capacity Constraint

(1-p)*x + p*y
<=
0.20
0.16
<=
0.16


Figure 10. Excel solution incorporating the capacity
constraint


6.2 Note on practical considerations
For all practical purposes, a business setting often prefers speed
and expediency of delivery, to an optimal solution, as classifier s
performance is near optimal. In case of the attrition problem, we
were lucky, in that one of the existing classifiers (0.14, 0.43) was
in close proximity of the optimal solution (0.15, 0.44) and still
within the feasible region. A selection of near optimal solution
was, in this case, a simple decision because this was a relatively
simple problem with just one additional constraint.


6.3 New performance constraints
While analyzing the new optimal solution, we notice that the true
positive rate is 0.44. In other words, out of all instances of the
positive class, only 44% (less than a hal0 is classified correctly.
This is not a desirable performance. 54% of defecting customers
remain unrecognized and without being contacted will leave the
bank. Solution analysis (see Appendix) shows that the capacity
constraint is binding at the optimal point. We cannot increase the
True Positive rate y, without leaving the feasible region and
violating the capacity constraint. Any further improvements in the
attfitors recognition rate will require additional resources so that
the capacity constraint can be relaxed. The optimization template
set up in Excel allowed us to play a number of
what if
scenarios. In a final compromise, the line of business decided to
relax the capacity constraint to allow for 30% of the population to
be classified as positive. In return, we added a new
missed




610

opportunity constraint, which required that the True Positive rate
is no less than 60%. The optimal solution for this problem with
two additional constraints is shown in Figure 11. The feasible
region is now the triangle contained between the lines: the
classifier constraint, the performance constraint y _>.6 and the
relaxed capacity constraint
(l-p) x+p y_<.3

Atbition model with capacity and performance constraints


0.7



0.65



0.6



0.55
0.2
0.35
fC
0.25
0.3 ·
FPrate


Figure 11. Feasible region with two additional constraints


The optimal solution (0.29, 0.65) found by the Excel Optimizer is
shown in Figure 12.

Error Cost
per unit


cO'in)
c(Nly) ~Ratep
I

$
30 J~2~575 0.037.._,__5

Data Points
Slopes

FPrate TP rate
xl
yl
rn
0
0
0.06
0.25 3.83
0.14
0.43 2.31
0.23
0.58 1.66
0.33
0.70 1.35
0.42
0.82 1.28
0.53
0.9~ 0.72
0.64
0.96
0.55
0.75
0.98 0.21
0.88
o.gg 0.08
t.00
1 0.04
Cost
Decision variables




Constraints


-mx+y
,<=
-m'x1 +yl
0.00

'0,011
<=
:. 0.10 ~'
·
0.17
,':
<=
0,19
0.26
<=
,
0.26 ·
0.28
~;,
<=
~0,29
0.44:
<=
:' 0.S2
:::~
~0.~,9
~:i.: <=
O.61
0.so
....
<=
~:o.~::
:.i:::::~:::0ie3
i: : ::
<=
~.: :0:92: ....
:: :: 0:64
<=
:0:0S

Capacity Constraint
I1 p :gp'y
<=
030
i
:i
<=
;
030:'
:
Missed OpportunityConstraint
~,
>=
0.60
0165! !,
>=
0i60


Figurel2. Excel solution with two additional constraints


Other examples of frequently used performance constraints could
include:
-
limited size of a direct mail campaign
-
restrictions in processing capacity for responders to a
campaign
-
limited expense of incentives for responders
-
limited capacity of response processing systems (indirectly
resulting in limitinga campaign size)
In each case, we need to express the constraints in terms of
decision variables to create the new convex hull to intersect with
the one created by the classifiers set.

Note, that in this formulation we do not need to know the
intersection points of ROCCH with the constrains convex hull.
The optimal solution can be found with no explicit knowledge of
vertices of the combined convex hull.


7. CONCLUSION
Linear Programming is a part of a broad field of constrained
optimizations called Mathematical Programming. Mathematical
Programming is a rich discipline and its applications have been
growing fast in the past several decades.

Once a problem has been framed as a mathematical programming
problem, we gain a powerful tool. More importantly, we can draw
from that field s rich legacy. Some future developments may, for
example,
include
multi-dimensional
problems
where
a
classification problem has more than two classes. Computational
difficulties grow fast with problem s dimensions, but techniques
and methods of Linear Programming can help overcome
computational issues. We could also consider problems involving
non-linear cost function. The theory of Non-linear Programming
may guide us in minimizing non-linear costs, such as piece wise
linear or quadratic. Other areas of pursuit may include methods to
deal with classifier and/or constraint uncertainty. Stochastic
Programming methods could perhaps be employed to deal with
optimal solutions if the standard errors of classifiers are need to be
taken into account.

At the same time we are not losing the main benefits of ROCCH.
We maintain the benefit of visualization, except the cost function
now slides along the convex hull. The benefit of being able to
choose the optimal classifier at the run time if constraints change,
can be maintained as well. The selection process would proceed
as follows:
Input new costs and constraints
Intersect a constraints space with the ROC space
Input all competing points
Solve the optimization problem


· ACKNOWLEDGEMENTS
Thanks to all who helped to inspire, develop and formulate the
above thoughts. In particular my gratitude goes to Entire Boros,
Stan Matwin and Vera Helraan who generously shared their
knowledge and experience and provided feedback throughout the
various stages of this work.


· REFERENCES
[1] Bazaraa, Moktar S. at ai. Linear Programming and
Network Flows. John Wiley & Sons, Inc., 1997.
[2] Murty, Katta G. Operations Research: Deterministic
Optimization Models. Prentice Hall, Inc. 1995.
[3] Provost, F., Fawcett, T. Robust Classification Systems
for Imprecise Environment.Proceedings of the Fifteenth
InternationalConference on Artificial Intelligence 98.
[4] Provost, F., Fawcett, T. Robust Classification for
Imprecise Environment Machine Learning, Vol. 42, No.3,
2001, pp.203-231




611

10. APPENDIX
Answer report and sensitivity analysis report produced by an
Excel Optimizer for the attrition problem with capacity constraint.


Ml~rooo1~Excel 8.0eAnswer Ropol*
Work,sheet:[ROCCHl.yJ~]Optimizetion
ReportCreated:2/28/20028:16:12PM

TargetCell(Max)
Coil
Name
Odginli'Vllue
Fi.iI Vlluo
SD$4 Max
0.00
38.33


Adi,..,stableCeils
Cell
Name
Odgln'l]'~alue
Flnll Value
$E$4 x
0.00
0.15
~#~
y
o.oo
0.44

Con~trints
ceil
Him
$057
-rnx + y
,$p$8 -r~÷#
SDSg -mx+y
SD$10 -mx+y
$D$11 - rnx+y
$0512 -rnx + y
$D$13 - nr~(+ y
!D~14,,,-mx * y
$0S15 - nix + y
SD$16 -mx+~
.$D$20 (1-p)*x+p*y --
0.16
$F$4 y
0.44
$E$4 x
0.15
Clll Vilul
Formula
" Status
Slack
0.00
SD$7<:$F$7 Bindinli
0
-0.14
$058<=$F$8
NotBinding
0,14
0.09
$D$9<=$I=$9 NotBinding
0,01
0,19
~D$10<=SFSlOBinding
0
0.24
$O$11<=$F$11Not Binding
0,02
02.5
SD$12<=SF$12NotBinding
'0.04
0.33
$O$13<=$F$13Not Binding
0.19
0.38
SD$14<=$F~14NOtBinding
0.25
0.41
SD$15<=SF$15NotBinding
0.41
0.43
SDS16<=$F$16Not Binding
0,49
$DS20<=$F$20Binder__
0
$F$4<=1
Not Binding
0.56
$E$4~=1
Not Binding
0.88
Ltoro~(t E~cd Ue , ~ y
~,~
Worksh~:/ROCCHI.J4s/C~;,,~.,~a




Cd
Name
i
Col
C o S
Jim


$F$4 y
0.~
0
~.6
1~
~.12


Cor,~ ;..,
Rnal elm
Con/mr
Nlalbie
Mmvabl/
Cell
Nm
Vduo
~
RKSkb
~
'$D$7 - n'~c+y
000
0
0
1E+30
0
...~
-r~+y
-0.14
0
0
IE.t-30
0.14
-rn0(+y
0.09
0
0.10
1E+30
0.01
~10
-rr~+y
0.19
91.77
0.19
0.01
0.47



~13
-mx+y
0.33
0
......0.52
1E.¢-30
0.19
$13514 -ni(+y
0.36
0
0.61
1E+30
0.26

~m....
o.
o
e.
o
n.,
.$D$20 II..p/'X+ p~
0.16
127.87
.......0.16
0.(36
0.01




612

