Fast Ordering of Large Categorical Datasets for Better
Visualization


AlinaBeygelzimer
Department of Computer
Science
University of Rochester
Rochester, NY
beygel@cs,rochester.edu
[Extended Abstract]

Chang-ShingPerng
IBM T. J. Watson Research
Center
Hawthorne, NY
perng@us.ibm.com
Sheng Ma
IBM T. J. Watson Research
Center
Hawthorne, NY
shengma@us.ibm.com



ABSTRACT

An important issue in visualizing categorical data is how
to order categorical values. The focus of this paper is on
constructing such orderingsefficientlywithout compromising
their visual quality.


1.
INTRODUCTION
Visual representation has become increasingly important
for the analysis and exploration of large collections of mul-
tidimensional data [14, 5, 1], because visual perception is
remarkably good at identifying spatial relationships.
The
goal is to present the information in such a way that re-
veals interesting trends present in the data. Many domains
contain categorical attributes, for example, host names in
system management data or city names in census data. The
lack of a natural ordering of categorical values makes it diffi-
cult to use standard visual techniques such as scatter plots or
parallel coordinate plots: there are exponentially many ways
in which the values can be totally ordered (and thus mapped
to the axes in the visual space), and each ordering provides
a different visual quality. To illustrate the importance of
having a proper ordering, consider a dataset [12] contain-
ing over 10,000 events of 20 types, generated by 160 hosts
in a three-day period.
Figure 1 shows two different scat-
ter plots with the x-axis denoting the time of an event and
the y-axis representing the host that generated the event.
The ordering on the y-axis in Figure la) is a random per-
mutation of host names, while the ordering in Figure lb)
was constructed algorithmically. Although the plots illus-
trate the same dataset, they provide qualitatively different
visualizations (see Section 4).
Although the problem has been studied before [12, 11, 2],
efficiency and accuracy issues have not been adequately ad-
dressed due to the inherent complexity of the problem. First,
a natural discrete formulation is NP-hard; hence finding the


Permission 1omake digital or hard copies of all or part of this work lbr
personal or classroom use is granted without lee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, to republish·to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
KDD 01 San Francisco CA LISA
Copyright ACM 2001 1-58113-391-x/01/08...$5.00
I
B ·
·
°
: ,I
:, ·.i -
·
. !...~_.
."
.,!
" ·° ·.d....
-..i,,
'~ "i
i
; ....
,




.-4r.
'4";
'
";


a) random ordering
~5
®.
d#:
. .':.'."
. ..
~ .~,...~.o~''."~'L~ ~"~"i'/

*#




!
".'..i

"~
os
·
°
·




b) spectral ordering

Figure 1: Two scatter plots of the same dataset contain-
ing over 10,000 events of 20 different types generated
by
160 hosts in a three-day period.



exact solution in most likelyinfeasible. Second, an effective
visualization must use geometric (visual)proximity to cap-
ture the relationshipsbetween data objects, so that similar
objects are placed near each other in the ordering. Any al-
gorithm that explicitlycomputes this underlying similarity
matrix, inevitablyhas quadratic complexity, and thus scales
poorly·
In order to avoid the inherent intractabilityofprevious for-
mulations, we relax the discretenessconstraint and consider
the corresponding continuous optimization problem solvable
exactly using the spectral method [9]. Such continuous re-
laxationshave been successfullyused in variouscontexts, e.g.
graph partitioning[10],sparse matrix reordering [3],cluster-
ing [7].Finding the spectralinformation, however, is rather
expensive (superlinear).To reduce the complexity, we use a
multi.level approach. The data is modeled as a graph. First,
the original graph is approximated by a sequence of increas-
ingly coarser graphs. Then the coarsest instance is ordered
using the spectral algorithm. Finally, the ordering is propa-
gated back by interpolating through the sequence of interme-
diate graphs. Real data is usually highly over-determined,
i.e. contains multiple values with almost identical similarity
structures. The multilevel approach emerges very naturally
in this context: the coarsening hides all unimportant, re-
dundant details about the graph, while preserving only the
most crucial connectivity information. Moreover, the spec-
tral method is particularly good at capturing globalproper-
ties of graphs. On a finer level, however, it is often worse




239

than even simple local heuristics; therefore, the refinemen£
phase can be used to improve the ordering.
In order to circumvent the second problem, i.e. avoid the
quadratic complexity of explicitly computing all pair-wise
similarities, we propose a fast way of constructing an im-
plicit adjacency representation of the dataset. In particulm.,
we show that it suffices to observe only a small portion of
the similarity matrix (of size linear in the number of objects,
as opposed to quadratic), and still be able to coarsen and
order the dataset in such a way that preserves the most cru-
cial structural information. In all existing schemes, coars-
ening is done by coalescing disjoint subsets of vertices of
the finer graph into multi-vertices of the coarser graph. In
contrast, we propose a new coarsening scheme, in which
multi-vertices are not necessarily disjoint. We argue that
allowing multi-vertices to overlap is quite natural and im-
proves the efficiency of the subsequent uncoarsening and re-
finement phases, as well as the quality of the final ordering.
Our techniques are motivated by the canopy construction
of McCallum et al. [13]. Informally speaking, canopies are
(possibly overlapping) subsets of similar vertices. Our coars-
ening scheme is based on shrinking the canopies into multi-
vertices; this allows to absorb components that are strongly
connected, and thus forces similar values to be in close po-
sitions after the initial ordering is uncoarsened. Allowing
the multi-vertices to overlap simplifies the final uncoarsening
stage that orders the vertices within multi-vertices. We show
that with high probability, a small, uniformly selected subset
of vertices covers the entire graph. That is, if we uniformly
at random select a small subset of vertices around which
canopies are formed, then with high probability (over the
choice of these vertices), almost every vertex in the graph will
be close to at least one canopy. Thus the canopies provide a
good approximation of the similarity relations between the
values. Intuitively, this representation can be thought of as
categorical vector quantization.

1.1
Related work
A simple approach to ordering categorical values is to ei-
ther use some auxiliary numerical attribute, e.g. time (Xmd-
vTool [14], Diamond [6]).Such objectives are not related to
any visual measure, and thus can not, in general, guarantee
good visual quality. Ma et 81. [12] developed a clustering-
based approach to ordering.
The algorithm first clusters
the data, and then orders the clusters using combinatorial
optimization. One limitation of this approach is that the
clusters are inherently unordered, which requires additional
non-trivial methods for ordering the clusters, as well as the
elements within each cluster. Another approach proposed
by Ma et al. [11] is based on hierarchical ordering. The algo-
rithm iteratively groups closest pairs of points, progressively
forming a hierarchy of nested suborderings (aka agglomer-
ative clustering). However, such strict hierarchies are not
likely to capture complex relationships present in the data.
Furthermore, the algorithm uses only local properties of ver-
tices, and, as a result, is not particularly good at capturing
the global sructure of the data.
This becomes a problem
when the size and the complexity of the dataset is large.
Closely related to the ordering problem is the problem
of projecting high-dimeusional data onto a low-dimensional
space that itself can be used as a visual representation of the
data. However, there is a fundamental difference between
the objective of ordering and the goal of projection methods
(such as multi-dimensional scaling (MDS) or FastMap [8]).
Given a similarityfunction, projection methods try to pre-
serve the distances between the original objects (in respect
to this similarityfunction) as well as possible. We, on the
other hand, can hardly expect to have a good similarityfunc-
tion, because the dataset isnot well-understood (in fact, our
goal isto aid this understanding). Therefore, instead of pre-
serving the distances in respect to some specific similarity
measure, we expect our similarityfunction to only guide our
algorithms. We remark that this approach is highly similar
to that of Davidson et al. in Vxlnsight [4].

1.2
Contributions of this paper
Our contributions can be summarized as follows.
1. We adopt a continuous formulation of the ordering
problem. This allows to avoid the inherent complexity
of previous discrete formulations.
2. We develop an efficient multi-level algorithm, which
performs the ordering in three phases: (1) coarsens
the dataset reducing its size, and thus the complex-
ity of finding a good ordering; (2) orders the coarsest
instance using the spectral algorithm; (3) uncoarsens
and refines the initial ordering.
3. We present an algorithm for efficiently constructing an
implicit representation of the dataset, thereby avoid-
ing the quadratic complexity of explicitly computing
all pairwise similarities between the values. This rep-
resentation is then used to efficiently coarsen and or-
der the dataset. Moreover, it can be used to answer
queries concerning the neighborhood of most points in
time that is independent of the size of the dataset. The
complexity of constructing this representation is linear
in the size of the data.set.
4. We propose a new coarsening scheme that differs from
all existing schemes in that it allows multi-vertices to
overlap. This provides an interesting generalization of
graph coarsening and also gives valuable information
for the refinement phase.

We also develop a method for solving the multi-attribute
ordering problem by reduction to the single-attribute case.

2.
FORMULATION
OF THE PROBLEM
2.1
Definitions
Let A1,A2,...,Ak be a set of k attributes with respec-
tive domains D1,D2,...,Dk.
A data entry r is a k-tuple
(1-1,...,1"~), where ri E Di for all i, 1 < i < k. A dataset
r is a sequence of data entries. For example, consider r =
{(F, a}, (F, b), <F, a), (G, a), <G, c}, (H, a}, (H, c~, (H, c}},
where D1 = {F, G, H} is the set of hosts and D2 = {a, b, c}
is the set of corresponding event types. Then the data entry
(F, a} corresponds to an event of type a generated by host
F. We distinguish two types of similarity measures.
An intra-attribute similarity function 7i : D~ x D~ -+ [0, 1]
measures the similarity of values of the same attribute Ai,
e..g. the similarity of hosts. Then for each value x E D~, let
F~ denote the sequence of all data entries in F that have x
as the value of attribute A~ (we omit x from the entries).
For example, in our case F~ = {(a), (b), (a}} is the sequence
of events generated by host F.
Our intra-attribute similarity function is based on the
Dice's coefficient

~,(~,y) = 2 Ir__~n r~_AI
i
i
'
Ir~l + Irul




240

where f3 denotes the sequence intersection (sequences are
treated as multi-sets). In our example, "yl(G, H) = 4/5,
while ~fl(F, G) = 2/5 and ~/2(F, H) = 1/3.
We are also interested in the inter-attribute similarityfunc-
tion 7tJ : Dt x Dj -~ [(3,1], which captures the similarity of
values across different attributes, 1 < i < j < k. For x E D~,
y E D~, we define


Ir'~l
Ir~l

For example, the similarity of host F and event type a is
")'12(F,a) ----1/3.
We associate the dataset with its underlying pairwise sim-
ilarity graph, where the vertices represent the values to be
ordered. Notice that the vertices may represent the values of
different attributes. We call such graphs heterogeneous.
The
weight of an edge corresponds to the similarity of its end-
points; if both end-points represent the values of the same
attribute (e.g. both are hosts), then the intra-attribute simi-
laxity is used; otherwise, we use the inter-attribute similarity.
This representation allows to visually explore the relations
between different values not only within the same attribute,
but also across the attributes. Section 2.3 discusses how to
use the heterogeneous representation to order the values of
multiple categorical collections simultaneously.

2.2
Problem Formulation
Given a categorical attribute A with domain D and
the
inter-attribute similarity function 7 : D x D ~
[0,1], the
goal of the linear ordering problem is to find a permutation
of D such that

1
~r= arg min ~ E
"y(a,b)Or(a ) - lr(b)) 2.
(1)
a,bED

Clearly, the above formulation encourages similar values to
be in close positions in the permutation.

2.3
Ordering the values of multiple attributes
Most real datasets have multiple categorical attributes. It
is important to order these categorical sets in such a way
that not only is each ordering good, but the orderings are
also good in respect to each other~ i.e., the orderings have
to be consistent with each other, so that they can be used
together in constructing effective visualizations (for example,
in parallel coordinate plots). In order to extend the ordering
algorithm to multiple categorical attributes, we propose to
create a "heterogeneous" linear ordering that represents a
mapping of all (or a subset of) attributes onto a single line.
We use our example to make this notion clear. Instead of
ordering host names and event types independently, we map
their values to the set of objects {or = F, 02 = G,03
=
H, 04 = a, 05 = b, oo = c}. Now the problem simply reduces
to the problem of ordering a single set of values. Once the
ordering has been constructed, it can be mapped back to the
original values in an obvious way. For example, to obtain
the ordering of host names one would simply filter out the
ordering of objects corresponding to host names, i.e. the
ordering of 01, 02, o3. Notice that since we defined both
inter-attribute and intra-attribute notions of similarity, the
similarity between objects is well defined.
This allows us to focus on the single-attribute case. Hence,
in what follows, we may assume that we have a set of objects
(called points) $ = {1.... , n} and a similarity function q,(., .)
defined on pairs of objects from 8. Two points a,b E S are
said to be e-closeif7(a, b) ~_ 1 - e.

2.4 Continuous Formulation
As we discussed in the introduction, the discrete formu-
lation (1) is NP-hard; hence the solution is not likelyto be
obtained efficiently.To avoid the combinatorial complexity,
we allow the coordinates (the values of ~r) in Equation (1)
to be real. This yields a continuous optimization problem of
finding a real placement vector x = [xl....,xn] (where x~
is the placement of the i-th point) minimizing the weighted
sum of squared distances between the points,

·
1
x=argmxm~ E
"/(i,j)(xi-xj)
2,
such that Ex~
= 1.
ij~s
i
(2)
A normalization constraint is needed in order to avoid a triv-
ial solution when all the points axe mapped to zero. Clearly,
the above formulation encourages related points to be near
each other on the line, while pushes dissimilar points apart.
Hall [9] showed that the solution of the above problem is
just the second eigenvector of the weighted Laplacian ma-
trix associated with the similarity graph of the data. Even
though mapping the solution of the continous problem to
the closest discrete point does not necessarily yield the opti-
mal discrete solution, it provides a good approximation [3]).
However, finding the eigenvectors is rather expensive. More-
over, even before computing the eigenvectors, the algorithm
has to compute the underlying similarity matrix of the data,
which is infeasible for large datasets.


3.
THE ALGORITHM

3.1
An overview of our approach
The efficiencyof our algorithm is based on the following
key ideas. The firstis to reduce the size of the dataset by
coalescing (not necessarilydisjoint)subsets of similar points.
This serves two purposes. First,the spectral algorithm has
be run on a much smaller set of points, which significantly
reduces the complexity. Second, by absorbing groups of sim-
ilar points, we guarantee that these points will be in close
positions in the finalordering· The second idea is to avoid
the quadratic complexity of explicitly computing all pair-
wise similaritiesby constructing an implicit representation
that approximates the underlying similarity matrix. An im-
portant observation is that only a linear number of similar-
ities (instead of quadratic) have to be computed in order to
coarsen and order the dataset. In other words, in order to
obtain a high-quality ordering of points, we do not actually
have to compute most pairwise similarities. We also show
that a sample of size independent
of the size of the dataset
is sufficient to approximate the adjacencies of most points,
allowing to answer neighborhood queries in constant time.
The algorithm proceeds in three phases: First, the original
dataset is coarsened. Once the dataset is sufficiently small,
it is ordered using the spectral algorithm. Notice that this
requires the original spectral algorithm to be extended to
multi-vertices, which can be done by incorporating vertex
weights into the normalization constraint. Finally, the re-
sulting ordering of the reduced dataset is uncearsened and
refined.

3.2
Coarsening the graph




241

Our coarsening scheme is motivated by the canopies tech-
nique of McCallum et al. [13], proposed in the context of
clustering. The idea is to first use a cheap distance function
to divide the points into overlapping subsets called canopies.
Each canopy contains points that are within some small dis-
tance from each other. Hence the points that do not share at
least one canopy are far enough not to be in the same cluster,
and thus can be ignored in the final fine-tuning stage that
refines the cluster using a more accurate (and thus more
expensive) distance function.
McCallum et al. [13] proposes the following canopy con-
struction: Given an inner and an outer distance thresholds,
pick a point and compute its distance to all other points us-
ing an inexpensive distance function. Put all points within
the outer distance threshold into a canopy. Mark all points
that are within the inner distance threshold. Repeat from
any unmarked point, until all points are marked. Once the
canopies are constructed, the exact pairwise distances need
to be computed only between the points that share at least
one canopy. This allows to avoid a large number of (ex-
pensive) exact distance computations, because the canopies
exclude a large number of points in the dataset. It can be ar-
gued that the accurate clustering solution can be completely
recovered provided that the inexpensive distance function
and the thresholds satisfy certain properties. However, these
properties do not allow the construction to simultaneously
keep canopy sizes small (constant) and the construction cost
linear. In particular, it is required that for every traditional
cluster, there exists a canopy containing all of its elements.
Thus canopies are likely to contain O(n) points on average.
If we lose some clustering accuracy and bound the average
number of points in a canopy by a constant, the expected
number of canopies will in turn increase to O(n) making the
construction cost quadratic, because creating each canopy
requires O(n) (inexpensive) distance computations.
The above scheme is beneficial in the case when the exact
distance function is indeed much more expensive than the
approximate one (for example, it may be based on computing
the edit distance of strings or the time-warping distance of
temporal sequences). As we discussed in Section 1.1, we do
not expect our similarity function to be perfect; therefore,
we don't want it to be expensive. Intead, we are much more
interested in reducing the cost of the construction.

A new canopy construction algorithm
The original
canopies algorithm was rather conservative. This was neces-
sary in the context of clustering in order to guarantee that
no accuracy is lost due to approximation. Here we can afford
some slackness, which allows us to achieve better efficiency.
The first idea is to switch the role of the inner and the outer
distance thresholds. The second idea is to show that in or-
der to approximate the similarity relations between points,
it is sufficient to select a small random sample of vertices in
a canopy. A detailed description of the construction follows.
Our algorithm is given in Figure 21. The reader may think
of the sets z~ and A~~ as, respectively, the inner and the
outer spheres of influenceof the center point p E .4. Once p is
selected, the algorithm forms a new canopy Cp by uniformly
sampling a small subset of points from the inner sphere of

1 Here, and in the rest of the paper, c, d are application depen-
dent constants. Clearly, there is an accuracy/efficiency tradeoff:
the higher the constants, the more accurate (less efficient)the ap-
proximation. The choice of · (0 < · < 1) depends on the overaU
sparsity of the data: the denser the data, the smaller c.
Algorithm
CanopyConstruction(S,~)
Given
distance parameter ~,
approximation error 5.
begin

repeat cC 1ln(2c/6e) times:
begin
1. Uniformly at random select a point p E ~q-6.
Let ~4+--.ztU{p}.
2. For each q GS -27, compute f(p, q).
3. Denote the set of all q E ,-q-27that are e-dose
to p by A~. Similarly define A~.
4. Set27~-27UA~,
£~--CUA~.
5. Create a new canopy Cp by sampling c~/e
points uniformlyat random from A~.
Make p the center of Cp.
end
end

Figure 2: A new canopy construction


p. Notice that the original canopies algorithm would have
included all points in the outer sphere. Once the canopy
is formed, all points within the outer sphere are excluded
from being the center of (and thus forming) new canopies.
Our motivation behind this is that all the points in the outer
sphere ofp are already close enough to some canopy (namely
Cp), thus by excluding those points we give an opportunity
to other points to get approximated. When a point is close
to at least one canopy, we say that it is covered. By sam-
piing the neighborhood of 1ouniformly at random we get a
representative sample space that approximates all the points
in the neighborhood. The reason why we do not include all
the points within the inner sphere is that different regions
of the data space inevitably have different densities; hence
using the same distance parameter uniformly for all center
points would have resulted in a large variation in canopy
sizes; in particular, center points that belong to dense re-
gions of S are likely to contain O(n) points in their inner
spheres, while we need to keep the size of each canopy in-
dependent of n. Also notice that the number of canopies
is constant by construction. (It depends only on the qual-
ity of desired approximation and the distance parameter e.)
Figure 3 illustrates the construction.


÷




Figure 3: An illustration to the
construction in Figure 2.
Points that were
chosen to be included
in a canopy are shown
as bullets ("o"). Cen-
ter points are slightly
bigger than regu-
lar canopy points.
Notice that while
there are a lot of
points that do not
fall within any canopy
(shown as "+"), al-
most all of them are
covered, i.e. are at
distance at most 2e
to at least one center point. In fact, only two points are
missed. The points that are not covered are generally the
outliers (points lying in sparsely occupied regions, at a dis-
tance from the main clusters). For example, consider the
points D and E in Figure 3.
In this example, the size of each canopy is at most 4, i.e.
at most 4 points are sampled from the inner sphere of each




242

center point to form the corresponding canopy.
Now we are no longer guaranteed tllat each point appears
in at least one canopy; in fact, most of them will not, because
both the number of canopies and the number of points within
each canopy are independent of n. We will, however, show
that with high probability over the choice of center points,
every point will be sufficiently close to at least one canopy.
Indeed, if a point q belongs to Z, we know that it is e-close
to some p E .4, and thus, by the triangle inequality, it is
2e-close to every point in Cp. Since the points in C~ are
chosen uniformly at random from the e-neighborhood of p,
they are representative of the neighborhood, and will give a
good approximation of q for free. Similarly, if q E E-I, it is
2e-close to some center point, and thus at most 3e-close to
the points in the corresponding canopy. Otherwise (i.e. if
q E S-£), q's adjacency relations cannot be approximated
using the distances already computed. We need to bound the
probability that this bad case happens. The proof follows
the following sketch. A point is said to be dense if it has
a lot of neighbors (in some precise sense of "a lot"). We
first show that with probability at least 1 - ~, a random
set .A of size O(e-1 In(l/e)) covers almost all dense regions,
i.e. every dense point is e-close to at least one point in .A.
Then we show that almost all points in the set are close
to at least one dense point. Thus the construction yields a
constant number of canopies that with high probability (over
the choise of canopy centers) cover almost all points in the
set.

Coarsening scheme Once the canopies are constructed,
a natural way to coarsen the graph is to shrink them into
multi-vertices. The points within a canopy are close to each
other; hence by contracting the canopies we absorb subsets of
similar points, generating graphs that are loosely connected.
Notice that this is precisely what we want, because it forces
similar points to be near each other after the initial ordering
is uncoarsened. Of course, since the goal of coarsening is to
decrease the complexity of the graph, the average number of
points in a canopy should no longer be constant. Otherwise,
shrinking a constant number of canopies would reduce the
size of the graph only by a constant term.
NOW instead of sampling
from the inner spheres, we
. D
add all the points in the outer
sphere to the corresponding
·
B
C
. E
canopy. Notice that the multi-
A
vertices are not necessarily
·
e/
disjoint. The coarse graph,
however, isstillwelldefined.
·
This relaxationdistinguishes
our scheme from the stan-
dard coarsening methods. Fig-Figure 4: Shrinking the
ure 4 illustratesthe coUec-
canopies in Figure 3.
tionofmulti-verticesobtained
by shrinking the canopies in Figure 3. The size of each
multi-vertex corresponds to the number of original points
it contains. Multi-verticesthat overlap are connected by
lines. The similarityof two multi-verticesis defined as the
similarityoftheircentersweighted by the sizeofthe smaller
end-point. We argue that allowingthe multi-verticesto over-
lap simplifiesthe uncoarsening phase that orders the points
within multi-vertices.


3.3
Uncoarsening
Once the coarsegraph isconstructed,the spectralordering
algorithm can be used to finda high-qualityordering,which
is then projected back onto the originalgraph. Since the
verticesof the coarser graph represent groups of verticesof
the originalgraph, uncoarsening the ordering simply reduces
to ordering the verticeswithin each multi-vertex. Of course,
the resultingordering may not be optimal for the original
graph, because when the graph is coarsened only a fraction
ofpossibleorderingsisrepresented on the coarserlevel.This
problem can be alleviatedby locallyrefiningthe ordering. In
general, the ordering of the finergraph respects the order-
ing of the coarser graph. Now we can benefitfrom allowing
the multi-verticesto overlap. Iftwo multi-verticesare adja-
cent in the coarse ordering,then theiroverlap should clearly
be placed on the border between them in the uncoarsened
ordering. This way the overlaps "connect" the suborder-
ings corresponding to multi-vertices. The private parts of
the multi-verticescan be ordered arbitrary. The refinement
phase consistsof repeatedly picking a pair of verticeswithin
the same multi-vertex,computing theiraverage distance to
the points in the adjacent multi-vertex,and switching them
if a violationis found, i.e. ifthe vertex with the smaller
distance isfartheraway from the adjacent multi-vertex.


4.
EXPERIMENTAL RESULTS
Two datasets are used to test the effectiveness of our al-
gorithm. The first dataset is the event data discussed in the
introduction. The scatter plots in Figures la) and lb) show
the random ordering of hosts (on the y axis) and the order-
ing produced by our algorithm. Clearly, Figure lb) provides
a better visual quality and allows us to identify many in-
teresting patterns that can be used for fault diagnosis. For
example, the hosts in the bottom part of the ordering in Fig-
ure lb) generated a lot of "threshold violated" and "thresh-
old reset" events. Afterwards, it was determined that this
was caused by installation errors related to these hosts. Sec-
ond, the two cloud-like patterns in the top part of the plot
consist of "port-up" and "port-down" events generated as a
result of mobile users connecting to and disconnecting from
hubs. This characterizes a normal operation. Third, the
three spike-like patterns, happening every day at 2:00pm,
consist of "SNMP requests" (middle-up part of the plot)
and "authentication failures" (middle-low part of the plot).
Most likely, this is due to either an improperly configured
monitor or a systematic intrusion attempt. Finally, the sin-
gle spike in the middle of the plot corresponds to a series of
link-up and link-down events caused by a software problem
on a group of hubs. Clearly, all this information can not be
easily identified in Figure la).
Figure 5 shows the adjacency matrices of this dataset. If
the entry (i,j) is non-zero (shown as a black dot), then 7(i, j)
is above a certain threshold. Here we used 0.7' as the value
of the threshold. Because the algorithm encourages simi-
lar objects to be in close positions, we can clearly see that
this moves non-entries towards the diagonal. Notice that
adjacent rows are highly similar. In contrast, orderings con-
structed using other approaches, distribute the non-zero en-
tries across the entire matrix, making the identification of
patterns more difficult.
Our second dataset contains events collected from a pro-
duction IT environment. The dataset contains about 6,000
events of 90 different alarm types generated by 410 servers.
Figure 6 shows parallel coordinate plots of this dataset. The




243

~:" "~'.
"i::

~.
*
n
..4cJ'.~




(a) Random


:5n
_
4ii --
y_!m n-m
!



::~1!
I !1!1~1.:

(c) Alphabetical
......
..i .imljimll~*


-'l
! l
I ' ' . i '
'
t.J
,UI! ......]ll/l


(b) Time-based




(d) Spectral
(a) Random
(b) Time-based




Figure 5: Similarity matrices


left (vertical) axis represents the host name; the right axis
represents the event type. A line between the axes connect-
ing host z and alarm type y signifies an event of type y gen-
erated by the host x. As both host names and alarm types
are categorical, we need to order the values on both axes in
such a way that helps identify the relationships between the
values of the two attributes. The plots in Figures 6a-c) cor-
respond to random, time-based, and alphabetical orderings,
respectively. Figure 6d) shows the ordering produced by our
algorithm. Clearly, even though time-based and alphabeti-
cal orderings axe more structured compared to the random
ordering, they are still significantly worse than the ordering
in Figure 6d) (in terms of the number of edge crossings).
The plot produced by our algorithm allows to more easily
identify the relationships between groups of hosts and alarm
types.


5.
CONCLUSION
We have presented an efficient multMevel algorithm for or-
dering large categorical datasets. We show that we can effi-
ciently coarsen the dataset without computing most pairwise
similarities. The coarser version provides a good approxi-
mation of the original problem, inheriting all its essential
connectivity information.


6.
REFERENCES
[1] C. Ahlberg and E. Wistraad. IVEE: an information
visualization g~exploration environment. In IEEE
Symposium on information visualization, 1995.
[2] M. Ankerst, S. Berchtold, and D. Keim. Similarity
clustering of dimensions for an enhanced visualization of
multidimensionaldata. In IEEE Symposium on
Information Visualization, pages 52-60, 1998.
[3] S. T. Barnard, A. Pothen, and H. Simon. A spectral
algorithm for envelope reduction of sparse matrices.
(c) Alphabetical
(d) Spectral


Figure 6: Parallel plots


Numerical linear algebra with applications, 2(4):317-334,
1995.
[4] G. S. Davidson, B. Hendrickson, D. K. Johnson,
C. E. Meyers, and B. N. Wylie. Knowledge mining with
VxInsight: Discovery through interaction. Journal of
Intelligent Information Systems, 11:259-279, 1998.
[5] M. Derthick, J. A. Kolojejchick, and S. F. Roth. An
interactive visualization environment for data exploration.
In Proceedings of knowledge discovery in databases, 1997.
[6] S. Diamond. http://www.spss.com/software/diamond.
[7] P. Drineas) R. Kannan, A. Frieze, S. Vempala, and
V. Vinay. Clustering in large graphs and matrices. In
Proceedings of the lOth A CM-SIAM Symposium on
Discrete Algorithms, 1999.
[8] C. Faloutsos and K.-I. D. Lin. Fastmap: A fast algorithm
for indexing, data-mining and visualization of traditional
and multimedia datasets. In Proceedings of the 1995 A CM
SIGMOD International Conference on Management of
Data, pages 163-174, 1995.
[9] K. M. Hall. An r-dimensional quadratic placement
algorithm. Management Science, 17(3):219-229, 1970.
[10] B. Hendrickson and R. Leland. An improved spectral graph
partitioning algorithm for mapping parallel computations.
SIAM Journal on Scientific Computing, 16(2):452-469, 1995.
[11] S. Ma and J. Hellerstein. Systems and methods for ordering
categorical attributes to better visualize multidimensional
data. Technical Report YO1%8-1999-0237,IBM, 1999.
[12] S. Ma and J. L. HeUerstein. Ordering categorical data to
improve visualization. In Proceedings of the IEEE
Symposium on Information Visualization, 1999.
[13] A. McCallum, K. Nigam, and L. Ungar. Efficient clustering
of high-dimensionaldata sets with application to reference
matching. In KDD, pages 169-178, 2000.
[14] M. O. Ward. Integrating multiple methods for visualizing
multivariate data. In Proceedings of Visualization, 1994.



244

