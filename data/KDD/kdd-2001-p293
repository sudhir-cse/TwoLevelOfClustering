Mining Top-n Local Outliers in Large Databases*

Wen Jin
Anthony K. H. Tung
Jiawei Han
Intelligent Database Systems Research Lab
School of Computing Science
Simon Fraser University
Burnaby, B.C.,Canada V5A 1S6
Email: {wjin, khtung, hau}@cs.sfu.ca


ABSTRACT

Outlier detection is an important task in data mining with
numerous applications, including credit card fraud detec-
tion, video surveillance, etc. A recent work on outlier' detec-
tion has introduced a novel notion of local outlier in which
the degreeto which an object is outlying is dependent on the
density of its local neighborhood, and each object can be
assigned a Local Outlier Factor (I_OF) which represents
the likelihood of that object being an outlier. Although tim
concept of local outliers is a useful one, the computation of
LOF values for every data objects requires a large number
of k-nearest neighbors searches and can be computationally
expensive.
Since most objects are usually not outliers, it
is useful to provide users with the option of finding only
n most outstanding local outliers, i.e., the top-n data ob-
jects which are most likely to be local outliers according to
their I.OFs. However, if the pruning is not done carefully,
finding top-n outliers could result in the same amount of
computation as finding LOF for all objects. In this paper,
we propose a novel method to efficiently find the top-n local
outliers in large databases. The concept of "micro-cluster" is
introduced to compress the data. An efficient micro-cluster-
based local outlier mining algorithm is designed based on
this concept. As our algorithm can be adversely affected by
the overlapping in the micro-clusters, we proposed a mean-
ingful cut-plane solution for overlapping data. The formal
analysis and experiments show that this method can achieve
good performance in finding the most outstanding local out-
liers .


1.
INTRODUCTION
Outlier detection is an important data mining activity
with numerous applications, including credit card fraud de-

*The work was supported in part by the Natural Sciences
and Engineering Research Council of Canada (NSERC-
A3723) and the Networks of Centres of Excellence of Canada
(NCE]IRIS-3 and NCE/GEOID)


Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed tbr profit or commercial advantage and that
copies bear this notice and the lull citation-on the first page. To copy
otherwise, to republish, to post on servers or to redistribute to lists.
requires prior specific permission and/or a tee.
KDD 0l San l:rancisco CA USA
Copyright ACM 2001 1-58113-391-x/01/08...$5.00
tection, discovery of criminal activities in electronic com-
merce, video surveillance, weather prediction, and pharma-
ceut,ical research.
An outlier is an observation that deviates so much from
other observations so that it, arouses suspicions ttrat it is
generated by a different mechanism [6]. Studies on outlier
detectiou are numerous and carl be grouped into five general
categories. The first is distribution-based,
where outliers
are observations which deviate from a standard distribution
(e.g., Normal, Poisson, etc.) [1]. Outfier detection can also
be depth-based
which relies on the computation of differ-
ent layer's el"k-d convex hulls. In depth-based methods, out-
liers are objects in the outer layer of these hulls. The third
category of outlier detection method is distance-based.
In-
troduced by Knorr and Ng in [7], a distance-based outfier
in a data.set D is all object with pet% of the objects in D
having a distance of more than dmm away from it.
This
notion of distance-based outlier generalizes many notions
from distribution-based approaches and is further extended
in [9] so that outliers can be more efficiently discovered and
ranked.
In many clustering algorithms like [8], DBSCAN
[4], BIRCH [10] and CURE [5], outliers are by-products of
clustering and we refer to such outlier-detection method as
clustering-based.
While the above four categories of outlier detection are
interesting and useful in their own rights, our paper will
only fbcus on the fifth category which detect local outliers-
based on the local density of an object's neighborhood. We
refer to this category as density-based.
The concept of
a local outlier is an important one since in mauy applica-
tions, different portions of a dataset can exhibit, very differ-
ent characteristics, and it. is more meaningful to decide on
the outlying possibility of an object based on other objects
in its neighborhood.
In view of this, [3] defines the con-
cept of a local outlier factor(l_OF), wtfich is intuitively
a measure of difference in density between an object, and
its neighborhood objects. Unfortunately, the work done in
[3] requires the computation of LOF for all objects which is
rather expensive because it requires a large number of k-
nearest-neighbors queries. As it, is observed that many ob-
jects have I_OF values whirl1 are very close and are unlikely
to be interesting outliers, we believe that a system should
provide users with tire option of constraining a search to only
the top-rz local outliers instead of computing the LOF of ev-
er'y object in a database.
Given our motivation, it is obvious that finding COF for
all objects and then selecting the top-n among them is not a
solution in our consideration. However, if careful pruning is




293

not done, finding the top-n LOF can be as expensive as the
above naive method. This is because unlike global outlier
detection in which a data point can be immediately pruned
off if it cannot be an outlier, in local outlier detection the
points deletion may affect the LOF values of those objects
in its neighborhood. In short, we have a catch-22 situation
here: Without knowing where the possible local outliers are,
we cannot prune off our computation of density information;
however, without computing density information, we do not
know where the possible top-n outliers are.
In this paper, we first propose a novel method for top-
n local outliers mining that avoid computation of I.OF for
most objects if n << N, where N is the size of the database.
The main idea in our solution is to compress the data into
"micro-clusters" similar to those in [10] and efficiently es-
timate an upper bound and lower bound on I.OF of each
object in the database. By comparing the upper and lower
bound for the /OF of each object (micro-cluster), it, will be
immediately clear that many of the objects cannot be in the
list of top-n local outliers.
As such, only a small amount
of computation will be required for those candidate objects
that are possibly among the top-n.
As a central idea of our algorithm, we will need to esti-
mate the distance between a poinl, o to a gToup of point,s
in a micro-cluster.
Since each micro-cluster is represented
by a circle centered at the mean of it,s points, a very impre-
cise estimation will be made if the point o lies within the
circle.
To solve this overlapping problem, we introduce a
novel "cut-plane" method to identify the boundary between
a data point and a micro-cluster.
The paper is organized as follows. In section 2, we present
the motivation and definition.
In section 3, properties of
micro-cluster are introduced.
In Section 4, we propose a
micro-duster based top-n [_OF algorithm.
In section 5, a
performance evaluation is made and the results are analyze,].
Section 6 concludes the paper.


2.
DEFINITION OF LOCAL OUTLIERS
In this section, we briefly review the definition of local
outliers. Interested reader are referred to [3] for more details.
Let D be a database, p, q, and o be some objects in D,
and k be a positive integer.
We use d(p, q) to denote the
Euclidean distance between objects p and q.

DEFINITION 1. (k-dlstance of p)
The k-distance of p, denoted as k-distance(p) is defined as
the distance d(p, o) between p and o such that:

1. for at least k objects o' e D\{p} it holds that d(p, o') <
d(p, o), and

2. for at most (k - 1) objects o' E Dk{p} it holds that
d(p, o') < d(p, o)

o
Intuitively, k-distance(p) provides a measure on the spar-
sity or density around the object p. When the k-distance of
p is small, it means that the area around p is dense azad vice
versa.

DEFINITION 2. (k-distance neighborhood
of p)
The k-distance neighborhood o]p contains every object whose
distance from p is not greater than the k-distance, is denoted
as
Nk(p) = {q E D\{p}I d(p,q) <_k-distance(p)}
[]
Note that since there may be more than k objects within
k-distance(p), the number of objects in gk(p) may be more
than k. Later on, the definition of LOF is introduced, and its
value is strongly influenced by the k-distance of the objects
in its k-distance neighborhood.

DEFINITION 3. (reachabillty distance ofp w.r.t object
o)
The teachability distance of object p with respect to object o
is defined as

,'each-distk (p, o) = ma:,:{k-distance( o), d(p, o)}

r3
DEFINITION 4. (local reachabillty density of p)
The local reachability density of an object p is the inverse of
the average reachability distance from the k-nearest-neighbors
oSp.

lrdk(p) = l/[ E°e~
(') reach-distk(p, o) 1
[Ark(,) (p)]
m

Essentially, the local reachability density of an object p
is an estimation of the density at point p by analyzing the
k-distance of the objects in Nk(p). The local reachability
density of p is just the reciprocal of the average distance
between p and the objects in its k-neighborhood.
Based
on local reachability density, the local outlier factor can be
defined as follows.

DEFINITION 5. (local outlier factor of p)



trd~(r,)
LOF~.(p) = Eoe~k(,)
IN~.(p)r
C3
LOF is the average of the ratios of the local reachability
density of p and those ofp's k-nearest-neighbors. Intuitively,
p's local outlier factor will be very high if its local reacha-
bility density is much lower than those of its neighbors.


3.
ALGORITHM FORFINDING TOP-N LO-
CAL OUTLIERS
Given the earlier definition of local outliers, our problem
is to find the top-n outliers in terms of LOF values when n
and k are provided by the users. In this section, we will first
analyze this problem and then introduce various concepts
involved in our algorithm for finding top-r: local outliers ef-
ficiently.

3.1
Problem Analysis
From tile definitions given in the previous section, our
analysis shows that an upper bound and lower bound on the
LOF of an object p can be obtained if an upper and lower
bound on the local reachability density of p and objects in
Nk(p) are available. More specifically, we have (;tie following
theorem.

THEOaEM 3.1. Let lrdk(o).lower and lrdk(o).upper denote
the lower and upper bound on the local reachability density
of an object o respectively, and o E Nk(p), then

M in{ lrdk (o).lower }
Max{ lrdk (o).upper }
h'dk(p).upper
< LOF~.(p) <
lrdk(p).lower




294

Proof." Since LOF is the average of the ratios of lrdk (p)
and its k-neighborhood objects, this average must be higher
than Min{lrdk (o)lo E N, (p) }/lrd~ (p). By taking the lower
bound for lrdk(o) and the upper bound for Irdk(p), we re-
duce this estimate even further and thus obtain a lower
bound for LOFk(p).
With the similar reasoning, we ob-
tain an upper bound for LOF~ (p) by taking the maximum
of lrdh(o).upper and dividing it by lrdk(p).lower.
[]


As a result of Theorem 3.1, we have the following corollary
which we will use to find an upper bound and lower bound
for the local reachability density of every object.

COROLLARY 3.1. Given an objectp, o E Nk(p),its reach-
ability distance with respect to its k-neighborhood is bounded
by

1
1
Max{reach-distk(p, o)} -< lrdk(p) <_ Min{reach-distk(p, o)}

Explanation: Since the local teachability density of p is
the inverse of the average of reachability distance, it is not
difficult to see that the average will be lower than the max-
imum of reach-distk(p, o) and higher than the minimum of
reach-distk(p,o), o E Nk(p). Ttms taking their inverse will
give the respective upper and lower bound.
[]


Given the above corollary, we will now have to use the
following theorem to bound the maximum and minimum
value of reach-distk(p, o). We derive the following theorem.

THEOREM 3.2. Let the upper and lower bound on the
k-distance of object o be denoted by k-distance(o).upper and
k-distance(o).lower respectively.
We use d(o,p).upper and
d(o,p).lower to denote the upper and lower bound on the
distance between object o and p respectively. Then we have
the following two inequalities.

M ax( d(o, p).lower, k-distanee( o).lower ) < reaeh-dist(p, o)

and

reach-dist(p, o) < Max(d(o,p).upper, k-distance(o).upper)

[]

The above theorem is derived naturally based on the def-
inition of reachability distance. With the derivation of the
above theorem, we can see that the computation of an up-
per and lower' bound on tile LOF of all points in a dataset
is dependent on an efficient way to find an upper and lower
bound on the distance between points and the k-distance of
each point.

3.2 Concepts
We will now look at. the various concepts used in our al-
gorithm for finding top-n local outliers. The concept we use
is called "micro-clustering" which is similar to those in [10].
We compress the data into small clusters and represent each
small cluster using some statistical information. We define
a micro-cluster as follows.
DEFINITION 6. (Micro-Cluster)
A micro-cluster MC(n, c, r) is a summarized representation
of a group of data pl,... ,pn, which are so close together
that they are likely to belong to the same cluster.
Here,
n
c = Ei=tn p' , is the mean center while r = max{d(pl,c)},
i=1,... ,n, is the radius.
[]
To ensure that not too much accuracy is sacrificed by
using micro-cluster, we limit radius of each micro-cluster
to be below a user-specified threshold, maxradius. When a
micro-cluster with a radius higher than maxradins is found,
it is split by choosing two data that are farthest apart in
the micro-cluster as seeds and reassigning other data left to
the nearest seed. Since we use a micro-cluster to represent
a dataset in outlier detection, the distance measurement to
a micro-cluster must also be defined.
In the rest of this paper, let D be a database and M be a
set of micro-clusters. A point in D is denoted as p(pz, ..., p,~,)
and a micro-cluster in M is denoted as MC(n, c, r).

THEOREM 3.3. Let p be an object and MC(n,e,r) a micro-
cluster', lf p is greater than a distance of r from c, then the
minimum distance between an object p and a micro-cluster
MC will be:


DistM~,,(p, MC) = d(p, c) - r


and the maximum distance between p and MC will be:

DistM,~(p, M C) = d(p,c) + r



Figure 1 illustrates the maximum and minimum distance
between a point p and a micro-cluster, MC(n, c, r), when p
is not within a distance of r from c. Intuitively, any point
within the micro-cluster MC(n,c, r) will be at a distance
of at least DistMi,,(p, MC) and at most DistM~(p, MC)
from p. The situation is different if p is within a distance
of r from the center of the micro-cluster. In this case, while
the maximum distance remains unchanged, the minimum
distance must be set to 0.
Such a situation, however, is
undesirable since we will be estimating the k-distance of a
point based on its distance to its neighboring micro-clusters,
and a k-distance of 0 will mean extremely high density. To
overcome this problem, our solution is to ensure that each
object in a micro-cluster MC(n, c,r') is in fact nearest to c
than to the center of any other micro-clusters. This can be
achieved by first, forming micro-clusters using an algorithm
like BIRCH [10], fixing the centers of each micro-cluster and
then do a simple redistribution of the objects to the nearest
center'. Note tilat such a process carl be efficiently done since
the BIRCH structure resides in the main memory.
With
such an assumption, we will now define the concept of a cut
plane.

DEFINITION 7. (Cut-Plane)
Let MCi, MCj be two mictv-clusters, a cut-plane for MC~
and MCj, denoted as cp( MC,, MCj), is a hyperplane that
is perpendicular to the line between cl and c3 and divide the
line into exactly two halves.
[]

We illustrate a cut plane between two micro-clusters MCi
and MCj in Figure 2. With the definition of a cut-plane,
we will now define the minimum distance between p and a
micro-cluster MCj(nj,cj, rj) when p is within a distance of
rj fl'om cj.

DEFINITION 8. (Mill distance between all object and
a micro-cluster with overlapping)
Let p be an object within a micro-cluster MCi(n,, ci, ri). [f
p is within a radius of rj to a micro-cluster MCj (nj, ej, rj),




295

MC(n,c
,
~



D
s
t
~
~
i. ,
....--'

p
o
~
,
z


Figure 1: Min/Max
distance between an object and
a micro-cluster without overlapping.
:~C,(n
rc,r~
MC,(n,.c,,r,)




, cutplane


Figure 2: Minimum distance between an object and
a micro-cluster wlth overlapping.


i~
DistMax(MCI,MCj)
=i
MCi




DistMin(MCi,MCj)


Figure 3: Min/Max
distance of mlcro-clusters.



let d(p, cp( MCi, MCj ) denote the perpendicular distance of
p from cut-plane cp(MGi,MCj).
Then the minimum dis-
tance between an object p and a micro-cluster MC 3 is
d(p, cp(MC,, MCj)).
0

We illustrate the computation of the minimum distance
between p and an overlapping micro-cluster MCj in Figure
2. Note that such a formulation is based on the assumption
that p is always nearer to the center of MCi than MCj. We
now define the minimum and maximum distance between
two micro-clusters.

DEFINITION 9. (Min/max
distance between MCI and
MCj)
Let M Ci( ni, ci, ri) and M Cj(nj, ej, rj) be two micro-clusters,
then the minimum distance between MCi and MCj is,

DistMi~( MCi, MCj) = d(ci, cj) - r, - r~

and the maximum distance between MCi and MCj is,

DistM~(MCi, MCj) = d(ci, cj) + ri + "3

[]

Notice that t,he definition assumes the two micro-clusters
have no overlaps. When overlap occurs, then the minimum
distance between the two micro-clusters will be 0. Having
defined the minimum and maximum distance between two
micro-clusters, we will now provide a corollary which will be
used to find the upper arid lower bound tbr the k-distance
of an object with respect to the micro-clusters around it.
COP~OLLARY 3.2. Let p be an object and MC(n,c,r)
be
the micro-cluster that contains p. Let MC1 (m, cl, rl), ... ,
M Ct (nh c~, rz) be a set of micro-clusters that could poten-
tially contain the k-nearest neighbors of p. For ease of dis-
cussion, we will treat the other (n - 1) objects as micro-
clusters, i.e., each object oi is a micro.cluster MCI(1, oi, 0).
Thus we will now have l + n - 1 micro-clusters.

1. Let {Dist Mi~(p, M Ct )..... Dist Mi,,(p, MCt+~_x)} be
sorted in increasing order'.
Then a lower bound on
the k-distance of p, denoted as kmin-distance(p) will
be DistMir,(p, MCi) such that nl + ... + nl >_ k, and
nl+...+ni-1
<k

2.
Let { DistM~(p, MC, ),... , DistM~(p, MC,+,_, )} be
sorted in increasing order.
Then an upper bound on
the k-dislance of p, denoted as k........-distance(p) will
be DistM,,.~,(p, MCI) such that nl + ... + ni > k and
nl + ... + ni-i < k.

[]

Given a micro-cluster MC containing pt,... ,p,, we will
use k,~-distance(MC)
to denote Max(k,~-distance(pl),
.... km~-distance(pl )) and kmi~-distance( M C) to denote
gin(kmin-distance(pa )..... k,ni~-distance(pi )). We now de-
rive a bound for the internal reachability of a micro-cluster.


DEFINITION 10. (Internal
teachability
bound of a
micro-cluster)
We define the internal reachability bounds of
a
micro.cluster
MC(n, c, r) as follows,

1. r.~,( MC) = Max(2r, Max(k.~.~-distance(MC)))

2. r'm,n( MC) ----kmin-distar, ce( MC)
[]

Intuitively, given two objects p and o within micro-cluster
MC, rm.~(MC) avid rmi,~(MC) represent the maximun,
and minimum bound for reaeh-distance(p,o)
respectively.
This definition is used for estimating the reachability dis-
lance bound of a pair of data wittfin one nficro-cluster.


DEFINITION 11. (External reaehability bound of two
micro-clusters)
We define the external reachability bounds of a micro-cluster
MCi with respect to another" micro-cluster MC~ as follows,

1. rm,x(MCl, MCj) =
Max( Distma.~( M Ci, M C~), k,n~-distance( M Cj ))

2. ,'mi,(MCi,MC3)=
Max( Dist,~i, (M Ci, M Cj ), k,,n-distance( M Ci ))
[3

Intuitively, given two objects p and o within micro-cluster
MCi and MC~ respectively, r'ma~ and "mln represent~ the
maximum and minilnum bound ['or reach,-distance(p, o) re-
spectively. This definition is used for estimating the reach-
abillt,y distance bound of a pair ot" data in different micro-
clusters. One thing to note is tllat the external reachability
bound of two micro-clusters will NOT be affected by over-
lapping between the micro-clusters as long a.s we have good
estimation of the k-distance bound for the objects inside the
mlcro-clusters. Having introduced the various concepts, we
will have a look at our algorithm in tile nexL section.




296

4.
MICRO-CLUSTER-BASED ALGORITHM
In this section, we look at our micro-cluster-baaed algo-
rithm for finding top-n local outliers. The general steps of
our algorithm are as follows,

1. preprocessing,

2. computing LOF bound for micro-clusters, and

3. rank top-n local ontliers.

4.1
Preprocessing
In this step, the input data are pre-processed so that effi-
cient determination of k-distance bound can be done. Pre-
processing can further be divided into three steps as follows.

1. Load data into CF Tree.
A sequential scan is per-
formed on the database and the objects are inserted
into a CF-tree [10].
At the end of the process, the
centers of all the CFs are recorded and inserted into a
memory-based X-tree [2].

2. Fix CF node and generate micro-clusters.
A second
scan is then performed on the database and the objects
are assigned to their nearest centers computed in the
previous steps. In the process, the number of objects
and the radius of each micro-cluster are recorded.

3. Insert micro-clusters into X-tree. Each micro-cluster
is bounded by its MBR and is inserted into a different
memory-baaed X-tree f~om step (1).

4.2
Computing LOFBound for Micro-Clusters
To compute an upper and lower bound for micro-clusters,
it requires one scan through the database and one scan
through the micro-clusters. The scan through the database
will first estimate a bound for the k-distance for each micro-
cluster M C, i.e., km~=-distance( i C) and kmm-distance( i C).
The second scan will determine a bound for the reachabil-
ity distance and thus the LOF for each micro-cluster. We
describe the details as follows.

ALGORITHM 1. Algorithm k-distance-bound.
Input:
A set of micro.clusters MC1, . .. , MOt.
Output:
k,~a=-distance(MC~), k,,,-distance(MC~) 1 <
i<l.
1VIethod.

I. for each micro-cluster MCi do
~.
find a micro-clusters set P of MCi by Corollary 3.~;
3.
kmin-distance(MCi) = c~; k....... -distance(MCi) = O;
4.
for each object p in MCi do
5.
get krnin(p), kma.~.(p) w.r.t P 8:J objects in MCi;
6.
if kmln(P) < krnln-distance(MCi)
7.
k,nin-dista.nce(MCi)
=
kml.n(p);
8.
if k.... (p) > k.......-dista,nce(MCi)
9.
k,~.~-dista, nce(MCi) = #.m,=(p);

This algorithm is used to compnte the k-distance bound
for each micro-cluster.
Since the micro-clusters are in an
X-tree, step 2 can be efficiently performed [2] by examining
the MBR of the micro-clusters and computing the cut-plane
between two micro-clusters if needed.
After computing the k-distance bound for each micro-
cluster, we scan through the micro-clusters to compute the
LOF bound using Algorithm k-distance-bound.

ALGORITHM 2. Algorithm LOF-bound.
Input: MC1, ..., MCz and their k-distance-bound.
Output:
LOF bound for each micro-cluster.
Method:
/.for each micro-cluster MCI do
2. find a micro-clusters set Po] MCI by Corollary 8.2;
3. get internal rmaz(MOl)/rmin(MCi) bound for MOl;
4. LOF(Mel).upper = rmaz(Mel)/rmln(MCi);
LOY(MCi).lower = rmln(MCi)/rrna=(MOl);
5. for each micro-cluster MC7 E P do
6.
get external rmax(MCi, MC3),rmln(MCi,MC~);
7.
if LOF(Mel).upper < rmax(MCi, MCj)/rmi,:(MCi, MCj)
8.
LOF(MOl).upper = r.... (MCi,MCj)/rmln(MCI,MC3);
9.
if LOF(MCl).lower
>
rmin(MOi, MOj)/r .... (MCI, MCj)
10.
LOF(MCi).lower = rmi,(MCi,MCj)/rmax(MCi,MCj);
This algorithm is used to compute the LOF bound of a
micro-cluster. In steps 3 and 4, it first handles data within
one micro-chister.
Then in steps 6-t0, it considers those
potential neighbor micro-clusters to obtain their lower and
upper bounds.

4.3 Rank Town Local Outliers
Given an upper and lower bound for the LOF of micro-
cluster, we can easily rank Top-n local outliers.

ALGORITHM 3. Algorithm t~nk-TOPn-LOF.
Input: A set of MC1, . .. , MCl and their LOF_bound.
Output:
TOP-n local outliers.
Metlmd:

l.
Sort the first n micro-clusters in ascending LOF.Iower
order, and label the minimal one Min-TOPn-LOF;
2. for any other micro-clusters MCi do
3.
if LOF(MCi).upper
<
Min-TOPn-LOF
4.
then delete MCI;
5.
else if LOF(MCi).lower > Min-TOPn-LOF
6.
then delete the current sorted n-th micro-cluster;
7.
add MCI into current top n sorted micro-clusters;
8.
re-sort current n micro-clusters,
and label the n-th as Min-TOPn-LOF;
9. for any data in the remaining micro-clusters MC~ do
10.
calculate detailed LOF value;
11.
prune those that are impossible to become TOP.n LOFs;
12.
obtain TOP-n local outliers;
5.
EXPERIMENTS
We compare our micro-cluster TOP-n LOF algorithm with
X-tree-based LOF method [3]. We did experiments on both
real and synthetic data sets but will only show the result for
the synthetic data due to lack of space.
The experiments
are conducted on a PentiumllI-450 PC with 256MB main
memory under Windows NT4.0 and implemented in Visual
C++6.0. The cost, of time in the experiments includes build-
ing an index tree.


1600

1400

12oo

i lO00

,~ 800

i
600

z
400
--~-lopl0noC
~
·.l-..TopS0noCP
·..~-.TopS0
no
CP
~-" Topl0~lkCP
TopSOwi~ CP




0.3 0.5
1
1.5
2
3
4
5
6
7
6
Max Radius(% of data region)



Figure 4: No. of unpruned candidates vs max-radius

The synthetic datasets we used for our experiments are
generated using Gaussian random distribution. We evaluate
the performance based on three aspects.




297

I
2801
~
.
.
~




t30


80.
0.3
0,5
1
1.5
2
3
4
5
6
7
8

MaxRadius(%ofdataregion)
"~.-.T~CP

..-~'..-T~10v~hC~
10000
9000
8000
7000
6000
5000
E 4000
3000
2000
1000
0
~~:::::.~.
................

w -----~--'---~
.
~
~
.
.
.
-
d
l


100K 200K 300K 400K 500K 600K 700K 800K 900K
SizeofData


Figure 5: Running time vs max-radius


First, we investigate the pruning effect of TOP-n LOF
under the different max-radius of micro-clusters. Here the
max-radius is viewed as the percentage of the radius of the
dataset. Tire number o[" candidate micro-clusters are plot-
ted against max-radius for n = 10, n = 50 and n = 80
in Figure 4 for two versions of our algorithm, one with cut
plane (abbreviated as "with CP") and one without (abbrevi-
ated as "no CP"). When a cut-plane is considered, it shows
that more micro-clusters will be pruned since much of the
overlapping data, which originally could not be separated,
now has a discernible distance from other data. In addition,
when the max-radius of a micro-cluster increases, less micro-
clusters will be pruned since more accuracy is lost and the
lower and upper bounds for the LOF of these micro-clusters
are more interleaved. Subsequently, as max-radius continue
to increase, the number of unpruned candidates decrease,
this is due to the overall decrease in the total number of
micro-clusters rather than an effect of our pruning.
Second, we plot the runtime of our' algorithms against tire
size of micro-clusters in Figure 5. It is clear that when cut-
plane is used, the runtime is usually lower than when it is not
used since more micro-clusters have been pruned. We can
also see although there are very few candidate micro-clusters
when the max-radius is large, the running time at these
values still increase. This is because search and computation
time for each of these micro-cluster is large as each of them
contains more data points,
Third, to investigate the scalability of our algorithm with
regard to dimensionality and dataset size, we plot the run-
ning time of our algorithm for a top-10 local outlier query
against the size of the data~set in Figure 6. The dimension-
ality of the dataset is set at 2, 10 and 20. The graph shows
that the micro-cluster TOP-n LOF mining method outper-
forms naive X-tree-based method by a big margin especially
for high number of dimensions.


6.
CONCLUSIONS
In this paper', we have proposed a novel and efficient
method for mining top-n local outliers. The strength of the
method is at that it avoids computation of L0F for most
objects if n << N, where N is the size of the database. The
formal analysis and performance evaluation show that our
method is not only efficient in computation but also effective
at ranking most understandable local outliers.
Finding top-n local outliers is a new and promising re-
search topic in data mining. The future work may include
finding strong local outlier- groups and finding (nested) local
Figure 6: Running time vs.dataset size


outlier at multiple levels of granularity.

Acknowledgment. Thanks to Helen Pinto and Joyce Lain for
proof reading the initial version of this paper.


7.
REFERENCES
[1] V. Barnett and T. Lewis. Outliers in Statistical Data.
John Wiley & Sons, 1994.
[2] S. Berchtold, D. Keim, and H.-P. Kriegel. The X-tree:
An efficient and robust, access method for points and
rectangles. In Proc. 1996 Int. Conf. Very Large Data
Bases (VLDB'96), pages 28-39, Bombay, India, Sept.
1996.
[3] M. M. Breunig, H. P. Kriegel, R. T. Ng, and
J. Sander. Loft Identifying density-based local outliers.
In Proc. ~000 ACM-SIGMOD Int. Conf. Management
of Data (SIGMOD'O0), Dallas, Texas, 2000.
[4] M. Ester', H.-P. Kriegel, J. Sander, and X. Xu. A
density-bmsed algorithm for' discovering clusters in
large spatial databases. In Proc. 1996 Int. Conf.
Knowledge Discovery and Data Mining (KDD'96),
pages 226-231, Portland, Oregon, Aug. 1996.
[5] S. Guha, R. Rastogi, and K. Shim. Cure: An efficient
clustering algorithm for large databases. In Proc. I998
ACM-SIGMOD Int. Conf. Management of Data
(SIGMOD'98), pages 73-84, Seattle, WA, June 1998.
[6] D. Hawkins. Identification of Outliers. Chapman and
Hall, London, 1980.
[7] E. Knorr and R. Ng. Algorithms for mining
distance-based outliers in large datasets. In Proc. 1998
Int. Conf. Very Large Data Bases (VLDB'98), pages
392-403, New York, NY, Aug. 1998.
[8] R. Ng and J. Han. Efficient and effective clustering
method for spatial data mining. In Proc. 1994 Int.
Conf. Very Large Data Bases (VLDB'94), pages
144-155, Santiago, Chile, Sept. 1994.
[9] S. Rama.~wamy, R. Rastogi, and K. Shim. Efficient
algorithms for mining outliers from large data sets. In
Proc. ~000 A CM-S1GMOD Int. Conf. Management of
Data (SIGMOD'O0), Dallas, Texas, 2000.
[10] T. Zhang, R. R.amakrishnan, and M. Livny. BIRCH:
an efficient dal,a clustering method for' very large
databases. In Proc. 19.96 A CM-SIGMOD Int. Conf.
Management of Data (SIGMOD'96), pages 103-114,
Montreal, Canada, June 1996.




298

