Similarity Measure Based on Partial Information of
Time Series
Xiaoming Jin
YuchangLu
Chunyi Shi
The StateKey Laboratoryof IntelligentTechnologyand System
Dept. of ComputerScienceand Technology,TsinghuaUniversity
xmjinOO@mails.tsinghua.edu.cn lyc@tsinghua.edu.cn
scy@est4.cs.tsinghua.edu.cn



ABSTRACT
Similarity measure of time series is an important subroutine in
many KDD applications. Previous similarity models mainly focus
on the prominent series behaviors by considering the whole
information of time series. In this paper, we address the problem:
which portion of information is more suitable for similarity
measure for the data collected from a certain field. We propose a
model for the retrieval and representation of the partial information
in time series data, and a methodology for evaluating the similarity
measurements based on partial information. The methodology is to
retrieve various portions of information from the raw data and
represent it in a concise form, then cluster the time series using the
partial information and evaluate the similarity measurements
through comparing the results with a standard classification.
Experiments on data set from stock market give some interesting
observations and justify the usefulness of our approach.


Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications - data
mining;
1.5.3 [Pattern
Recognition]: Clustering - similarity
measures


Keywords
Time series, Similarity measure, Partial information


1. INTRODUCTION
Time series constitute a large part of data stored in many
information systems, e.g. stock price, telecommunication data,
weather data, astronomical data [15], medical data [4], audio data,
etc. Recently, there has been a lot of interest in mining time series
data. In many data mining problems, such as similarity queries,
cluster, classification, etc., similarity measure is an important
subroutine.

There have been several efforts to develop effective and efficient
similarity models. Most previous approaches consider the whole
information of a time series, or partial information that is derived
by ignoring minor series behaviors for the sake of efficiency. Such
models focus on the prominent series behaviors and evaluate the



Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requiresprior specificpermission and/or a fee.
SIGKDD "02,July 23-26, 2002, Edmonton, Alberta, Canada.
Copyright2002 ACM 1-58113-56%X/02/0007...$5.00.
S3




50
100
150
200
·
$4


$3


$2


81


50
100
150
200


Figure I. Examples
Figure 2. Partial series behaviors

associations between the overall shapes of time series. However, in
many real applications, a "good" similarity measure means what
consists best with not the shape of the time series but the human
feeling. On such occasions, a common observation is: using
normalized data could provide more accurate results than using the
raw data [2]. :Such normalized data, representing minor series
behaviors, contain only a portion of all the information of the raw
data. We could also decompose time series by other criterion,
whereupon derive other portion of information. Then can we find
another potion of information that is more meaningful for
similarity measure? We believe it is true for certain classes of
applications.

Example 1: If remove the overall series behaviors from the time
series in Figure 1, the remainder series movements would like that
in Figure 2. Such partial information could reveal another kind of
relations about these time series, furthermore, about the systems
that generated these time series. Therefore, in some applications,
it's more meaningful to consider ($2, $3, $4) similar than consider
($1, $2) similar as what is done using the common approach.

Another promotion of our research is to facilitate dimension
reduction. Since the real data sets tend to be very large, much
similarity measure related work focuses on retrieving partial
information that centralizes the most energy [1][6][10][13] for
similarity measuring, indexing, querying, etc. However, there isn't
any detailed and accurate evaluation of similarity measurements
using different portions of information. The evaluation is of crucial
impotence when the goal is to measure the similarity based on
subjective feeling, because it could enable us go beyond the
criterion of energy centralization and choose the most efficient
partial information.

Accurate evaluations on the meaningfulness of various portion of
information could help designing a more effective and more
efficient similarity measurement. To our knowledge, this problem
has not been weU considered in the KDD field. Even the model
that can retrieve and represent the partial information in time series
has not been well defined. To fill this gap, we propose such a
model in this paper, which includes time series decomposition,




544

representation, and distance measurement. By using this model, we
introduce a methodology for evaluating similarity measurements
based on partial information. Experiments on a data set from stock
market give some interesting observations and justify the
usefulness of our approach.

This paper is organized as follows: Section 2 discusses some
related work on similarity measure. Section 3 presents the model
for retrieving and representing the partial information. Section 4
describes the method for evaluating the partial information and
introduces the setup of our experimental system. Section 5 presents
the experimental results and some discussions. Finally, Section 6
offers some concluding remarks.


2. RELATED WORK
The simplest similarity measurements are Euclidean distance (L2)
and other Lp norms. A similarity model that uses non-overlapping
ordered similar subsequence was proposed in [5]. Clearly, the
above methods use the whole information. The most meaningful
series behaviors are likely to be influenced, or even flooded by the
overall series movement. Therefore it's difficult to discover really
useful associations between time series by using the whole
information.

In [1][13], the Discrete Fourier transform (DFT) was applied on
the time series, and then only the first few Fourier coefficients
were preserved for dimension reduction. For the same purpose,
discrete wavelet transform (DWT) was used in [6] and singular
value decomposition was used in [10]. These approaches use
partial information, but the goal is to approximate the whole
information and to reveal the prominent behaviors of the original
time series. In our approach, which portion of information will be
used is not determined by whether it can best approximate the
original time series, but by whether it is meaningful for similarity
measure.

Another known approach that uses partial information is the
segment-based methods [9][11] where the time series was
represented by piecewise segments and the shape of each
individual segment was represented by a real-value function. Such
methods seem alike to what we address, but is essentially different.
It decomposed the time series through the time dimension, so it's
still the overall series behaviors that predominate each segment.

In [2], various known methods for similarity measure were used in
clustering stock data. Then verified which measure was best for the
data sets by analyzing the results. However, no method that
concerned partial information, except normalization, was involved.
In our experiments, we used the same data sets and applied the
cluster methods that were proved optimal in [2]. This makes it easy
for us to compare our approach with the known best methods.


3. RETRIEVAL AND REPRESENTATION
OF PARTIAL INFORMATION
The problem of designing a representing model is to retrieve the
partial series behaviors for similarity measure. We define a
representing model by decomposing the problem into three sub-
problems: 1) get the partial information, ensure the information of
different interest be derived; 2) represent the partial information in
a compressed form, which make the amount of resulting data as
less as possible; and 3) a similarity measuring strategy, in which
most previous similarity model could be applied. In section 3.1, the
problem is formally defined in a general form. Then, by using
discrete transform, we give a practical method in section 3.2.
3.1 General Description
A time series X=(X(1),...,X(N)) is a sequence of real numbers, each
number represents a value at a time point. In this paper, all time
series are of the same length.

Definition 1: Use a rule F to decompose X into a set of time series
X',, i.e. X~(X'I ..... X'r), so that X(n)=Zk X'~(n). Then X'k is the
k-th F-based component of X.

Definition 2: Segment X into a set of sub-series XF(X(Jr-
r+l),...,)((jr)) through sliding a time window of width r. Let X~k
be the k-th F-based component of sub-series Xj. Use an one-to-one
series-value mapping rule T to map each X)k to a value RkQ), i.e.
X)~R~'). Then the sequence Rk=(Rk(1)..... Rk(N/r)) is the k-th
representing sequence.

Definition 3: Given the components (X',) of time series X,
K=(K1,...,Kw) that is the orders of all the representing sequences of
interest, and Ak=(Ab...,Ar) where A, is the degree of user's interest
to the n-th component, the sequence ~n (Ax,X'xn) is the portion of
partial information of interest.

Definition 4: Given the k-th representing sequence Rkof time series
X, K=(Kb...,Kw) that is the orders of all the representing sequences
of interest, and Ak=(Ab...,Ar) where A, is the degree of user's
interest to the n-th component, The sequence

R(m) = Ax
RK
(Kr,/wq)
MOD(m,W)
MOO(re.W)
I
I


is the full representing sequence (FRS) of the partial information
~. (AK.X'K.).We denote it as: FRS(X)=R.

Here MOD(m,K) denotes the remainder ofm/K and ['a'] denotes the
smallest integer value which is larger than a. In Definition 3 and
Definition 4, vector K represents which components are evolved in
the partial information, and A. represents the degree of user's
interest on the n-th component.

Definition 5: Given two time series X, Y. The micro distance (MD)
between Xand Y is the distance between FRS(X) and FRS(Y). We
denote it as MD(X,Y)=D(FRS(X), FRS(Y)).

D(X,Y) in Definition 5 could be any distance measurement
currently in use. For example, we could use Euclidean distance:
D(X,Y)=LE(X,Y). Then MD(X,Y)=L2(FRS(X), FRS(Y)).

To sum up, a representing model for the partial information can be
summarized by <Decomposition method: F, Representation
method: T, Distance measurement: D>. The first one F is for
retrieving the
partial
information and
storing the
partial
information in a set of components. The second one T is for the
compact storage and the efficient process of the partial information.
The storage expense for FRS that represents the partial information
is NW/r.
It accomplishes
significant storage
compression
comparing with storing all the components directly when the
storage expense is NW. Finally, D is for measuring the distance of
the partial information. MD is the similaritymeasurement based on
partial information. By a careful selection of T and D, the FRS can
be distance-preserved with the original partial information, i.e.

D(FRS(X), FRS(Y)) = D(Z. (AK,,X'K.),E. (Ax.g'x.))

Only in this occasion, the micro distance is exactly equal to the
distance of the partial information, which means we could
calculate the distance by using less data.

Example 2: Consider the time series in Example 1, let the window
width to be 8 which is correspond to the period of the local




545

fluctuating. We use F that is: decomposing the time series to two
components, of which one is the local fluctuating movement S'l,
and another is the global movement S'2. Then S' 1is what is shown
in Figure 2, and S'2 is the remainder. Since each local fluctuating
movement is of the same shape, we use T that is: map S',~ to the
same value a if there is such fluctuating movement and 0 otherwise;
anddo not map S'~2. Then the 1st representing sequence is:

{~
S'jl = fluctuation
R1(J) =
otherwise

Suppose we are only interested in the local fluctuating movement,
so let K=(l) and AN(l,0). Then the full representing sequence of
the partial information of interest is: FRS(X)=Ri. Finally we use L2
as the distance measure D, i.e. MD(X,F)=L2(FRS(X), FRS(Y)). It's
easy to see, the distance is not preserved unless a is set to be the
largest singular value of a single fluctuating sub-series. In addition,
the length of the FRS(X) is 200/8. So we need process only 1/8
data when accessing the representing sequence FRS(X), comparing
with accessing the portion of partial informationS'l directly.


3.2 Practical Method
Above arc general definitions on the representing model. Further
investigation of effective <F T D> is needed to fill the gap in
application. We propose using orthonormal discrete transform as F
and T, and Euclidean distance as D.

Let X be a time series of length N, and r be the width of the sliding
window. Then the j-th sub series of X is XF(X(/r-r+I),...,X(/r)),
N/r~l>_l. Let H be the transform matrix of a given orthonormal
discrete transform. The results of the r point transform of sub-
series Xj is sequence Tfik), given by TFH.Xj. We denote the results
of discrete transform of time series X and Y by DT(Xj)=3f/~,
DT(Yj)ffiYT~.Then the k-th component of Xis:

X'k(n) =/~,/,l(k). IB~ (n - (In / r]- 1). r)

where

=
M,.-:

The k-th representing sequence is:

Rk(m)=Tm(k)

FRS(X) can be calculated as:

R(m) = AX~oo,.., l~./w](Xuoo¢m.v¢))

After FRSs of the time series have been derived, we use the
Euclidean distance to calculate MD.

Here, we would like a transform 1) that can preserve the distance;
2) that is easy to compute; 3) that can represent interesting pattern
easily and efficiently. The distance-preservationrequirement is met
by orthonormal transform and Euclidean distance [7]. By using
Parseval's theorem, it is easy to prove that the above method is
distance preserved [3], i.e.


MD(X,Y) =
(XTj (K,)- YTj(Kn))2Ax, 2


W
W
= L2
Zx'
.
n=l
n=l
This enable us c~dculate the Euclidean distance between FRS (X)
and FRS (Y), which is compact, to measure the distance between
the partial information in Xand that in Y.

Widely used orthonormal transforms form two classes: 1) the data-
dependent ones, like the Karhunen-Loeve (K-L) transform and 2)
the data-independent ones, like discrete Foruie transform (DFT),
discrete Cosine transform (DCT), discrete Walsh transform, and
discrete wavelet transform (DWT). For data-dependent transforms,
if the data set is changed, a recomputation of the transform matrix
is required to avoid performance degradation, requiring expensive
data reorganization. So we prefer data-independent transforms. At
the same time, the response time of our method will improve with
the ability of the transform to concentrate the information of
interest. The fewer the resulting coefficients that contain the
interested information are, the more efficient the method for
accessing the partial informationwill be.

Note that our approach can be applied with any transform that fits
our requirements. Among them, we used DCT in our experiments
because it is widely used in many areas, its code is readily
available and it does a good job of meeting all of our requirements.


4.
SYSTEM
SETUP

4.1 Evaluation of Similarity Measurement
Based
on Partial
Information
We used a common strategy to evaluate the performances of
similarity measurements based on partial information. First, cluster
the data set using the given partial information. Then evaluate the
measurement by comparing the resulting clusters to a standard
classification of the data set.

Clustering is the process of grouping a set of objects into classes of
similar objects. Commonly, the clustering process is based on the
distance of objects. Cluster by partial information is: use partial
information to measure the similarity of time series, i.e. calculate
MD; then based on the similarity, each time series is grouped into
a cluster. The problem includes three sub-problems: 1) get the
partial information and represent it by a compact version, 2)
compute the distance based on partial information, and 3) use a
distance based cluster method to cluster the time series. The first
sub-problem could be solved by time series decomposition and
representation that is presented in section 3. Consider the second
sub-problem, our methodology to deal with partial information
enables us use the Euclidean distance between FRS (X) and FRS (Y)
as the distance between the partial information in X and that in Y.
Finally, We could solve the third sub-problem by clustering the
FRS sequences instead of the original time series or the
components.

Our strategy has no constraint on the clustering algorithm, any
common distance based clustering method could be used, e.g.
recursive k-means, agglomerative and divisive hierarchical
clustering, BIRCH, CUBE, Chameleon, etc. [8], and neither the
clustering algorithm nor
the
similarity measurement need
modifying. We used hierarchical agglomerative clustering (HAC)
because the nmaaber of resulting cluster can be pre-defined and it
can be easily implemented. HAC starts by placing each object in
its own cluster and then merges atomic clusters with minimum
inter-cluster distance into larger and larger clusters, until certain
termination conditions are satisfied. We specified the desired
number of clusters as the termination condition. There are several
roles for agglomeration, such as minimum distance, maximum




546

distance,meandistanceand averagedistance,etc. Among them,
maximumdistancewas used in our experimental system.

Alter the FRSs have been clustered, the following modified FI
model was used to compare the resulting clusters with a standard
classification: given clusters C = C1 ...
Ck and standard
classification S=Si...St,their similarity is calculated as follows:

Sim(Ci, Sj) = 2lfinSj}/(IC,I+ ISjl)

and

Sim(C'S)=l ,~maxSim(Ci,Sj)l/k


The similarity measure will return 0 if the two clusters are
completely different and 1 if they are the same. And this measure
is not symmetric.


4.2 Data and Preprocessing
We used the Standard and Poor 500 index (S&P) historical stock
data at http://kumo.swcp.com/stocks/,
in which daily price
movements of approximately 500 stocks had been collected daily
over the time period of one year. Among them, 449 stocks, of
which the data of all trading days were available, were used. Data
of each stock are a set of time series, which indicate the opening
price, closing price, trading volume, etc. We only considered the
time series of closing price. The S&P data also contain the official
S&P classification which groups the stocks into total 109 groups
based on their primary business focus, e.g.

Advanced Micro Devices: Electronics(Semiconductors)

Aetna Inc. (New): HealthCare(ManagedCare)

We first combined the 109 member clusters into 73 super clusters
based on the main-classification, e.g. "Computers(Software&
Services)","Computers(Software)",and"Computers(Hardware)".
This information was then used as the standard classification in our
experiments.

There is a broad consensus that similarity measurement with
proper preprocessing could give better results. For example, time
series can have different baselines and scaling factors [5][12].
From human interpretation, if two time series are with similar
behaviors running at different levels or with different scale, they
should be treated as similar. In order to compare our approach with
the previous measurements accurately, we first preprocessed the
data using methods that were proved optimal in [2]. The
preprocessing stage consists of two steps, which are mapping and
normalization. In the first step, the original time series is mapped
as Y(n)=X(n+l)-X(n).In the second step, the normalization is done
as follows: split each sequence into windows, and then, divide the
vector of each window by its L2 norm.


4.3 Overall Method
Our overall experimental method is as follows: For each time
series, DCT was applied to decompose the time series and to
represent the partial information. We used a binary sequence
E=(Et ..... Er) to represent the chosen portion of information,
where r is the window width. Each E, could be either 1 or 0, which
means we used or did not use the (n+l)-th component in similarity
measure. Note that we did not use the mean value, which is the
first component. Then generated all possible portions of partial
information by varying E. Since the first component was not
involved, the total number ore is 2rl-1. Each time, a generated E
was used to calculate K. Then together with a user-specified d,
FRSs of each time series were generated. FRSs were sequentially
used in calculating MD, and furthermore were used in clustering as
we proposed in section 4.1 (see section 3 for the definition of K
and A). Finally, the similarities between each resulting cluster and
the official S&P classification were calculated, including Sim (S,C),
Sim (C,S), and average similarity.

In our experiments, the window width in preprocessing stage and
partial information retrieving stage was both set to be 10, A was set
to be (1,1,1,1,1,1,1,1,1).


5. RESULTS AND DISSCUESSION

5.1 Experiments on Single Data Set
Results are shown in Table 1, 2, 3 and 4. Among them, Table 1
shows the results using various amounts of data. Note that, result
with ID: 639 in Table 1 is the result on all the nine components, i.e.
the whole information of the normalized data. The performance of
this similarity measurement is the same as that of the common
method currently in use, which uses the optimal preprocessing
setup. Table 2 lists the first 20 results with the largest average
similarity in all the results, i.e. the top 20 "best" results. Table 3
lists the first 20 results with the smallest average similarity in all
the results, i.e. the first 20 "worst" results. Table 4 shows the
results when the partial information with the same data amount
(10% comparing with the original data set) was used. We also
show the avg. Sim of each individual portion of information in
Figure 3, together with the relative amount of data that was used.

We could get several interesting observations form the results:

First of all, similarity measure on different components gave
different results. This is apparent in practice and is widely known
in many research areas. The best result (ID: 711) was obtained
when only a portion of the information is used. Furthermore,
Different portion
of information
contributed
differently to
similarity measure. This could be explained by the criterions of
generating the standard classification. Different components of the
price movements implicate different characters of the stocks. And
the subjective classification is only based on some of the characters.

We can get more interesting observations by comparing the
frequency of each used components. In Table 2, the frequencies, i.e.
the times of being used, of the components are (11 15 14 10 19 10
14 17 14), which means the first component was used for 11 times,
the second component was used for 15 times, etc. This indicates
the 5th component was more frequently used when "good" results
were produced. In Table 3, the frequencies are (3 3 3 3 2 4 4 5 5),
which show the 5th component was used for only 2 times when the
"worst" clusters were produced. This observation gives an actually
useful knowledge, which is using partial information that includes
the 5th components are apt to produce good results. More research
work is needed in order to fully understand this observation. But
we believe acquiring such knowledge will help constructing more
effective similarity measurement. For example, we could modify
the measurement by increasing the weight of the 5th component.

Another observation is about the relationship between the amounts
of used components, i.e. amount of data accessed, and the
performance of the corresponding similarity measurement. There is
an assertion in the areas of time series indexing and querying that
more information involved could give better results. However,
Figure 3 indicates this turn out to be only a general rule. By
analyzing the results, we could observe that how many components




547

Table 1. Experiential results, using various amount of data
Table 3. Experiential results, 20 worst ones



639
.548575
.5221937 .535384
90
1
1
1
1
1
1
1
1
1
895
.5068562 .4964326 .501644
80
1
1
1
1
1
1
1
1 0
1023 .4834336 .4726601 .478047
70
1
1
1 1
1
1
1 0
0
1087 .4951615 .4785663 .486864
60
1
1
1 1
1
1 0
0 0
1119 .493847
.4813504 .487599
50
I
1
1 1
1 0
0
0
0
1135 .4830775 .4663766 .474727
40
1
1
1
1 0
0
0
0 0
1143 .4740183 .4641361 .469077
30
I
1
1 0
0
0
0
0 0
1147 .4314948 .4213433 .426419
20
1
1 0
0
0
0 0
0
0
1149 .3765412 .3667394 .371640
10
1 0
0~0
0
0
0
0 0


Table 2. Experiential results, 20 best ones



glll~l~lli~ll~il~lOIillillOIZIIlllO00l
"~ll'~r.~'~'ntw.'en ~.~ui~lOEIOOigllXilllillll
lall


:*~'J~t:~'1'GB'*~ ~"I~WIIEli
OlgllglnlglOlglO|

,a*s~m.~:~*~m~:dSl~r.~alOOillOOIglOOIgl|




:~.*~'~'*"'~":'o~'~':"~ai~'~'ma'OOlgllgllgllgligliglllE



was considered did not have the decisive effect on the performance
of similarity measurement. Consider result ID: 639 in Table 2, the
most information (90%) were used. But the result was not the best
one. In the experiment ID: 711, only 70% of all the information
was used, but provided the best results. Sometimes, the difference
was so big that a measurement outperformed another one that
accessed much more data. For example, one result was: (ID: 814,
Avg. Sim:0.49, Data%:30).
But another results when double
amount of data were involved was worse, which was (ID: 773, Avg.
Sire: 0.46, Data%: 60). In addition, the results in Table 4 also
suppose the conclusion: Partial information with the same amount
of data may produce results of distinctly different quality.

Finally, we could get the conclusion: using first few components
is not the best choose for similarity measure. A widely used
dimension reduction method is that keep only the first few
components, which are seen as "strongest". However in Table 4,
the best result is not ID: 1149 when the first component was
involved, but is ID: 1134. Similarly, the worst one is not ID: 894
when only the last component was involved..This is mainly
because the first few components were not still the strongest ones
after the preprocessing stage had been applied on the raw data.


5.2
Experiments
on Two Data Sets

The goal of this experiment is to evaluate the consistence of the
performances concerning the same portion of information on
11086 .3685369
.3535208
.3610289
894
.3606749
.367043
.363859
1118 .3754304
.3630577
.3692441
1149 .3765412
.3667394
.3716403
1022 .3828205
.3710613
.3769409
1142 .3854209
.370132
.3777764
1146 .3861199
.3733439
.3797319
10
0 0 0 0]0
0 1 0 0
10
0 0 0 010
0 0 0
1

10
OiO 010
0
1 0 0 0
10
1 0 0 ~ 0 0 0 0 0
10
000
00010
10
0,0
0 1 0 0 0 0 0
10
0 0 1 0 0 0 0 0 0
1054
.394694
.3883391
.3915165
20
0 0 0 0 0
1 1 0 0
11148 .399101
.3871556
.3931283
10
0'1
010
0 0 0 0 0
1134
.3954
.3985346
.3969673
10
0 0 0l 0 1 0 0 0 0
1110 .4200318
.4064898
.4132608
20
0 0 0!1
0
1 0 0 0
1130 .4200035
.410384
.4151937
20
0 0
1 0
1 0 0 0 0
957
.4179261 .4165671
.4172466
30
1 0 0 ~ 0 0 1 1 0
892
.4276956
.4098543
.418775
20
0 1 0
: 0 0 0 0 1
1019 .4211325
.4230531
.4220928
30
1 1 0 010
0 0
1 0
886
.4308223
.4149195
.4228709
20
0 0 0 1 0 0 0 0 1
958
.4270174
.419221
.4231192
20
0
0
0
0
0
0
1
!
0

766
.4243701 .4221905
.4232803
20
0 0 0 0 0 0 0
1 1
862
.4264641
.4234081
.4249361
20
0 0 0 010
1 0 0 1
1114 .4285116
.4236687
.4260902
20
0 0 1 010
1 0 0 0


Table 4. Experiential results, using same amount ofdata



1149 .3765412
.3667394
.3716403
1148
.399101
.3871556
.3931283
1146 .3861199
.3733439
.3797319
1142 .3854209
.370132
.3777764
1134
.3954
.3985346
.3969673
1118 .3754304
.3630577
.3692441
1086 .3685369
.3535208
.3610289
1022 .3828205
.3710613
.3769409
894
.3606749
.367043
.363859
10
1 o 0 0 0 0 0 0 0
1o
0 1 0 0 010 0 0 0
10
0 0
1 0 010
0 0 0
10
0 0 0 1 0 0 0 0 0
lO
o o o Oil olo o o
10
0 0 0 0 0
1 0 0 0
10
0 0 0 010
~ I 0 0
10
O000iO
010
10
0 0 0 0 0 0'0
0 1


different data set. In addition to the data set used in section 5.1,
another data set was used, which is from the same source and with
the same format, but of the different time duration. Both the price
movement and the cluster information of the two data sets are
different. Each time we retrieved the same portion of information
from both data sets. Then used it in clustering alternatively. Hence
the avg. Sims of the two resulting clusters that used the same
portion of information composed a similarity-pair. We show all the
similarity-pairs in Figure 4, where the X-axis corresponding to the
resulting
avg.
Sire
on
the
first
data
set
and
the
Y-axis
corresponding to the avg. Sim on the second one.

It's easy to see that all the similarity-pair lies near the Y=X line.
This means the same portions of information of different data set
frequently gave the results with similar quality. If using partial
information of one data set produced good results, it likely to also
produced good results that used the same portion of information of
the other data set, and vice versa.

The results verified the usefulness of our approach. By applying
the proposed method on the historical data, we could find which
part of information is more meaningful or more efficient for
similarity measure in that application area. The choice of partial
information relates to the amount of involved data that determine
the corresponding time complexity, and the accuracy of the
corresponding measurement. Since the evaluation of the similarity
measurements based on various partial information is accurate and




548

0.35



0.5



u~ 0,45
0"351-
+
~
;




0.4r,
S
v_.
r
r
'


i
. i
i
i
1
i
i
i
r
0
50
100
150
200
250
300
350
400
450
500
Experiment Ie

Figure 3. Average similarity and amount of involved data




035


+
+
+
+

.=



'5 0.45
+
'+'
++
+
++
+
÷~+
++

0.4




0.35
+
+
++
'




i
0.3
0.35
0.4
0.45
0.5
0.55
0.6
A~erageSim of experiment 1

Figure 4. Experimental results on two data sets

quantitative. We could balance the two performance factors of
accuracy and efficiency by browsing the results, and finally choose
one for developing further similarity model. For example, if the
goal is to reduce the dimensionality, then we could browse all
results, which are produced using fewer data, to find the partial
information that has been used to produce the best result. Similarly,
when the goal is to develop an accurate measure strategy, we
browse all the good results and find one portion of information
concerning the fewest data.


6.
Conclusion
In this paper, we propose a model for retrieving and representing
the partial information in time series data, and a methodology for
evaluating
the
similarity
measurements
based
on
partial
information.

The experimental results give some interesting observations. First,
similarity measurement that use different portion of information
gives different results, and different portion, of information
contributes differently to similarity measure. Second, how many
components are considered does not have the decisive effect on the
similarity measure. Finally, using the first few components is not
the best choose for similarity measure. These observations could
help designing a more effective and more efficient similarity
measurement. Furthermore, the results of experiments on two data
sets justified the usefulness of our approach. Our methodology
gives the accurate and quantitative evaluation on the similarity
measurements based on partial information. This enable us balance
the two performance factors including accuracy and efficiency, and
finally find the optimal portion of information for further KDD
applications.


7. ACKNOWLEDGMENTS
The research has been supported in part of Chinese national key
fundamental research program (No, G1998030414) and Chinese
national fund of natural science (No. 79990580)


8. REFERENCE
[1] Rakesh A, Christos F, Efficient similarity search in
sequence databases. FODO 1993, 1993.
[2] M. Gavrilov, D. Angnelov, P. Indyk, R. Motwani.
Mining the stock market: which measure is best? Proc.
of the 6th ACM SIGKDD, 2000.
[3] X. Jin, Y. Lu, C Shi, Micro similarity query in time
series database. Proc. of PAKDD 2001, 2001
[4] Juan P. Caraca-Valente, Ignacio Lopez-Chavarrias,
Discovering similar patterns in time series. Proc. of the
6thIntl. Cone on KDD, 2000.
[5] R. Agrawal, K.I. Lin, H. S. Sawhney, K. Shim, Fast
similarity search in the presence of noise, scaling and
translation in time-series databases. The 23~ Intl. Cone
on Very Large Data Bases, 1995.
[6] D. Hull. Improving text retrieval for the routing
problem using latent semantic indexing. Proc. Of the
17thACM-SIGIR Conference, 1994.
[7] K.
Fukunaga,
Introduction
to
statistical
pattern
recognition. Academic Press, 1990, 2nd Edition.
[8] J.
Han,
M.
Kambr.
Data
mining,
concept
and
techniques. Academic Press, 2000.
[9] Hagit S, Stanley B. Z. Approximate queries and
representations for large data sequences. Proc. of the
12~ Intl. Cone on data engineering, 1996.
[10]K. Chan, A. Fu. Efficient time series matching by
wavelets.
Proc. of the
15th Intl. Conf. on Data
Engineering, Sydney, Australia, 1999.
[I1]E.J.
Keogh
and
M.J.
Pazzani,
An
enhanced
representation of time series which allows fast and
accurate
classification,
clustering
and
relevance
feedback. Proc. of the 4thConf. on KDD, 1998.
[12]Dennis D, Mining multivariate time-series sensor data
to discover behavior envelopes. Proc. of the 3ra Intl.
Cone on KDD, Newport Beach, California, 1997.
[13]D.
Rafiei
and
A.O.
Mendelzon,
Similarity-based
queries for time series data. In Proc. Of ACM SIGMOD
Int. Cone On Management of Data, 1997.
[14]Jagadish H. V., Alberto O. M, Tova M, Similarity-
based queries.
Proc. of the
14th ACM SIGACT-
SIGMOD-SIGART
symposium
on
principles
of
database system, 1995.
[15]M.K. Ng, Z. Huang. Data-mining massive time series
astronomical data: challenges, problems and solutions.
Information and Software Technology, 41,1999.




549

