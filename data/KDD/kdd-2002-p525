Mining Complex Models from Arbitrarily Large Databases
in Constant Time


Geoff Hulten
Dept. of ComputerScience& Engineering
UniversityofWashington,Box352350
Seattle,WA 98195-2350, U.S.A.
ghulten @cs.washington.edu
Pedro Domingos
Dept. of ComputerScience& Engineering
Universityof Washington,Box352350
Seattle,WA 98195-2350, U.S.A.
pedrod @cs.washington.edu


ABSTRACT
In this paper we propose a scaling-up method that is applica-
ble to essentially any induction algorithm based on discrete
search. The result of applying the method to an algorithm
is that its running time becomes independent of the size of
the database, while the decisions made are essentially iden-
tical to those that would be made given infinite data. The
method works within pre-specified memory limits and, as
long as the data is iid, only requires accessing it sequen-
tially. It gives anytime results, and can be used to produce
batch, stream, time-changing and active-learning versions
of an algorithm. We apply the method to learning Bayesian
networks, developing an algorithm that is faster than previ-
ous ones by orders of magnitude, while achieving essentially
the same predictive performance. We observe these gains on
a series of large databases "generated from benchmark net-
works, on the KDD Cup 2000 e-commerce data, and on a
Web log containing 100 million requests.


Categories and Subject Descriptors
H.2.8 [Database Management]:
Database Applications--
data mining; 1.2.6 [Artificial Intelligence]: Learning--in-
duction; 1.5.1 [Pattern Recognition]: Models--statistical;
1.5.2 [Pattern Recognition]: Design Methodology--clas-
sifter design and evaluation

General Terms
Scalable learning algorithms, subsampling, Hoeffding bounds,
discrete search, Bayesian networks


1. INTRODUCTION
Much work in KDD has focused on scaling machine learn-
ing and statistical algorithms to large databases. The goal is
generally to obtain algorithms whose running time is linear
(or near-linear) in the size of the database, and that only ac-
cess the data sequentially. So far this has been done mainly



Permissionto makedigital or hard copies of all or part of this work for
personal or classroomuse is granted withoutfee providedthat copies are
not madeor distributedfor profitor commercialadvantageand that copies
bear thisnotice and the full citationon the firstpage. Tocopyotherwise,to
republish,to post on serversor to redistributeto lists,requirespriorspecific
permissionand/ora fee.
SIGKDD02 Edmonton,Alberta,Canada
Copyright2002ACM 1-58113-567-X/02/0007...$5.00.
for one algorithm at a time, in a slow and laborious process.
We believe that this state of affairs can be overcome by de-
veloping scaling methods that are automatically (or nearly
automatically) applicable to broad classes of learning algo-
rithms, and that scale up to databases of arbitrary size by
limiting the quantity of data used at each step, while guar-
anteeing that the decisions made do not differ significantly
from those that would be made given infinite data.
This
paper describes one such method, based on generalizing the
ideas initially proposed in our VFDT algorithm for scaling
up decision tree induction [3]. The method is applicable to
essentially any learning algorithm based on discrete search,
where at each search step a number of candidate models or
model components are considered, and the best one or ones
are selected based on their performance on an iid sample
from the domain of interest. Search types it is applicable to
include greedy, hill-climbing, beam, multiple-restart, looka-
head, best-first, genetic, etc. It is applicable to common al-
gorithms for decision tree and rule induction, instance-based
learning, feature selection, model selection, parameter set-
ting, probabilistic classification, clustering, and probability
estimation in discrete spaces, etc., as well as their combina-
tions.
We demonstrate the method's utility by using it to scale
up Bayesian network learning [8]. Bayesian networks are a
powerful method for representing the joint distribution of a
set of variables, but learning their structure and parameters
from data is a computationally costly process, and to our
knowledge has not previously been successfully attempted
on databases of more than tens of thousands of examples.
With our method, we have been able to mine millions of
examples per minute. We demonstrate the scalibility and
predictive performance of our algorithms on a set of bench-
mark networks and on three large Web data sets.

2. A GENERAL METHOD FOR SCALING
UP LEARNING ALGORITHMS
Consider the following simple problem. We are given two
· classifiers A and B, and an infinite database of iid (inde-
pendent and identically distributed) examples. We wish to
determine which of the two classifiers is more accurate on
the database. If we want to be absolutely sure of making
the correct decision, we have no choice but to apply the two
classifiers to every example in the database, taking infinite
time. If, however, we are willing to accommodate a prob-
ability 5 of choosing the wrong classifier, we can generally
make a decision in finite time, taking advantage of statis-




525

tical results that give confidence intervals for the mean of
a variable. One such result is known as Hoeffding bounds
or additive Chernoff bounds [9]. Consider a real-valued ran-
dom variable r whose range is R. Suppose we have made
n independent observations of this variable, and computed
their mean ~. One form of Hoeffding bound states that, with
probability 1
5, the true mean of the variable is at least

e, where E = · ~
Let ~ be the difference in ac-
V
2n
'
curacy between the two classifiers on the first n examples in
the database, and assume without loss of generality that it
is positive. Then, if ~ > ~, the Hoeffding bound guarantees
that with probability 1
5 the classifier with highest accu-
racy on those n examples is also the most accurate one on
the entire infinite sample. In other words, in order to make
the correct decision with error probability 5, it is sufficient
to observe enough examples to make e smaller than ~.
The only case in which this procedure does not yield a
decision in finite time occurs when the two classifiers have
exactly the same accuracy.
No number of examples will
then suffice to find a winner. However, in this case we do
not care which classifier wins. If we stipulate a minimum
difference in accuracy r below which we are indifferent as to
which classifier is chosen, the procedure above is guaranteed
to terminate after seeing at most n =
[½(R/r)21n(1/6)]
examples.
In other words, the time required to choose a
classifier is constant, independent of the size of the database.
The Hoeffding bound has the very attractive property
that it is independent of the probability distribution gen-
erating the observations. The price of this generality is that
the bound is more conservative than distribution-dependent
ones (i.e., it will take more observations to reach the same
5 and e). An alternative is to use a normal bound, which
assumes ~ is normally distributed.
(By the central limit
theorem, this will always be approximately true after some
number of samples.)
In this case, e is the value of r
for which ~((r
~)/a,.) = 1
5, where 4,0 is the standard
normal distribution function. 1 Either way, we can view a
bound as a function f(5, n) that returns the maximum e by
which the true mean of r is smaller than the sample mean
~, given a desired confidence 1
5 and sample size n.
Suppose now that, instead of two classifiers, we wish to
find the best one among b. Making the correct choice re-
quires that each of the b
1 comparisons between the best
classifier and all others have the correct outcome (i.e., that
the classifier that is best on finite data also be best on infi-
nite data). If the probability of error in each of these deci-
sions is 5, then by the union bound the probability of error
in any of them, and thus in the global decision, is at most
(b
1)5. Thus, if we wish to choose the best classifier with
error probability at most 5", it suffices to use 5 = 5*/(b
1)
in the bound function ff(5, n) for each comparison.
Simi-
larly, if we wish to find the best a classifiers among b with
probability of error at most 5", a(b
a) comparisons need
to have the correct outcome, and to ensure that this is the
case with probability of error at most 5, it suffices to require
that the probability of error for each individual comparison
be at most 6*/[a(b
a)].
Suppose now that we wish to find the best classifier by a
search process composed of d steps, at each step considering
at most b candidates and choosing the best a. For the search


1If the variance a~ is estimated from data, the Student t
distribution is used instead.
process with finite data to output the same classifier as with
infinite data with probability at least 1
5", it suffices to
use 5 = 6*/[da(b
a)] in each comparison. Thus, in each
search step, we :need to use enough examples nl to make
cl = f(n~,5*/[da(b
a)]) < r~, where ri is the difference
in accuracy between the ath and .(a + 1)th best classifiers
(on ni examples, at the ith step).
As an example, for a
hill-climbing search of depth d and breadth b, the required
5 would be 5*/db. This result is independent of the process
used to generate candidate classifiers at each search step, as
long as this process does not itself access the data, and of
the order in which search steps are performed. It is appli-
cable to randomized search processes, if we assume that the
outcomes of random decisions are the same in the finite- and
infinite-data cases. 2
In general, we do not know in advance how many examples
will be required to make El < ri for all j at step i. Instead,
we can scan ~xn examples from the database or data stream
at a time, and check whether the goal has been met. Let
f l(e, 5) be the inverse of the bound function, yielding the
number of samples n needed to reach the desired e and 5.
(For the Hoeffding bound, n = [½(R/e)2 In(I/5)].)
Recall
that ~" is the threshold of indifference below which we do
not care which classifier is more accurate.
Then, in any
given search step, at most c = If l(r, 6)/~n] goal checks
will be made. Since a mistake could be made in any one of
them (i.e., an incorrect winner could be chosen), we need
to use enough examples ni at each step i to make el =
f(n,,5°/[cda(b
a)]) < r,.
Notice that nothing in the treatment above requires that
the models being compared be classifiers, or that accuracy
be the evaluation function. The treatment is applicable to
any inductive model -- be it for classification, regression,
probability estimation, clustering, ranking, etc. -- and to
comparisons between components of models as well as be-
tween whole models. The only property required of the eval-
uation function is that it be decomposable into an average
(or sum) over training examples. This leads to the general
method for scaling up learning algorithms shown in Table 1.
The key properties of this method are summarized in the
following theorem (A proof can be found in [10]). Let teen
be the total time spent by L* generating candidates, and
let ts,t be the total time spent by it in calls to Select-
Candidates(.M), including data access. Let L ~ be L with
ITI = c~, and U be the first terminating condition of the
"repeat" loop in SelectCandidates(.M) (see Table 1).

THEOREM 1. If ts~l > teen and ITI > c~xn, the time
complexity of L" is O(db(a+cAn) ). With probability at least
1
5", L* and L°~ choose the same candidates at every step
for which U is satisfied. If U is satisfied at all steps, L* and
L°° return the same model with probability at least 1
5".

In other words, the running time of the modified algorithm
is independent of the size of the database, as long as the
latter is greater than f
l(r, 5*/[cda(b
a)]). A tie between
candidates at some search step occurs if r is reached or the
database is exhausted before U is satisfied. With probability
1
5", all decisions made without ties are the same that
would be made with infinite data. If there are no ties in any

2This excludes simulated annealing, because in this case the
outcome probabilities depend on the observed differences in
performance between candidates. Extending our treatment
to this case is a matter for future work.




526

Table 1: Method
for scaling up learning algorithms.


Given:
An iid sample T = {X1, X2,..., XiTI}.
A real-valued evaluation function E(M, x), where
M is a model (or model component) and x is
an example.
A learning algorithm L that finds a model by per-
forming at most d search steps, at each step
considering at most b candidates and selecting
the a with highest E(M, T) = ~
~XeTE(M, X).
A desired maximum error probability 5*.
A threshold of indifference 7".
A bound function f(n, 5).
A block size An.

Modify L, yielding L °,
by at each step replacing the selection of the a
candidates with highest E(M, T) with a call
to SelectCandidates(.A4), where .M is the set of
candidates at that step.



Procedure
SelectCandidates(./t4)
Let n = 0.
For each M E .M, let ~(M) = 0.
Repeat
If n + An > ITI then let n' = ITI.
Else let n ~= n + An.
For each M E .M
r~t
Let E(M) = E(M) + ~,=,~+t E(M, X,).
Let ~(M, T') = ~(M)/n'.
Let A/g = {M E .M : M has one of the a highest
E(M,T') in M}.
n.~n
t.

c =
Ft"
~(~-,6"/[cd~@
a)])/An].
Until [Y(M~ e M', Mr e M
M')
f(n, 5"/[cda(b a)]) < E(M,,T')
E(M~,T')]
or f(n,J*l[cda(b
a)]) < r or n = ITI.
Return M ~.



of the search steps, the model produced is, with probability
1
~*, the same that would be produced with infinite data.
An alternative use of our method is to first choose a maxi-
mum error probability ~ to be used at each step, and then re-
port the global error probability achieved, 5" = 5 ~i~=1 cciai
(bi
ai), where ci is the number of goal checks performed
in step i, bl is the number of candidates considered at that
step, and ai is the number selected. Even if 5 is computed
from c, b, a and the desired J* as in Table 1, reporting the
actual achieved 5" is recommended, since it will often be
much better than the original target.
Further time can be saved by, at each step, dropping a
candidate from consideration as soon as its score is lower
than at least a others by at least f(n,5*/[cda(b
a)]).
SelectCandidates(.h4) is an anytime procedure in the sense
that, at any point after processing the first An examples, it
is ready to return the best a candidates according to the
data scanned so far (in increments of An). If the learning
algorithm is such that successive search steps progressively
refine the model to be returned, L* itself is an anytime pro-
cedure.
The method in Table 1 is equally applicable to databases
(i.e., samples of fixed size that can be scanned multiple
times) and data streams (i.e., samples that grow without
limit and can only be scanned once). In the database case,
we start a new scan whenever we reach the end of the
database, and ensure that no search step uses the same ex-
ample twice. In the data stream case, we simply continue
scanning the stream after the winners for a step are chosen,
using the new examples to choose the winners in the next
step, and so on until the search terminates.
When learning large, complex models it is often the case
that the structure, parameters, and sufficient statistics used
by all the candidates at a given step exceed the available
memory.
This can lead to severe slowdowns due to re-
peated swapping of memory pages.
Our method can be
easily adapted to avoid this, as long as m > a, where m
is the maximum number of candidates that fits within the
available RAM. We first form a set .Mr composed of any
m elements of .A4, and run SelectCandidates(]~41). We then
add another m
a candidates to the a selected, yielding
.M2, and run SelectCandidates(.M2). We continue in this
way until .M is exhausted, returning the a candidates se-
lected in the last iteration as the overall winners. As long
as all calls to SelectCandidates0 start scanning S at the
same example and ties are broken consistently, these win-
ners are guaranteed to be the same that would be obtained
by the single call SelectCandidates(J~). If k iterations are
carried out, this modification increases the running time of
SelectCandidates(/~4) by a factor of at most k, with k < b.
Notice that the "new" candidates in each .A41do not need to
be generated until SelectCandidates(.A4~) is to be run, and
thus they never need to be swapped to disk.
The running time of an algorithm scaled up with our
method is often dominated by the time spent reading data
from disk. When a subset of the search steps are indepen-
dent (i.e., the results of one step are not needed to perform
the next one, as when growing the different nodes on the
fringe of a decision tree), much time can be saved by us-
ing a separate search for each independent model compo-
nent and (conceptually) performing them in parallel. This
means that each data block needs to be read only once for
all of the interleaved searches, greatly reducing I/O require-
ments.
When these searches do not all fit simultaneously
into memory, we maintain an inactive queue from which
steps are transferred to memory as it 'becomes available (be-
cause some other search completes or is inactivated).
The processes that generate massive data sets and open-
ended data streams often span months or years, during which
the data-generating distribution can change significantly,
violating the iid assumption made by most learning algo-
rithms. A common solution to this is to repeatedly apply
the learner to a sliding window of examples, which can be
very inefficient. Our method can be adapted to efficiently ac-
commodate time-changing data as follows. Maintain ~(M)
throughout time for every candidate M considered at ev-
ery search step.
After the first w examples, where w is
the window width, subtract the oldest example from ~2(M)
whenever a new one is added. After every An new exam-
ples, determine again the best a candidates at every previ-
ous search decision point. If one of them is better than an
old winner by J*ls (s is the maximum number of candidate




527

comparisons expected during the entire run) then there has
probably been some concept drift. In these cases, begin an
alternate search starting from the new winners. Periodically
use a number of new examples as a validation set to compare
the performance of the models produced by the new and old
searches. Prune the old search when the new models are
on average better than the old ones, and prune the new
search if after a maximum number of validations its models
have failed to become more accurate on average than the
old ones. If more than a maximum number of new searches
is in progress, prune the lowest-performing ones. This ap-
proach to handling time-changing data is a generalization of
the one we successfully applied to the VFDT decision-tree
induction algorithm [11].
Active learning is a powerful type of subsampling where
the learner actively selects the examples that would cause
the most progress [1]. Our method has a natural exten-
sion to this case when different examples are relevant to
different search steps, and some subset of these steps is in-
dependent of each other (as in decision tree induction, for
example): choose the next An examples to be relevant to
the step where U is currently farthest from being achieved
(i.e., where f(n,~*/[cda(b
a)])
minij{max(r,E(Mi,T')
E(Mj, T')}} is highest).
In the remainder of this paper we apply our method to
learning Bayesian networks, and evaluate the performance
of the resulting algorithms.

3.
LEARNING BAYESIAN NETWORKS
We now briefly introduce Bayesian networks and meth-
ods for learning them. See Heckerman et al. [8] for a more
complete treatment. A Bayesian network encodes the joint
probability distribution of a set of d variables, {xl .... , xd},
as a directed acyclic graph and a set of conditional probabil-
ity tables (CPTs). (In this paper we assume all variables are
discrete.) Each node corresponds to a variable, and the CPT
associated with it contains the probability of each state of
the variable given every possible combination of states of its
parents. The set of parents of xl, denoted par(xi), is the set
of nodes with an arc to xi in the graph. The structure of the
network encodes the assertion that each node is condition-
ally independent of its non-descendants given its parents.
Thus the probability of an arbitrary event X = (xl,..., xd)
can be computed as P(X)
d
= I-[i=1P(xilpar(x')) · In gen-
eral, encoding the joint distribution of a set of d discrete
variables requires space exponential in d; Bayesian networks
reduce this to space exponential in maxie(1,...,d} Ipar(xl)l.
In this paper we consider learning the structure of Bayesian
networks when no values are missing from training data. A
number of algorithms for this have been proposed; perhaps
the most widely used one is described by Heckerman et al.
[8]. It performs a search over the space of network struc-
tures, starting from an initial network which may be ran-
dom, empty, or derived from prior knowledge. At each step,
the algorithm generates all variations of the current network
that can be obtained by adding, deleting or reversing a sin-
gle arc, without creating cycles, and selects the best one
using the Bayesian Dirichlet (BD) score (see Heckerman et
al. [8]). The search ends when no variation achieves a higher
score, at which point the current network is returned. This
algorithm is commonly accelerated by caching the many re-
dundant calculations that arise when the BD score is applied
to a collection of similar networks. This is possible because
the BD score is decomposableinto a separate component for
each variable. ]In the remainder of this paper we scale up
Heckerman et aL's algorithm, which we will refer to as HGC
throughout.

4.
SCALING UP BAYESIAN NETWORKS
At each search step, HGC considers all examples in the
training set T when computing the BD score of each candi-
date structure. Thus its running time grows without limit as
the training set size ITI increases. By applying our method,
HGC's running time can be made independent of ITI, for
ITI > f l(r,a), with user-determined r and 6. In order
to do this, we must first decompose the BD score into an
average of some quantity over the training sample. This is
made possible by taking the logarithm of the BD score, and
discarding terms that become insignificant when n ~ oo, be-
cause the goal is to make the same decisions that would be
made with infinite data. This yields (see Hulten & Domin-
gos [10] for the detailed derivation):

d
logBDoo(S,T) = E ~loglS(x,~lS, par(x,,))
(1)

e=l
i=1


where xi, is the value of the ith variable in the eth example,
and P(xi,lpar(xi,)) is the maximum-likelihood estimate of
the probability of xi~ given its parents in structure S, equal
to nljk/nlj if in example e the variable is in its kth state and
its parents are in their jth state. Effectively, when n ~ co,
the log-BD score converges to the log-likelihood of the data
given the network structure and maximum-likelihood pa-
rameter estimates, and choosing the network with highest
BD score becomes equivalent to choosing the maximum-
likelihood network. The quantity ~ia=x log P(xi~IS,par(xl,))
is the log-likelihood of an example X~ given the structure S
and corresponding parameter estimates. When comparing
two candidate structures $1 and $2, we compute the mean
difference in this quantity between them:

1 "
d
AL(S1, S2) = ~ E E l°g lS(xilS" par(x') )
e=l
i=1




n
e=l
i=1

Notice that the decomposability of the BD score allows this
computation to be accelerated by only considering the com-
ponents corresponding to the two or four variables with dif-
ferent parents in $1 and $2. We can apply either the normal
bound or the Hoeffding bound to ~xL(S1,Sz). In order to ap-
ply the Hoeffding bound, the quantity being averaged must
have a finite range. We estimate this range by measuring
the minimum non-zero probability at each node P~ and use
~d
e
as the range ~.,i=1 log~, where c is a small integer,z
After the structure is learned, the final nljk and nlj counts
must be estimated. In future work we will use the bound
· from [6] to determine the needed sample size; the current al-
gorithm simply uses a single pass over training data. Togeth-
er with the parameters of the Dirichlet prior, these counts
induce a posterior distribution over the parameters of the
network. Prediction of the log-likelihood of new examples is

3A value may have zero true probability given some parent
state, but this is not a problem, since such a value and parent
combination will never occur in the data.




528

carried out by integrating over this distribution, as in HGC.
We call this algorithm VFBN1.
HGC and VFBN1 share a major limitation: they are un-
able to learn Bayesian networks in domains with more than
100 variables or so. The reason is that the space and time
complexity of their search increases quadratically with the
number of variables (since, at each search step, each of the d
variables considers on the order of one change with respect
to each of the other d variables). This complexity can be
greatly reduced by noticing that, because of the decompos-
ability of the BD score, many of the alternatives considered
in a search step are independent of each other.
That is,
except for avoiding cycles, the best arc change for a par-
ticular variable will still be best after changes are made to
other variables. The VFBN2 algorithm exploits this by car-
rying out a separate search for each variable in the domain,
interleaving all the searches.
VFBN2 takes advantage of
our method's ability to reduce I/O time by reading a data
block just once for all of its searches. The generality of our
scaling-up method is illustrated by the fact that it is applied
in VFBN2 as easily as it was in VFBN1.
Two issues prevent VFBN2 from treating these searches
completely independently: arc reversal and the constraint
that a Bayesian network contain no cycles. VFBN2 avoids
the first problem simply by disallowing arc reversal; our ex-
periments show this is not detrimental to its performance.
Cycles are avoided in a greedy manner by, whenever a new
arc is added, removing from consideration in all other search-
es all alternatives that would form a cycle with the new arc.
VFBN2 uses our method's ability to cycle through searches
when they do not all simultaneously fit into memory. All
searches are initially on a queue of inactive searches. An in-
active search uses only a small constant amount of memory
to hold its current state. The main loop of VFBN2 transfers
searches from the head of the inactive queue to the active
set until memory is filled. An active search for variable x~
considers removing the arc between each variable in par(xi)
and xl, adding an arc to xi from each variable not already
in par(x~), and making no change to par(xl), all under the
constraint that no change add a cycle to S. Each time a
block of data is read, the candidates' scores in each active
search are updated and a winner is tested for as in Select-
Candidates() (see Table 1). If there is a winner (or one is
selected by tie-breaking because 7"has been reached or T has
been exhausted) its change is applied to the network struc-
ture. Candidates in other searches that would create cycles
if added to the updated network structure are removed from
consideration. A search is finished if making no change to
the associated variable's parents is better than any of the
alternatives. If a search takes a step and is not finished, it
is appended to the back of the inactive queue, and will be
reactivated when memory is available. VFBN2 terminates
when all of its searches finish.
HGC and VFBN1 require O(d2vk) memory, where k is the
maximum number of parents and v is the maximum number
of values of any variable in the domain. This is used to store
the CPTs that differ between each alternative structure and
S. VFBN2 improves on this by using an inactive queue to
temporarily deactivate searches when RAM is short. This
gives VFBN2 the ability to make progress with O(dvk) mem-
ory, turning a quadratic dependence on d into a linear one.
VFBN2 is thus able to learn on domains with many more
variables than HGC or VFBN1. Further, HGC and VFBN1
perform redundant work by repeatedly evaluating O(d2) al-
ternatives, selecting the best one, and discarding the rest.
This is wasteful because some of the discarded alternatives
are independent of the one chosen, and will be selected as
winners at a later step. VFBN2 avoids much of this redun-
dant work and learns structure up to a factor of d faster
than VFBN1 and HGC.
We experimented with several policies for breaking ties,
and obtained the best results by selecting the alternative
with highest observed BD score. First, it limits the number
of times an arc can be added or removed between each pair
of variables to two. Second, to make sure that at least one
search will fit in the active set, it does not consider any
alternative that requires more than Mid MB of memory,
where M is the system's total available memory in MB.
Third, it can limit the number of parameters used in any
variable's CPT to be less than a user-supplied threshold.

5.
EMPIRICAL EVALUATION
We compared VFBN1, VFBN2, and our implementation
of HGC on data generated from several benchmark net-
works, and on three massive Web data sets.
All experi-
ments were carried out on a cluster of 1 GHz Pentium III
machines, with a memory limit M of 200MB for the bench-
mark networks and 300MB for the Web domains. In or-
der to stay within these memory limits, VFBN1 and HGC
discarded any search alternative that required more than
Mid2 MB of RAM. The block size An was set to 10,000.
VFBN1 and VFBN2 used the normal bound with variance
estimated from training data. To control complexity we lnn-
ited the size of each CPT to 10,000 parameters (one param-
eter for every 500 training samples). VFBN1 and VFBN2
used ~ = 10 9 and r = 0.05%. All algorithms started their
search from empty networks (we also experimented with
prior networks, but space precludes reporting on them).

Benchmark Networks For the benchmark study, we gen-
erated data sets of five million examples each from the net-
works contained in the repository at http://www.cs.huji.ac.-
il/labs/compbio/Repository/. The networks used and num-
ber of variables they contained were: (Insurance, 27), (Wa-
ter, 32), (Alarm, 37), (Hailfinder, 56), (Muninl, 189), (Pigs,
441), (Link, 724), and (Munin4, 1041). Predictive perfor-
mance was measured by the log-likelihood on 100,000 inde-
pendently generated test examples. HGC's parameters were
set as described in Hulten ~ Domingos [10]. We limited the
algorithms to spend at most 5 days of CPU time (7200 min-
utes) learning structure, after which the best structure found
up to that point was returned.
Table 2 contains the results of our experiments. All three
systems were able to run on the small networks (Insurance,
Water, Alarm, Hallfinder).
Because of RAM limitations
only VFBN2 could run on the remaining benchmark net-
works. The systems achieve approximately the same likeli-
hoods for the small networks. Further, their likelihoods were
very close to those of the true networks, indicating that our
scaling method can achieve high quality models. VFBN1
and VFBN2 both completed all four runs within the allot-
ted 5 days, while HGC did so only twice. VFBN2 was an
order of magnitude faster than VFBN1, which was an order
of magnitude faster than HGC. VFBN2 spent less than five
minutes on each run. This was less than the time it spent
doing a single scan of data to estimate parameters, by a fac-
tor of five to ten. VFBN2's global confidence bounds ~* were




529

better than VFBNi's by at least an order of magnitude in
each experiment. This was caused by VFBNi's redundant
search forcing it to remake many decisions, thus requiring
that many more statistical bounds hold. We also ran HGC
on a random sample of 10,000 training examples. This vari-
ation always had worse likelihood than both VFBN1 and
VFBN2. It also always spent more time learning structure
than did VFBN2.
For the large networks, we found VFBN2 to improve sig-
nificantly on the initial networks, and to learn networks with
likelihoods similar to those of the true networks. Not sur-
prisingly, we found that many more changes were required
to learn large networks than to learn small ones (on these,
VFBN2 made between 566 and 7133 changes to the prior
networks). Since HGC and VFBN1 require one search step
for each change, this suggests that even with sufficient RAM
they would learn much more slowly compared to VFBN2
than they did on the small networks. We watched the ac-
tive set and inactive queue during the runs on the large data
sets. We found the proportion of searches that were active
was high near the beginning of each run. As time progressed,
however, the size of the CPTs being learned tended to in-
crease, leaving less room for searches to be active. In fact,
during the majority of the large network runs, only a small
fraction of the remaining searches were active at any one
time. Recall that VFBN1 and HGC can only run when all
remaining searches fit in the active set. For these networks
the 200 MB allocation falls far short. For example, on a typ-
ical run on a large network we found that at the worst point
only 1.31% of the remaining searches were active. Assuming
they all take about the same RAM, HGC and VFBN1 would
have required nearly 15 GB to run.

Web Applications In order to evaluate VFBN2's perfor-
mance on large real-world problems, we ran it on two large
Web traces. The first was the data set used in the KDD
Cup 2000 competition [12]. The second was a trace of all
requests made to the Web site of the University of Wash-
ington's Department of Computer Science and Engineering
between January 2000 and January 2002.
The KDD Cup data consists of 777,000 Web page requests
collected from an e-commerce site. Each request is anno-
tated with a requester session ID and a large collection of
attributes describing the product in the requested page. We
focused on one of the fields in the log, "Assortment Level
4", which contained 65 categorizations of pages into product
types (including "none"). For each session we produced a
training example with one Boolean attribute per category
- "True" if the session visited a page with that category.
There were 235,000 sessions in the log, of which we held out
35,000 for testing.
The UW-CSE-80 and UW-CSE-400 data sets were created
from a log of every request made to our department's Web
site between late January 2000 and late January 2002. The
log contained on the order of one hundred million requests.
We extracted the two training sets from this log in a manner
very similar the KDD Cup data set. For the first we iden-
tiffed the 80 most commonly visited level two directories on
the site (e.g.
/homes/faculty/and/education/undergrads/).
For the second we identified the 400 most commonly vis-
ited Web objects (excluding most images, style sheets, and
scripts). In both cases we broke the log into approximate
sessions, with each session containing all the requests made
Table 2:
Empirical results.
Samples is the total
number of examples read from disk while learn-
ing structure, in millions. Times in bold exceeded
our five day limit and the corresponding runs were
stopped before converging.


Network
Algorithm
Log-Likel
Samples
Minutes
Insurance
True
13.048
-
-
HGC
13.048
320.00
2446.08
VFBN1
13.069
16.69
39.72
VFBN2
13.070
0.52
1.02
Water




Alarm




Hallfinder




Muninl


Pigs
True
HGC
VFBN1
VFBN2
True
HGC
VFBN1
VFBN2
True
HCC
VFBN1
VFBN2

True
VFBN2
True
12.781
12.783
12.805
12.795
10.448
10.455
10.439
10.447
49.115
54.403
48.885
48.889

37.849
38.417
330.277
375.00
35.80
0.88


279.93
15.07
0.81


123.12
23.80
0.17


0.95
5897.53
360.45
1.85


7200.15
87.87
2.92


7200.62
194.97
3.22


47.38


VFBN2
319.559
1.24
636.98
Link
True
210.153
-
-
VFBN2
232.449
12.29
2451.95
Munin4
True
171.874
-
-
VFBN2
173.757
4.61
3003.28

KDD-Cup
Empty
2.446
-
0.00
HGC
2.282
69.59
6345.13
VFBN1
2.272
12.05
335.37
VFBN2
2.301
0.42
15.40
UW-80
Empty
1.611
-
0.00
HGC
1.346
75.53
7201.05
VFBN1
1.273
14.93
457.18
VFBN2
1.269
1.98
12.08
UW-400
Empty
7.002
-
0.00
VFBN2
4.556
3.63
905.30


by a single host until an idle period of 10 minutes; there
were 8.3 million sessions. We held out the last week of data
for testing.
The UW-CSE-400 domain was too large for VFBN1 or
HGC. HGC ran for nearly five days on the other two data
sets, while VFBN2 took less than 20 minutes for each. The
systems achieved similar likelihoods when they could run,
and always improved on their starting network. Examining
the networks produced, we found many potentially interest-
ing patterns. For example, in the e-commerce domain we
found products that were much more likely to be visited
when a combination of related products was visited than
when only one of those products was visited.


6.
RELATED WORK
Our method falls in the general category of sequential
analysis [17], which determines at runtime the number of
examples needed to satisfy a given quality criterion. Other
recent examples of this approach include Maron and Moore's




530

racing algorithm for model selection [13], Greiner's PALO al-
gorithm for probabilistic hill-climbing [7], Scheffer and Wro-
bel's sequential sampling algorithm [16], and Domingo et
al.'s AdaSelect algorithm [2]. Our method goes beyond these
in applying to any type of discrete search, providing new
formal results, working within pre-specified memory limits,
supporting interleaving of search steps, learning from time-
changing data, etc. A related approach is progressive sam-
pling [14, 15], where successively larger samples are tried,
a learning curve is fit to the results, and this curve is used
to decide when to stop. This may lead to stopping earlier
than with our method, but stopping can also occur prema-
turely, due to the difficulty in reliably extrapolating learning
curves.
Friedman et al.'s Sparse Candidate algorithm [5] alter-
nates between heuristically selecting a small group of po-
tential relatives for each variable and doing a search step
limited to considering changing arcs between a variable and
its potential relatives. This procedure avoids the quadratic
dependency on the number of variables in a domain. Fried-
man et al. evaluated it on data sets containing 10,000 sam-
ples and networks with up to 800 variables.
This paper describes a general method for scaling up learn-
ers based on discrete search.
We have also developed a
related method for scaling up learners based on search in
continuous spaces [4].

7.
CONCLUSION AND FUTURE WORK
Scaling up learning algorithms to the massive data sets
that are increasingly common is a fundamental challenge
for KDD research. This paper proposes that the time used
by a learning algorithm should depend only on the com-
plexity of the model being learned, not on the size of the
available training data. We present a framework for semi-
automatically scaling any learning algorithm that performs
a discrete search over model space to be able to learn from
an arbitrarily large database in constant time. Our frame-
work further allows transforming the algorithm to work in-
crementally, to give results anytime, to fit within memory
constraints, to support interleaved search, to adjust to time-
changing data, and to support active learning.
We use
our method to develop a new algorithm for learning large
Bayesian networks from arbitrary amounts of data. Experi-
ments show that this algorithm is orders of magnitude faster
than previous ones, while learning models of essentially the
same quality.
Directions for future work on our scaling framework in-
clude combining it with the one we have developed for search
in continuous spaces, improving the bounds by taking can-
didate dependencies into account, constructing a program-
ming library to facilitate our framework's application, and
scaling up additional algorithms using it. Future work on
VFBN includes extending it to handle missing data val-
ues, developing better mechanisms for controlling complex-
ity when data is abundant, and scaling VFBN further by
learning local structure at each node (e.g., !n the form of a
decision tree).
Acknowledgments We thank Blue Martini and Corin An-
derson for providing the Web data sets and the donors and
maintainers of the Bayesian network repository. This re-
search was partly supported by NSF CAREER and IBM
Faculty awards to the second author, and by a gift from the
Ford Motor Co.
8.
REFERENCES
[1] D. Cohn, L. Atlas, and R. Ladner. Improving
generalization with active learning. Machine Learning,
15:201-221, 1994.
[2] C. Domingo, R. Gavalda, and O. Watanabe. Adaptive
sampling methods for scaling up knowledge discovery
algorithms. Data Mining and Knowledge Discovery,
6:131-152, 2002.
[3] P. Domingos and G. Hulten. Mining high-speed data
streams. In Proc. 6th A CM SIGKDD International
Conf. on Knowledge Discovery and Data Mining, pp.
71-80, Boston, MA, 2000.
[4] P. Domingos and G. Hulten. Learning from infinite
data in finite time. In Advances in Neural Information
Processing Systems 14. MIT Press, Cambridge, MA,
2002.
[5] N. Friedman, I. Nachman, and D. Pedr. Learning
Bayesian network structure from massive datasets:
The "sparse candidate" algorithm. In Proc. 15th Conf.
on Uncertainty in Artificial Intelligence, pp. 206-215,
Stockholm, Sweden, 1999.
[6] N. Friedman and Z. Yakhini. On the sample
complexity of learning Bayesian networks. In Proc.
1Pth Conf. on Uncertainty in Artificial Intelligence,
pp. 274-282, Portland, OR, 1996.
[7] R. Greiner. PALO: A probabilistic hill-climbing
algorithm. Artificial Intelligence, 84:177-208, 1996.
[8] D. Heckerman, D. Geiger, and D. M. Chickering.
Learning Bayesian networks: The combination of
knowledge and statistical data. Machine Learning,
20:197-243, 1995.
[9] W. Hoeffding. Probability inequalities for sums of
bounded random variables. Journal of the American
Statistical Association, 58:13-30, 1963.
[10] G. Hulten and P. Domingos. A general method for
scaling up learning algorithms and its application to
Bayesian networks. Technical report, Department of
Computer Science and Engineering, University of
Washington, Seattle, WA, 2002.
[11] G. Hulten, L. Spencer, and P. Domingos. Mining
time-changing data streams. In Proc. 7th ACM
SIGKDD International Conf. on Knowledge Discovery
and Data Mining, pp. 97-106, San Francisco, CA,
2001.
[12] R. Kohavi, C. Brodley, B. Frasca, L. Mason, and
Z. Zheng. KDD-Cup 2000 organizers' report: Peeling
the onion. SIGKDD Explorations, 2(2):86-98, 2000.
[13] O. Maron and A. Moore. Hoeffding races:
Accelerating model selection search for classification
and function approximation. In Advances in Neural
Information Processing Systems 6. Morgan Kaufmann,
San Mateo, CA, 1994.
[14] C. Meek, B. Thiesson, and D. Heckerman. The
learning-curve method applied to model-based
clustering. Journal of Machine Learning Research,
2:397-418, 2002.
[15] F. Provost, D. Jensen, and T. Oates. Efficient
progressive sampling. In Proc. 5th A CM SIGKDD
International Conf. on Knowledge Discovery and Data
Mining, pp. 23-32, San Diego, CA, 1999.
[16] T. Scheffer and S. Wrobel. Incremental maximization
of non-instance-averaging utility functions with
applications to knowledge discovery problems. In
Proc. 18th International Conf. on Machine Learning,
pp. 481-488, Williamstown, MA, 2001.
[17] A. Wald. Sequential analysis. Wiley, New York, 1947.




531

