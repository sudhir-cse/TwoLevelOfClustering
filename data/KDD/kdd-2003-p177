Cross-training: Learning probabilistic mappings
between topics

Sunita Sarawagi
IIT Bombay
sunita@it.iitb.ac.in
Soumen Chakrabarti


IIT Bombay
soumen@cse.iitb.ac.in
Shantanu Godbole


IIT Bombay
shantanu@it.iitb.ac.in

ABSTRACT
Classification is a well-established operation in text mining.
Given a set of labels A and a set DA of training documents
tagged with these labels, a classifier learns to assign labels
to unlabeled test documents. Suppose we also had available
a different set of labels B, together with a set of documents
DB marked with labels from B. If A and B have some se-
mantic overlap, can the availability of DB help us build a
better classifier for A, and vice versa? We answer this ques-
tion in the affirmative by proposing cross-training: a new
approach to semi-supervised learning in presence of multiple
label sets. We give distributional and discriminative algo-
rithms for cross-training and show, through extensive exper-
iments, that cross-training can discover and exploit proba-
bilistic relations between two taxonomies for more accurate
classification.
Categories and subject descriptors: I.2.6 [Artificial in-
telligence]: Learning; I.5.2 [Pattern Recognition]: De-
sign Methodology - classifier design and evaluation
Keywords: Semi-supervised multi-task learning, Document
classification, EM, Support Vector Machines.

1.
INTRODUCTION
Document classification is a well-established area of text
mining. A document classifier is first trained using docu-
ments with preassigned labels or classes picked from a set
of labels, which we call the taxonomy or catalog. Once the
classifier is trained, it is offered test documents for which it
must guess the best label/s. Depending on the application,
the label may correspond to a broad topic (e.g., a topic in
the Yahoo! directory), a product category, or a user's per-
sonal taste in books, CDs, or Web sites.
Support Vector Machines (SVMs) [8], nearest-neighbor
classifiers [19], maximum entropy classifiers [14], and naive
Bayes (NB) classifiers [13] are some of the commonly used
document classifiers.
If all content creators and users agreed on a single cat-
alog of universal labels, text classification could also help
tag content with unambiguous semantic annotations. The
Web, however, has evolved without central editorship. It


Contact author.

Supported by the Infosys Fellowship Award from Infosys Tech-
nologies Limited, India.



Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGKDD '03 August 24-27, 2003, Washington, DC, USA.
Copyright 2003 ACM 1-58113-737-0/03/0008 ...$5.00.
is unclear if universal standards will emerge outside specific
application segments, and even for those segments, there is
a need to consolidate legacy data into content organized as
per the agreed-upon standards. These standards, moreover,
are far from static.
A few examples will illustrate the current scenario. An
e-commerce site which consolidates catalogs of goods and
services may want to organize them according to the (still
evolving) codes being developed in cooperation between EC-
CMA and UNSPSC (see http://www.eccma.org/unspsc/).
Meanwhile, vendors may have their own custom/legacy codes
which generally evolve over time.
As another example, consider Yahoo! and Dmoz. Both
cover the Web and have evolved to similar taxonomies, but
show non-trivial differences. Among other artifacts, taxon-
omy inversion is rampant in the Regional categories; what
is Reference.Education.Colleges_and_Universities.Asia.
India in one may be Regional.Asia.India.Education in
the other; in fact, they sometimes coexist in the same taxon-
omy! Other relationships are also common: Dmoz.Recreation.
Outdoors.Speleology overlaps Yahoo.Recreation.Outdoors.
Caving, but there are important non-overlapping sub-topics.
Given that documents are inherently a conglomeration of
concepts, we believe that mappings between content-based
taxonomies will be complex, uncertain and noisy. There-
fore, text searching, ranking, and mining tools must exploit
any available relationships, even probabilistic ones, between
diverse meta-data standards.

Our contributions: We introduce a general semi-supervised
learning framework called cross-training which can exploit
knowledge about label assignments in one taxonomy B to
make better inferences about label assignments in another
taxonomy A. Cross-training generalizes several existing clas-
sification algorithms, while also comparing favorably with
their accuracy on a host of related applications. Apart from
increased classification accuracy, the benefits include a bet-
ter understanding of probabilistic relationships between tax-
onomies, and more experience with encoding heterogeneous
features for learning algorithms.
We propose two cross-training algorithms. One uses Ex-
pectation Maximization (EM) [5]. The other uses Support
Vector Machines [16, 7, 8]. Through a detailed experimen-
tal study using real-life and semi-synthetic data from Yahoo!
and Dmoz, we show that cross-training is decisively better
than the best classifier we could induce on A or B alone.
Neither of our approaches dominates the other across all
data sets. Cross-training also compares favorably with one
recent algorithm [1] for mapping taxonomies, as well as an
earlier classification algorithm which could exploit a pool of
unlabeled documents [15].
Cross-training is related to multi-task learning [18, 4], but
quite different from co-training [3]. We will discuss these and
other related work in §6. The connection between cross-



177

training and supervised learning with discrete/categorical
attributes is discussed in §7.
Outline: We start with a formal definition of the cross-
training problem in §2, and then present the two major
classes of cross-training algorithms in §3 and §4. We conduct
a detailed experimental evaluation of the accuracy of vari-
ous algorithms in §5, review related work in §6, and make
concluding remarks in §7.

2.
PROBLEM SETTING AND
TERMINOLOGY
Like most recent text classifiers, our system models each
document as a bag (multi-set) of words. Term t occurs n(d,t)
times in document d. In contrast with prior work, there
are two sets of class labels A and B. A document can be
associated with a pair of labels (cA,cB) where cA  A and
cB  B. We will also denote these labels, for a specific
document d, as cA(d) and cB(d).
Flat taxonomy assumption: In our setting, a taxonomy is
a flat set of class labels. We apologize for continuing to use
the term "taxonomy" which connotes hierarchical relations
between concepts or topics. Generalizing our approach to
that setting is left for future work.
Owing to the dichotomy of labels, training and testing
processes must be defined more carefully than in standard
document classification. There are two distinct learning sce-
narios, which we discuss separately.

2.1 Mapping half-labeled documents
The mapping problem can arise in a e-commerce setting
where catalogs of goods and services of one site need to be
integrated with those of another site. A could represent
the target taxonomy, and B the taxonomy used at the data
source.
A training document has exactly one of cA(d) and cB(d)
known; we call such documents half-labeled. The system
trains on half-labeled documents. During deployment, a new
document comes with exactly one of the labels known, and
the system has to estimate the missing label.
For benchmarking, we use a test set which is fully-labeled,
and hide each label in turn, comparing the hidden label with
the system's guess. The accuracy of the system is the frac-
tion of documents that it assigns to the correct hidden label.
Thus, mapping is a symmetric scenario.
DA (respectively, DB) is the set of documents with A-
labels (respectively, B-labels), and DA - DB and DB -
DA are the half-labeled instances available to an algorithm.
DA  DB is used for testing the algorithm.
Validationandtuning: As in earlier work [1], we will (some-
times) assume that some fraction of fully-labeled data can be
sampled and made available to the system to help it tune its
parameters and validate its models. This is called the tune-
set. Typically, the available set of fully-labeled documents
is partitioned into a tuneset and a test set, the tuneset used
to fine-tune the system's models and parameters, and the
test set used to evaluate the system. This split is done ran-
domly, many times, and the average accuracy is reported.
(The above discussion may hint that the tuneset ought to be
a small portion of the fully-labeled data, but in earlier work
[1] large tunesets have been used. We report experiments
with both choices, to make a fair comparison.)

2.2 Classifying zero-label documents
Several personal bookmark managers [11, 9] need to train
document classifiers on bookmarks organized into personal
topic directories (say, B) with the intent of mapping sub-
sequently visited pages to those topics. Because bookmark-
ing and annotation takes effort, people bookmark far fewer
pages than they visit.
Nigam et al. [15] showed that if training data is scarce,
a pool of unlabeled documents can be used to induce more
accurate classifiers. (We discuss their method in §3.1.2.)
Unlabeled documents are plentiful and easy to collect. Ex-
tending Nigam et al.'s work, we note that plentiful labeled
data is available as well, e.g., from Web topic taxonomies.
The catch is that those taxonomies (say, A) may differ sub-
stantially from the target taxonomy--exactly the situation
we are setting out to address. We wish to evaluate if the ad-
ditional label data can be exploited to improve our accuracy
further.
Evaluation: As with mapping, training documents are half-
labeled but one taxonomy, say A, has significantly more doc-
uments than B and the goal is to improve the B classifier
using A-labeled documents. In contrast, each test document
d has only one label (say from B) and even that is hidden
from the system. The system must guess cB(d). Accuracy
is defined as before. We call this the "zero-label" setting.

3.
DISTRIBUTIONAL CROSS-TRAINING
Distributional classifiers fit a generative model to the train-
ing data, and use this model to predict labels for test cases.
E.g., a naive Bayes (NB) classifier posits that a document
is generated by first fixing a label by invoking a (typically
multinomial) prior distribution on labels, and then creating
the document by invoking a term (feature) distribution con-
ditioned on the label just chosen. In the next subsection, we
discuss two settings with this generative framework: com-
pletely supervised and partially supervised learning of a sin-
gle label. Then we propose our main algorithms for learning
label-pairs.

3.1 Preliminaries

3.1.1 Naive Bayes (NB)
In NB classification for a single taxonomy with label set C,


Pr(c|d) =
Pr(c,d)
Pr(d)
=
Pr(c)Pr(d|c)
Pr(d)
(1)

 Pr(c)Pr(d|c)
 c
td
n
(d,t)
c,t
,

where c  C is the label, d is the test document, t occurs
n(d,t) times in d, c is the fraction of documents tagged c
(also called the prior probability of c), and c,t are multino-
mial probability parameters [13, 1], estimated from training
documents as

c,t =
+
dDc
n(d,t)

T
(
+
dDc
n(d,)
),
(2)


where T is the vocabulary or feature set, Dc is the set of
training documents marked with label c, and 0 <   1 is
the Lidstone's smoothing parameter [17] ( = 1 corresponds
to the well-known Laplace's smoothing). Having estimated
model parameters from training data, the goal is to find the
best class arg maxc Pr(c)Pr(d|c) for test documents.

3.1.2 Expectation maximization (EM1D)



178

The NB classifier needs each training document to be
marked with one label. Can we make use of additional docu-
ments with no label information (such as the test documents
themselves) or partial label information (e.g., that a docu-
ment was generated from one among a restricted subset of
labels)?
A classic approach to estimating distributions over miss-
ing values is Expectation Maximization (EM) [5]. Nigam
et al. [15] use EM to induce a document classifier starting
from a few labeled and many unlabeled documents (Fig-
ure 1). Because this algorithm is designed for only one label
set, we will call it EM1D.


1: Use labeled documents to induce a naive Bayes
classifier with parameters 
2: while model  has not stabilized to satisfaction do
3:
set up new model parameters 
4:
collect contributions from labeled documents to 
5:
for each unlabeled document d do
6:
E-step: calculate the class probabilities
Pr(c|d,) based on current parameters
7:
M-step: if term t occurs n(d,t) times in d, let d
"contribute" a fractional term count of
Pr(c|d)n(d,t) to the next estimate c,t
8:
end for
9:
Re-estimate new cluster model parameters 
10:
  
11: end while


Figure 1: Using standard EM ("EM1D") for semi-
supervised learning of document labels.


3.2 EM2D: Cross-trained naive Bayes
We extend Nigam et al.'s EM algorithm to EM2D by cre-
ating a 2d grid of class labels taken from the product set
C = A × B. We assume a standard mixture model [5] for
document generation. First the label pair (cA,cB) is picked
with probability Pr(cA,cB), and then a conditional term dis-
tribution Pr(d|cA,cB) is sampled to generate the document.
Thus,

Pr(d) = Pr(cA,cB)Pr(d|cA,cB).
(3)

We assume the term distribution to be multinomial, extend-
ing parameters c,t to cA,cB,t. Likewise, parameters cA,cB
express the prior probability of a document being generated
from label-pair (cA,cB).
Thus, each document belongs to exactly one cell of this
grid1. However, in the mapping scenario (§2.1) each training
document comes with exactly one label, which determines
either the row or the column where the training document
belongs, but not both. Thus, each document d identifies
a subset Cd  C to which it potentially belongs, and for
  Cd, we are given that Pr(|d) = 0. We force this con-
straint in the E-step shown in Figure 1, limiting the con-
tributions from a training document to its correct row or
column, and scaling the E-variables to add up to 1 over the
row or column.

3.2.1 Initialization
The EM algorithm [5] guarantees only a locally optimum
solution to the E and M variables. It is important to start
the iterations from a reasonably good initial estimate of .

1
It is possible that a document belongs to more than one class in
a single taxonomy; handling such cases is left to future work.
DA­DB
DB­DA
DADB


70/30 split

#features
Lidstone
70/30 split

#features
Lidstone




Rainbow
classifier
for A
Rainbow
classifier
for B
Filter



Filter
Filter
train
train
test




DB­DA
B known
A guessed
DA­DB
A known
B guessed
Sample
Tune-set




Evaluate
R
em
aining




EM2D
Init

EM2D
Iterate


Final
model



Figure 2: Design and evaluation of EM2D.

In EM2D, we have two resources at our disposal to achieve
good initialization.
The first option is to train two naive Bayes classifiers on
DA - DB and DB - DA (see §2.1 for their definition), and
derive guessed B-labels for DA - DB and guessed A-labels
for DB - DA. The basic naive Bayes classifiers also help
us (via random 70%/30% train/validate splits) to choose an
initial number of features (in decreasing order of information
gain) and an initial value of the Lidstone parameter  in
equation (2). These initial steps are shown near the top of
Figure 2.
The second option is to use the tuneset of fully-labeled
documents to seed the initial  distribution. However, the
tuneset is generally rather small. Using only the tuneset
would generally fail to populate all the cells of the label grid
adequately. It is probably best to use both options in the
rare case that fully-labeled data is available.

3.2.2 Update rules
Suppose a training document d has  = cA(d) known but
cB(d) unknown. Then
cB
Pr(,cB|d,) = 1. Using the
standard multinomial model in equation (1), we can write


Pr(,cB|d,) =
,cB
td
n
(d,t)
,cB,t


,
td
n
(d,t)
,,t
.
(4)


This completes the specification of the E-step, although
some care is required to preserve numerical precision. For
the M-step, we set


, =
1
|D|
d:cA(d)=
Pr(,|d,)
+
d:cB(d)=
Pr(,|d,)
, (5)


which is simply the expected fraction of documents occupy-
ing label cell (,). Likewise, we set



,,t =
 +
d:cA(d)=
n(d,t)Pr(,|d,)
+
d:cB(d)=
n(d,t)Pr(,|d,)



 +
d:cA(d)=
n(d,)Pr(,|d,)
+
d:cB(d)=
n(d,)Pr(,|d,)
(6)



179

This expression closely resembles equation (2), except that
again, contributions to term counts are weighted by the
probability of each document occupying label cell (,), like
in step 7 of Figure 1.
Damping: In the EM2D setup, different documents have
different quality and extent of label information. Tuneset
documents plug into exactly one known (,) slot and pre-
sumably have the most reliable label information. Training
documents have one label pinned by human input, which is
assumed to be reliable, but the other label is not as reliable.
In the zero-label setting (§2.2), test documents have neither
label known, but may still help the classifier gain accuracy
by participating in the EM iterations "completely floating"
over the label grid.
In the update equations above, we have given one vote
to each document. However, it is common [15] to use a
damping factor L  1 to scale down the contribution of
documents whose labels we consider less reliable. It is as
though a fully-labeled document is worth one vote, but a
singly labeled document is worth only L = 0.5, say. Thus,
L can be thought of as an instance scaling mechanism like
in boosting. It does not invalidate the theory of EM in any
way. The best value of L can be set by cross-validation.
Early stopping: The NB generative model is a very crude
approximation to reality. Therefore, maximizing data like-
lihood using EM may not improve classification accuracy in
all cases. It is common to use a tuneset to stop EM itera-
tions in case classification accuracy over (cross-) validation
data is found to drop [15].

3.2.3 Deployment
The half-label setting is simple. Given a test document
d with cA =  known, we simply find Pr(,cB|d) for all
candidates cB, and report the best. The zero-label setting
gives us at least two distinct options: EM2D with guesses
and EM2D with model aggregation.
EM2D with guesses (EM2D-G): To classify to target tax-
onomy B, we first apply an A-classifier to the test document.
The guessed A-label now lets us deal with the zero-label test
instance as if it were a mapping problem. Obviously, we
should use the best possible A-classifier.
EM2D model aggregation (EM2D-D): After EM2D iter-
ations are over, we use the final values of the E-variables to
prepare a new classifier for target taxonomy B. More specif-
ically, each training document d  DA - DB has associated
E-values Pr(cB|d,cA). Just like in EM, we let d "contribute"
its term counts in proportion to this probability to label cB.
Documents in DB contribute fully to their respective labels
in B. The resulting "aggregated" classifier for B is used to
classify test instances.

3.3 Stratified EM1D
If EM2D improves upon the accuracy of single-taxonomy
learners, that could be attributed to multiple reasons. Let
B be the target taxonomy in this discussion. The mapping
of A-labeled documents to B may improve simply because
of the extra documents in DA -DB, not because these doc-
uments are A-labeled. Whether this is the case can be easily
determined by calibrating EM2D against EM1D (run with
B as target labels) with the documents in DA -DB thrown
in as unlabeled documents.
Between EM1D and EM2D there are options which let us
use the A-labels, but in ways simpler than EM2D. E.g., we
can set up an EM1D instance for each   A. The B-labeled
documents are shared across all such instances. A-labeled
documents bearing the label  become unlabeled documents
for the instance corresponding to . The pseudo-code is
shown in Figure 3. We call this Stratified-EM.

1: for each label   A do
2:
train a B classifier  using EM1D with DB - DA
as the labeled set and {d  DA - DB|cA(d) = } as
the unlabeled set.
3: end for
4: for each test document labeled (cA, ?) do
5:
use the EM1D model cA to predict cB
6: end for


Figure 3: Stratified EM to exploit A-labels while
classifying for B.

If EM2D beats both EM1D and Stratified-EM, we can
conclude that the mutual "corrections" of term distributions
in EM2D are somehow vital to its higher accuracy.

4.
DISCRIMINATIVE CROSS-TRAINING
The classifiers discussed thus far aim to fit a class-conditional
generative distribution Pr(d|c) (or Pr(d|cA,cB)), and use
Bayes rule to estimate Pr(c|d) (or Pr(cA|d,cB) etc.). In
contrast, discriminative classifiers seek to directly fit a re-
gression function from the document to scores for label(s).
In this section we will discuss cross-training using two dis-
criminative classifiers. The first (new) approach uses Sup-
port Vector Machines (SVMs), which have been reported to
do well for text data [7]. The second (prior) approach [1]
combines distributional and discriminative aspects.

4.1 SVM-based cross-training
Linear SVMs: Suppose we are given a vector representation
of n documents. Each vector has a component for each fea-
ture (in our case, a term) which is proportional to the num-
ber of times the term occurs in the document2. Document
vectors are usually scaled to unit L2 norm. Each document
vector is associated with one of two labels, +1 or -1. The
training data is thus {(di,ci),i = 1,...,n},c  {-1,+1}.
A linear SVM finds a vector w and a scalar constant b
such that for all i, ci(w · di + b)  1, and w is minimized.
This optimization corresponds to fitting the thickest possible
slab between the positive (c = +1) and negative (c = -1)
documents. In case the training samples are not linearly
separable, it is possible to trade off the slab width for the
number of misclassified training instances.
If the data has more than two labels, it is common to cre-
ate an ensemble of yes/no SVMs, one for each label. During
training, a document marked c is a positive example for the
SVM associated with c, and a negative example for all other
SVMs. This is called the "one-vs-rest" ensemble approach.
During testing, for a test document d, each SVM evaluates
its regression function; the SVM corresponding to label c
evaluates wc·d+bc. The label chosen is arg maxc(wc·d+bc)
(other policies can also be used).
Inducing a SVM classifier involves a complex, iterative
numerical optimization. Several implementations of SVM

2
SVMs have been commonly used on the standard TFIDF repre-
sentation. We used both TF and TFIDF representations scaled
to unit norm and found similar results.



180

are publicly available, including Sequential Minimum Opti-
mization (SMO) [16, 7] and SVMlight [8].
Cross-training SVMs: If A-labels are good predictors of B-
labels, one way to enhance a purely text-based SVM learner
for B is to allocate, over and above a column for each token
in the training vocabulary, |A| extra columns, one for each
label in A. A document d  DB - DA is submitted to a
text-based SVM ensemble for A, called S(A,0), which gives
it a score wcA · d + bcA for each class cA  A.
These scores can be inserted into the |A| new columns,
either as-is, or after some simple transformation, such as
taking the sign of the score, or converting the largest score
to +1 and the rest to 0 or -1 (we use the latter option in
our experiments), and scaling ordinary term attributes by a
factor of f(0  f  1) and scaling these label attributes by a
factor of 1 - f. Document vectors are always scaled to unit
L2 norm.
The parameter f, which can be chosen through cross-
validation on a tuneset, decides the relative importance of
label and term attributes. We evaluated f from 0 to 1 in
steps of 0.05 and set f = 0.95. These cross-trained SVMs
are denoted by SVM-CT.




S(A,0)




S(B,1)
DA­DB

Train
DB­DA
Test




 
t
¡
 
cA
¡
cB




 
t
¡
 
cB
¡
cA




S(A,2)
DADB




E
valuate




Figure 4: Cross-training SVMs.

Documents in DB -DA thus get a new vector representa-
tion with |T|+|A| columns where |T| is the number of term
features. They also have a supervised B label. These are
now used to train a new SVM ensemble S(B,1). The docu-
ment tables and how they are used to train and test SVMs
are shown in Figure 4. We can obviously repeat the process
iteratively in a ping-pong manner, each classifier providing
synthetic columns for the other. The complete pseudo-code
is shown in Figure 5.
Our experiments show that SVM-CT does outperform
SVM, making effective use of the label attributes (although
there is little improvement beyond the first ping-pong round).
SVM-CT is also better than the distributional cross-training
methods in about half the cases. SVM (which uses text
alone), in turn, is much better than the baseline NB clas-
sifier. Moreover, inspecting the components of w along the
label dimensions derived by SVM-CT gives us some inter-
esting insights into various kinds of mappings between the
label sets A and B. We will return to these observations
in §5.
1: Represent each document as a vector d in term space
and d = 1
2: Build one-vs-rest SVM classifiers S(A, 0) and S(B, 0)
for DA - DB and DB - DA using text tokens only
3: for i = 1, 2,... do
4:
for each document d  DB - DA do
5:
Apply S(A,i - 1) to d, getting a vector A(d) of
|A| scores (see text)
6:
Concatenate vectors d and A(d) into a single
training vector with label cB(d), with relative
term-label weight determined by f and
maintaining d = 1
7:
Add this vector into the training set for a
one-vs-rest SVM classifier S(B,i)
8:
end for
9:
Similarly, use S(B,i - 1) to get B(d) and induce a
new one-vs-rest SVM classifier S(A,i) for all
d  DA - DB
10: end for


Figure 5: Cross-training SVMs.


4.2 The A&S mapping algorithm
Agrawal and Srikant (A&S) [1] proposed a hybrid distri-
butional/discriminative classification algorithm by enhanc-
ing the prior estimation of NB (equation 1). Let the target
label-set be C and the source label set be S (to be consis-
tent with their notation). In the mapping setting, classi-
fying document d entails finding arg maxc Pr(c|d,s), where
the source label s  S is supplied and the target label c is
sought.
Given d and s are fixed, arg maxc Pr(c|d,s) is equal to
arg maxc Pr(c|s)Pr(d|c,s), which A&S approximate as Pr(c|s)
Pr(d|c), using the conditional independence assumption shown
underlined (which is theoretically debatable, but seems to
work in practice). All that remains is to propose parametric
forms for Pr(c|s) and Pr(d|c). Pr(d|c) is modeled exactly as
in equation (1), i.e., Pr(d|c) 
td
n
(d,t)
c,t
. All c,t are pre-
estimated by a C-trained classifier which has no knowledge
of S-labels. (This is the distributional/generative part.)
The key innovation of A&S is to propose a parametric
form for Pr(c|s) depending on inter-label relations. Let Nc
be the number of C-labeled documents in the training set
for C. As in EM2D, A&S use a C-trained classifier to guess
classes of S-labeled documents; let G(s,c) be the number
of documents with source label s that this classifier assigns
to target label c. The overall score uses a tuning parameter
R  0 and is given by

Pr(c|d,s)  Nc G(s,c)R
td
n(
d,t)
c,t
, or
(7)

log Pr(c|d,s) = constant + log Nc + R log G(s,c) +

td
n(d,t) log c,t.

Note that (once the c,ts are fixed) R is the only tunable
parameter here. R = 0 coincides with standard NB on the
master labels. Taking logs, we see that (like SVM) A&S is
also a linear discriminant learner. A&S use a tuneset to set
the best value of R, which can be chosen in two ways.
Random sampling: A fraction (varying between 10% and
90% in our experiments) of the fully-labeled documents is
sampled to create a tuneset. The remaining documents are
used as the test set. We evaluate a range of choices for R 
{0,1,3,10,30,100,...} against the tuneset. The accuracy is
averaged over dozens of such samples.



181

Active learning: The system repeatedly samples the fully-
labeled documents. For each sample d, it varies R to see
if R makes any difference to the estimated C-label. If it
does, d is placed in the tuneset; otherwise it is put in the
test/calibration set. A&S report that 5­10 actively chosen
samples are adequate to pick a suitable R.

5.
EXPERIMENTS
All our algorithms were coded in a few thousand lines of
simple C++. A&S, EM2D, and variants were run on (over
1GHz) Pentium3 servers with 1­3GB of RAM. The models
fit easily in tens of megabytes of RAM. We scanned the
documents sequentially and did not need to hold document
vectors in memory. The SVM implementation we used did
load document vectors into memory. A&S, EM2D and its
variants generally trained faster than SVM and SVM-CT.

5.1 Data collection and preparation
We collected example URLs from Dmoz and Yahoo!. Their
intersection has 110926 documents, less than 10% of either's
total size. This supports our claim that double-labeled doc-
uments are hard to find. Like A&S [1] we selected five
data sets: Autos, Movies, Outdoors, Photo, and Software.
However, their sub-topics and training examples were not
available to us. Therefore, for each data set, in each of the
two taxonomies, we picked immediate children as labels such
that there were at least 10 URLs in common with a label
of the other taxonomy. We then added in a few additional
labels from each taxonomy. Finally we went back to the orig-
inal Dmoz and Yahoo! sources to collect all URLs within the
chosen label sets; some 70­80% of the fetches succeeded.




Data set
|D
A
-
D
B
|



|A
|
|D
B
-
D
A
|



|B
|
|D
A

D
B
|




Autos
3589
31
3138
24
184
Movies
8003
33
11420
27
1222
Outdoors
8739
26
1540
39
181
Photo
2895
8
438
22
95
Software
9851
51
2383
25
264
Bookmarks
47247
154
365
7
1289

Figure 6: Sizes of various document and label sets in
our collected data. The first five benchmarks Autos
to Software are used primarily for studying the half-
labeled mapping scenario, and Bookmarks is used
for studying the zero-label scenario.

Figure 6 shows various properties of our main data sets.
We felt uncomfortable about the small test sets, but A&S re-
ported intersections of similar small sizes, and we also found
human labeling (based on page text alone) to systematically
reject pages with relatively unreliable text, exactly those
cases where A&S and cross-training are likely to shine.
The Bookmarks data set was created mainly to study zero-
labeled classification. We collected and inspected a dozen or
so bookmark files published on the Web. We found it very
common for bookmark authors to collect URLs into coherent
topics. Usually, these topics had strong correspondence with
one or few topics in Yahoo!/Dmoz. However, the number of
URLs per topic was small (say 3­20), exactly the scenario
we painted at the outset. We derived sample bookmark
topics B from these bookmark collections, and populated
them from Yahoo! (A) URLs, and removed them from DA.

5.2 The naive Bayes baseline
We used naive Bayes (NB) classifier in the Rainbow pack-
age [12]. We created two Rainbow classifiers, one for A labels
using DA - DB, the other for B labels using DB - DA.
Apart from providing a strawman, NB runs are used to
set the Lidstone parameters and the feature sets for A and
B. Consider the classifier for A. We first created a random
70%/30% train-test split of DA-DB. Rainbow ingested the
70% training subset and listed features in decreasing order
of information gain (w.r.t. the labels). In an outer loop,
we chose from A between 0.1 and 1 in steps of 0.1. In an
inner loop, we chose a prefix TA of the feature list of size
10% through 90% in steps of 10% (similarly for B). We
then used the 30% validation data to pick the best values
for A,B,TA,TB. Finally, the NB baseline is obtained by
subjecting the held-out DADB to these optimized Rainbow
classifiers. Figure 7 shows various accuracy statistics.

Data set
|T|
 70/30
DA  DB
Autos
A
10000
0.1
39.3
46.5
B
10000
0.2
59.2
65.6
Movies
A
8385
0.5
44.6
43.0
B
64434
0.2
50.9
41.0
Outdoors
A
2142
0.2
79.8
77.1
B
813
0.5
68.0
78.0
Photo
A
27969
0.5
68.7
40.9
B
325
1.0
49.6
35.5
Software
A
40000
0.1
40.0
47.8
B
17000
0.1
58.4
54.3

Figure 7: Naive Bayes baseline accuracy with op-
timized choices of |T|, the number of features, and
, the smoothing parameter. A is Dmoz and B is
Yahoo!. Percent accuracy is shown for 70/30 cross
validation and the unseen DA  DB test set.

All other distributional cross-training algorithms used these
optimized values of  and T. In particular, EM2D used
TA  TB as the feature set, and the average of A and B.
Feature selection and the choice of  matters a great deal
for most data sets. Given high-dimensional data like text,
feature selection would likely be helpful for any learning
method, but the benefit from tuning  is large mainly be-
cause the naive Bayes model results in terrible estimates of
the joint distribution, and any "fix" to the innumerable s is
likely to help. Whereas the two classifiers can each optimize
TA, TB, A and B in an unconstrained manner, EM2D is
stuck with a single feature set and a single value of , which
puts it at a disadvantage.

5.3 SVM and cross-training
We used SVMlight [8] in one-vs-rest ensemble mode,
with a linear kernel and default settings for all parameters.
Documents were represented as unit vectors (§4.1). f was
set to 0.95 as explained earlier in §4.1.
Figure 8 compares the accuracy of SVM and SVM-CT
with the NB baseline. In most cases, SVM beats NB. This
is consistent with folk wisdom that SVMs generally per-
form better than NB on text classification tasks. More in-
teresting is the observation that SVM-CT sometimes has
higher accuracy than SVM, which shows that it is possible



182

Figure 8: Comparative evaluation of NB, SVM and
SVM-CT (cross-trained SVM).


for SVM-CT to exploit additional information from label-
derived columns.
We made two additional studies of SVM-CT. First, we
checked that the average magnitude of w for ordinary term
features was always lower than the average magnitude of w
for label-derived features. Recall that |wt| is a measure of
how strongly the feature t can influence the decision of the
SVM, i.e., the sensitivity of the SVM to feature t.
Second, we tabulated the B (respectively, A) labels cor-
responding to the highest and lowest w values of various
A (respectively, B) classifiers. We wanted to observe the
mappings learned between the classes in the two taxonomies
using cross-trained SVMs3. During cross-training, the label
information was transformed into a vector of 1 and -1 values
as mentioned in §4.1. In addition, a new dimension called
none-of-the-above (NOTA) was introduced, whose value was
set to 1 when all label scores obtained from B (respectively,
A) were negative and all label dimensions were set to -1.
The purpose of NOTA is explained shortly.
The results are shown in Figure 9. We show some Dmoz
(respectively, Yahoo!) class labels along with the Yahoo!
(respectively, Dmoz) class labels which had the greatest pos-
itive and negative influence in predicting the said Dmoz (re-
spectively, Yahoo!) class. All positive couplings are very
meaningful; some negative couplings are fairly intriguing
too.
The Outdoors dataset for both taxonomies contains the
class ScubaDiving which maps to its namesake in the other
taxonomy with a large positive component along w. Such
one-to-one mappings are symmetric and expected. Even
when there is no direct one-to-one correspondence between
the labels (or there is a containment relationship) such as
between Yahoo.Movies.Genres.SciFi-Fantasy and Dmoz.
Movies.Series.StarWars, SVM-CT seems capable of ex-
tracting that information. On the other hand when the
Dmoz.Software.Accounting class really has no relevant class
in the Yahoo! taxonomy, the synthetic NOTA class indicates
this by the high value of |wNOTA|.
The superclass Dmoz.Photo.Techniques&Styles duly maps
to finer categories in Yahoo.Photo, such as Pinhole Photog-
raphy, 3D Photography, and Panoramic Photography. The
Dmoz to Yahoo! mapping gives high positive weights to
most of these child classes as seen in Figure 9. Such instruc-

3
See http://www.cse.iitb.ac.in/~soumen/doc/kdd2003/svmct.
html for examples
Dataset
Dmoz.
Maps to Yahoo.
Weight
Autos
News&Magazines
News&Media
0.147
Volkswagen
-0.156
Movies
Genres/Western
Titles/Western
0.242
Titles/Horror
-0.052
Outdoors
Scuba Diving
Scuba
5.878
Snowmobiling
-0.647
Photo
Techs&Styles
Pinhole Ph'graphy
2.796
3D
0.964
Panoramic
0.921
Organizations
-1.184
Software
Accounting
NOTA
0.156
Screen Savers
0.103
OS/Unix
-0.171
Dataset
Yahoo.
Maps to Dmoz.
Weight
Autos
Corvette
Chevrolet
0.981
Parts&Accessories
-0.266
Movies
SciFi&Fantasy
Series/Star-Wars
1.123
Reviews
-0.824
Outdoors
Scuba
Scuba Diving
4.822
Wildlife
-0.437
Photo
Pinhole Ph'graphy
Techs&Styles
0.4842
Photographers
-0.270
Software
OS/MSWindows
OS/MSWindows
0.018
NOTA
-0.001
OS/Unix
-0.008

Figure 9: Dmoz and Yahoo topic mappings learned
with cross-trained SVMs


tive parent-child, or one-to-many mappings emerge in spite
of our assumption of flat taxonomies.
Another interesting mapping is from Yahoo.Software.OS.
MSWindows to multiple high positive weights to classes in
the Dmoz taxonomy. Here, the NOTA class can be inter-
preted as clearly separating all the Windows related classes
above it from the Unix related classes below it within Dmoz.
Software.

5.4 EM2D for mapping
Figure 10 shows the accuracy of EM2D in comparison with
NB. EM2D is significantly better than NB with a maximum
gap of 30% for the Movies dataset and average gap of 10%.
This is reassuring, but in this section we wish to analyze
carefully why this is the case.




Figure 10: EM2D vis-a-vis Stratified-EM1D, EM1D,
and NB. For EM1D we used its best damping pa-
rameter, L = 0.01.


There are two potential sources of information from DA-
DB which may improve the accuracy of classifying into B
given d and cA. The first is simply the addition of a bunch
of documents, even if they are not labeled with B-labels
and even if we ignore their A-labels. If this were the only
source of extra information, EM1D should be able to match
EM2D, which is clearly not the case. Therefore, knowledge
of A-labels of specific documents is vital.



183

As we discussed in §3.3, A-labels can be used by Stratified-
EM, which simply creates one instance of EM1D for each
distinct label   A. Figure 10 also shows that with only two
minor exceptions, EM2D beats both EM1D and Stratified-
EM. This despite the fact that each EM1D has denser data,
lowering the variance of the parameter estimates compared
to EM2D. These measurements help us establish that

· There is information available in cross-training which
EM1D cannot exploit, and
· Stratified-EM, a relatively straight-forward extension
to EM1D, does not work as well as EM2D.

5.5 Sensitivity to initial guesses of labels
EM2D, like EM1D, finds locally optimum values of the
total data likelihood. Hence the final accuracy is sensitive
to the initial assignment of half-labeled data in the 2D label
grid.
Given the baseline classifiers trained on A (using docu-
ments in DA - DB) and B (using documents in DB - DA),
it is natural to initialize EM2D by submitting documents in
DA - DB to the B-classifier and vice versa. We may use
a "hard" assignment to the best guess, or a "soft" or frac-
tional assignment based on the probabilities emitted by the
baseline classifiers.
However, these are not the only options. How will EM2D
behave if each document in DA - DB is assigned uniformly
over each label in B? In general, how sensitive is EM2D to
perturbations and errors in the initial E-estimates?
To test EM2D's resilience, we randomly picked a fraction q
of documents (with A-labels, say) and replaced their guessed
scores for B-labels with a uniform distribution smeared over
all B-labels. The remaining fraction 1 - q of documents
are added to the EM2D system as before. Thus, q = 1
corresponds to full uniform assignment.
Obviously, the effect of smearing a fraction q depends on
the accuracy of the guesses in the first place. Therefore we
repeat the smearing experiments for varying level of start-
ing guess accuracy. (We fake different guess accuracies by
random flips in guesses. Note that these "flips" are distinct
from the "smear.")




Figure 11: The effect on EM2D of smearing the ini-
tial guesses of a fraction of half-labeled training doc-
uments. The y-axis shows final EM2D accuracy.

In Figure 11 we show the change in accuracy with in-
creasing fraction of smeared guesses on the Movie dataset,
with two different settings of guess accuracy: the default as
shown in Figure 7 and a second setting where 70% of the
guesses have been pre-flipped to a random label (this results
in guesses of very poor quality). These plots show that
· When the guesses are reasonably accurate, uniform as-
signment is worse than assignment based on guessed
probabilities, which makes eminent sense.
· EM2D can handle limited (q = 0.15 to q = 0.20 for
this data) smearing, beyond which accuracy starts to
drop.
· When the accuracy of guesses is too poor, smearing
a fraction of the guesses (q = 0.10 in this case) can
improve accuracy. This was somewhat unexpected,
and made sense only in hindsight.

5.6 Highly asymmetric scenarios
All the data sets we collected have relatively balanced sizes
of DA-DB and DB-DA. How well can EM2D do in highly
unbalanced settings, especially w.r.t. the sparsely-populated
taxonomy?
To answer this question, we (arbitrarily) picked B as the
taxonomy to be decimated, and sampled DB - DA down to
300 documents. (Actually, we decimated to 200, 300, and
5% of the original. Results were similar.) DA -DB was left
unchanged.
The small size of DB - DA led to a poor baseline B-
classifier. Therefore, the guessed B-labels for the documents
in DA-DB had a large error rate. Because information flow
is bidirectional in EM2D, poor B guesses reduced overall
accuracy. We propose three fixes for this problem:

· Taking our cue from Figure 11, we smear documents
in DA -DB over all B-labels. Documents in DB -DA
continue to use the A guesses.
· Like A&S, we use a tuneset sampled from DA  DB.
Specifically, we sampled 5% of fully-labeled documents.
· We set the damping factor L (§3.2.2) so as to restore
the relative weights of DA - DB and DB - DA to the
same ratio as in the original dataset. This would mean
L  0.05 on documents in DA - DB.




Figure 12: EM2D on a small sample of 300 docu-
ments from DB - DA.

Figure 12 shows that the accuracy gain of EM2D over
NB in highly asymmetric settings can in fact be higher (av-
erage 11.4%) than in more balanced data (average 10% in
Figure 10), provided EM2D is initialized properly. Nigam
et al.'s experience [15] seems to corroborate that the gains
from semi-supervised learning are larger when labeled data
is limited.

5.7 Comparison with the A&S algorithm
For our A&S implementation, we fixed the feature set to
TATB as found by Rainbow, and also fixed the  parameter



184

to one that gave the best accuracy for Rainbow for each of
A and B prediction.
Making a fair comparison between A&S and EM2D in-
volves exposing to EM2D at least the fully-labeled tuneset
that A&S uses. In fact, it is very difficult to compare the
active-learning version of A&S with EM2D in a principled
way, because A&S inspects fully-labeled documents (not the
labels, but the text) outside the tuneset as well. (EM2D is
not designed for active learning.) Therefore, we focused on
the randomly sampled tuneset paradigm only, because that
could be used with both A&S and EM2D.
In addition, given the large skew between half-labeled and
fully-labeled populations, we used damping to re-scale them
to the same effective size (see §3.2.2 for more details).




Figure 13: Accuracy of the A&S algorithm com-
pared with EM2D for 10% and 90% tuneset (T) and
A&S active learning (AL).

In Figure 13 we present the accuracy of A&S with 10%
and 90% randomly sampled tuneset as well as a tuneset of
size 10 picked by active learning (AL) from the entire test
set of fully-labeled documents. Broadly, A&S and EM2D
are comparable, but EM2D edges over A&S by a maximum
of 20% and an average of 4% for the 10% tuneset and 2%
for the 90% tuneset. When EM2D loses to A&S, the gap is
very small.

5.8 EM2Dforclassifyingzero-labeldocuments
In this scenario, we are required to finally produce a classi-
fier for B which does not depend on the test instances being
labeled with A labels. In §3 we discussed two methods for
deploying EM2D in this setting: EM2D-D, a model aggre-
gation method, and EM2D-G which is essentially EM2D,
except that the A-labels are supplied as guesses from an
A-classifier.
Figure 14 shows the accuracy for EM2D-D, EM2D-G,
EM1D and NB, for various sizes of labeled training sets,
and two choices of the damping factor L discussed at the end
of §3. These numbers are for the Bookmark data set. The
accuracy values were averaged over three random choices of
the training set for each choice of training set size.
As the fraction of training data is increased, the benefit
of semi-supervised learning reduces, which is obvious. The
damping factor does essential damage control when there are
many labeled documents, but can hurt when the labeled set
is very small. These observations corroborate with earlier
EM1D results by Nigam et al. [15].
Unlike Nigam et al., EM1D could improve beyond NB
only for the smallest training sets in our case. One possi-
Figure 14: EM2D with guessing is the best methods
for classifying zero-label documents. NB accuracy
is shown only once for each size of the training set,
because it does not change with L.


ble reason is that the unlabeled Yahoo! dataset, from which
EM1D adds instances, is significantly different, and has many
more irrelevant classes, compared to the initial labeled data
in our Bookmark dataset. Nigam et al.'s experiments drew
unlabeled and labeled documents from the same distribu-
tion.
Finally, we were surprised to see that EM2D-G performed
better than EM2D-D. Recall that EM2D-D is really a 1d
classifier, which should reduce data sparsity and improve the
reliability of its parameter estimates compared to EM2D.
Despite this benefit, model aggregation appears to hurt.
Even a noisy guess at the A-label, followed by a row-conditioned
classification, outperforms the aggregated model.


6.
RELATED WORK
In recent years, EM-like semi-supervised learning has been
enhanced in several ways and applied to a number of set-
tings.
Extending beyond EM1D, Liu et al. [10] and Yu et al. [20]
consider the realistic situation where, apart from labeling
only a few samples, the user is also unlikely to spend the
effort to mark negative samples. Their EM-like algorithms
can work on a set of positive examples P and a mixed pool
of samples M which may contain both positive and negative
instances.
Cross-training is related to multi-task learning or life-long
learning, in which information (features, models, etc.) from
one learning task is used for another. Thrun [18] discusses
how to cluster learning tasks (not instances) and pick, for a
given task, those other tasks that are likely to be related to
the current one. Information from those tasks are then used
to influence the distance function in a nearest-neighbor clas-
sifier. Caruana [4] discusses how to use multi-task learning
in neural networks, and Baxter [2] provides a PAC analysis.
Cross-training is a two-task setting with no instance sub-
mitted to more than one task. The similarity between tasks
falls out naturally as we estimate ,.
A recent approach to semi-supervised learning (which might
appear superficially similar to cross-training) is co-training,
proposed by Blum and Mitchell [3]. In co-training, too,
there are two learners, but, unlike cross-training, the learn-
ers have to use disjoint subsets of attributes, and assign la-
bels from only one taxonomy. Each learner picks unlabeled
training instances that it is most confident about classify-
ing correctly, and makes it a labeled training instance for
the other learner. Co-training and cross-training are quite



185

different things: two label sets are central to our formula-
tion, and our approach depends on modeling a single term
distribution conditioned on a pair of labels.
Doan et al. [6] study a related problem of identifying map-
ping between labels of two taxonomies (called ontologies in
the paper). Their goal is to find for each label in one tax-
onomy, the label most similar to it in the other taxonomy.
In contrast, our goal is to assist classification in one cata-
log without necessarily committing on a specific mapping
relation with another catalog.

7.
DISCUSSION AND FUTURE WORK
We have presented cross-training, a new technique for
using sample documents from one taxonomy to improve
classification tasks for another taxonomy. We have pre-
sented two algorithms for cross-training: a probabilistic al-
gorithm based on EM and a discriminative algorithm based
on SVMs.
Extensive experiments with real-life Web data show that
our approach definitely beats baseline classifiers in each tax-
onomy, which is not very surprising. More reassuring are
the observations that show our approach to compare favor-
ably with the best existing approach, while providing a more
sound foundation.
In principle, a sufficiently powerful supervised learner that
can handle discrete categorical attributes can be used di-
rectly for cross-training. In practice, specifically for text
data, the large number of dimensions and the heterogeneity
across term and label attributes pose challenges. Luckily, in
cross-training, the label attribute can take only a few values.
Therefore (in decision tree terms) we can first stratify the
data on the label attribute and then build distributions for
each label value.
Our work suggests several natural research directions. Might
it be possible to improve EM2D using a better generative
model whose E-scores are not as close to 0/1 as NB? Might
tempering or annealing let EM2D reach better local optima?
Are we estimating the number of clusters in EM properly?
The "correct" number of EM clusters may be as high as
|A||B| (if the taxonomies are truly "orthogonal"), but is
generally much smaller (perhaps even smaller than |A| and
|B| for poorly separated labels). Can SVM-CT be improved
further by designing better kernels? Is it an accident that
neither of EM2D and SVM-CT dominates the other in ac-
curacy? If not, can we predict which is likely to do better
for a given problem? Can we design cross-trained classi-
fiers which combine the strengths of the distributional and
discriminative approaches? Finally, it would be useful to ex-
tend the algorithm for real taxonomies (as against flat sets
of classes).
Acknowledgments: The research was supported in part by
IBM Corporation, Tata Consultancy Services, and the Min-
istry of Information Technology. We thank Kinshuk Jerath
for collecting part of the data and for writing a prelimi-
nary version of the A&S algorithm; Ganesh Ramakrishnan,
Deepa Paranjpe, and Roger Menezes for helping us clean
the data; and an anonymous reviewer for pointing out ap-
proaches for learning multiple tasks.

8.
REFERENCES
[1] R. Agrawal and R. Srikant. On integrating catalogs. In
World Wide Web, pages 603­612, 2001.
[2] J. Baxter. A model of inductive bias learning. Journal of
Artificial Intelligence Research, 12:149­198, 2000.
http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/
volume12/baxter00a.pdf.
[3] A. Blum and T. M. Mitchell. Combining labeled and
unlabeled data with co-training. In Computational
Learning Theory, pages 92­100, 1998.
[4] R. Caruana. Multitask learning. Machine Learning,
28:41­75, 1997. http://www.cs.cmu.edu/afs/cs.cmu.edu/
user/caruana/pub/papers/mlj97.ps.
[5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum
likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, B(39):1­38, 1977.
[6] A. Doan, J. Madhavan, P. Domingos, and A. Halevy.
Learning to map between ontologies on the semantic Web.
In WWW, pages 662­673, Honolulu, Hawaii, May 2002.
http://www2002.org/CDROM/refereed/232/.
[7] S. Dumais, J. Platt, D. Heckerman, and M. Sahami.
Inductive learning algorithms and representations for text
categorization. In 7th Conference on Information and
Knowledge Management, 1998. Online at http:
//www.research.microsoft.com/~jplatt/cikm98.pdf.
[8] T. Joachims. Making large-scale SVM learning practical. In
B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods: Support Vector Learning. MIT Press,
1999. See http://www-ai.cs.uni-dortmund.de/DOKUMENTE/
joachims_99a.pdf.
[9] W.-S. Li, Q. Vu, D. Agrawal, Y. Hara, and H. Takano.
PowerBookmarks: A system for personalizable Web
information organization, sharing and management.
Computer Networks, 31, May 1999. http:
//www8.org/w8-papers/3b-web-doc/power/power.pdf.
[10] B. Liu, W.-S. Lee, P. S. Yu, and X. Li. Partially supervised
classification of text documents. In ICML, volume 19,
pages 387­394, Sydney, Australia, July 2002.
[11] Y. S. Maarek and I. Z. Ben Shaul. Automatically
organizing bookmarks per content. In Fifth International
World-Wide Web Conference, Paris, May 1996.
[12] A. McCallum. Bow: A toolkit for statistical language
modeling, text retrieval, classification and clustering.
Software available from
http://www.cs.cmu.edu/~mccallum/bow/, 1998.
[13] A. McCallum and K. Nigam. A comparison of event models
for naive Bayes text classification. In AAAI/ICML-98
Workshop on Learning for Text Categorization, pages
41­48. AAAI Press, 1998. Online at
http://www.cs.cmu.edu/~knigam/.
[14] K. Nigam, J. Lafferty, and A. McCallum. Using maximum
entropy for text classification. In IJCAI-99 Workshop on
Machine Learning for Information Filtering, pages 61­67,
1999. See http://www.cs.cmu.edu/~knigam/ and
http://www.cs.cmu.edu/~mccallum/papers/
maxent-ijcaiws99.ps.gz.
[15] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text
classification from labeled and unlabeled documents using
EM. Machine Learning, 39(2/3):103­134, 2000. See
http://www-2.cs.cmu.edu/~mccallum/papers/
emcat-mlj2000.ps.gz.
[16] J. Platt. Sequential minimal optimization: A fast algorithm
for training support vector machines. Technical Report
MSR-TR-98-14, Microsoft Research, 1998. Online at http:
//www.research.microsoft.com/users/jplatt/smoTR.pdf.
[17] E. S. Ristad. A natural law of succession. Research report
CS-TR-495-95, Princeton University, July 1995.
[18] S. Thrun and J. O'Sullivan. Discovering structure in
multiple learning tasks: The TC algorithm. In L. Saitta,
editor, Proceedings of the 13th International Conference on
Machine Learning ICML-96, San Mateo, CA, 1996. Morgen
Kaufmann.
[19] Y. Yang and X. Liu. A re-examination of text
categorization methods. In Annual International
Conference on Research and Development in Information
Retrieval, pages 42­49. ACM, 1999. Available from
http://www-2.cs.cmu.edu/~yiming/publications.html.
[20] H. Yu, J. Han, and K. C. Chang. PEBL: Positive example
based learning for Web page classification using SVM. In
KDD, volume 8, Edmonton, Canada, July 2002.
http://www-faculty.cs.uiuc.edu/~kcchang/Papers/
pebl-kdd02.pdf.




186

