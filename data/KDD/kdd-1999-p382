Using Approximations
to Scale Exploratory
Data Analysis in Datacubes


Daniel Barbara'
Xintao
Wu
George Mason University
*
Information
and Software Engineering Department
Fairfax, VA 22303
(dbarbara,xwu)C!gmu.edu



Abstract
Exploratory
Data Analysis
is a widely
used technique
to
determine
which factors have the most influence
on data
values in a multi-way
table, or which cells in the table can
be considered anomalous with respect to the other cells. In
particular,
median polish is a simple,
yet robust method
to perform
Exploratory
Data Analysis.
Median
polish is
resistant
to holes in the table (cells that have no values),
but it may require
a lot of iterations
through
the data.
This factor
makes it difficult
to apply
median
polish
to
large multidimensional
tables, since the I/O requirements
may be prohibitive.
This paper describes a technique
that
uses median polish over an approximation
of a datacube,
easing the burden of I/O. The results obtained are tested for
quality, using a variety of measures. The technique scales to
large datacubes and proves to give a good approximation
of
the results that would have been obtained by median polish
in the original data.


1
Introduction

Exploratory
Data Analysis (EDA) is a technique [7, 81
that uncovers structure
in data.
EDA is performed
without
any a-priori
hypothesis
in mind:
rather
it
searches for "exceptions"
of the data values relative
to what those values would have been if anticipated
by an statistical
model.
This statistical
model fits
the rest of the data values rather
well and can be
accepted as a description of the dataset. When applied
to multidimensional
tables,
EDA also uncovers the
attribute
values that have the greatest effect on the data
points.
As pointed out by Ill],
these exceptions can
be used by an analyst as starting points in the search
for anomalies, guiding the analyst work across a search

This work has been supported
by NSF grant IIS-


Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the fuil citation on the tirst page. TO copy
otherwise, to republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
KDD-99
San Diego CA USA
Copyright ACM 1999 I-581 13-143-7/99/08...$5.00
space that can be a very large.
A widely used data model for On-Line
Analytical
Processing (OLAP)
is the datacube [6]. A datacube is
a multidimensional
data abstraction,
where aggregated
measures of the combinations
of dimension values are
kept.
At any level of aggregation,
datacubes can be
viewed as multi-ulay
tables,
that can be subjected to
EDA.
Two traditional
ways of performing
EDA
in
tables are median polish and mean polish [8].
Both
methods try to fit a model (additive
or multiplicative)
by operating on the data table, finding and subtracting
medians (means) along each dimension
of the table.
Starting with one dimension, the method calculates the
median (mean) of each "row" i and subtracts this value
from every observation in the row. Then, the method
moves to the next dimension and uses the table resulting
from the previous operation to find medians (means) in
each row and subtract them from the entries in this row.
Of course, doing that changes the medians (means) on
the previous dimension rows.
The process continues
with each dimension and iteratively
cycles through the
set of dimensions.
In principle,
the process continues
until all the rows in each dimension have zero median
(mean), hence the number median (mean) polish.
(In
practice, the process stops when it gets sufficiently
close
to that goal.) One gets two kinds of information
from
this process. First, one gets the eflect that each row in
each dimension has on the model, given by the algebraic
sum of the medians (means) that have been subtracted
in that row at every step. Secondly, one gets residuals
in each cell of the table, which tell us how far apart that
particular
cell is from the value that would have been
predicted by the model being fit.
It has been pointed out that the main drawback of the
mean polish method is its lack of resistance to outliers
(i.e., cells that do not fit the model well).
This lack
of resistance manifests itself specially
when "holes,"
e.g., missing cells, are present [8]. This is particularly
troublesome
for datacubes,
which are usually
sparse

lwe
use the term
Yaw"
to refer to the set of cells in the
hypercube
which
share. the same attribute
value for one of the
dimensions




382

(i.e.,
not every cell has a value).
On the other hand,
although
median polish is more resistant to outliers
and holes, its performance
is very adversely affected
by holes which increase substantially
the number of
iterations
needed by the process [S]. Increasing
the
number of iterations
can drastically
impose enormous
I/O demands (by requesting many passes over a large
dataset) and therefore render the process impractical
for large datacubes.
Previous work in using EDA for
datacubes [ll] has chosen to use mean polish, precisely
for being less demanding on the number of iterations
needed to finish.
This paper explores a method that benefits from the
high robustness of median polish, while at the same
time scales well for large datacubes.
Our method
uses statistical
models to approximate
subsets of the
datacube,
eliminating
the need to do several passes
over the dataset in order to compute the medians.
However, by doing so, the row effects and the residuals
computed
are only an approximation
of those that
would have been computed by applying median polish
to the dataset.
We show that this tradeoff of lesser
I/O
demands versus accuracy in the results is very
beneficial.
This paper is organized as follows. In Section 2, we
present the basis of our method. Section 3 presents the
experimental
results obtained by our method using real
and synthetic datasets. Finally, Section 4 presents the
conclusions and directions of future work.

2
Our method
As we pointed out before, median polish takes every
row in the table and computes its median, substracting
it from every cell's value to make the row's median
equal to zero. It is easy to see that this method cannot
scale well.
During each iteration,
we need full access
to the entire dataset.
If the dataset does not fit in
main memory, this implies severe I/O demands: a row
of the table would be brought to memory only to be
replaced by other portions
of the dataset later, and
brought
back in the next iteration.
In this section
we describe in detail how we scale the median polish
procedure to large datacubes. The aim of our method
is to avoid having to bring data cells to main memory
repeatedly in order to compute the medians.
We do
that by characterizing
portions
of the core cuboid of
the datacube 2, employing statistical models that can be
later used to estimate the values of the individual
cells.
To avoid incurring
large errors by using the estimated
cell values, we retain all the cell values whose estimated
values are too erroneous (farther
away from the real
value by more than a preestablished
threshold).
We

%he core cuboid
is the set of cells of finest granularity;
any
other
cuboid
in the datacube
can be computed
from the core
cuboid by aggregating
on one or more of the dimensions.
then keep the model parameters (for each portion of the
core cuboid) in main memory, along with the retained
set of cells (or at least as much of these values as we can
fit in memory) and start the median polish procedure.
In each iteration, the procedure will use, for a given cell,
one of the following values: a) the estimated value given
by the corresponding
model, if the cell is not retained
or, b) the actual cell value, if the cell has been retained.
Hence, this method minimizes
I/O by not having to
fetch cells from the disk too often (if all the retained
cells fit in main memory, the I/O requirements
after
modeling the portions of the core cuboid drop to zero).
Of course, the usage of the estimated value brings as
a consequence that the results of the median polish
procedure are only approximate.
However, as we shall
show in the results section, the results are extremely
close to performing median polish in the real datacube,
even when large estimation errors are allowed.
It is important
to point out that while our descrip-
tions and experiments have been performed using the
core cuboid, it is straightforward
to use our data struc-
tures and method to perform approximate
EDA in any
other cuboid of the datacube. The models used to de-
scribe portions of the core cuboid, can be used to per-
form aggregations to obtain cells in any other cuboid,
and our method can then operate in these aggregations.
The steps involved in performing
our approximate
method can be summarized as follows. First, we need to
select chunks of the core cuboid that will be described
by models, and decide which cells need to be retained.
Second, we need to organize the model parameters
and retained cells to efficiently
access them during the
median polish run.
And finally,
we need to evaluate
the quality of the results obtained by the approximate
method. In the rest of this section, we describe how we
solved each of these issues.

2.1
Dividing
the core cuboid
In order to select the chunks in which we divide the
core cuboid we use a density based approach called
hierarchical
clustering
[9] to get high density portions
of the cube. We assume that the core cuboid has been
computed (an algorithm
such as the one presented in
[lo] is well suited for the task).
Given a d-dimensional
datacube with
dimensions
A
=
{&,As,...&},
we assume it be a set of
bounded, totally
ordered domains space Accordingly,
s
=
//AlI] x llA2ll x ... x IIAdll,
is the possi-
ble number of cells in the core cuboid.
The non-
empty cells in the cuboid are a set of multidimen-
sional points,
V
=
{l?~,z)^z,~~~,&}.
where each
?yi =
{Vil,Viz,"'
,zlid, mi},
with vij the i-th dimen-
sion value and rni the value of the cell.
Initially,
we partition
the space of the cuboid into
non-overlapping
rectangular
lst-level
chunks
which




383

have the same chunk size {pi,.
' . ,PA}. We use ct to
denote the i-th chunk in the first level. The size of any of
the lst-level chunks is given by di = n ,$ . Clearly, we
require that /J: 5 llAi[l for all i; moreover, we choose
not to divide those dimensions with small domains (for
them we make pi
=
IIAilj).
At any point during
the process of partitioning
we may decide to further
divide a 1-st level chunk into several 2nd level chunks,
and successively an j-th level chunk into j + 1-th level
chunks. The size of an j-th level chunk is predetermined
to be (~1, ... ,pi},
for j
=
1, ... ,MAXLEVEL,
where MAXLEVEL
is the maximum
level of any
chunk.
There are three possible states for a chunk
after completing
the partitioning:
STAT-NULL
for a
chunk with no cells, STAT-SPARSE
for a chunk with
very few cells, and STAT-MODL
for a chunk has been
modeled. There are three parameters used to drive the
partitioning
process:

l
(Y : this marks the minimum
acceptable density for
chunks measured by the number of cells in the chunk
divided by the chunk's size.

.p:
this is the maximum
error level tolerated
in
the estimation
process.
That is if y is the value
of a cell and jj the value of the estimation
by the
model describing the chunk, the relative error given
by ]y - cl/y must be less than or equal to /?. Any
cell whose estimation
error surpasses p is declared
an outlier .

l
y : this is the maximum percentage of outlier cells
allowed in the chunk, which is measured as the
number of outliers divided by the number of cells
in the chunk.

The algorithm
(whose pseudo-code cannot, for rea-
sons of space, be presented here, but can be found in
[3]) proceeds to divide the initial
cuboid in 1-st level
chunks, assigning cells to each one and proceeds to clas-
sify each chunk, further
dividing
it if necessary, until
the chunk is declared STAT-SPARSE,
STAT-NULL
or
STAT-MODL.
It is worth noticing
that the algorithm
will read the number of cells in the core cuboid into
memory only once. Then each chunk that is neither
sparse not null will be read into memory (with its re-
spective cells) and processed (subdivided,
modeled) un-
til no more processing is needed for the chunk. If each
chunk fits in memory, then it is guaranteed that the
each cell will be read into memory only once, making
the input activity
equivalent to two passes of the data
and the write activity
also equivalent to two passes of
the data. If a chunk does not fit in memory several reads
and writes to the chunk will be needed, making the total
read activity
loosely bound by MAXLEVEL
number
of passes through the data. (In practice the I/O activity
will be much less, since there will be child chunks that
fit in memory.) Moreover, we need to point out that all
this I/O activity
takes place before the EDA is under-
taken. (Chunks and their models can be stored as part
of the cuboid and reused for further analysis and other
uses, such as approximate
query processing and other
types of data mining [l, 21.) A point we need to make
in this subsection is that of modeling. The general tech-
nique is quite independent
of the model chosen for the
chunks. Of course, there will be some models that are
better suited for specific classes of data and therefore
will produce smaller estimation
errors. However, as we
will show in Section 3, the results of the approximate
median polish are quite robust, in spite of the errors
incurred by the modeling process. For our prototype,
we chose a simple linear regression model in the number
of dimensions.
The regression was computed using the
marginal values (total sum of measure values per "row"
of each dimension).


2.2
Median
Polish
Algorithm
Using
the
Chunk
Models

Our algorithm to perform median polish (whose pseudo-
code cannot be presented here for space reasons, but
can be found in [3]) proceeds as follows.
First, both
the chunk list and a number of chunk structures
(as
many as the memory buffer allocated for such purpose
allows) are prefetched into main memory. The process
of polishing medians is conducted until a predetermined
number of steps is carried out or more than a predefined
number of rows have median equal to 0. (In practice,
we do not demand that the median be equal to zero, but
rather less than a small value.) In case the chunk used
is of the type STAT-SPARSE,
the algorithm
reads the
real cell values, stored in the chunk structure, otherwise
(if the chunk is of the type STAT-MODL,
the cell values
can be either estimated by using the model parameters,
or read if the cell happens to be an outlier.
As cells
are used or estimated (i.e., taken from the outliers or
modeled),
they are placed in a set r.
Once all the
chunks in the list have been processed, the cells in T are
processed one by one, subtracting
to the cell value all
the median values for all the dimension attribute
values
that define the cell. In this way, the values kept in the
chunks correspond, at every step, to those of the original
cuboid. This makes it unnecessary to write a chunk to
disk, if we need to reclaim the buffer space occupied by
it. After subtracting
the medians, the cell value reflects
the residual at that step of the computation.
With these
values, we compute the median of the row and subtract
that from the corresponding
effect. If the entire set of
values T does not fit in memory, we can perform the
classic divide-and-conquer
approach used to compute
order statistics.
We want to revisit the issue of the chunk description
size here. As we said in Section 2.1, the ability to keep




384

many (or all) the chunks in memory decreases with
the size of the chunks.
This, of course, assumes that
the regions covered by chunks whose state is STAT-
MODL are indeed dense (i.e., contain few holes).
If
this is not the case, the zero (or non-zero 3, cells
need to be indexed, making the description
larger. So
our procedure is more effective for data that is highly
clustered in subspaces along the datacube. Fortunately,
this is the case in many real datasets.
(Commercial
systems exploit
this property:
for instance,
Arbor
Software's Essbase, stores dense and sparse regions of
the core cuboid using different
data structures
[5]).
However,
as the results
will
show, we still
get a
substantial
benefit in performance if the density in the
clusters is low.

2.3
Quality
Measures
Since our procedure is approximate,
we need to define
ways to compare the results
with
those that
are
obtained by applying the median polish method to the
base cuboid. In this subsection we describe some of the
measures we use for that purpose.
(The complete set
can be found in [3].)
Let us define the sets E: and Et: as the sets of the Ic
topmost ranked effects and the k lowest ranked effects
in row i respectively, where k =
I] E+ 11 =
11Ei: 11.
Given the equivalent sets &
and I$:, obtained by the
approximate
median polish, we can define the positive
and negative effect recall (Re+,Re-)
in the manner
shown in Equations 1 and 2.



Ret
=
11E? II J$- 11
2
IIE: II
, for i = 1,2;..,d
(1)



Re; = 11E,' " `%' 11
II`-57II
, for i = 1,2,...,d
(2)

Finally for the residuals we define the set R as the set
of the lc-th largest residuals, with I] R 1 = ,+computed
by the standard median polish and R defined for the
residuals obtained with the approximate
cuboid. Then
we define the Recall (Ret) in Equation 3.

^
Ret =
]I Tl G if" ]I, with ]I R II = II fi II
(3)


3
Results
The experiments were conducted in a SUN Ultra 2, with
two processors, and 500 Mbytes of RAM. We conducted
the experiments in several datasets, but we only report
the results for two of them in this paper.

3the choice of which one we should
index depends on which
ones are fewer.
ICI Re:
Rel
Re,
1 Re,
10 I
1 I
1 I
1 ] 0.60




Figure 1: Positive and negative effect recall values for
dataset l.(lc is the size of the set of largest positive or
largest negative effects.)




Figure 2: Recall values for the residuals of dataset 1 for
two different p values.



3.1
Dataset
1
The first dataset used is taken from the U.S Census
Bureau data [4]. It is a two dimensional
table that
contains the population
for 227 countries from the year
1950 to the year 2049 (future years had been projected
by the Census Bureau).
There are 22700 tuples in it
and no holes.
The experiment
in dataset 1 was conducted
with
(Y =
0.01 ( which is really irrelevant
here, since the
set has no holes) and y = 0.2.
Figure 1 shows the positive and negative effect recall
values for dataset 1. Only one value of p
=
0.1 is
reported in the table.
Figure 2 shows the recall values for dataset 1 (for
two different values of ,0). The results for recall are
again very high across the spectrum of experiments,
showing that most of the largest residual effects of the
standard method will be captured by an equal sized set
of residuals in the approximate
method.

3.2
Dataset
2
This is a synthetic
dataset whose parameters can be
changed to drive a series of experiments.
It is important
to point
out that
we use this dataset to test the
scalability
of the method as the number of non-zero
cells in the datacube grows. The quality measures for
this dataset are of secondary importance.
The dataset
has four dimensions
with corresponding
cardinalities
320,160,80 and 40.
The non-zero
cells values are
distributed
with a normal distribution
of mean 20 and
variance 4. We choose the size of the dataset in number
of non-zero cells, the size of a chunk and the density
of the dense chunks (cx). The remaining non-zero cells
not attached to a dense chunk are randomly
placed in
the remaining
(spare) chunks.
Also, the placement of




385

83
s
P
c2.5-
G
a
=c
2-
B
:t
* 1.5 -



l-



0.5 -

I(*
r
_______------
---=>Y__-___--_--_----
._-.__.-
-.-.__
_._._._,_,_,_,_
-
-,_._._
_._
0
,
0
200
4w
600
eQ0
1000
12x7
1400
,wJ
,eol
2lm


Figure 3: Execution
time for the standard and approx-
imate method for dataset 3. The Y axis shows the ex-
ecution time in seconds; The X axis shows the number
of non-zero cells in thousands.


dense chunks is done randomly.
We conducted all the
experiments with a p = 0.2, y = 0.2, and two values
of Q!(0.33 and 0.95).
Figure 3 plots the execution
time of the standard
method and the approximate
method for chunk den-
sities of 33% and 95%. As can be observed, the approx-
imate method scales well with the size of the dataset,
while the execution time of the standard method gets
to be impractical
for large sizes (more than 10 hours
for 2 million
non-zero cells). It is also worth pointing
out that in the high density case, the approximate
al-
gorithm performs better since there is no need to index
the non-zero cells in the chunk. Nevertheless, in the case
of low density, the approximate
algorithm
still outper-
forms the standard by a ratio of 1O:l (as opposed to a
ratio of more than 3O:l in the dense case).


4
Conclusions

In this paper we have described an approximate method
to do EDA, using the median polish procedure over
datacubes. We have shown that the method scales well
with the size of the cube, allowing the user to perform
EDA in cases where the cost of doing the standard
algorithm
would be prohibitive.
The price one pays for using the approximate
method
is a decrease on the quality
of the results.
However,
as shown in Section
3, the quality
of the results
obtained by the approximate
method is good. We are
currently
continuing
this work by looking at methods
to improve the quality of our results (by using different
modeling techniques), while retaining
the scalability
of
the method.
Also, we want to explore techniques to
visualize the results of the EDA. Moreover, we are also
examining
other EDA and mining techniques that can
take advantage of approximate
methods.

References
PI



PI




PI



PI

151


PI




[71


PI


PI

WI


PI
D. Barbara
and M.
Sullivan.
Quasi-Cubes:
Exploiting
Approximations
in Multidimensional
Databases.
SIGMOD
Record, 26( 3)) September
1997.

D. Barbara
and M. Sullivan.
Quasi-Cubes:
A
Space-Efficient Way to Support Approximate
Mul-
tidimensional
Databases. Technical Report ISSE-
TR-98-03,
George Mason University,
Information
and Software Engineering
Department,
October
1998.

D. Barbara and X. Wu.
Using Approximations
to Scale Exploratory
Data Analysis in Datacubes.
Technical
Report
ISE-TR-99-02,
George Mason
University,
Information
and Software Engineering
Department,
March 1999.

U.S.
Census
Bureau.
Population
data.
http://www.census.gov/main/www/access.html.

Arbor Software Coorporation.
Application
Man-
ager User's Guide. Essbase Version 4.0.

J. Gray, A., Bosworth,
A. Layman,
and H. Pira-
hesh. Data cube: A reiational
aggregation operator
generalizing group-by, cross-tabs and sub-totals. In
Proceedings of the 12th International
Conference
on Data Engineering,
pages 152-159, 1996.

D.C.
Hoaglin,
F. Mosteller,
and J.W.
Tukey.
Ex$oring
Data Tables, Trends and Shapes. Wiley,
1985.

D.C. Hoaglin, F. Mosteller,
and J.W. Tukey.
Un-
derstanding Robust and Exploratory
Data Analysis.
Wiley, 1986.

A.K.
Jain
and R.C.
Dubes.
Algorithms
for
Clusteting Data. Prentice Hall, 1988.

K.A. Ross and D. Srivastava.
Fast Computation
of Sparse Datacubes.
In Proceedings of the 23rd
VLDB Conference, Athens, Greece, 1997.

S. Sarawagi,
R.
Agrawal,
and
N.
Meggido.
Discovery-driven
Exploration
of
OLAP
Data
Cubes.
In Proceedings of the International
Con-
ference on Extending Data Base Technology, pages
168-182,1998.




386

