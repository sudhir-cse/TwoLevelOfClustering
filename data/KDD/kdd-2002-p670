B-EM: A Classifier Incorporating Bootstrap with EM
Approach for Data Mining


Xintao Wu
UNC at Charlotte
9201 Univ. City Blvd
Charlotte, NC 28223
XWU @ uncc.edu
Jianping Fan
UNC at Charlotte
9201 Univ. City Blvd
Charlotte, NC 28223
jfan @ uncc.edu
Kalpathi R. Subramanian
UNC at Charlotte
9201 Univ. City Blvd
Charlotte, NC 28223
krs@ uncc.edu



ABSTRACT
This paper investigates the problem of augmenting labeled
data with unlabeled data to improve classification accuracy.
This is significant for many applications such as image clas-
sification where obtaining classification labels is expensive,
while large unlabeled examples are easily available. We in-
vestigate an Expectation Maximization (EM) algorithm for
learning from labeled and unlabeled data. The reason why
unlabeled data boosts learning accuracy is because it pro-
vides the information about the joint probability distribu-
tion. A theoretical argument shows that the more unlabeled
examples are combined in learning, the more accurate the
result. We then introduce B-EM algorithm, based on the
combination of EM with bootstrap method, to exploit the
large unlabeled data while avoiding prohibitive I/O cost.
Experimental results over both synthetic and real data sets
show that the proposed approach has a satisfactory perfor-
malice.


Categories and Subject Descriptors
H.4.m [Information Systems]: Miscellaneous


Keywords
Expectation Maximization, Classification, Supervised and
Unsupervised learning, Bootstrap Method


I.
INTRODUCTION
Classification has been identified as an important problem
in data mining field. There has been focus on algorithms [17,
8, 13] that can build classifier over large labeled training sets.
The intuition there is that by building classifier over large
training data sets, we will be able to improve the accuracy
of the classification model. One key difficulty with these
current algorithms is that the assumption requires a large,




Permission to makedigitalor hard copies of all or part of this workfor
personal or classroomuse is grantedwithoutfee providedthat copiesare
not madeor distributedforprofitor commercialadvantage and that copies
bearthisnoticeand the fullcitationon the firstpage. Tocopyotherwise,to
republish,toposton serversortoredistributetolists,requirespriorspecific
permissionand/ora fee.
SIGKDD '02 Edmonton,Alberta,Canada
Copyright 2002ACM 1-58113-567-X/02/0007 ...$5.00.
often prohibitive, number of labeled training records to learn
accurately.
However, in many modern classification applications such
as text categorization, web categorization and image classi-
fication, the training datset may not be assumed to be fully
labeled. On the contrary, the training data set usually may
contain only very few labeled examples and a large num-
ber of unlabeled examples due to the fact hand-labeling is
expensive while collecting unlabeled records is trivial espe-
cially for those involving online sources.
Consider the problem of training an image classifier to au-
tomatically classify the images. Given the growing volume
of online images available through the internet, this problem
is of great practical significance. Each image is associated
with a large number of visual features such as color, texture,
shape etc which can be extracted automatically by feature
extraction tools. Classification problem here involves learn-
ing a mapping from a known feature space to a set of discrete
semantic class labels.
In this paper, we investigate an algorithm that learns
to classify data more accurately by combining unlabeled
records to augment the available labeled training records.
The reason why unlabeled samples boost learning accuracy
is, in brief, the unlabeled samples provide the information
about the joint probability distribution over feature values
of the records [4]. Here, we assume the labeled training ex-
amples may only be a small fraction of total training data
set. The labeled part can be easily fitted in memory while
the total volume of training set may be too large to be fitted
in memory. We argue those traditional classification algo-
rithms such as neural networks [12], statistical models [111,
decision trees [16] and genetic models [10] suffer the accu-
racy here as they assume a sufficient set of labeled training
data. The specific approach we describe here is to extend
conventional learning algorithms over large data sets by us-
ing Expectation Maximization (EM) to dynamically derive
pseudo-labels for unlabeled image into supervised learning.
For cases where a large volume of unlabeled records is avail-
able, we propose a novel algorithm, B-EM, based on a combi-
nation of two well known learning algorithms: the bootstrap
method [5] and EM [7] algorithm. B-EM greatly reduces the
number of database scans while achieving the satisfactory
accuracy (bounded in a small range with a high confidence
level).
The remainder of the paper is structured as follows. In
Section 2, we survey existing work on EM in supervised and




670

unsupervised learning. In Section 3, we formally introduce
how to extend EM over labeled and unlabeled training data.
We present our new algorithm B-EM which is applicable to
large training data set. In Section 4, we present experimen-
tal results and performance evaluation over both synthetic
and real data sets. We present the conclusion and address
the future work in Section 5.


2.
RELATED WORK
The EM algorithm [7] is a general technique for finding
maximum likelihood estimates for parametric models when
the data are not fully observed. EM has been well studied
for unsupervised learning and has been shown to be supe-
rior to other alternatives for statistical modeling purposes
[6]. The well-known AutoClass project[6] investigates the
combination of the EM algorithm with naive bayes classifier
where they emphasize how to discover clusters for unsuper-
vised learning over unlabeled data. Bradley et al. present
a scalable clustering framework where they apply EM al-
gorithm to the data summary instead of the original data
[2]. The algorithm presented effectively scales to very large
databases as it requires at most one scan of the database.
However, it is unknown how to apply the above algorithm
for supervised learning.
Ghahramani et al. present a framework [9] based on max-
imum likelihood density estimation where EM is applica-
ble both for supervised and unsupervised learning problems.
For example, by estimating the joint density of the input and
class label using a mixture model, the classification problems
can be thought learning a mapping from an input space into
a set of discrete class labels.
The theoretical work [3, 4] shows use of unlabeled data
can improve parameter estimates of mixture model. The
results can be highlighted as following 1) unlabeled data
does not improve the classifiaton results in the absence of
labeled data; 2) the classification error approaches the bayes
optimal solution at an exponential rate in the number of
labeled examples given if infinite amounts of unlabeled data
are available; 3) the labeled data can be exponentially more
valuable than unlabeled data in reducing the probability
of classification error; 4) the additional unlabeled samples
should always improve the performance.
Nigam et al. introduce an algorithm for learning from
labeled and unlabeled text, based on the combinations of
EM with a naive bayes classifer and show that the accuracy
of learned text classifiers be improved by augumenting a
small number of labeled training documents with a large
pool of unlabeled documents [14].


3.
OUR METHOD

3.1
General Approach and Notation
We are given n records in a training set S = SIUSu. Each
record s~ takes the form si =< xi, yi >, where xi is an asso-
1
· ,xid >, which is depicting
ciated d-attribute vector, < xi ," ·
d measurements made on the data from d attributes, rep-
sectively, At,.. · , A~, yi denotes the class label of records
from m classes C = {cl,... ,cm}. The record si E S l comes
with the known class label yi E C, and for the rest of the
records, in subset S ~', the class label yi is unknown.

1x~ contains the relevant information required for measuring
the similarity between the data.
Now the learning task is, given S = S zUS t', how to build
a classifer which can predict yi based on xi for new data
si E S t, where St is test data sets . Note the traditional
classifcation approach is to build the classifer only based on
labeled training data SL
In this paper, we will apply a mixture model for charac-
terizing the nature of the data and classifiers. The mixture
model follows two commonly used assumptions about the
data: 1) the data are produced by a mixture model; 2) each
record only belongs to one class and there is a one-to-one cor-
respondence between the components in the mixture model
and classes.


m

P(xiI0) = E
P(cs)P(xilcs' 0J)
(1)
j=l

The mixture model, as shown in Equation 1 has two parts:
the first part gives the interclass mixture probability P(cs)
that an example sl is a member of class c5, independently
of anything else we may know of the data; the second part
P(xi]c5, 05), shows the data in each class cj are then modeled
by a class distribution (component), giving the probability
of observing the instance attribute values xi, conditional on
the assumption that instance si belongs in class c5.
The interclass pdf is a Bernoulli distribution characterized
by the class number m and the probabilities of each class.
As the distribution of each class is unknown, the multi-
dimensional Gaussian distribution is usually assumed for it
provides a good approximation for unknown distributions.
In this paper, we assume all attributes xi are continuous
variables2.
Equation 2 shows a d-dimensional Gaussian distribution
for class c5, where j = 1,... ,m, #5 is d-dimensional mean
vector and E5 is d × d covariance matrix, the superscript T
indicates transpose , I~jl is the determinant of ~j and ~-1
is its matrix inverse.


1
1
P(xle~) =,/~.~5e~p{-
~(x - ~)*~j-~(~, - ~)}

v
,--~
,-~,
(2)
In this setting, each record, si, is created by first selecting
a component according to priors P(cj), then, second, having
the mixture component generate the record according to its
own distribution P(xilcj, 0j) with parameters 0j 3.
Under this framework, classification problems can be solved
by estimating joint density of the known attributes and class
label using a mixture model and computing a maximum like-
lihood estimate of 0, i.e., finding the parameterization that
is most likely given our S. EM is a widely used iterative
technique which can concurrently generate probabilistically-
assigned labels for the unlabeled data, and a more probable
model with smaller parameter variance that predicts these
same probabilistic labels.

3.2
EM Expansion with Unlabeled Examples
When traditional classifier approach such as naive bayesian,
decision trees etc. is given a small set of labeled training

2Discrete or categorical data can be modeled as generated
by a mixture of multinomial densities and similar derivations
for the learning algorithm can be applied.
3Under the assumption of Gaussian distribution, 0j includes
pj, ~j.




671

data, classification accuracy suffers. This section shows, by
augmenting this small set with a large set of unlabeled data
and combining the two sets with EM, we can improve the
parameter estimates and hence classification accuracy.
Consider the probability of all the labeled and unlabeled
training data, S = S ~t2 S~' under the two part model frame-
work. The probability of the whole data is simply the prod-
uct over all the data shown as,


P(SIO) = ~ P(sdO)
i=1

here we assume one record is independent with the others.
The likelihood of an unlabeled record can be characterized
as the sum of total probability over all mixture components.



P(xl0) = ~
P(xlci, O~)P(cs)
j=l

For the labeled record, we are given the label yi and thus
do not need to sum over all class components as shown,


P(x]0) = P(xlcj, 0j)P(ci)

When combining both labeled and unlabeled records, the
probability of the whole training data set is shown as Equa-
tion 3.


Is~l rn
P(SIO) =
H ~ P(c~)P(xdc~'O~)
i~1
j:l

Isq
× H P(cj = y,)P(xtlcj, Oj).
(3)


By the maximum likelihood principle, the best model of
the data has parameters that maximize P(OIS). Equation
4 shows the log likelihood of the parameters given the data
set.


log(P(OlS)) = log(P(O)/P(S)) +
Isul
m
log~ P(c~)P(x, lcj, Oj) +
i=1
j=l

Isq
~tog(P(cj = y,)P(x, lcj,0~) )
(4)

i=1

Note the first part is a constant for P(S) is a constant
and maximum likelihood estimation assumes that P(O) is a
constant.
However, the second part of this equation has a log of
sums, it is not easily maximized numerically.
Intuitively,
it is unclear which component of the mixture model gen-
erated a given record and thus which parameters to adjust
to fit the feature value of that record. When all the class
labels in S t are given, we could express this complete log
likelihood of the parameters without a log of sums [9]. We
introduce the binary indicator z~i where zij = 1 iif yi = cj
else zij = 0 in Equation 5. By introducing a hidden vari-
able z that indicate which record was generated by which
component, then the maximization problem decouples into
a set of simple maximizations.
log(P(OlS,z))
= lo9(P(O)/P(S)) +

~ zijlog(P(cjlO))P(x, lcj, 0j)(5)
i=1
j=l

Since z is unknown, the log(P(O}S,z)) can not be utilized
directly.
The EM algorithm can be used to find a local
maximum likelihood parameter by an iterative procedure
through the following two steps.

·
E-step, which corresponds to calculating probabilistic
labels P(c~lx~,0) for every record by using the cur-
rent estimate of 0. Equation 6 shows how to compute
E [zi~Ixi, 0(k)] (we denote as h~)), the probability that
Ganssian j, as defined by the parameters estimated at
step k, generated data xi.

·
M-step, which corresponds to calculating a new maxi-
mum likelihood estimate for parameter 0 given the cur-
rent estimates for P(cjIx~, 0). As shown in Equation 7
and 8, the M-step re-estimates the means and covari-
ances of the Gaussians using the data set weighted by
the h!~)
"'t3
"




^
!

h!~) =
I~1 -= ~p{-½(x~ - -J~(~)'T~J
~-~'(~)~^, _ f~))}
*3
^
_!


(6)

g-,n
h(k)v
..(k+l) = z-~i=X'°i~ ~i
(7)



= E?=l h}?(x,
-
-
,
E7:1
(8)


The parameter 0 generated by EM that locally maximizes
the probability of all the data (both the labeled and unla-
beled) will be used to label the test data with the largest
posterior probability.

3.3
EM Over Large Unlabeled Data Sets
In this paper, we assume that the size of labeled data set
S z is small, hence, it can be easily fitted in memory. While
the size of unlabeled data S~ can be too large to be fitted in
memory, computing a mixture model over large databases
via standard EM would not be acceptable as hundreds of it-
erations or more may be required during iterative EM refine-
ment step. One straightforward approach is to sample un-
labeled records as many as the memory can hold. As shown
from theoretical work [3, 4], the more additional unlabeled
data, the more accuracy we achieve. Although guaranteed
to converge, a general bound on the number of unlabeled
data required for a given training data set is not available.
As shown in experiment results, the training data set gener-
ated by class distributions with larger variances needs more
unlabeled data to converge. In this paper, we combine the
EM with bootstrap methods [5] to classify with large sizes
of training set.
The resulting B-EM algorithm is very straightforward and
can be outlined at a high level as follows:




672

1. Build an initial classifier by estimating the parameters
of model from the labeled data only.

2. Repeat M times

· Obtain a radom bootstrap sample from unlabeled
data, filling in the memory buffer.

· Repeat until the parameters 0z do not change

- Apply the current classifer to probabilistically
label the unlabeled data in the buffer.

- Recalculate the classifier parameters 0z given
the probabilistically assigned labels.

3. compute 0* = ~i=1~M 0z.

4. Apply the 0* to probabilistically label all the unlabeled
data.

^ Ideally, we would like to say that /9" is very close to the
0 computed over all records in training set. The bootstrap
principle [5] shows the two estimates converges the same
when bootstrap steps M is sufficiently large (e.g., 100).
We can see the B-EM needs at most two scans of data
while achieving almost the same accuracy. When the train-
ing data set is totally unordered, we get bootstrap samples
by randomly fetching from disk block. In this case, we only
need one scan (the step 4 which labels all unlabeled data).
When the training data set is not totally unordered, we need
one more read scan to generate M bootstrap samples and
write each samples to disk. In this case, the total number
of scans is bounded by two. It is also worth pointing out
B-EM also saves CPU cost. The explaination has two parts.
First, for B-EM, the number of unlabeled data invloved in
EM iterative steps is less than standard EM. Second, EM
empirically tends to need fewer iterations with less data.

3.4
Discussion
In this paper, we also combine EM with naive bayesian
approach which assumes the values of the attributes are con-
ditionaly independent of one another when the number of
dimensions is too large. The explaination has two parts.
First, in estimating P(ej]x~, 0) as shown in Equation 6, we
need compute, ~-t, the inverse of covariance matrix. When
the values of the attributes axe not conditionaly indepen-
dent, the covariance matrix Ez is singular which causes the
estimation overflow. Second, we can reduce computation
cost in evaluating P(x[cj, 0j) involved in EM.
Under the naive bayesian assumption, the class distribu-
tion shows as,


d

PCxlc¢,0~) = 1Y~P(xkicJ, 07)
k=l


The probabilities P(x k[cj) can be estimated from the train-
ing sample, where each attribute Ak is assumed to have a
normal distribution with mean #~t and standard variance

values for attributes A for train-
a~ respectively, given the
k
ing samples of class ca.


4.
EXPERIMENTAL RESULTS
The experiments are conducted on a Dell PowerEdge 4400,
with two processors and 1G bytes of RAM.
I
I
I
I
I
I
¢0
O~
~0
100
t~
140
~
all~lmtoxm




Figure 1: Classification accuracy on the synthetic
data set DS1 (10k records, 10 classes), both with
and without unlabeled images



-~- m~utun~5~r~cU
I÷
,




Nurnbwdiibqdadr~cli



Figure 2: Classification accuracy on the synthetic
data set DS4 (10k records, 10 classes), both with
and without unlabeled images


4.1
Synthetic Data

4.1.1
Classification Quality Evaluation
In this section, we show using unlabeled training data in
EM do improve the classification accuracy.
The synthetic data sets (DS1, DS2, DS3, DS4) we gen-
erate in this experiment consists of m = 10 classes. Each
class is defined by a multi-dimensional Gaussian distribution
(d = 25) which is characterized by mean vector # and co-
variance matrix E. For all these four data sets, the Gaussian
means are chosen uniformly on [0.0, 10.0]. For DS1, the di-
agonal covariance marries are chosen uniformly on [0.8, 1.2].
The distribution of DS1 is the same as the data set gener-
ated in [2] and it is expected the clusters are fairly separated.
We then generate DS2, DS3 and DS4 by varying the vari-
ance (multiplying a constant 5, 8, 10). Note the larger the
variance of multi-dimensional Gaussian distribution, the less
separated as more outliers are generated by the Gaussian
distributions with large variance.
For each data set, we generate 10k records (each class has
lk records). Varying sizes of random subsets are labeled and
the remaining subsets are used as unlabeled records.
Figure 1 and 2 show the effect of using EM with unlabeled
data over synthetic data sets DS1 and DS4 respectively. The




673

vertical axis indicates the accuracy rate which is computed
on the basis of the remaining unlabeled records, and the
horizontal axis indicates the amount of labeled data used in
training. We vary the amount of labeled training data, and
compare the classification accuracy of EM without unlabeled
data with EM with unlabeled data.
EM with unlabeled
data performs significantly better.
When the variance of
Ganssian distribution increases, the accuracy rate of both
traditional EM and EM with unlabeled data decreases as
shown in Figure 2.
An important point here is that the outliers in labeled
data can hurt the performance. For example, in DS4, when
the number of labeled data is less than 120, the EM with-
out unlabeled data performs better accuracy than EM with
unlabeled data. However, when the labeled data increases
(the affect of outlier decreases), the EM with unlabeled data
achieves better. The reason is, as Shahshahani et al. [18]
point out, that although in theory the additional unlabeled
samples should always improve the performance, in practice
this might not always be true. As the unlabeled samples
might contain outliers due to the deviation of the real world
situations from the models that are assumed. Such outliers
can hurt the performance.
In figure 3, we hold the number of labeled data constant
as 100 (the class distribution is the same as DS2), and vary
the number of unlabeled data in the horizontal axis. The
experiment results show that more unlabeled data natually
improve the accuracy rate. The exponential rate of conver-
gence towards the limiting rate is evidenced by the approx-
imate linear trend in the semilog Figure 3.

4.1.2
Scalability Evaluation of B-EM
In this experiement, we examine the scalability of B-EM
as the size of input data set increases and compare the run-
ning time of B-EM with that of EM. The data sets in this
evaluation have d = 25 attributes and has k = 10 class (the
description of class distribution is the same as DS3). We
generate the data sets with the size 100k, 200k, 500k, 1M
and 5M records respectively. For all data sets, we fix the
labeled records as 120.
B-EM was run with memory size of 1000 records while
EM was not constrained to a limited RAM requirement.
We note that for data set with 25 continuous attributes,
the 5M records need 1G memory (ignoring any other RAM
requirements). The bootstrap step M is 100.
Figure 4 shows the overall running times of the algorithms
as the number of records in the data sets increases from 100k
to 5M. As can be observed, the B-EM method scales well
with the size of the data set, while the execution time of
the standard EM gets to be impraticial for large sizes (more
than 2 hours for 5M records). It is also worth pointing out
that B-EM is running faster than the full in-memory EM
algorithm for data sets that fit in memory: 100k, 200k and
500k records as the explaination is given in the algorithm
section. Note the classification accuracy of B-EM and EM
for this experiment is almost the same (the difference is less
than 0.5%).

4.2
Corel Data
Our studies here have focused on building classifier through
EM over the Corel images collection [15]. The 68,040 images
are cataloged into broad categories. Four sets of features,
color histogram, color histogram layout, color moments, and
II




Figure 3: The effect of varying the number of un-
labeled data.
Classification accuracy is shown on
synthethic data set with 100 labeled records, and
varying amounts of unlabeled records




feint,, ~uri~,~r~r~
p)



Figure 4: Execution time vs. number of records for
EM and B-EM


co-occurence texture are available online.
In this experiment, we extract 6003 images and get 12
classes. The experiment is done over the combined feature
set (41 features) of color histogram and color moment.
Figure 5 shows the effect of using EM with and without
unlabeled images. EM with unlabled images performs better
when only a small number of labeled images are available.
When we have sufficient number of labeled images, as shown
in right figure in Figure 5, the unlabeled images do not help
improve the performance.


5.
CONCLUSIONS
This paper :investigate the question how unlabeled data
may be used to improve the accuracy of classifier when few
labeled data is available. This is an important question in
many applications such as document categorization and im-
age classification where the cost of labeling is very high and
the hugh volume or unlabeled data is easily available. We
then present a sealable classification algorithm, B-EM, to
exploit the huge volume of unlabeled data while avoding
I/O cost.
The theoretical model shows unlabeled data can be used
to improve the accuracy of classifiers when 1) the probability
distribution that generates the underlying data can be de-




674

45



40



35



~ 3o

~ 25


2O,




15



10
I ---I--- without unlabeled
images
+
with unlabeled
ima~les




410
6=0
80
1(~0
1;0
140
45


40



35




~ 25


20
I_.~__
without
unlabeled
images
with unlabeled
ima~le8




15



100
500
1000
1600
2000
Number of labeled
images
Number of labelecl image8


Figure 5: Classification accuracy on the corel data set (6003 images, 12 classes), both with and without
unlabeled images


scribed as a mixture distribution and 2) there is a one-to-one
correspondence between the components and class labels.
However, the complexity of real-world applications will not
be completely captured by statistical models and the real-
world data is not totally consistent with the assumptions of
the model. For example, we assume one Gaussian distribu-
tion over feature space for mamal in our experiment with
Coral data set. This assumption is clearly not true. Our fu-
ture work will investigate how to build mixture models over
concept hierarchy.
Another interesting direction for future work with unla-
beled data is how to build an incremental learning algorithm
for stream data [1] where the unlabeled data is infinitely
available. The incremental algorithm may expliot the un-
labeled test data received in the testing phase to improve
performance on the later test data.


6.
ACKNOWLEDGMENTS
The authors would like to thank Michael Ortega-Binderberger
for his help in providing the labels of Corel images.


7.
REFERENCES

[1] S. Babu and J. Widom. Continuous queries over data
streams. SIGMOD Record, 30(3), Sept 2001.
[2] P. S. Bradley, U. M. Fayyad, and C. A. Reina. Scaling
clustering algorithms to large databases. In
Proceedings of the Fourth ACM KDD International
Conference on Knowledge Discovery and Data Mining,
pages 9-15, August 1998.
[3] V. Castelli and T. Cover. On the exponential value of
labeled samples. Pattern Recognition Letters,
6:105-111, 1995.
[41 V. Castelli and T. Cover. The relative value of labeled
and unlabeled samples in pattern recognition with an
unknown mixing parameters. IEEE Transaction on
Information Theory, 42(6), 1996.
[5] B. Chapmann and R. Tibshirani. An Introduction to
the Bootstrap. Monograph on Statistics and Applied
Probability. Chapman and Hall, 1993.
[6] P. Cheeseman and J. Stutz. Bayesian classification
(autoclass): Theory and results. Advances in
Knowledge Discovery and Data Mining, 1996.
[7] A. Dempster, N. Laird, and D. Rubin. Maximum
likelihood from incomplete data via the em algorithm.
Journal of the Royal Statistical Society, 39(1):1-38,
1977.
[8] J. Gehrke, V. Ganti, R. Ramakrishnan, and W.-Y.
Loh. 13oat-optimistic decision tree construction. In
Proceedings of the SIGMOD Conference, pages
169-180, 1999.
[9] Z. Ghaharmani and M. Jordan. Supervised learning
from incomplete data via an em approach. Advances
in Neural Information Processing Systems 6, 1994.
[10] D. Goldberg. Genetic Algorithms in Search,
Optimization and Machine Learning. Morgan
Kanfmann, 1999.
[11] M. James. Classification Algorithms. Wiley, 1985.
[12] R. Lippmann. An introduction to computing with
neural nets. IEEE ASSP Magazine, 4(22), 1987.
[13] M. Mehta, R. Agrawal, and J. Risanen. Sliq: A fast
scalable classifier for data mining. In Proceedings of
the Fifth International Conference on Extending
Database Technology, March 1996.
[14] K. Nigam, A. Mccallum, S. Thrun, and T. Mitchel.
Text classification from labeled and unlabeled
documents. Machine Learning, 39(2/3):103-134, 2000.
[15] M. Ortega-Binderberger. Cord image features.
http:/ /kdd.ics.uci.edu /databases/ CorelFeatures / CorelFeatures.html.
[16] J. Quinlan. Induction of decision trees. Machine
Learning, 1:81-106, 1986.
[17] J. Sharer, R. Agrawal, and M. Mehta. Sprint: A
scalable parallel classifier for data mining. In
Proceedings of the 2Pnd VLDB Conference, pages
544-555, 1996.
[18] 13. Shanhshanhani and D. Landgrebe. The effect of
unlabeled samples in reducing the small sample size
problem and mitigating the hughes phenomenon.
IEEE Transactions on Geoscience and Remote
Sensing, 32(5):1087-1095, 1994.




675

