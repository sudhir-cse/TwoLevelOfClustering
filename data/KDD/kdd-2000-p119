The IGrid Index: Reversing the Dimensionality Curse For
Similarity Indexing in High Dimensional Space


Charu C. Aggarwal
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598

charu@watson.ibm.com
Philip S. Yu
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598

psyu@watson.ibm.com


ABSTRACT
The similarity searchand indexing problem is wellkno wn
to be a di cult one for high dimensional applications. Most
indexing structures show a rapid degradation with increas-
ing dimensionality whic hleads to an access of the entire
database for each query. Furthermore,recent researchre-
sults sho w that in high dimensional space, even the concept
of similarity may not be very meaningful. In this paper,
we propose theIGrid-index; a method for similarity index-
ing which uses a distance function whose meaningfulness is
retained with increasing dimensionality. In addition, this
technique shows performance which is unique to all known
indexstructures; thepercentageofdataaccessedisinversely
proportional to the overall data dimensionality. Th us, this
technique relies on the dimensionality to be high in order to
provide performance e cient similarity results. The IGrid-
index can also support a special kind of query whic hwe
refer to as projected range queries; a query whic his in-
creasingly relevantfor very high dimensional data mining
applications.

Categories and Subject Descriptors
H.2.8 Database Management: Database Applications

General Terms
Dimensionality Curse, Indexing

1. INTRODUCTION
The similarity search problem is de ned as follows: for a
giventargetrecordinmulti-dimensionalspace, ndtheclos-
est record to itbased on somepre-de neddistance measure.
This technique ndsapplications in numerousdomainssuch
as spatial databases, multimedia systems, data mining, and
image retrieval 4, 5. With the increasing availabilityof
large repositories of very high dimensional data in numer-
ous application domains, it becomes increasingly important
to dev elop e cient query processing and similarity indexing
techniques for such data.
A number of techniques such as KDB-Trees, kd-Trees, and
Grid-Files are discussed in the classical database literature
26 for indexing m ultidimensionaldata. Many of these
techniques wereinitially proposed in the contextof low-
dimensional spatial applications. Starting with the seminal
workof Guttman on R-Trees 18, considerable workhas
been done on nding multi-dimensional index structures in
the database arena. Variants of R-Trees such as R-Trees,
R+-Trees,and Hilbert R-Trees 7, 21, 27, are able to re-
solvevariouskindsofqueriesmoree ciently,byusingbetter
packing and split methods to build the tree structure. All
of these methods partition the data into ranges which are
parallel to the original axis system. Subsequently, methods
such as SS-Trees 28 were proposed which do not necessar-
ily partition the data using ranges from the original set of
attributes. Other prominent indexes and query processing
methods may be found in 5, 20, 24, 28.
The abovetechniques generally workwellfor low dimen-
sional problems, though they degrade rapidly with increas-
ing dimensionality, so that eac h query requires the access of
almost all of the data; consequently specialized techniques
for high dimensional similarity indexing such as X-Trees,
SR-Trees and TV-Trees 11, 22, 23 havebeen proposed.
Because of the inheren tdi culty of exact nearest neigh-
bor search for v ery high dimensional data, some interesting
methods have also been proposed for approximate nearest
neigh borsin these cases 8, 17. Other relevantmethods
include the pyramid technique 9, whic hhavebeen pro-
posed for high dimensional range queries. Despite these in-
dexing and query processing methods, it has been observed
that with almost any technique, when the dimensionality is
above 15 or 20, then all of the data is accessed.In fact, re-
cent results 6 sho w that a simple sequential scan performs
better than any of the space partitioning methods on uni-
formly distributed data when the dimensionality is larger
than 610.1 This behavior has been validated with empirical
testing for signi cantly lowerdimensionalities 15. Con-
sequen tly, a technique called the VA-File 6 was proposed
whic h assumes that a sequential scan is inevitable and tries
to build an index by compressing the time required for this
sequen tial scan to between 12:5,25 of the data. This
is achieved by scanning a compressed representation of the
data; some additional time overhead is required for resolv-
ing the con icts due to the information lost in compression.
This method showsmore promising results than the best
1
In practice this threshold is well below 610, since the worst
caseanalysisisbasedonverycrudeestimationsandbounds.
Permission to make digital or hard copies of part or all of this work or
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers, or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD 2000, Boston, MA USA
© ACM 2000 1-58113-233-6/00/08 ...$5.00




119

space partitioning methods.
Recentresearchresultsshowthatinhighdimensionalspace,
even the concept of proximity may not be very meaningful
12. Theseresultsshowthatforcertainclassesofcommonly
used similarity functions such as the Lp-norm, the nearest
and furthest neighbor are of the same relative distance to
the query point for large classes of data distributions. Sev-
eral additional interestingproperties oftheLp-normmaybe
found in 1. The lack of relative contrast in terms of simi-
larity is somewhat undesirable, since it is not clear whether
or not the nearest neighbor is meaningful under such cir-
cumstances. For many high dimensional data mining ap-
plications, even the concept of similarity is a heuristic one,
and is not well de ned. There is not much consensus and
literature on nding distance functions which result in the
mostmeaningfulde nitionofsimilarity. Thecommonuseof
the Euclidean distance metric for indexing structures arises
fromtheirinitialapplicabilitytospatialdatabasesforwhich
the L2-norm has special meaning. For many other high di-
mensional application domainssuchas information retrieval
IR, categorical data and market basket data, the impor-
tanceofdesigningqualitativelye ectiveandmeaningfuldis-
tance functions using the aggregate statistical behavior of
the data has been well understood and appreciated 13, 16,
25. These techniques have however not been applied to the
multidimensional indexing problem in the quantitative do-
main, where many applications have continued to use the
increasingly irrelevant Lp-norm.
Thelackofde nitenessinmeasurementofsimilarityforhigh
dimensional data has been explored in recent work, where
the distance distributions of the data to the query point in
di erent projections are examined in order to identify inter-
esting query-speci c features 19. These identi ed features
areusedinordertodeterminethemostsimilarobjects. This
techniqueis called projected nearest neighbor search and can
be veryvaluable in understanding dimensional selectivity in
the neighborhood of a query point. Since it is a di cult
problem to nd the best combination of dimensions in the
neighborhood of a query-point, the technique is slower than
a sequential scan. Thus, this system is very useful for deal-
ing with the quality issue of nearest neighbor search and
understanding the data by providing locally interesting
combinations of dimensions which are discriminatory pro-
jections. On the other hand, the technique is not really
focussed on the problem of rapid query resolution. In order
to provide such capability, we need to develop a meaningful
similarity function which can be expressed in closed form,
and can be used in conjunction with an e ective index. To
thise ect, wedevelopaclass ofdistancefunctionswhichare
somewhatsimilartotheLp-norminlow dimensions,butbe-
come increasingly di erent with greater dimensionality. In
addition, thedevelopmentofaclosedformdistancefunction
has implications for a wide variety of other di cult high di-
mensional data mining problems which rely on proximity
concepts.
This paper shows how the aggregate summary behavior of
the data may be used in order to design a qualitatively ef-
fective function which computes distances in a more exible
way than the straightforward summation of distances over
all dimensions. We will provide a theoretical analysis of the
meaningfulness of this distance function, which shows that
it does not su er from the lack of discrimination caused by
high dimensional sparsity as discussed in 12. This theo-
retical analysis is supplemented with qualitative empirical
evidence which suggests that this method leads to more
meaningful results. A key aspect of this class of distance
functions is that it turns out to be index-friendly; we will
designaclass of indexstructuresIGridandIGrid+based
on these functions. These index structures show the sur-
prising behavior of the reversal of the dimensionality curse
in terms of performance; in other words the percentage of
data accessed is inversely proportional to the dimensional-
ity. This is in contrast to the behavior of almost all known
indexing structures and algorithms.
This paper is organized as follows. The remainder of this
section discusses the background and de nition of the simi-
larity function. In section 2, we introduce the IGrid index.
Atheoreticalanalysisisdiscussedinsection3. Section4dis-
cusses how inter-attributecorrelations maybe used in order
to improve the similarity calculations. In section 5, we will
discuss the application of the IGrid-index to a special kind
of query which we refer to as projected range queries. Em-
pirical results are presented in section 6. Section 7 discusses
an overview of the results of this paper.

1.1 Contributions of this paper
This paper discusses an indexing technique for similarity
search which is both qualitatively e ective and shows per-
formance e cient behavior. To this e ect, we show that
as the dimensionality increases it becomes more important
to compute similarity functions in more exible ways than
is the case with the Lp-norm. The discrimination behavior
of the similarity function is further enhanced by using the
aggregate inter-attribute correlation behavior. We provide
empirical evidence that this enhanced similarity measure is
moremeaningfulthantheeuclideanmetricforrealdatasets
in high dimensional data mining applications. At the same
time, this similarity function can be indexed e ectively by
a method called the IGrid-index which shows the surpris-
ingly pleasant behavior of improved performance with in-
creasing dimensionality. This behavior is rooted in the fact
that the index and the distance measure exploit the greater
statistical information available in high dimensional data in
a more e ective way. Thus, it solves the problem of mean-
ingful and performance e cient similarity indexing in one
uni ed framework. We also show how the IGrid-index can
be utilized for e ective resolution of a special kind of query
called the projected range query; a query which becomes
more relevant than the full dimensional query for very high
dimensional applications.

1.2 Background and Motivation
In this section, we will illustrate some of the recent results
12 which show that in high dimensional space, the ratio of
therelativedistancesofthedi erentpointstoagiventarget
converges to one. Inorder to do so, we will establish certain
notations and de nitions:
d: Dimensionality of the data space
N : Number of data points
Xd : Data point from d-dimensional data distribution Qd
distdXd: Distance of Xd from the origin 0;::: ;0



120

Dmind;Dmaxd : Nearest Farthest distance of N points to
origin
E X ;var X : Expected value variance of random variable
X
Yd !p c: Yd converges in probability to c as d1

Theorem 1. Beyer et al. 12 If:
limd!1 var

distdXd
E distdXd

=0 , then:
Dmaxd,Dmind
Dmind
!p 0.

Proof. See 12 for proof of a more general version of
this result.

This result shows2 that under certain conditions on the dis-
tancefunctionsanddatadistribution,thedi erencebetween
themaximumandminimumdistances toa giventargetthe
origin3 in this case does not increase as fast as the nearest
distancetoanypointinhighdimensionalspace. Thismakes
aproximityquerymeaninglessandunstablebecausethereis
poor discrimination betweenthe nearest andfurthest neigh-
bor. Thus, in very high dimensional space a small relative
perturbation of the target point in a direction away from
the nearest neighbor could easily change the nearest neigh-
bor into the furthest neighbor.
These results are valuable not just from the perspective of
meaningfulness but also from the performance perspective
of indexing. Most indexing methods work by using some
kind of partitioning hierarchical or at of the data set.
This partitioning is then used in order to perform e ective
pruning of the data set. The key idea is that if it is already
known that some neighbor X is close enough to the target,
thenonecanpruneawayanentirepartitionbyshowingthat
the optimistic distance bound to that partition is no better
than the distance to X 24. The results in 12 show that in
high dimensionality the nearest and farthest neighbor have
very similar relative distances to the target. Consequently,
the optimistic bounds used by most index structures are
usually not sharp enough for any kind of e ective pruning
in partition based methods. These results also show how
approximate nearest neighbor indexes 8, 17 are a ected
by increasing dimensionality; when the ratio of nearest to
furthest distance is almost even, then the quality of an -
approximate solution which is within a pre-speci ed ratio
1 + of the best solution may be as bad as the furthest
neighbor. Thus, the excellent -dependent performance re-
sults of these techniques come at an increasing qualitative
price with greater dimensionality and xed . In addition,
theseresultsexposethemeaningfulnessissuesofavastnum-
ber of problemssuchas clustering whichrelyon theconcept
of proximity;somerecentresults 2, 3 showhowtheseprob-
lemscanbeunderstoodmoree ectivelybyusingpointsand
dimensions in a more exible way.
2
Otherresults 8 also show thatundercertain di erentpre-
conditions, themeaninglessnessbehaviorisnotquiteasbad.
However,someevidenceofthedegradationhasbeendemon-
strated by empirical testing for real distributions 12.
3
Wehaveusedtheoriginasthetargetinthiscaseinorderto
simplify presentation. See 12 for a more general statement
of results.
1.3 The Similarity Function
Oneofthereasonsforthelackofdiscriminationbetweenthe
nearest and furthest neighbor is the fact that for every pair
of points there are dimensions with varying distances to the
corresponding values in the target. The dominant compo-
nents of distance functions such as the Euclidean metric are
the dimensions on which the points are farthest apart; for
the particular case of high dimensional data, this results in
very poor measurement of similarity. This is because when
thedimensionalityis high, eventhemostsimilar records are
likely to have a few feature values which are well separated
because of noise e ects and sparseness of the data; the ex-
actdegreeofdissimilarityonthesefewnoisydimensionswill
determine the order of distances to the target. In general,
for a given feature, we expect the values for two randomly
pickedrecords to bereasonably well separated average sep-
aration along that range; there is no interesting statisti-
cal information in this fact. For distance functions such as
the Lp-norm, the results of 12 show that the averaging ef-
fects of the di erent dimensions many of which are noisy
start predominating with increasing dimensionality. A dif-
ferent and complementary view of similarity would be one
inwhichaprede nedproximitythresholdisde nedforeach
dimension, and the overall similarity is de ned both by the
number and quality of similarity along the dimensions on
which the two records are more proximate than this thresh-
old. Thus, the similarity function is directly a ected by
thenumberof dimensionswhich havethisinterestingly high
level of proximity, and beyond a certain quality threshold,
the exact degree of dissimilarity on a given dimension is not
considered relevant. Since the meaningfulness problem is
sensitive to the data dimensionality, the criterion for pick-
ing this proximity threshold is also dependent on the data
dimensionality. Speci cally, it is derived using a theoreti-
cal analysis of meaningfulness; we will revisit this issue in a
later section.
The analysis indicates that this de nition of similarity con-
tinues to retain its meaningfulness for higher dimensionali-
ties in terms of the relative contrasts in distances to a given
target. In addition, we will provide evidence using several
high dimensional real data sets that the quality of the near-
est neighbor returned by such a technique is as good or bet-
ter than that provided by the Lp-norm. At the same time,
it is possible to design index structures which are able to
prune away a large part of the data while searching for a
closest neighbor to the target.
We will rst de nea simpledistance similarity function in
which higher numbersimplygreater similarity; later we will
show how to modify this distance function in order to use
proximitythresholdsontheindividualdimensions. LetX =
x1;:::xd and Y = y1;:::yd be two sets of coordinates
in d-dimensional space, so that xi;yi 2 li;ui for some set
of lower and upper bounds li and ui. Then, the distance
similarity function IDistX;Y betweenX and Y is given
by:

IDistX;Y=
"
d
X

i=1

1,
jxi,yij
ui,li

p1
=p
1

The presence of ui ,li in the denominator studentizes or
normalizesthedistanceswithrespecttothedi erentranges



121

of the coordinates.
In order to incorporate the concept of proximity threshold-
ing in the similarity function, we discretize the data into
several ranges. Speci cally, we assume that each dimension
is dividedinto kd equi-depth4 ranges. Each of these is a con-
tiguous range of values, such that a given range contains a
fraction 1=kd of thetotal numberof records. Speci cally, we
denote the jth range for dimension i by Ri;j . In order to
emphasize thedependencyto bedeterminedlater of kd on
the data dimensionality, we have used the dimensionality d
in the subscript.
Let X = x1;:::xd and Y = y1;:::yd be two records.
Then the set of dimensions on which the two records are
similar are those which share the same ranges. Thus, for
dimension i, if both xi and yi belong to the same range
Ri;j , then the two records are said to be in proximity on
dimension i. The entire set of dimensions on which the two
records lie in the same range is referred to as the proximity
set. Let S X;Y;kd be the proximity set for two records
X and Y for a given level of discretization. Furthermore,
for each dimension i 2 S X;Y;kd , let mi and ni be the
upper and lower bounds for the corresponding range in the
dimension i in which the records X and Y are in proximity
to one another. Then, for a given pair of records X and Y
and a level of discretization kd, the similarity between the
records is given by:

PIDistX;Y;kd=
2

4
X

i2S X;Y;kd

1,
jxi,yij
mi,ni

p3
5
1
=p
2

Notethatthevalueoftheaboveexpressionwillvarybetween
0 and jS X;Y;kd j, since each individual expression in the
summation lies between 0 and 1.
The above use of the similarity function guarantees a non-
zerosimilaritycomponentonlyforthosedimensions,inwhich
thetworecordsareproximateenough. Theuseofequi-depth
partitionsensuresthattheprobabilitythattworecordshave
a component in the same partitions given by 1=kd. Thus,
on the average the above summation is likely to have d=kd
components. For more similar records, the number of such
dimensionswill begreater, andeachsuchindividual compo-
nent is also likely to contribute more to the similarity value.
The above function leads to the ignoring of theexact degree
of dissimilarity on the distant dimensions: we will see from
our empirical tests, that for the case of high dimensional
datathiscreates asparsity noise reductionwhichoutweighs
the e ects of information loss.

1.4 Use of Equi-Depth Ranges
Theuseofequi-depthrangesasopposedtoequi-widthranges
has considerable signi cance. In real applications, the data
may be distributed in a very non-uniform way across the
di erent attributes. As a result, simple distance functions
such as the Lp-norm fail to take the aggregate behavior of
the data into account while measuring similarity. The use
4
In equi-depth ranges, each range contains an equal num-
ber of records. In equiwidth ranges, each range contains
a similar length of values covered. The reason for picking
equi-depth ranges will become clear soon.
of equi-depth partitions ensures that when a particular re-
gion is very dense, then the categorical range value is much
smaller. The idea here is that the proximity of two records
for a given feature value should not be treated in a uniform
way across the entire range, but it should be based on how
close these features are with respect to the behavior of the
entire data set. Thus, the use of equi-depthranges creates a
an implicit normalization of the data bytaking into account
the aggregate behavior in that range. This kind of normal-
ization is similar in spirit to idf-normalization techniques
25 used in IR applications. Another advantage of the use
of equi-depthranges is the ability to predict and control the
indexing behavior using the discretization parameter; an is-
sue which we will revisit in later sections.

2. THE IGRID INDEX
The use of ranges in the similarity function provides consid-
erable advantages in theuse of an invertedindex in order to
perform the similarity calculations. The IGrid-index In-
verted Grid index is based on the use of the inverted index
on a grid representation of the data. The division of the
data into ranges automatically creates a grid structure, in
which each record belongs to a particular cell. We create an
inverted representation of the data as follows:
1 For each range j for each dimension i we maintain the
lower and upper bounds of the range Ri;j .
2Foreachrangeforeachdimension,wemaintainalistof
the record identi ers which lie in that range. Note that the
useofkd equi-depthpartitionsalongeachdimensionensures
that the length of each inverted list will be N=kd, where N
is the total number of records.
3Alongwitheachentryinthelistofrecordidenti ers,we
keep the actual coordinate for the corresponding dimension
in that record.
Note that the size of the inverted representation of the data
is comparable to the size of the original database. Once
we have constructed this invertedrepresentation of thedata
from the grid structure, the calculation of similarity is very
simple. For the target record, we ndthe appropriate range
for each dimension. Then we examine the corresponding
d lists. Since there is a total of d  kd lists, the percent-
age of data accessed is small when the value of kd is large.
The method of calculating similarity is very similar to the
method often used in information retrieval applications in
using theinvertedrepresentation for calculating thesimilar-
ity based on word frequencies 25.
LetT =t1:::tdbeagiventargetrecord,andletmi andni
be the upper and lower bounds for the corresponding range
for dimension i. We say that a record is touched by the tar-
get, if it lies on at least one of the d lists corresponding to
the ranges in the target record. The hash table maintains
a record of the similarity value of all the records which are
touched by the target. An entry is added to the hash table
the rsttimeitisencounteredduringtheexaminationofthe
data. While examining each entry on the lists with corre-
sponding coordinate value x0, we calculate the contribution
of that component which is equal to


mi,ni,jti,x
0
j
mi,ni

p
as
de ned in Equation 2 to the similarity function and keep
addingthatvaluetotheappropriateentryinthehashtable.
For the purpose of our results, we used the value p= 1. At



122

the end of the process, the hash table entry with the largest
similarity value is the closest neighbor. The method can
easily be generalized to nd the multiple nearest neighbors.
Anobservationtobekeptinmindisthattheinvertedrepre-
sentationalwaysreturnstheIDsoftherecordsasopposedto
therecordsthemselves. However,itisalsoassumedthatany
reasonablequerywouldhaveanoutputwhichissigni cantly
smaller than the original database for example, for a simi-
larity query one may ask for the most similar record out of
an enormous database of records. Thus, a small amount of
constant timeis requiredin order to access the records from
thedatabaseusingtheIDsofthereturnedrecordsusingthe
natural index by ID, an overhead which is independent of
database size and dependent only on the size of the output.
If it may be assumed that the users are interested only in
online queries which have responses that are small enough
for interactive search and exploration, then this overhead is
asymptotically negligible for larger and larger collections
of data.

2.1 Performance
The performance of this technique improves with increasing
kd; a value which we will determine by meaningfulness con-
siderations. This is because exactly d out of the dkd of the
lists are accessed, and each list is of the same size. Thus, a
fraction 1=kd of the entire inverted index is accessed. How-
ever, in order to make a fair comparison, we would also
need to compare the inverted index size to that of the orig-
inal database. Note that each Record Identi er occurs on
exactly d lists; therefore the total number of identi ers is
N  d. Along with each identi er in an inverted list, we
store the actual value for the corresponding feature of that
record. This requires the storage of another N d values in
all. Thus,thetotalspacerequirementoftheindexis2Nd,
whereas theoriginal database requires Nd space. This cor-
responds to a storage andhence performance overhead5 of
100. In this case, since the overhead is independent of the
value of kd, it follows that the larger the value of kd used,
the better the performance. In the nextsection, we will dis-
cuss how kd is determinedby meaningfulness considerations
in a given dimensionality d.

2.2 Avoiding Hash Table Overflow
Animportantissue in thistechniqueis to avoidthehash ta-
ble over ow in the process of similarity calculations. This is
because the potential number of hash table entries is likely
to be very large, and the hash table is always maintained
in memory. In order to actually implement the system, the
entire database is not indexed as one entity. Instead, the
database is divided into chunks. An inverted index is built
separately for each chunk, and the most similar record is
found one by one for each chunk. At the end, the best
match among all chunksis reported. Note that this division
ofdatabaseintochunksdoesnotchangetheperformancebe-
havior in termsof disk accesses from theinvertedindex, but
iteliminatesthepossibilityof hashtableover ows. Thesize
of each chunk is determined by kd times the total number
5
We do factor in these overheads in the performance results
of the empirical section. We will see that in spite of these
overheads, theperformanceoftheindexissubstantiallybet-
ter than competing methods.
of entries in the hash table. This eliminates the possibil-
ity of an over ow because exactly 1=kd of the entries in the
inverted index are accessed without counting repetitions.

3. THEORETICAL DETERMINATION OF
THE PROXIMITY THRESHOLD
Note that the similarity function is highly in uenced by the
number of dimensions in which the record lies in the same
range as the target. In general, we expect that the nearest
record will have large cardinality of the proximity set. It is
insightful to look at a crude approximation of the similarity
functionin whichonlythecardinality of theproximityset is
used for the purpose of measuring similarity. The discrim-
ination behavior of this function will provide considerable
insight into the case when the more re ned method of using
the actual coordinates are applied.
In order to analyze the discrimination behavior, let us con-
sider two records X and Y which are picked from uniformly
randomly distributed data. From the perspective of index-
ing and meaningfulness degradation, uniformly distributed
data is the most di cult case with increasing dimensional-
ity. Then, on any given dimension the event that the two
records lie in the same range is a bernoulli random variable
with success parameter 1=kd. Speci cally, let us de ne the
bernoulli random variable Mi as follows:
Mi =0 : xi;yi not in same bucket for dimension i
Mi =1 : xi;yi in same bucket for dimension i
The random variable Mi has a mean of 1=kd and a vari-
ance of 1=kd1,1=kd. Let L be the random variable,
which is the sum of this bernoulli variable over the d di-
mensions. Thus, we have L =
P
di=1
Mi. Note that if we
assume that X and Y are drawn from uniform distribu-
tions, then the values of Mi will also be independent and
identically distributed. In such a case, when the dimen-
sionality is high, the distribution of L approaches a normal
distribution which has a mean of d;kd = d=kd, and a
standard deviation of d;kd =
p
d=kd1,1=kd. The
pre-condition of Theorem 1 when interpreted in this con-
text would imply that the meaninglessness behavior is ex-
hibited when limd!1 d;kd=d;kd = 0. Consequently,
for the purpose of this analysis, we will analyze the be-
havior of the ratio d;kd=d;kd in order to measure
the e ects of the discretization parameter on this result.
The higher this ratio, the greater the level of discrimina-
tion among the distances to the di erent records. Using the
above analyzed values of d;kd and d;kd, we obtain
d;kd=d;kd =
p
kd,1=d. This value increases with
the discretization parameterkd. Thus, an increase in kd im-
proves the meaningfulness by ignoring the exact degree of
dissimilarity on the sparse dimensions; on the other hand,
picking kd too large may result in loss of information along
with noise reduction. Such a tradeo is handled by picking
theminimumvalueof kd dependentondsothattheprecon-
dition of Theorem 1 is violated. In other words, we would
like:
limd!1pkd ,1=d 0
3
This impliesthatweshouldpickkd whichis atleast linearly
dependentond. Consequently,forthepurposeofthispaper,



123

we will use kd =dde, where  is some constant. Note that
picking  =1 results in a distance function which is exactly
similar to the Lp-normfor 1-dimensional data, but becomes
increasingly di erent with increasing dimensionality. Simi-
larly, picking  = 0:5 and p = 1 creates a distance function
which is similar to the L1-norm for 2-dimensions but it be-
comes di erent in higher dimensionalities. Assuming that
Lp-norm distance functions work well for low dimensional
problems, these observations provide su cient guidance to
a good choice of  to lie in the region of 0.5 or 1.
For this value of the discretization parameter the value of
d;kd=d;kd remains almost constant and in fact in-
creases slightly with increasing dimensionality. As we shall
see later inthe empirical section, this also has a direct e ect
on the meaningfulness behavior of the similarity function
which does not vary much with increasing dimensionality.
Since we showed earlier that the performance of the IGrid-
index improves with increasing kd, a choice of kd = dde
results in an index structure for which the performance im-
proves with increasing dimensionality.

4. HEURISTICIMPROVEMENTSWITHAT-
TRIBUTE CORRELATIONS
In high dimensional space, many of the attributes are cor-
related with one another. Inter-attribute correlations have
often been used for designing distance functions in categor-
ical domains where there is no natural ordering of attribute
values. Insuch cases, theuseof inter-attributesummaryin-
formationprovidestheonlypossible insightintothesimilar-
ity of objects by examining whether commonly co-occuring
inter-attribute values are present in the two objects 13,
16. This insight is equally relevant even for quantitative
domains of data where a natural ordering of attribute val-
ues exists. The use of aggregate data behavior in order to
measure similarity becomesmoreimportantfor highdimen-
sional data, where there may be considerable redundancies,
dependencies, and relationships among the large number of
attributes 10. Sincealotoftheproximityinformationmay
be hidden in the aggregate summary behavior of the data,
the use of the linearly separable Lp-norm may be a poor
representation of the similarity, when considered in light of
the aggregate statistical behavior. We note here that some
data domains such as text factor the correlation behavior
indirectly into the distance function by using data transfor-
mation techniques such as Latent Semantic Indexing 14.
The rststepistodeterminepairsof attributeranges which
are strongly correlated with one another. Let us de ne one
new pseudo-attribute corresponding to each range. Thus,
there are a total of kd d pseudo-attributes, each of which
corresponds to a range from the original set of attributes.
Letusdenotethesepseudo-attributesbya1,a2,:::,akdd. A
pseudo-attributeisa0-1valueindicatingwhetherornotthe
corresponding attribute in the record contains that range.
Exactly d of the pseudo-attributes take on the value of 1,
whereas the others take on the value of 0. Thus, in terms of
this new representation, a given record Z = z1:::zd can
beexpressedasthesetfi1;:::idg, where, foreachr 21;d,
air takesonthevalueof 1for therecordZ. Foreach pairof
pseudo-attributes ai and aj, we calculate the corresponding
support. The support of a pair of pseudo-attributes is equal
to the percentage of records which take on the value of 1 for
both of these pseudo-attributes. Thus, this value is speci c
to the aggregate behavior of the entire data set which is
beingindexed. Letusdenotethissupportbysij. Thelarger
the value of this support, the greater the level of correlation
between these pairs of pseudo-attributes. Thus, if we have
a pair of records X and Y, such that X contains ai and Y
contains aj, and sij is high, then this is evidential of the
similarity between X and Y. Since there are a total of kdd
pseudo-attributes, atotalof kd2,d2

suchsupportvaluesare
computed. We pick a fraction f of the largest such values
of the inter-attribute correlation. These are the strongly
connected pairs of components.
Let X = x1;:::xd and Y = y1;:::yd be two sets of
records. Then, let fxi1:::xidg and fyi1:::yidg be the in-
dices of the corresponding d pseudo-attributes which take
on the value of 1. Then, the similarity between the records
X and Y is equal to thenumberof strongly connected pairs
amongthepseudo-attributesinX andY. Thus,iftheinter-
attribute correlation similarity is denoted by CIDist;,
then we have:
CIDistX;Y=Cardinality of S where:
S =fxip;yiq:axip and ayiq are strongly connectedg
The total similarity between two records is the sum of the
proximity-threshold based similarity and correlation-based
similarity. Thus, we have DistX;Y = PIDistX;Y +
CIDistX;Y. Each of the two records X and Y contains
exactly d pseudo-attributes. Thus, there are a total of d
d,1combinationsofthesepseudo-attributes,fromrecords
X and Y. Since only a fraction f of these possibilities are
strongly connected on the average, it means that if the data
is uniformly distributed, the contribution of CIDist; to
the similarity function is likely to be f dd,1. Recall
that the contribution of the PIDist component of the sim-
ilarity function is of the same order of magnitude as the
proximity set, which in turn is again expected to be d=kd.
Consequently, by choosing f = c=d,1kd, we obtain
a similar order of magnitude for the inter-attribute corre-
lations as well. The exact value of the constant c will de-
termine the weightage given to the correlation component
in the similarity measure, though the exact analysis of the
process for nding a value of c which results in the most
meaningful notion of similarity is beyond the scope of this
paper. For the purpose of our experiments, we found c = 1
to be appropriate.6

4.1 The IGrid+-index
The inverted representation also allows for an e cient cal-
culation ofcorrelation-based similarity. Foreachof thekdd
pseudo-attributes, we maintain the lists of strongly con-
nected pseudo-attributes. Note that since there are a total
of f k2d,d2

=cdkd=2 strong connections, it follows that
the size of the list for each of the dkd pseudo-attributes is
c on the average. We shall refer to these lists as the adja-
cency lists for each pseudo-attribute. Since the size of each
such list is so small typically less than 5, when c is cho-
sen to be 1, these lists can be maintained in main memory.

6
Since the expected value of PIDistX;Y is of the same
order as the size of the proximity set, a value of c = 1
balances PIDist; and CIDist; approximately.



124

Thus, the memory requirement for maintaining these adja-
cency lists is of the order of cdkd=2. The new expanded
index with these lists will be denoted as IGrid+. The same
hash table method is used in order to perform the similar-
ity computations in this case also except that we also need
to access all the inverted lists for all the pseudo-attributes
whicharestronglyconnectedtothepseudo-attributesinthe
target, and add one to the corresponding hash table entries
for those records. Thus, for each pseudo-attribute, we ac-
cess the adjacency lists of this pseudo-attribute, and ndall
the other pseudo-attributes which are strongly connected.
Then, the inverted lists of these strongly connected pseudo-
attributes are accessed. This will result in a performance
overhead factor of at most 1 +c, since for each pseudo-
attribute in the target, an additional c inverted lists may
need to be explored on the average. Thus, when c is chosen
to be 1, the total performance overhead is onlya factor of 2.
Since we know that the disk access percentage is inversely
proportional to dimensionality, it follows that for very high
dimensional data such a constant factor would always be
o set bytheasymptoticscalability behavior withincreasing
dimensionality.

4.2 Edge Effects of Grid Discretization
Theprocessofdiscretizingintorangeshasconsiderableedge
e ects because two adjacent intervals will contain values
which are very close to one another. These edge e ects can
beeliminatedbyusingasecondlevelofdiscretization, where
each of the kd discretized intervals are further subdivided
into l equi-depth ranges. The inverted list for each of the
l ranges is maintained separately. Thus, in this case there
will be a total of kd l inverted lists, each of which contain
N=kd  l Record Identi ers. For each target record, rst
the most re ned ranges corresponding to them are found,
and then the dl,1=2e inverted lists on either side of each
of these re ned ranges are explored. The resulting method
ensures that for each attribute in the target, the proximity
threshold explored on either side is symmetric. This elim-
inates the edge e ects associated with discretization. The
larger the value of l picked, the less the edge e ects; from
practical considerations we found the use of l =3 su cient.
The amount of data accessed by the technique is not af-
fected by this further level of discretization; the number of
lists explored is multiplied by a factor of l, whereas the size
of each of those lists is reduced by the same factor.

5. PROJECTED RANGE QUERIES
The primary focus of this paper is for developing an index
structure which shows e ective performance and qualita-
tive behavior for similarity search in high dimensional data.
However,theapplicabilityofthistechniqueextendstorange
queriesinlowdimensionalprojections; akindofquerywhich
becomes increasingly relevant for high dimensional data.
Thetraditionalmethodofperformingrangequeriesspeci es
full dimensional ranges a lower and upper bound for each
dimension, and uses these in order to nd the best match.
As the dimensionality increases, it becomes increasingly un-
likely that all ranges are relevant in a query. For example,
one may wish to specify only the ranges for a small number
of dimensions say 3 or 4, and for the remaining dimen-
sions, it is assumed that the entire range of values ought to
be considered. For such queries, traditional indexes such as
the pyramid technique are no longer applicable, since they
lead to the access of all the data.
The grid structure and inverted representation of the data
gives an easy technique for resolving such queries as well.
This is because for each projected range speci ed by the
user, one needs to examine only those lists whose ranges
have a non-zero intersection with the user-speci ed range.
For a given dimension, the union of all the records in the
corresponding equi-depthattribute grid ranges are relevant.
Then the intersection of the records for the di erent pro-
jecteddimensionsspeci edbytheuserresultsintherelevant
sets of records.
In order to consider the advantages of such a technique, let
us consider the case of 1000-dimensional data, in which the
level of discretization kd has also been chosen to be 1000.
Let us consider an example in which a user picks 4 of the
dimensions, and for each dimension, picks a range which
results in the access of a fraction q = 0:1 of the inverted
lists for that dimension. Thus, as a result the total fraction
of the inverted index accessed will be equal to 0.1*4 1000.
Thisisonlyequalto0:04ofthedata. Infact, ifweassume
that for most practical applications, users are only likely to
pick a small number of constant dimensions for projected
range queries irrespective of the data dimensionality, then
the performance of the range query will also be inversely
proportional to dimensionality. More speci cally, it is easy
ofverifyusingtheargumentssimilartothoseabovethatthe
fraction of the index accessed is equal to r q=d, where r
is the small number of projected dimensions speci ed by
the user, q is the average fractional speci city of each range
percentage of inverted lists accessed for that dimension,
and d is the total number of dimensions.

6. EMPIRICAL RESULTS
In this section, we will discuss the empirical results which
show thattheIGrid-indexis a meaningful andperformance
e cient technique for performing similarity search in very
high dimensional data. In particular, since our technique
changes the criterion for measuring similarity, it is very im-
portant to provide some understanding of the quality of the
nearest neighbor found. This presents considerable chal-
lengesbecauseoftheinherentdi cultyinmeasuringquality
and meaningfulness directly.
In order to measure the qualitative performance, we used
a technique which we refer to as the class stripping tech-
nique. We obtained some of the data from theUCI machine
learning7 repository. The data was rst cleaned in order to
take care of missing values, categorical attributes and non-
continuous values. These data sets correspond to classi ca-
tion problems consisting of a set of feature variables and a
specialvariablewhichisdesignatedastheclassvariable. We
stripped o the class variables from the data set and found
the = 5 nearest neighbors to each of the records in the
data set using di erentsimilarity methods. In each case, we
tested the number of records which matched with the class
variable of the target record. If a similarity method is poor
in discriminatory power, then it is likely to match unrelated
randomrecordsandtheclassvariablematchingisalso likely
7
http: www.cs.uci.edu ~ mlearn



125

Table 1: Meaningfulness Behavior of the nearest neighbor
Data Set Dimensions Random Euclidean PIDist IGrid PIDist+CIDist IGrid
+

Mechanical Analysis 8
60
293
354
386
Musk 160
65
255
636
671
Breast Cancer 14
1499
2535
2619
2671
Segmentation 19
144
688
755
802
Ionosphere 34
926
1371
1538
1606




0
100
200
300
400
500
600
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35




OVERALL DATA DIMENSIONALITY
RELATIVE
MEANINGFULNESS




Figure 1: Meaningfulness Behavior with increasing
dimensionality L
2
-norm

to be poor. This kind of behavior would be exhibited by a
random see rst column of Table 1 distance function in
which the distance between two records is a uniformly dis-
tributedrandomnumber. However,whenthenearestneigh-
bor is more meaningful in the feature space, it is also likely
to match the class variable closely. We concede that these
results are evidential in nature, since the exact relationship
between the feature variables and class variable is unknown
for these data sets. However, a consistent improvement on
the class variable accuracy by using the new measure for
locality in the feature space does tend to be strong evidence
for meaningfulnessofthenearestneighbor, sincethenearest
neighborwas foundwithoutanyinformationabouttheclass
variable. The above results were obtained using  = 1 and
c=1.
As we can see from Table 1, our similarity measures signi -
cantly increase the nearest neighbor accuracy; the addition
of correlation based measures improves the overall behav-
ior even further. Since the traditional indexes are designed
with respect to the Euclidean distance metric; this shows
that IGrid index will perform qualitatively at least as well
as other indices which rely on traditional distance norms.
Another interesting examination would be to test how
the meaningfulness measure de ned in 12 varies with in-
creasing dimensionality. In order to do so, we generated
uniform random distributions of N = 100 points in increas-
ing dimensionality, and tested both our similarity function
and the euclidean distance metric for meaningfulness. The
meaningfulnessratiowasde ned8 by
Dmaxd,Dmind
Davgd
. Thisis

8
Note that in the euclidean distance metric, lower numbers
imply greater similarity, whereas in our metric higher num-
0
100
200
300
400
500
600
5.2
5.4
5.6
5.8
6
6.2
6.4
6.6
6.8
7




OVERALL DATA DIMENSIONALITY
RELATIVE
MEANINGFULNESS




Figure 2: Meaningfulness Behavior with increasing
dimensionality PIDist




0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80




OVERALL DATA DIMENSIONALITY
ACCESS
COST
(PERCENT
OF
ORIGINAL
DATABASE
SIZE)
IGrid (Exact)
IGrid+ (Average Case)
VAFile Lower Bound




Figure 3: Performance Scalability Increasing Di-
mensionality




126

100
200
300
400
500
600
700
800
900
1000
0
2
4
6
8
10
12
14




OVERALL DATA DIMENSIONALITY
ACCESS
COST
(PERCENT
OF
ORIGINAL
DATABASE
SIZE)
IGrid (Exact)
IGrid+ (Average Case)
VAFile Lower Bound




Figure 4: Performance Scalability Increasing Di-
mensionality




0
10
20
30
40
50
60
70
0
2
4
6
8
10
12
14
16
18
20




OVERALL DATA DIMENSIONALITY
ACCESS
COST
(PERCENT
OF
ORIGINAL
DATABASE
SIZE)
IGrid or IGrid+




Figure 5: Projected Range Query Performance
w.r.t. Data Dimensionality




0
2
4
6
8
10
12
14
16
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35




NUMBER OF USER-SPECIFIED DIMENSIONS (TOTAL DIMENSIONALITY=1000)
ACCESS
COST
(PERCENT
OF
ORIGINAL
DATABASE
SIZE)
IGrid or IGrid+




Figure 6: Projected Range Query Performance
w.r.t. User-speci ed dimensionality
similar to that introducedin Section 1.2, exceptthat we use
the average distance value in the denominator for the sake
of robustness. This ratio converges to 0 with increasing di-
mensionality for large classes of data distributions 12. An
example of such a metric is the Euclidean distance metric
for which the meaningfulness ratio is illustrated in Figure
1. However, the PIDist function retains its meaningful-
ness see Figure 2 with increasing dimensionality because
it intentionally ignores the distant and noisy dimensions to
the target while measuring similarity. As predicted by our
theoretical analysis, the meaningfulness actually improves
slightly with increasing dimensionality. This is also guaran-
teed by our choice of discretization parameter which results
in the violation of Beyer's pre-condition for meaningfulness.
As evident from the results on the real data, this actually
improves the quality of similarity by measuring the number
andqualityofsimilarityofthelimitednumberofdimensions
onwhichthetworecordsareverysimilar,ratherthanletting
the sparse and distant dimensions dominate the similarity
function. When the dimensionality is high, this turns out
to be a more robust measure of similarity than traditional
distance norms.

6.1 Performance of Similarity Queries
In the previous subsection we discussed the qualitative be-
havior of the the nearest neighbor in terms of meaningful-
ness. In this section, we will discuss the performance of the
nearest neighbor method in terms of the percentage of data
accessed. An important observation is regarding the space
overheadinvolvedintheinvertedrepresentationofthedata.
The inverted representation of the data requires the storing
of the ID values on each list, as well as the exact coordinate
forthecorrespondingdimensioninthatIDvalue. Sinceeach
record occurs on exactly d of the dkd lists, it follows that
the total space required to store the inverted representation
is N d; exactly the same as the original database represen-
tation. However, the storage of both the coordinate value
and Record Identi er for each entry in the inverted repre-
sentation requires an additional 100 overhead. This over-
headis signi cant fromtheperformance point of view, since
greater storage in data representation translates to greater
access cost. Consequently, in all subsequent performance
charts, the performance access results have been presented
in terms of the size of the original data. Thus, if the entire
inverted index is accessed then the corresponding perfor-
mance coordinate on the chart would be 200. One of the
interesting observations about the IGrid-index is that the
exact access fraction of the index can be predicted analyti-
callyas2=ddeoftheoriginaldatabasesizeirrespectiveof
the nature of the underlying data distribution; in the case
of the IGrid+, the average case behavior is within a fac-
tor 1+c of the performance of the IGrid-index. The exact
curvefortheIGrid-indexandtheaverage-case curveforthe
IGrid+-index are presented in Figures 3 and 4 for di erent
data dimensionalities for the case when  =1 and c=1.
Since we present the results for very high dimensional data
whichisbeyondtherangeof traditional indexingtechniques
such as the X-Tree, we compare the results of our method
withthoseoftheVA-Fileforsimilarityqueries. TheVA-File
bers imply greater similarity. However, since our intention
is only to check the relative behavior of meaningfulness, we
can use the same function for both.



127

Table 2: Accuracy For Di erent Values of 

Accuracy 1-nearest neighbor
1
160
0.5
162
0.25
157
Euclidean
102

6 worksontheassumptionthatforveryhighdimensionali-
ties, spacepartitioningmethodsareoutperformedbysimple
sequential scan methods. Consequently, the VA-File com-
presses the data to about 12:5 ,25 of the original size,
and then uses the sequential scan on the compressed data
in conjunction with some overhead required for the con ict
resolution arising from the information lost in compression.
The relative time for con ict resolution increases with di-
mensionality for the method discussed in 6; therefore this
technique is not free of the dimensionality curse. In our ex-
periments, we compare the IGrid-index against the VA-File
lower bound, whichignoresthetimerequiredforcon ictres-
olution, and compares the least possible percentage of data
accessed by the VA-File versus the IGrid-index. Thus, in
reality, our results are likely to outperform the VA-File by
an even greater margin. As in the case of the qualitative
comparison with the Euclidean distance metric, we evalu-
ate both the indices IGrid and IGrid+. The index IGrid+
requires an additional amount of disk I O, since it also ac-
cesses the inverted lists corresponding to the strongly cor-
related pseudo-attributes. However, when the dimension-
ality is high enough, the IGrid-index always requires much
smaller amount of I O than the VA-File because of its in-
version with dimensionality. In fact, for all dimensionalities
30 or above, boththeIGridandIGrid+ indices outperform
theVA-File. Animportantpointtobenotedisthatbecause
of the e ect of increasing dimensionality on the data repre-
sentation and discretization, all of the performance results
are completely independent of the nature of the underly-
ing distribution and vary as 1=d. Details are illustrated in
Figures 3 and 4.

6.2 Parametric Stability
Our analysis in earlier sections indicates that the discretiza-
tion parameter kd = d  de should be linearly increasing
with dimensionality. Note that a choice of  equal to 0:5
or 1 creates a distance function which is quite similar to
the Lp-normfor 1-dimensional and 2-dimensional problems,
but is increasingly di erent for high dimensional problems.
These low dimensional cases in which the Lp-norm works
well provided us the reference point for picking  =1 in all
experiments of this paper. It is useful to see how stable the
nature of the similarity function was to small changes in the
value of this parameter. We would like to have a similarity
function which does not change too dramatically with small
changes in . The accuracy values of the IGrid-index for
the musk data set for di erent values of  are illustrated in
Table 2.The accuracy valuesdo notchangetoo dramatically
forvaluesofbetween0.25and1,thoughtheaccuracyvalue
of the Euclidean function is signi cantly di erent. On ex-
amining the actual points which were returned as the near-
est neighbors, we found that the set of nearest neighbors
returned for these di erent values of  were highly overlap-
ping 80,90, whereas the set of neighbors returned by
the Euclidean function was very di erent.
It is important to understand that an index which is built
for a particular value of  = 0 can automatically resolve9
queries for all values of  = 0= for any value of  1.
This is achieved by accessing the de closest ranges to the
current range for each dimension while performing the sim-
ilarity calculations. This means that it is desirable to con-
struct the index for larger values of , which automatically
provides the user the capability to interactively change the
similarity function in order to examine di erent kinds of so-
lutions. Suchinteractiveabilitymayprovidethekeytohigh
dimensional similarity searches which are often heuristically
de ned.

6.3 Performance of Projected Range Queries
We also study the behavior of the technique with respect
to projected range queries. For the purpose of projected
range queries, we picked n = 4 dimensions at random, and
speci ed a bounding rectangle whose side was drawn from
an exponential distribution with average side of 10 of the
total. The results for uniformly distributed data are illus-
trated in Figure 5. The use of equi-depth ranges ensures
that the curve can be generalized to arbitrary distributions
when the bounding rectangle is drawn in such a way that
the depth fraction of points enclosed in that range of each
sideof theboundingrectangleis drawnfromthesameexpo-
nential distribution. As we can see, the percentage of data
accessed rapidlydecreases with dimensionalityandis infact
inversely proportional to it. Other index structures cannot
handle such partial queries at all in high dimensional space,
since the entire range is relevant for the other dimensions.
Thisresultsinallthedatabeingaccessedinspacepartition-
ing methods. The improvementinbehavior of theprojected
range query with dimensionality is particularly pleasing in
light of the increased relevance of such queries in very high
dimensional data.
We also tested how the performance varied with increasing
number of projected dimensions picked by the user. The
average range speci city for each side was again chosen to
be 10 and the total numberof dimensions d was 1000. As,
we can see from Figure 6, even for a relatively large number
of dimensions speci ed from the range query, the speci city
of retrieval continued to be very high.

7. WHERE IS THE PARADOX?
Inthispaper,wediscussedtheIGrid-index,amethodwhose
performance improves with increasing dimensionality of the
data. These results are in contradiction with all the perfor-
mance results for other indexes in high dimensional space.
Thekeyhere istounderstandthatmostindexingstructures
and algorithms have been developed in the past based on
particular distance norms such as the Lp-norm, which have
natural physical interpretations in low dimensional space,
but are poor representations of similarity in high dimen-
sional space because of the noise e ects of sparsity. For
example, even though the euclidean distance metric has a
natural physical interpretation for spatial databases in 2- or
3-dimensions, the results of 12 show its meaninglessness in
9
This is only true for IGrid but not for IGrid+.



128

high dimensional space because of poor contrast in the dis-
tances to the di erent points. This is also the reason that
the high dimensional index structures and algorithms can-
notpruneawaylargesubsetsofpointseasily whilesearching
for the nearest neighbor; there is no discrimination to be-
gin with. In high dimensional data mining applications, the
notion of similarity itself is heuristical to begin with; there-
fore itmakessense to choose measures whichlead to greater
contrast between the di erent points. The understanding of
meaningfulnessof thenearestneighboriscritical indevelop-
ing distance measures which use only a small fraction of the
least noisy information available in high dimensional data
in order to measure similarity. The use of such a strategy in
order to measure similarity has a surprisingly pleasant side
e ect; in high dimensional space one has greater exibility
in picking the information which provides better statistical
evidenceofsimilarityratherthannoise; thereforebypicking
ahigherqualitythresholdforde ningtheproximityset, one
is able to continue to obtain meaningful nearest neighbors,
while improving the indexing performance.

8. REFERENCES
1 C. C. Aggarwal, A. Hinneburg, D. A. Keim. On The
Surprising Behavior of Distance Metrics in High
Dimensional Space. IBM Research Report, RC 21739,
2000.
2 C. C. Aggarwal et al. Fast Algorithms for Projected
Clustering. ACM SIGMOD Conference Proceedings,
pages 61 72, 1999.
3 C. C. Aggarwal, P. S. Yu. Finding Generalized
Projected Clusters in High Dimensional Spaces. ACM
SIGMOD Conference Proceedings, pages 70 81, 2000.
4 C. C. Aggarwal, J. L. Wolf, P. S. Yu. A New Method
For Similarity Indexing of Market Basket Data. ACM
SIGMOD Conference Proceedings, pages 407 418, 1999.
5 S. Arya. Nearest Neighbor Searching and Applications.
Ph. D. Thesis, University of Maryland, College Park,
MD, 1995.
6 R. Weber, H.-J. Scheck, S. Blott. A Quantitative
Analysis and Performance Study for Similarity Search
Methods in High Dimensional Spaces. VLDB Conference
Proceedings, pages 194 205, 1998.
7 N. Beckman, H.-P. Kriegel, R. Schneider, B. Seeger.
The R*-Tree: An E cient and Robust Method for
Points and Rectangles. ACM SIGMOD Conference
Proceedings, pages 322 331, 1990.
8 K. P. Bennett, U. Fayyad, D. Geiger. Density-Based
Indexing for Approximate Nearest Neighbor Queries.
ACM SIGKDD Conference Proceedings, pages 233 243,
1999.
9 S. Berchtold, C. Bohm, H.-P. Kriegel. The Pyramid
Technique: Towards Breaking the Curse of
Dimensionality. ACM SIGMOD Conference Proceedings,
pages 142 153, 1998.
10 B.-U. Pagel, F. Korn, C. Faloutsos. De ating the
Dimensionality Curse Using Multiple Fractal
Dimensions. ICDE Conference Proceedings, pages
589 598, 2000.
11 S. Berchtold, D. Keim, H.-P. Kriegel. The X-Tree: An
Index Structure for High Dimensional Data. VLDB
Conference Proceedings, pages 28 39, 1996.
12 K. Beyer et al. When is Nearest Neighbors
Meaningful? ICDT Conference Proceedings, pages
217 235, 1999.
13 G. Das, H. Mannila, P. Ronkainen. Similarity of
Attributes by External Probes. KDD Conference
Proceedings, pages 16 22, 1998.
14 S. Deerwester et al. Indexing by Latent Semantic
Analysis. Journal of the American Society for
Information Science, 416: pages 391 407, 1990.
15 U. Shaft, J. Goldstein, K. Beyer. Nearest Neighbor
Query Performance for Unstable Distributions. Technical
Report TR 1388, University of Wisconsin at Madison,
1998.
16 V. Ganti, J. Gehrke, R. Ramakrishnan. CACTUS-
Clustering Categorical Data Using Summaries. ACM
SIGKDD Conference Proceedings, pages 73 83, 1999.
17 A. Gionis, P. Indyk, R. Motwani. Similarity Search in
High Dimensions via Hashing. VLDB Conference
Proceedings, pages 518 529, 1999.
18 A. Guttman. R-Trees: A Dynamic Index Structure for
Spatial Searching. ACM SIGMOD Conference
Proceedings, pages 47 57, 1984.
19 A. Hinneburg, C. C. Aggarwal, D. A. Keim. What is
the Nearest Neighbor in High Dimensional Spaces?
VLDB Conference Proceedings, 2000.
20 R. Jain, D. A. White. Similarity Indexing: Algorithms
and Performance. SPIE Storage and Retrieval for Image
and Video Databases IV, 2670: pages 62 75, 1996.
21 I. Kamel, C. Faloutsos. Hilbert R-Tree: An improved
R-Tree Using Fractals. VLDB Conference Proceedings,
pages 500 509, 1994.
22 N. Katayama, S. Satoh. The SR-Tree: An Index
Structure for High Dimensional Nearest Neighbor
Queries. ACM SIGMOD Conference Proceedings, pages
369 380, 1997.
23 K.-I. Lin, H. V. Jagadish, C. Faloutsos. The TV-tree:
An Index Structure for High Dimensional Data. VLDB
Journal, 34: pages 517 542, 1994.
24 N. Roussopoulos, S. Kelley, F. Vincent. Nearest
Neighbor Queries. ACM SIGMOD Conference
Proceedings, pages 71 79, 1995.
25 G. Salton, M. J. McGill. Introduction to Modern
Information Retrieval. Mc Graw Hill, New York.
26 H. Samet. Design and Analysis of Spatial Data
Structures. Addison Wesley, 1989.
27 T. Sellis, N. Roussopoulos, C. Faloutsos. The R+
Tree: A Dynamic Index for Multidimensional Objects.
VLDB Conference Proceedings, pages 507 518, 1987.
28 D. A. White, R. Jain. Similarity Indexing with the
SS-Tree. ICDE Conference Proceedings, pages 516 523,
1996.



129

