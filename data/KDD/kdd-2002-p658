Discovery Net: Towards a Grid of
Knowledge Discovery


V. (~urSin M. Ghanem
Y. Guo M. K6hler
A. Rowe J. Syed
P. Wendel
ImperialCollegeof Science,Technologyand Medicine
180 Queen'sGate, London
mmg@doc.ic.ac.uk


ABSTRACT

This paper provides a blueprint for constructing collabo-
rative and distributed knowledge discovery systems within
Grid-based computing environments. The need for such sys-
tems is driven by the quest for sharing knowledge, informa-
tion and computing resources within the boundaries of sin-
gle large distributed organisations or within complex Virtual
Organisations (VO) created to tackle specific projects. The
proposed architecture is built on top of a resource federation
management layer and is composed of a set of different re-
sources. We show how this architecture will behave during a
typical KDD process design and deployment, how it enables
the execution of complex and distributed data mining tasks
with high performance and how it provides a community
of e-scientists with means to collaborate, retrieve and reuse
both KDD algorithms, discovery processes and knowledge
in a visual analytical environment.


1.
INTRODUCTION
This paper proposes an architecture to support the KDD
process in a Grid-enabled distributed computing environ-
ment. The approach is generic but originates from the needs
of the knowledge discovery processes in the bioinformatics
industry, where complicated data analysis processes are con-
structed using a data-pipelined approach. At different stages
of the discovery pipeline researchers need to access, integrate
and analyse data from disparate sources, in order to use that
data to find patterns and models, and feed these models to
further stages in the pipeline. At each stage, new analysis is
conducted by dynamically combining new data with previ-
ously developed models.
As a motivating example, consider an automated labora-
tory experiment, where a range of sensors produces large
volumes of data about the activity of genes in cancerous
cells. A short time series is produced that records how each




Permission to make digital or hard copies of all or part of this work for
personal or classroomuse is granted without fee provided that copies are
not made or distributedfor profitor commercialadvantageand that copies
bear this noticeand the full citationon the firstpage:To copy otherwise,to
republish,topost on serversorto redistributeto lists,requiresprior specific
permissionan~or a fee.
Copyright2002ACM 1-58113-567-X/02/0007...$5.00.
gene responds to the introduction of a possible drug. The ini-
tial requirement of the analysis is to filter interesting time
series from uninteresting ones; one approach is to use clus-
tering [5]. If a group of interesting genes is found then a
crucial step in the scientific discovery process is to verify
ff the clusters can be explained by referring to existing bi-
ological knowledge. Bioinformatics researchers have made
available a significant amount of information on the Internet
about various biological items and processes (Genes, Pro-
teins, Metabolism and Regulation). These semi-structured
resources can be accessed, from remote online databases over
the Internet, through a range of search mechanisms, includ-
ing a key based lookup to biosequence similarity searches.
The need to integrate this information within the discovery
process is inevitable since it dictates how the discovery may
proceed. Furthermore, recording an audit trail of how this
was acquired and used is essential since it allows researchers
to document and manage their discovery procedures, re-use
the same procedure in similar scenarios, and in many cases it
may help them in managing intellectual property activities
such as patent applications, peer reviews and publications.
Another feature of such discovery pipelines is that the
analysis components used can be tied to remote computing
resources, e.g. similarity searches over DNA sequences exe-
cuting on a shared high performance machine. New services
and tools for executing similar operations are continually
being made available over the Internet for access by various
researchers. Also, the discovery process itself is almost al-
ways conducted by teams of collaborating researchers who
need to share the data sets, the results derived from these
data sets and, more importantly, details about how these
results were derived.
This data-pipelined approach is gaining grounds beyond
life sciences, where similar needs arise for cross-referencing
patterns discovered in a dataset, with patterns and data
stored in remote databases, and for using shared high per-
formance resources. Examples abound in the analysis of het-
erogeneous data in fields such as geological analysis, environ-
mental sciences, astronomy, and particle physics. Support-
ing the data-pipelined knowledge discovery process requires
KDD tools that flexibly operate in an open system allowing:

· Dynamic retrieval and construction of data sets,

· Execution of data mining components on distributed
computing servers,




658

· Dynamic integration of new servers, new databases
and new algorithms within in the KDD process.

The above requirements can be contrasted to the ser-
vices offered by existing tools that mainly focus on extract-
ing knowledge within closed systems such as a centralised
database or a data warehouse where all the data required
for an analysis task can be materialised locally at anytinm,
before feeding them to data mining algorithms and tools
that were predefined at the configuration stage of the tool.
Recently [12], the Grid, a novel IT infrastructure, has been
proposed to provide a well-defined resource management in-
frastructure for virtual organisations (VO) and allow end
users to share both information and computing resources in
secure environments. Grid concepts and tools offer a flexible
but secure computing infrastructure that meets some of the
requirements of the data-pipelined knowledge discovery pro-
cess. However, a gap still exists between the services offered
by Grid-based methodologies and the requirements of the
KDD process.
This paper proposes a service layer and architecture that
aims to make best use of existing KDD practice [1] and tools
[2, 8] as well as making use of existing Grid infrastructure
and concepts.so as to bridge the gap between the traditional
KDD process (from its definition to its deployment as an
application for knowledge discovery) and its mapping onto
the VO's resources.


2.
BACKGROUND
Increased research demands in fields such as high energy
particle physics, astronomy and environmental modelling
led to the exploitation of fast networks and large-scale dis-
tributed computing techniques. This at first took place ex-
clusively within the scientific community (Seti project [19])
where individual computers were given processing tasks in
their 'spare time' with the results being assembled in one
centralised location. Very soon this idea of a global comput-
ing platform was dubbed 'The Grid', and numerous research
groups (many of which are now in the Global Grid Forum
]15]) started devoting time and effort to developing archi-
tectures to support this new infrastructure.
Initial Grid work was directed purely towards 'processor-
stealing' algorithms, concentrating on hardware resources
that could be shared, like CPU, memory and bandwidth,
but this soon changed to encompass a far broader, softer,
class of resources, like programs, data sources, knowledge
repositories etc. In their seminal paper [12} Foster, Kessel-
man & Tuecke state that the actual problem that the Grid
is trying to solve is 'coordinated resource sharing and prob-
lem solving in dynamic, multi-institutional virtual organisa-
tions', where virtual organisations are considered to be any
formal or informal communities that are sharing a certain
set of resources under some well-defined rules. Furthermore,
they define the layers of a Grid architecture with lower levels
providing middlewaxe support for higher level application-
specific services, thereby opening the door to the develop-
ment and porting of more ambitious systems, like Knowledge
Discovery or bioinformatics platforms [16], onto the Grid.
At the present stage, most of the current Grid middleware
products suffer from lack of interoperability and an insuffi-
cient focus on the software dimension of the problem. Even
The Globus Toolkit 2110], widely acknowledged to be the
most sophisticated middleware available, does not run on
Windows, so we are still far from being able to completely
abstract over any middleware platform in a commercially
viable system. Still, there are some running grids that can
be useful sources for further information, such as European
Data Grid [6], EuroGrid [7], NASA Information Power Grid
[17] and a number of university projects.
Fortunately, the next version of Globus (GT3) will include
an Open Grid Services Architecture (OGSA) [11] imple-
mentation that combines the Grid work performed with the
broad experience of Web service technologies [21] to provide
an industry-usable platform. Using WSDL [22] and UDDI
[20] it effectively provides the service-based IT infrastruc-
ture level that can be used in constructing and providing
application layer of Knowledge Discovery and Data Mining
services.
Regardless of the implementation details, it is our belief
that the next generation of data mining tools will be run-
ning on the Grid, therefore the KDD community should
participate in the project and influence the work so as to
ensure that the resulting environment will be well-suited to
its needs as well. The aim of this paper is to propose a way
forward and provide the first step towards the next genera-
tion of data mining architectures, Grid enabled and capable
of meeting the challenges posed by the evolved distributed
computing landscape.


3.
KNOWLEDGE DISCOVERY SERVICES
In the context of this paper, we use the term Knowledge
Discovery Service to describe the building blocks used in a
data-pipelined KDD process. As with the traditional KDD
process[91 the Grid-based version spans all activities from
data collection to modelling and deployment. This definition
thus encapsulates KDD algorithms as well as components
that extract data from a database. Regarding these compo-
nents as services is essential since it allows us to separate
their definition from their implementation. In this paper,
we also use the term Knowledge to be any structure that a
Knowledge Discovery Service needs as input or generates as
output.

3.1
Service Types
We classify knowledge discovery services into two cate-
gories: Computation servicesand Data services. A computa-
tion service allows users to define and compose their analysis
processes by assembling together data preparation and data
mining algorithms. In contrast, a data service allows users
to define their analysis data set as a composition of rela-
tional tables queried from databases and other data sources
available from the VO which can augment the training set
with additional information.

3.1.1
Computation Services
A computation service can be seen as a classical KDD al-
gorithm. Its input and output can be any number of Knowl-
edge objects. Each of its inputs can be linked one or more
other service's output allowing the definition of distributed
and parallel data mining processes. Since a computation ser-
vice has an implementation, it can also have resources con-
straints such as:

· Location: A service might be bound to a particular re-
source as either its implementation is platform-specific
(e.g. a high performance parallel implementation) or




659

that the service is licensed only for a particular ma-
chine.

· Platform/OS constraints: Even if a service is not
bound to a particular location and its implementation
can be downloaded, it might execute only on a specific
platform or operating systems.

3.1.2
Data Services

In contrast to computation services, which enable the com-
position of functions as discovery processes, data services
do not provide implementations, instead they provide meta-
information for composing data sets from heterogeneous and
distributed data sources.
Data services are used to model data for analysis. In a
large VO the required data is produced from different de-
vices and is retrieved through different protocols (RDBMS,
XML on Web servers, etc...) in different locations. The infor-
mation required for a specific task is a composition of these
sources, in effect an inner-join operation over heterogenous
and distributed data. A data service describes its functional-
ity using metadata descriptors for the source of information
it will attempt to use in order to materialise the data set.
The metadata also describes the form of the data in terms
of the different features and their data types. Features from
different data services can be extracted and composed in a
similar way to computation services to create a Knowledge
Schema which is effectively a composition of data services.
This schema can then be incorporated at any point of the
discovery process.

3.1.3
Service type
In order to de-couple resources from service definitions
and service implementations, we define four types of services
that are applicable to both computation and data services:

· A Resource-bound service (e.g. parallel implementa-
tion, optimised native code, a special data source or
data sinks)

· A Resource-free
service (e.g. pure Java code for al-
gorithms, simple data processing and visualisation or
standard input )

· A Template service, that will be matched, at execution
time, to a service (either machine-dependant or free).

· A Composed service (e.g. a composition of services
which can also be seen as a discovery process)

3.2
Service composition
Any non-triviai discovery process requires the composition
of existing services to create discovery processes by combin-
ing different services into larger, more complicated, ones. In
a Grid-based environment, this may involve the augmenta-
tion of a training data set using a data service, accessing
external data sources that are driven by the knowledge ex-
tracted by the computation service, etc. In this case, data
services can be composed to create a view of the data that
is used for analysis and to compose computational services
to develop an analysis pipeline.
Composition may be specified by the end-nser through a
GUI for component composition that provides access to a li-
brary of services including data importation, data cleaning,
data normalisation, mining and statistical algorithms, data
selection and data exportation. Internally, however, allow-
ing such composition requires the use of a service definition
language.

3.3
Description and registration
Each of the services provided to a user can have several im-
plementations; e.g. specialised implementations for specific
platforms. Each service must thus be catalogued and cate-
gorised, afiowing the user to browse the available services, or
even locate and retrieve them through queries. This requires
each discovery process to be well-defined through a descrip-
tor registered with a specific server. This registration mech-
anism allows new services to be added to system through
service descriptors including: Input and Output types; Pa-
rameters, Constraints (range of validity, discrete set of possi-
ble values, optional or required, ..); Registration information
and Service type.
The registration information can include the following:

· Factory: An object that allows a client to retrieve a
reference to the service, or to download the service if
it can be instantiated on the client machine.

· Category: Define the location of the service in the
hierarchy of services available.

· Keywords: Used to index and retrieve this service, a
set of keywords is associated with it.

· Description: A human-readable description of the
functionality provided by the service.

3.4
Service location resolution
The location where each service of a discovery process is
executed may have a strong impact on the overall perfor-
mance of the process execution. When dealing with very
large data sets, it is more efficient to keep as much of the
computation as near to the data as possible. Therefore a
location resolution method must be provided to take that
particular constraint into account. This service / location
resolution method can be achieved as follows:

1. Recursively match any template service with a well-
defined service, favouring if a choice needs to be made,
resource-bound services over other types as they would
be either native or high-performance implementations.

2. For any remaining resource-free service, the location is
decided based on the location of preceding services. If
a conflict arises because more than one service precede
then the best resource (as defined and chosen by the re-
source management for components layer) is selected.
For that, we plan to use the ICENI infrastructure de-
veloped at the London e-Science Centre [13].


4.
DISCOVERY NET IMPLEMENTATION
Our architecture is built on top of the basic Grid ser-
vices, and therefore we will assume throughout that this
lower layer of services, dealing with security and communi-
cation issues, is robust and stable. However, this does not
mean that the data available on one of the grid's computa-
tional node can be moved to another node since the data set
itself can have restricted permissions. Neither does it mean
that the connection is necessarily fast enough to favour mov-
ing data to another machine instead of mining it locally.




660

Client
Discovery Services




]Meta-informaUon~'"~-"
participating~
Server
Resources
Computational



Data

KEGG




Figure i: Components of the Discovery Net



4.1
Overall architecture
Figure 1 shows the different components of the Discov-
ery Net architecture. In the shown diagram, the user uses
client tools to construct and define his discovery procedures.
Knowledge Servers are knowledge bases which allow one user
to store / retrieve or publish knowledge. The Resource Dis-
covery Server (Resource Service Lookup Server) is a knowl-
edge base of published service definitions. It also performs
the role of resource resolution server, as resources are never
requested directly from a client but only through a request
to resolve the location of a particular service (if this ser-
vice is not bound to a particular location or is a template
service). The Discovery Meta-information Server is used to
store information about each type of knowledge.

4.2
Representing processes
A standard means is required to allows users to specify
the required tasks This representation should be exteusible
with respect to the discovery services it can specify, as well
as not being bound to specific resources as these will be
resolved by the lookup server. To meet this need we devel-
oped an XML-based language called called Discovery Pro-
cess Markup Language (DPML).
DPML represents a process as a data flow graph of nodes,
each representing a service. DPML is typically authored us-
ing a visual programming interface. Each node descriptor
contains three parts: service parameters (identifier and user
set parameters) history (record of past parameter settings)
and notes (user comments about operation).
For the purpose of executing tasks only the first section of
this descriptor is necessary. However, our implemented client
software allows users to annotate their process descriptions
and automatically record how parameters are set to create
a comprehensive representation of the discovery process.
The semantics of the service descriptor are defined by the
owner of the namespace specified for the service parameters.
This allows for an unambiguous description of what a service
provides and for multiple equivalent implementations to be
mapped to it using the lookup service. An example node
descriptor is shown in Figure 2 for a simple pre-processing
operation that deletes columns from its table input.

4.3
Resource Discovery Server
The resource discovery server (aka lookup server) is a cen-
tral part of the Discovery Net architecture as it allows the
services to register their descriptors, the other components
of the architecture to emphbrowse/ retrieve these descrip-
<node idffi"399876356"
name-"Decision
Tree">
<DTree xmlns-"http://~w.doc.ic.ac.uk/kensington"
algorithm-"KTree"
weight-"2"
confldence-"25">
<input>
<attribute namem"age " typef"continuous"
/>
<attribute nameffi"sex" typem"categorical"
/>
<attribute nameffi"heart_rate" ¢ype~"continuous"
/>

</input>
<output>
<attribute name-"healthy" type-"categorical"
/>
</output>
</DTree>
<history>
<change username-"demo"
date-"2001-08-17 16:06:10"
commentffi"Node created" />
<change usernamef"demo"
date-"2001-08-1? 16:06:12"
property-"Confidence"
old-"[25]" new-"J26]" />
</history>
<notee>
<note username-"demo" date-"2001-08-17
16:06:20">
Creating classifier to characterise healthy patients.
</note>
<process name-"CRISP-DM"
step-"4.3 Build Model" />
<execution date-"2001-08-1?
16:07:02"
duration-"O0:00:lS"
/>
<location x-"l?4" y-"187" />
</notes>
</node>


Figure 2: DPML Representation of a computation
service



tors, the resolution of the location of services to be deployed
and mapped on computational resources for execution.
As described in section 3.1.3, some services might not be
bound to a particular resource, therefore any resource willing
to participate and provide computational power must addi-
tionally register with the Resource Discovery Server which is
then responsible for resolving the service location if needed.
An overview of the location resolution method was presented
in Section 3.4.

4.4
Knowledge Server
The Knowledge Server stores and provides access to dis-
covery processes performed within the Discovery Net. It
warehouses the VO's process information (past experience)
and allows this knowledge to be re-used. Its three main func-
tions are:

Storage service The server acts as a repository for the VO
allowing users to store and retrieve their processes.

Reporting service The server can provide the knowledge
stored in many human- readable formats by transform-
ing the warehoused data.

Application generation service Users are able to easily
package their existing procedures as services in their
own right, 'deploying' their process as an application
on the resource discovery server. This involves specify-
ing which properties from the user's original procedure
they wish others to modify at run-time.

4.4.1
Process repository
By archiving all the processes stored within the VO the
knowledge servers contain a structured representation of the




661

activities and knowledge of the VO. We contrast with the
ERP system SAP [18] where a range of static business pro-
cesses are already implemented. The activity of discovery, by
its very nature, does not have a set of universal pre-existing
processes for finding knowledge: it is the role of the VO to
create these. The VO's knowledge servers provide a means
for creating such a resource dynamically.

4.4.2
Dynamic reporting
The DPML format allows additional information to be
stored about how a discovery process has been authored (a
complete change history), the motivation and justification
for why operations were performed, as well as the tagging
of steps to indicate that a process model or methodology
was being followed. Since we store information in an XML
based format we are able to transform our process data into
human-readable reports on demand. These reports are pro-
vided over the web and can include applets and other inter-
active elements to allow users to explore discoveries. Addi-
tionally, this store of structured information enables meth-
ods to find useful knowledge about how an organisation con-
ducts discoveries, recta-mining the repository for insight into
how people make discoveries.

4.4.3
Application deployment
Once users have constructed successful processes they wish
to be re-used they can publish them as new services. This
involves two steps.
First, the parameters of the process that they wish to be
modified at runtime need to be defined. This is analogous
to declaring properties for the deployed application 'compo-
nent '.
Second, the new application service needs to be regis-
tered with the lookup server.
Once registered this new process can be used as a service
in its own right or as a part of a more complex process.
To enable access for users who have no particular client
software (eg bench scientists) these published procedures
can be accessed via the web using a familiar forms based
interface.
As an additional feature, each deployed DPML procedure
can be wrapped within a WSDL (Web Services Description
Language) component to make use of the existing web ser-
vice technologies and enable deployment over Grid-based
OGSA services. This is achieved by providing the WSDL
descriptor for each component.

4.5
Meta-information Server
The Meta-Information(MIS) server is responsible for the
management of data types used by services in the Discov-
ery Net. Data types are represented as different type defi-
nitions for operations in the system. For example, an ana-
lytical component can define the output of its operation as
a PMML[3] model. Similarly data sources available in the
system can register database metadata to the MIS so that
the data service can be type-checked, composed and finally
queried. The main roles of the Meta-Information Server are:

Storage service Type definitions are stored when a new
type definition is registered. The resource discovery
server and clients are able to retrieve these definitions
to check types and browse the available data services.

Type checking To verify that a composed data analysis
process had the correct input types for each operation.

Data composition To allow complex data views to be ob-
tained by selecting features from different databases
and joining them in a single data source.

4.5.1 TypeDefinition
Data services in the discovery process differ in structure,
implementation and content. The highest level of structure is
the Relational Database where tables of known types are or-
ganised uniformly. Semi-structured databases include collec-
tions of documents written in a mark-up language or other
pre-defined format. Unstructured databases include collec-
tions of Free text or image files. A mixture of these different
types of data services is required in some scientific processes.
The current version of the MIS uses the XML Schema lan-
guage to model type definitions. The semi-structured nature
of XML allows the easy modelling of tabular data, and also
has the benefit of being able to model highly nested and un-
structured databases. The XML schema language also pro-
vides the necessary typing mechanisms required for accurate
recta-data provision.
The mapping to and from the databases that do not have a
natural XML format is performed by wrappers. A database
wrapper is a transformation layer that operates between the
databases and the Discovery Net. The wrapper transforms
the data from its natural form into the XML representation
that was described by the schema representing the database.

4.5.2
Data Service Composition - Knowledge Schema
A Type Definition describes the available features from
a specific database. When browsing the Meta-Information
Server, the client's view is of a single, large, virtual ware-
house of data. This contains a collection of data sources that
are each described by their individual Type Definitions. A
data analysis task is unlikely to require all of the data avail-
able within the Discovery Net, so the role of the Knowledge
Schema is to represent a selection of features from different
information sources.
A Knowledge Schema is generated by selecting the indi-
vidual features from the Type Definition and using them
as end user information, eg. a description of a biological
function, or for cross-referencing them with another type
definition from another database. Once the complex schema
required for the specific task has been built and the user has
specified the information required for the interpretation of
the analysis task, a personal warehouse has been described.
To realise the individual warehouse the task is passed back
to the computation grid services that perform information
retrieval tasks. The computation processes that are involved
in the rea~sation include Federated Database approaches
such as Discovery Link[4] from IBM or the functional signa-
ture approach seen in geneticXchange[14].


5.
SUMMARY AND CONCLUSIONS
In this paper, we have presented the Discovery Net, an
architecture that provides a recta-level interface to data
sources within the Grid infrastructure, and described how it
can be used to fit traditional KDD processes into this new
environment. Our approach is based on the service model
concept that allows any service-based architecture of the
emerging Grid technologies (e.g. Jini/Globus/OGSA) to be
used. The Knowledge Discovery Services presented here are




662

the building blocks of the KDD process. They are imple-
mented using multiple strategies but only require a lookup
mechanism for effectively determining their optimal execu-
tion strategy; this can be achieved using existing Grid mid-
dleware technologies.
A prototype of the proposed architecture is currently be-
ing implemented within the Discovery Net project, a UK
e-science pilot project for using new Grid-based architec-
tures as an infrastructure for knowledge discovery services
in global collaborative research projects.




Figure 3: Composed services in systems biology


Figure 3 illustrates a composed service for an applica~
tion in systems biology research producing an integrated
genomics , proteornics and metabonomics model for insulin
resistance. In this application, multi-mode biology informa-
tion are dynamically integrated and analysed. More than
10 discovery algorithms are used, including sequence anal-
ysis, gene expression clustering (using K-means, SOM and
EM algorithms), gene expression classification for disease
profiling, metabolic pathway search, literature analysis, im-
age processing for 2D-gel protein identification,spectroscopy
analysis for protein sequences and cell level metabolites ac-
tivities, PCA and PLS based analysis for the correlations
of gene expression and expression metabolites. This novel
integrated analysis utilises the flexible service composition
functionalityof Discovery Net.
Our immediate goals include deployingour system in fur-
ther real-world situations and observing the practical diffi-
culties that are absent from the development environment.
Furthermore, we are considering implementing some ele-
ments of the Grid middleware ourselves to achieve better
support for KDD resources, hoping that it will eventually
become a part of a global standard.


6.
ACKNOWLEDGEMENTS
We would like to thank the EPSRC, the DTI, the UK e-
Science core programme and InforSense for supporting this
work.


7.
REFERENCES
[1] P. Chapman, J. Clinton, T. Khabaza, T. Reinartz,
and R. Wirth. The CRISP-DM process model, March
1999.
[2l Jaturon Chattratichat, John Darlington, Yike Guo,
Stefan Hedvall, Martin Kohler, and Jameel Syed. An
architecture for distributed enterprise data mining. In
Proceedings of the 7th Conference on High
Performance Computing and Networking Europe,
1999.
[3] The Data Mining Group. [http://www.dmg.org].
[4] Discovery link
http: / /www.ibm.com/solutions/lifesciences/.
[5] M. Eisen, P. Spellmaaa,P. Brown, and D. Botstein.
Cluster analysis and display of genomewide expression
patterns. Proc. Natl. Acad. Sci., 95:14863-14868, 1998.
[6] European datagrid project,
http://www.eu-datagrid.org/.
[7] Eurogrid, http://www.eurogrid.org/.
[8] Usama Fayyad. Knowledge discovery in databases: An
overview. In Nada Lavr~ and S~o D~eroski, editors,
Proceedings of the 7th International Workshop on
Inductive Logic Programming, volume 1297 of LNAI,
pages 3-16, Berlin, September 17-20 1997. Springer.
[9] Usama Fayyad, Gregory Piatetsky-Shapiro, and
Padhraic Smyth. Knowledge discovery and data
mining: Towards a unifying framework. In Proceedings
of Second International Conference on Knowledge
Discovery and Data Mining. AAAI Press, 1996.
[10] Ian Foster and Carl Kesselman. The globus toolkit. In
Ian Foster and Carl Kesselman, editors, The Grid:
Blueprint for a New Computing Infrastructure, pages
259--278. Morgan Kaufmann, San Francisco, CA, 1999.
Chap. 11.
[11] Ian Foster, Carl kesselman, Jeffrey M. Nick, and
Steven Tuecke. The physiology of the grid an open
grid services architecture for distributed systems
integration. Technical report, 2002.
[12] Ian Foster, Carl Kesselman, and Steven Tuecke. The
anatomy of the Grid: Enabling scalable virtual
organization. The International Journal of High
Performance Computing Applications, 15(3):200-222,
Fall 2001.
[13] Nathalie Furmento, Anthony Mayer, Stephen
McGough, Steven Newhouse, and John Darlington. A
component framework for HPC applications. Lecture
Notes in Computer Science, 2150, 2001.
[141 geneticxchange http://www.geneticxchange.com/.
[15] Global grid forum, http://www.gridforum.org/.
[16] Carole Goble. The low down on e-science and grids for
biology. Comparative and Functional Genomics, pages
365-370, 2001.
[17] Nasa power grid, http://www.ipg.nasa.gov/.
[181 Sap http://www.sap.com/.
[19] Seti institute, http://www.seti.org/.
[20] Uddi http://www.uddi.org.
[21] Web services technology
http://www.w3.org/2002/ws/.
[22] Web service description language
http://www.w3.org/tr/wsdl.




663

