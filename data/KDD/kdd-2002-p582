A Robust and Efficient Clustering Algorithm based on
Cohesion Self-Merging

Cheng-Ru Lin and Ming-Syan Chen
Department of Electrical Engineering
National TaiwanUniversity
Taipei, Taiwan, R.O.C.
mschen@cc.ee.ntu.edu.tw, owenlin@arbor.ee.ntu.edu.tw


ABSTRACT

Data clustering has attracted a lot of research attention in
the field of computational statistics and data mining. In
most related studies, the dissimilarity between two clusters
is defined as the distance between their centroids, or the dis-
tance between two closest (or farthest) data points. How-
ever, all of these measurements are vulnerable to outliers,
and removing the outliers precisely is yet another difficult
task. In view of this, we propose a new similarity measure-
ment, referred to as cohesion, to measure the inter-cluster
distances. By using this new measurement of cohesion, we
design a two-phase clustering algorithm, called cohesion-
based self-merging (abbreviated as CSM), which runs in lin-
ear time to the size of input data set. Combiningthe features
of partitional and hierarchical clustering methods, algorithm
CSM partitions the input data set into several small subclus-
ters in the first phase, and then continuouslymerges the sub-
clusters based on cohesion in a hierarchical manner in the
second phase. As shown by our performance studies, the
cohesion-based clustering is very robust and possesses the
excellent tolerance to outliers in various workloads. More
importantly, algorithm CSM is shown to be able to cluster
the data sets of arbitrary shapes very efficiently, and provide
better clustering results than those by prior methods.
Index Terms: Data mining, data clustering, hierarchical
clustering, partitional clustering


1.
INTRODUCTION
Data mininghas attracted a significantamount of research
attention due to its usefulness in many applications, includ-
ing selective marketing, decision support, business manage-
ment, and user profile analysis, to name a few [3]. Among
others, data clustering is one of the most active research
areas [8]. The data clustering techniques can be used to
perform similarity search, pattern recognition, trend analy-
sis, grouping, classification, and so forth [3].
In general, there are two types of attributes associated




Permissionto makedigitalor hardcopiesof all or pai-tof this workfor
personalor classroomuse is grantedwithoutfee providedthatcopiesare
not made or distributed for profit or commercial advantage and that copies
bearthisnoticeand the fullcitationonthefirstpage. Tocopyotherwise,to
republish,topostonserversortoredistributetolists,requirespriorspecific
permissionand/ora fee.
SIGKDD'02 Edmonton,Alberta,Canada
Copyright2002ACM1-58113-567-X/02/0007...$5.00.
with data clustering, i.e., numerical attributes [2][14][16] and
categorical attributes [6][15]. Numerical attributes are those
with finite or infinite number of ordered values, such as the
height of a person or the x-coordinate of a point on a 2-D
domain. On the other hand, categorical attributes are those
with finite unordered values, such as the occupation or the
blood type of a person. In this paper, same as in most prior
works, we focus only on the clustering of numerical data.
Many data clustering algorithms have been proposed in
the literatures. These algorithms can be categorized into
nearest neighbor clustering, fuzzy clustering, partitional clus-
tering [9], hierarchical clustering, artificial neural networks
for clustering [7], and so on. Hierarchical clustering algo-
rithms can usually find satisfiable clustering results. With
a hierarchical structure of the input data set, a hierarchi-
cal algorithm is able to obtain different clustering results for
different similarity requirements. However, most of the hier-
archical algorithms are very computationaUy intensive and
require much memory space. Both of the two well-known hi-
erarchical algorithms, i.e., single-link and complete-link al-
gorithms, require the time complexity of O (n2log n) and
the space complexity of O (n2), where n is the number of
input points. Algorithm CURE [5], which is an improve-
ment of the single-linkalgorithm, though being able to lead
to good clustering results, is in essence very costly in its
computational overhead. Despite the sampling technique
may be employed to accelerate the computation, sampling
has an uncertainty in its applications, and suffers from the
drawback of not being able to keep all features of the original
data set [13].
On the other hand, most partitional clustering algorithms
run in linear time, i.e., of time complexity O (n).
With
better efficiency, the clustering quality of a partitional al-
gorithm is, however, not as good as that of a hierarchical
algorithm in general. The k-means clustering algorithm is
one of the most famous partitional clustering algorithms.
Although being widely used, the k-means algorithm suffers
from some deficiencies, including the dependency on the in-
put order, the tendency to result in local minimum, and
the limited applicability to only the data set consisting of
isotropic clusters (i.e., a circle in 2-D, or a sphericity in 3-D).
Several clustering methods have been proposed to com-
bine the features of these two categories of clustering al-
gorithms. In general, these algorithms first partition the
input data set into m subclusters, where m > k and k is the
predetermined number of clusters. Then these algorithms
construct a hierarchical structure based on these m subclus-




582

(a) Step 1:Form severval small subclusters




(b) Step2:Mergesmall subclusters into clusters



Figure 1: An illustration of a hybrid clustering al-
gorithm.



ters. As shown in Figure l(a), the data set is partitioned
into 15 subclusters. Next, as shown in l(b), these subclus-
ters are grouped into 2 clusters. This hybrid idea of cluster-
ing is first proposed in [10] where a multilevel algorithm is
developed. At the first level, the multilevel algorithm parti-
tions the data set into several partitions, and then performs
the k-means algorithm on each partition to obtain several
subclusters. In a subsequent level, this algorithm uses the
centroids of the subclusters identified in the previous level as
the new input data set and continues to perform on the new
data in a hierarchical manner. This process continues un-
til exactly k clusters are determined. Finally, the algorithm
performs a top-down process to map all points of each sub-
cluster to the corresponding final cluster which the centroid
of these points belongs to in a higher level. However, repre-
senting a subcluster by only one point makes the multilevel
algorithm not applicable to some cases especially when the
dimension of a subcluster is in the same order of the corre-
sponding final cluster. In addition, algorithm BIRCH is one
of the most efficient hierarchical clustering algorithms [16].
However, the use of square Euclidean distance as its sim-
ilarity measurement makes algorithm BIRCH be properly
applicable to data sets only consisting of isotropic clusters.
Specifically, chain-like and concentric clusters are difficult to
be identified by algorithm BIRCH [5][12].
Consequently, it is noted that in most related studies, the
dissimilarity between two clusters is defined as the distance
between their centroids, or the distance between two closest
(or farthest) data points. However, these measurements are
vulnerable to outliers, and removing the outliers precisely is
yet another difficult task. This is the very reason that most
prior clustering methods do not perform stably for various
types of inputs. In view of this, we propose a new similarity
measurement, referred to as cohesion, to measure the inter-
cluster distances. By using cohesion, we design a two-phase
clustering algorithm, called cohesion-based self-merging (ab-
breviated as CSM), which runs in linear time to the size of
input data set. Combining the features of partitional and
hierarchical clustering methods, algorithm CSM partitions
the input data set into several small subclusters in the first
phase, and then continuously merges the subclusters based
on cohesion in a hierarchical manner in the second phase. It
is shown by our performance studies that the use of cohesion
as the measurement enables algorithm CSM to be not only
very robust to the existence of outliers but also able to lead
to better clustering results than those by most prior meth-
ods. In a specific comparison with algorithm CURE which
is known to be one excellent clustering method, algorithm
CSM is able to achieve the clustering results of the same
quality as CURE while only incurring a much shorter, in
orders, execution time.
The rest of the paper is organized as follows. Section 2
presents the preliminaries. Algorithm CSM is presented in
Section 3. Performance studies are conducted in Section 4.
This paper concludes with Section 5.


2.
PRELIMINARIES

Given a desired number of clusters k, data clusteringis the
process of partitioning the input data points into k clusters
so that the points in each cluster are similar to one another
and are different from the points in other clusters.

2.1
Related Works
In this subsection, we review several prior algorithms re-
lated to this study.

2.1.1
Hierarchical Clustering Algorithms
As its name implies, a hierarchical clustering algorithm
establishes a hierarchical structure as the clustering result.
With the hierarchical structure, we can obtain different clus-
tering results for different similarity requirements.
Most existing hierarchical clustering algorithms are vari-
ations of the single-linkand complete-link algorithms. Both
algorithms require the time complexity of O (n2log n) and
the space complexity of O (n2). Owing to their good quality
of clustering results, hierarchical algorithms are widely used,
especially in the document clustering and classification. The
outline of a general hierarchical clustering algorithm is given
below.
Hierarchical Clustering Algorithm

1. Initially, each data point forms a cluster by itself.
2. The algorithm repetitively merges the two closest clus-
ters.
3. Output the hierarchical structure constructed.
A single-linkclustering algorithm is different from a complete-
link clustering algorithm in the inter-cluster distance mea-
surement, i.e., Step 2. The single-link algorithm uses the
distance between the two closest points of the two clusters
as the inter-cluster distance, i.e.,

d(Ci,Cj) = min{d(o,,oj) [o, 6 Ci,oj · Cj },

while the complete-link algorithm uses the distance of two
farthest points as the inter-cluster distance, i.e.,

d(C~,C~) = max {d(o,o,) Io~ · C~,oj ~ C, }.

However, the single-linkalgorithm suffers from so-called chain-
ing effect and the complete-link clustering algorithm has




583

problems in dealing with particular shapes such as two con-
centric circles.
Algorithm CURE [5], which is an improvement of the
single-link clustering algorithm, adopts techniques of sam-
pling to accelerate its computation. By randomly sampling
the input data, algorithm CURE is performed only on a
subset of the original data. Algorithm CURE selects sev-
eral scattered data points carefully as the representatives of
individual clusters. In each iteration, it merges the two clus-
ters with the minimal distance, where the distance of two
clusters is defined as the distance of the closest representa-
tives of the two clusters. For the merged cluster, algorithm
CURE shrinks these representatives toward the center of the
new cluster in order to eliminate the effects of outliers and
avoid the link effects.
It is known that CURE is one of the most successful clus-
tering methods for clustering the data of any shape. How-
ever, without the random sampling part which has an un-
certainty as noted before, the time complexity of algorithm
CURE is essentially O (n2log n), where n is the size of in-
put data set. Moreover, algorithm CURE utilizes a spatial
searching data structure, kd-tree, for searching the near-
est representatives. However, as pointed in [1], this kind
of searching structures does not work well in a high dimen-
sional domain. This fact limits the applicabilityof algorithm
CURE.

2.1.2
PartitionalClusteringAlgorithms
The k-means algorithm is one popular partitional algo-
rithm for data clustering. Several algorithms, such as CLARA
and CLARANS [11], are variations of the k-means algo-
rithm. However, these clustering algorithms suffer from the
tendency of resulting in local minimum and are likely to ob-
tain different clustering results with different initial states.
The clustering results are also dependent on the sequence
of the input data. The adoption of Square Euclidean dis-
tance or Euclidean distance as the similarity measurement
causes the application of these algorithms to be limited to
data sets consisting of only isotropic clusters, thus refraining
these algorithms from being used in many real applications.
Nevertheless, the partitional algorithm has the advantages
on execution time and space requirement. The outline of
the k-means algorithm is given as follows.
Algorithm K-Means
1. Initially, select k centroids arbitrarily for each cluster C~,
i e [1,k].
2. Assign each data point to the closest cluster based on the
distance to the centroid of the cluster.
3. Calculate the centroid cl of cluster Ci, i E [1,k].
4. Repeat Step 2 and Step 3 until no points change between
clusters.

2.1.3
HybridClusteringAlgorithms
Since the complexity of hierarchical data clustering is rel-
atively high, several improved algorithms have been pro-
posed, such as the hybrid clustering algorithm proposed in
[10] and algorithm BIRCH [16]. The hybrid algorithm in
[10] is a multilevel algorithm. In the first level, the input
data set is divided into Px partitions equally. Then, the
hybrid algorithm performs a k-means algorithm to get C1
clusters in each partition. In level i, where i > 1, the cen-
troids of the clusters obtained in level i - 1 are taken to
this level for merging. These centroids are partitioned into
P~ partitions. In each partition, the hybrid algorithm per-
forms a hierarchical clustering algorithm to get Ci clusters.
This process continues until exactly k clusters are identi-
fied. This algorithm is shown to be very efficient in both
aspects of computation and memory space. Finally, the al-
gorithm performs a top-down process to map all points of
each subcluster in level i to the cluster which their centroid
belongs to in level i + 1. However, using only one point to
represent the w:holecluster may easily lose some information
about the distributions of clusters, which are important to
the similarity of two clusters.
Another hybrid clustering algorithm, BIRCH, is designed
to deal with a large data set. Algorithm BIRCH uses cluster
features (CF) to represent a cluster. Given the CF of a clus-
ter, one can ob~tainthe centroid, radius and diameter of that
cluster easily (in constant time). Further, the CF vector of
a new cluster formed by merging two subclusters can be di-
rectly derived from the CF vectors of the two subclusters by
algebra operations. Algorithm BIRCH is one efficient clus-
tering algorithm. However, adopting CF vector to represent
a cluster will unavoidably suffer from the same problem as
the k-means algorithm. As a result, BIRCH is appropriate
for data sets consisting of only isotropic clusters.

3.
COHESION-BASED SELF-MERGINGAL-
GORITHM
In this section, we describe the details of cohesion-based
self-merging algorithm (abbreviated as CSM). In Section
3.1, we propose a new measurement for the similarity of two
subclusters, cohesion, which is intrinsically different from
other prior measurements. Cohesion is more appropriate
for inter-cluster similarity measurement because it does not
judge the similarity of two subclusters by only some data
points. Rather, the cohesion measurement takes the dis-
tributions of t:he two clusters into account. In Section 3.2,
based on cohesion, we devise a new clustering algorithm,
CSM, which fully utilizes the features of cohesion.

3.1
Similarity Measurement Between Subelus-
ters
As shown in Figure 2, although the distance between the
centroids of the two subclusters in Figure 2(a) is the same
as that in Figure 2(b), the two subclusters shown in Figure
2(b) are more inclined to be merged into a new cluster. In
addition to the distance between centroids, another method
to measure the inter-cluster distance of two subclusters, Ci

and Cj, is the average complete distance, i.e., ~
for
ic, l×lcj I
'

each pi E Ci and pj E Cj. However, this complex computa-
tion has its own drawback and cannot even distinguish the
two cases sho~rn in Figure 2. Note that the average complete
distances of these two clusters are very close to each other,
i.e., 20.40 in Figure 2(a) and 21.57 in Figure 2(b). (In con-
trast, as can be verified later, the corresponding cohesions
are 0.013 in Figure 2(a) and 0.189 Figure 2(b), indicating
the better capability of distinguishing one from another by
using the cohesion measurement.) Several alternatives, such
as the distance between the closest (or farthest) points of the
two clusters, could be employed to redeem this deficiency.
However, those measurements are very vulnerableto random
noises (outliers). Consequently, we propose a new similarity
measurement, namely cohesion, based on the joinability of
a data point to another cluster. First, same as in [16], we




584

O
(a)Twoclusterswithsmallercohesion


·
'- <'-




(b)TwoClusterswithgreatercohesion



Figure 2: Illustration for subclusters that have dif-
ferent cohesion values.


define the radius r of a cluster as follows.

Definition 1: Given a cluster Cl consisting of n data
points, pl,p2,. ..... ,p,, the radius r of Cl is defined as




where c is the eentroid of the cluster, i.e., e = ~--
"~,
and d (pi, c) is the Euclidean distance between pi and
C.

Then, with the radius, we define the joinabilityof one data
point to another cluster, which can, in essence, be viewed
as the intention of a data point to be joined into another
cluster.

Definition 2: Given a data point oi of a cluster Ci and
another cluster Cj, the joinability ofpl to Cj is defined
as

join(pi, Cj ) = Exp (- [d(Pi' ~ ) - d (p~' cJ)])


where ci and cj are the centroids of Ci, and C~ respec-
tively, and ri is the radius of clusters Ci.

With the notion of joinability, we define the cohesion of
two clusters as follows.

Definition 3: The cohesion of two clusters Ci and Cj is
defined as


join (p, Cj) + ~ "join (p, C,)
chs (Ci, Cj) = .,ec,
.,ec~
Ic, I + Icjl

It is noted that the time complexity of this computation
is linear to the size of the two clusters, i.e., O (IC~l + Ic, I).
Example 3.1: Consider the two subclusters, Cll = {A, B}
and Cl2 = {C, D, E}, shown in Figure 3.
The centroid
OCOD(14,12)
(12,11)

OB(6,7)
OE(16,7)
OA(4,5)



Figure 3: An illustration of computation of cohesion.


of first cluster Cll, cl, is (5,6) and the centroid of clus-
ter Cl2, c2, is (14, 10). The radii of these two clusters, rl

r2, are ~/2 and ~/-~ respectively. Therefore, byand
the

definition of joinability, we can derive that join (A, Clz) =
Exp(--Id(A'cl)~d(A'e2)') = Exp(---~ -~) =
0,001 and

jo
n(n, Cl
) =
=
0.002.
Simi-

larly, join (C, Cll) = Exp (-Id(C'c2)-d(c'cl,'2 )l) = 0.095 and

the joinabilities of D, and E according to Cll are 0.038,
and 0.064 respectively. Thus, the cohesion of these two sub-
clusters is chs (Cll, CI2)= (0.001 + 0.002 + 0.095 + 0.038 +
0.064)/5 = 0.0398.
It is worth mentioning that this similarity measurement
of cohesion is robust to the existence of outliers due to the
following two reasons: (1) Using the cohesion measurement,
instead of only a few of points, all of the related points of the
two subclusters are considered to evaluate this inter-cluster
similarity; (2) According to the cohesion measurement, the
contribution of each point is limited to be in the range of
[0, 1]. This important feature limits the possible impact of
outliers.

3.2
Algorithm CSM
Now, we describe the proposed algorithm based on cohe-
sion self-merging as follows.
Algorithm CSM

//Input: The input data set, the size of the data set, n, the
number of subclusters, m and the desired number of clus-
ters, k.
//Output: The hierarchical structure of the k clusters.
1. Initially select rn centroids arbitrarily.
2. Assign each point to the closest subcluster based on the
distance to the centroids.
3. Obtain the centroid of each subcluster.
4. Repeat Step 2 and Step 3 until no points change between
clusters.
5. Obtain the cohesions, chs (Ci, Cj) for all pairs of subclus-
tars, Ci and Cj.
6. Build a heap, QCHS, with the cohesions of all combina-
tions.
7. Extract the maximal cohesion, say chs(Ci,Cj),
from
QCHS.
8. If Ci and Cj do not belong to the same subcluster, then
merge the two subclusters which they belong to into a new
subcluster.
9. Repeat Step 7 and Step 8 until the number of clusters is
equal to k.
Example 3.2: Consider the data set shown in Figure
4. Recall that better visual effect can be achieved if these




585

[oChJstt~
]xCluster
I· ChJst~r



Figure 4: An illustrative example of algorithm CSM.
(a) DaRtSet I
(b) Data Set 2



figures are viewed in colors. We apply algorithm CSM with
k = 2 and m = 6 on it.
As shown in Figure 4, in the
first phase, algorithm CSM partitions the data set into 6
subclusters.
In the second phase, algorithm CSM tries to
merge the 6 subclusters into two clusters by their cohesions.
The cohesions between any two subclusters in this example
are shown in Table 1.



Cll
Cl2
Clz
el4
Cl5
Cl6
Cll
0.001
0.000
0.001
0.293
0.268
Cl2
0.001
0.298
0.284
0.014
0.011
Cl3
0.000
0.298
0.286
0.002
0.001
Cl4
0.001
0.284
0.286
0.009
0.004
Cl5
0.293
0.014
0.002
0.009
-
0.288
Cl6
0.268
0.011
0.001
0.004
0.288
-


Table 1. The all cohesions in Example 3.2


As shown in Table 1, the largest cohesion is chs (Cl2, Cl3) =
0.298. Therefore, we extract the cohesion entry from QcHs
and merge Cl2 and Cl3 into a new subcluster. Then, we ex-
tract the maximal entry in the Qcns, i.e., chs (Cll, Cls) =
0.293, and then merge the Cll and Cl5. Similarly, we merge
Cl5 and Cle and then Cl3 and Cl4 Finally, the output
of algorithm CSM is the two clusters, {Cll, Cls, Cle} and
{C/2, Cl3, Cl4}.
Algorithm CSM is a two-phase clustering algorithm. In
the first phase, i.e., from Step 1 to Step 4, algorithm CSM
adopt the k-means algorithm to divide the input data set
into m subclusters.
In Step 5, it obtains the cohesions of
these rn subclusters identified in the first phase. Then, al-
gorithm CSM enters the second phase, i.e., from Step 5 to
Step 8, where it performs the clustering based on cohesion
merging to obtain the final clusters.



4. PERFORMANCE STUDIES
To assess the performance of algorithm CSM, we have
conducted a series of experiments.
These experiments are
performed on a computer with a 800Mhz Intel CPU and
512M of memory. In section 4.1, we compare the clustering
quality of CSM with several well-known clustering methods.
We implement the k-means clustering, single-link cluster-
ing, and complete-link algorithms. We also'implement the
clustering algorithm CURE with the outlier elimination en-
hancement as described in [5].


4.1
Experiment I: Clustering Quality of Algo-
rithm CSM
Figure 5: Data sets used in Experiment I.




-:,
·
¢,r-,
,a
".



(a) Cluster restdts of Data Set I
(b) Cluster results of Data Set 2




Figure 6: Clustering results of algorithm CSM.




n
m
k
avg time
Data Set 1
100000
32
5
84.3 sec
Data Set 2
2696
32
2
0.68 sec


Table 2. Execution details on each data set by CSM.

We perform our experiments on the data sets shown in
Figure 5. Data Set 1 is the same as the one used in CURE
[5]. As stated in [5], both algorithm BIRCH and single-link
clustering algorithm cannot partition this data set correctly.
The second data set is a famous single-link example. How-
ever, we add some random noises for experimental purposes.
The number of points and parameter setting for each data
set is shown in Table 2.
Here, we choose the parameter
m with which algorithm CSM can easily obtain the similar
clustering results each time.
Note that some of the data
sets contain too many points to be clearly shown in the pa-
per. Hence, we only show a sampled subset of the original
data sets. For the same reason, we show only the sampled
subsets of the clustering results while we indeed perform all
algorithms on the whole data sets.
The clustering results of algorithm CSM are shown in Fig-
ure 6. As shown, algorithm CSM is able to successfully par-
tition these data sets. We also applied other implemented
algorithms on these data sets. Note that because of their
time complexity, hierarchical algorithms, including CURE,
single-link, and complete-link, are applied on random sam-
pled subsets of some large data sets.
Among these algo-
rithms, only algorithm CURE can obtain the correct clus-
tering results on all data sets. Finally, we conduct a series
of experiments on the sensitivity analysis on the value of m.
In these experiments, it is shown that when the value of m
is between 5 and 16, the clustering results are similar to the
one shown in Figure 7(a), when the value of m is between
16 and 64, the results are similar to Figure 6(a), and when




586

"Z "+-"
"
·




.,



(a) Clustering results for small vlaues of rr
(b) Clustering results for large values ofm




Figure 7: Clustering results of CSM for various val-
ues of rn.




o ~
.
i
i
,
,
o
20000 40000 60GO0 80~)0 Io00c~
I000
200O 3000 4~0
$000

$i~ ofD~da$¢~
Slz~ of Data $¢X

(a) Macrocosm
(b)Microcosm




Figure 8: Execution times (in seconds) for data sets
of various sizes.


m is larger than 64, the clustering results are similar to the
one shown in Figure 7(b). It is seen that algorithm CSM
works similarly as a k-means algorithm with a smaller value
of m, and as a single-link algorithm with a larger value of
m.


4.2
Experiment II: Efficiency ofAlgorithm CSM
First, we show the relationship between the running time
and the size of input data set. We apply algorithm CSM on
sampled subsets of Data Set 1 of various sizes with m = 32.
The average execution times for these data sets are shown
in Figure 8. As shown in Figure 8(a), algorithm CSM runs
linearly to the size of input data set. Moreover, as shown in
Figure 8(b), algorithm CSM is much faster than other hi-
erarchical clustering algorithms. Note that algorithm CSM
attains the same powerful clustering capability as CURE
whereas only incurring a much shorter, in orders, execution
time than CURE.


5.
CONCLUSION
In this paper, we proposed a new similarity measurement,
cohesion, to measure the inter-cluster distances.
By us-
ing cohesion, we proposed a two-phase clustering algorithm,
CSM, whose time complexity is linear to the size of the in-
put data set.
Combining the features of partitional and
hierarchical algorithms, algorithm CSM is able to not only
resist outliers but also lead to similar clustering results as
algorithm CURE while incurring a much shorter execution
time complexity. A series of experiments conducted shows
the merit of CSM and its advantage over prior clustering
methods.


Acknowledgments
The authors are supported in part by the National Science
Council, Project No. NSC 90-2213-E-002-086 and NSC 90-
2213-E-002-116, Taiwan, Republic of China.
6.
REFERENCES
[1] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and
U. Shaft. When is "nearest neighbor" meaningful?
ICDT, pages 217-235, 1999.
[2] P. S. Bradley, K. P. Bennett, and A. Demiriz.
Constrained k-means clustering. MSR-TR-2000-65,
Microsoft Research, May 2000.
[3] M.-S. Chen, J. Han, and P. S. Yu. Data Mining: An
Overview from Database Perspective. IEEE Trans. on
Knowledge and Data Engineering, 5(1):866-883,
December 1996.
[4] R. C. Dubes. How many clusters are best? - an
experiment. Pattern Recognition 20, 6:645-663, 1987.
[5] S. Guha, R. Rastogi, and K. Shim. CURE: An
efficient clustering algorithm for large databases.
Proceedings of the ACM SIGMOD Conference on
Management of Data, pages 73-84, 1998.
[6] S. Guha, R. Rastogi, and K. Shim. ROCK: A Robust
Clustering Algorithm for Categorical Attributes.
Proceedings of the 15th International Conference on
Data Engineering, 1999.
[7] J. Hertz, A. Krogh, and R. G. Palmer. Introduction to
the theory of neural computation. Santa Fe Institute
Studies in the Sciences of Compexity lecture notes.
Addison- Wesley Longman Publ. Co., Inc., Reading,
MA., 1991.
[8] A. K Jain, M. N. Murty, and P. J. Flynn. Data
Clustering: A Review. ACM Computer Surveys, 31(3),
Sept. 1999.
[9] C.-R. Lin and M.-S. Chen. On the optimal clustering
of sequential data. Proceedings of the 2nd SIAM
International Conference on Data Mining, April 2002.
[10] N. M. Murty and G. Krishna. A hybrid clustering
procedure for concentric and chain-like clusters.
Internation Journal of Computer and Information
Sciences, 10(6):397-412, 1981.
[11] R. T. Ng and J. Han. Efficient and Effective
Clustering Methods for Spatial Data Mining.
Proceedings of the 20th VLDB Conference, 1994.
[12] Y.-J. Oyang, C.-Y. Chen, and T.-W. Yang. A study
on the hierachical data clustering algorithm based on
gravity theory. Proceedings of 5th European
Conference on Principles and Practice of Knowledge
Discovery in Databases, pages 350-361, 2001.
[13] C. R. Palmer and C. Faloutsos. Density biased
sampling: An improved method for data mining and
clustering. A CM SIGMOD International Conference
on Management of Data, pages 89-92, 2000.
[14] A. K. H. Tung, J. Han, L. V. S. Lakshmanan, and
R. T. Ng. Constraint-based clustering in large
databases. Proc. 2001 Int. Conf. On Database Theory,
Jan 2001.
[15] C.-H. Yun, K.-T. Chuang, and M.-S. Chen. An
efficient clustering algorithm for market basket data
based on small-large ratios. Proceedings of the 25th
International Computer Software and Applications
Conference(COMPSA C 2001), Oct. 2001.
[16] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH:
An Efficient Data Clustering Method for Very Large
Database. Proceedings of the ACM SIGMOD
Conference on Management of Data, pages 103-114,
1996.




587

