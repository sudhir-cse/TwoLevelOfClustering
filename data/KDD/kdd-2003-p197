Assessment and Pruning of Hierarchical Model Based
Clustering


Jeremy Tantrum


Department of Statistics
University of Washington
Seattle, WA 98195
tantrum@stat.washington.edu
Alejandro Murua
Department of Statistics
University of Washington
Seattle, WA 98195
murua@ieee.org
Werner Stuetzle


Department of Statistics
University of Washington
Seattle, WA 98195
wxs@stat.washington.edu


ABSTRACT
The goal of clustering is to identify distinct groups in a
dataset. The basic idea of model-based clustering is to ap-
proximate the data density by a mixture model, typically a
mixture of Gaussians, and to estimate the parameters of the
component densities, the mixing fractions, and the number
of components from the data. The number of distinct groups
in the data is then taken to be the number of mixture com-
ponents, and the observations are partitioned into clusters
(estimates of the groups) using Bayes' rule. If the groups are
well separated and look Gaussian, then the resulting clusters
will indeed tend to be "distinct" in the most common sense
of the word - contiguous, densely populated areas of feature
space, separated by contiguous, relatively empty regions. If
the groups are not Gaussian, however, this correspondence
may break down; an isolated group with a non-elliptical dis-
tribution, for example, may be modeled by not one, but
several mixture components, and the corresponding clusters
will no longer be well separated. We present methods for as-
sessing the degree of separation between the components of a
mixture model and between the corresponding clusters. We
also propose a new clustering method that can be regarded
as a hybrid between model-based and nonparametric cluster-
ing. The hybrid clustering algorithm prunes the cluster tree
generated by hierarchical model-based clustering. Starting
with the tree corresponding to the mixture model chosen by
the Bayesian Information Criterion, it progressively merges
clusters that do not appear to correspond to different modes
of the data density.


Categories and Subject Descriptors
I.5.3 [Pattern Recognition]: Clustering; I.5.1 [Pattern

Supported
by NSA grant 62-2948.
Supported
by NSF grant DMS-9803226 and NSA grant 62-
2948.




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGKDD '03 Washington, DC, USA
Copyright 2003 ACM 1-58113-737-0/03/0008...$5.00.
Recognition]: Models--Statistical; G.3 [Probability and
Statistics]: Multivariate Statistics


General Terms
Model-based Clustering


Keywords
Model-based Clustering, Nonparametric Clustering, Density
Estimation, Unimodality


1. INTRODUCTION AND MOTIVATION
The goal of clustering is to identify distinct groups in a
dataset X = {x1, . . . , xn}  Rm. For example, when pre-
sented with (a typically higher dimensional version of) a
dataset like the one in Figure 1a we would like to detect
that there appear to be two groups, and assign a group label
to each observation. (Throughout this paper we distinguish
between "groups" and "clusters", which are estimates for
the groups.)

Model-based clustering in a nutshell. To cast clustering
as a statistical problem we regard the data x1, . . . , xn as a
sample from some unknown probability density p(x). Model-
based clustering (see [9] and references therein) relies on the
premise that each group g is represented by a density pg(x)
that is a member of some parametric family, typically the
multivariate Gaussian distributions. In this case p(x) is a
Gaussian mixture:


p(x) =
G



g=1
g pg(x; µg, g) ,
(1)


where G is the number of groups, g is the prior probability
of group g, and p(x; µ, ) denotes the Gaussian density with
mean µ and covariance matrix . For fixed G we can esti-
mate the parameters g, µg, and g by maximum likelihood,
using the EM-algorithm [9, Chapter 2.8]. There are many
ways of estimating G [9, Chapter 6], e.g. by maximizing the
Bayesian Information Criterion (BIC) [11, 5]:

^
G = argmaxG (2 × L(G) - r log(n)) .
(2)

Here, L(G) is the log-likelihood of the best G component
model, r is the number of parameters of the model and n is
the number of observations.


197

While attractive conceptually, the straight-forward ap-
proach to mixture modeling -- fit models for many different
values of G using the EM algorithm, and then choose the
model that maximizes the BIC -- is slow. Following a sug-
gestion by Fraley and Raftery [5], we address this problem
by using a hierarchical approach: Find a model with G - 1
components by merging the two groups of the G component
model for which the merge leads to the smallest decrease in
log-likelihood. Among the sequence of models thus gener-
ated choose the one maximizing the BIC.
There have been several recent advances in extending the
normal mixture model to large datasets [2, 13].

A conceptual problem with model-based clustering.
Model-based clustering relies on the premise that mixture
components in the model correspond to distinct groups in
the data.
If the groups are Gaussian, then the resulting
clusters will indeed tend to be "distinct" in the most com-
mon sense of the word - contiguous, densely populated areas
of feature space, separated by contiguous, relatively empty
regions [3]. If the groups are not Gaussian, however, the cor-
respondence between groups and mixture components may
break down. An isolated group with a non-elliptical distri-
bution, for example, may be modeled by not one, but several
mixture components, and the corresponding clusters will no
longer be distinct. This problem is illustrated in Figure 1a.
Most observers would probably agree that the data in this
figure fall into two separate groups. The BIC criterion, how-
ever, chooses a mixture model with four components; Fig-
ure 1b shows regions containing 60% of the mass of each
component.

Contributions of the paper. We present diagnostic tools
for assessing the degree of separation between the compo-
nents of a mixture model and between the corresponding
clusters. We also propose an algorithm for pruning the clus-
ter tree generated by hierarchical model-based clustering.
The algorithm starts with the tree corresponding to the mix-
ture model chosen by the Bayesian Information Criterion.
It then progressively combines mixture components that do
not appear to correspond to different modes of the data den-
sity and merges the corresponding clusters. Each cluster in
the final partition may therefore be modeled by more than
one mixture component. The resulting procedure can be re-
garded as a hybrid between nonparametric and model-based
clustering: we look for modes in the data distribution using
the mixture model as a density estimate.



2. ASSESSING SEPARATION BETWEEN
MIXTURE COMPONENTS
Roughly speaking, we would expect mixture components
modeling different groups in the data to be well separated.
On the other hand, mixture components modeling parts of
the same group would be expected to exhibit significant
overlap.
We now put this concept in probability terms. Assume
that we have modeled the distribution of the observed data
by a mixture density p(x) =
g
gpg(x). We can generate
observations from this density by first generating a compo-
nent label Y with P (Y = g) = g, and then generating X
from pY . According to Bayes' rule, the posterior probability
(a)




1
2
3

4




(b)

Figure 1: Data set with fitted Gaussian mixture.
The modes of the mixture are indicated by the two
white dots. (This example is referred to as the run-
ning example in the remainder of the paper.)


P (Y = g|X) is

P (Y = g|X) =
gpg(X)
G
j=1
jpj(X)
.


Component g is well separated from all the other compo-
nents if P (Y = g|X) only takes extreme values, either close
to zero or close to one - one for observations actually gener-
ated from component g, and zero for all others.
Exactly evaluating the distributions of P (Y = g|X) for
the G components is generally impossible when the dimen-
sion m is larger than 1. To see why this is so, define the ran-
dom variable h(X) = P (Y = g|X). Its distribution function
Fh(u) is given by

Fh(u) = P (h(X)  u) =
I(h(x)  u) p(x)dx ,
(3)

where I(·) denotes the indicator function. Except in triv-
ial cases, (such as G = 2, 1 = 2) the region of feature
space defined by the indicator function has a complex shape
described in terms of conic sections. Hence in general this
integral can not be evaluated analytically and we resort to
Monte Carlo simulation.


198

In the following we present three methods for assessing
the separation between mixture components, based on the
posterior probabilities, the margins, and the misclassifica-
tion probabilities. All these were estimated by simulating
from the model.

2.1 Assessing separation using posterior prob-
abilities
Figure 2 shows rootograms of the posterior probabilities
P (Y = g|X) for the four components of the mixture model
in our running example.
(A rootogram is a variant of a
histogram where the heights of the bars encode the square
roots of the bin counts, instead of the bin counts themselves.
This makes low counts more visible.) The rootograms are
based on 20,000 data points generated from the estimated
mixture model. We have omitted the bin containing P (Y =
g|X) = 0, because it would have by far the largest bin count
and would obscure the information in the remaining bins.
The rootogram for component one (top panel) has a large
peak at P (Y = 1|X) = 1 and is essentially zero elsewhere,
indicating clear separation of component one from all the
other components. On the other extreme, the rootogram
for component four has no peak at P (Y = 4|X) = 1. This
is due to the fact that component four is completely over-
lapped by components two and three, and hence there is al-
ways a substantial posterior probability that an observation
generated from p4 might have come from p2 or p3. Further-
more, the significant mass away from P (Y = g|X) = 1 in
the rootograms for components two, three, and four shows
that these components are not well separated.




0.2
0.4
0.6
0.8
1.0
0
20
50




0.2
0.4
0.6
0.8
1.0
0
20
50




0.2
0.4
0.6
0.8
1.0
0
20
50




0.2
0.4
0.6
0.8
1.0
0
20
50




Figure 2: Running example: Rootograms of the pos-
terior probabilities P (Y = g|X) for X distributed ac-
cording to the mixture model. The bin containing
zero is not shown in order not to obscure the pattern
in the other bins.


2.2 Assessing separation using margins
An alternative to looking at the posterior probabilities is
to consider the margins. Let
^
Y (X) be the estimated com-
ponent label assigned to X by Bayes' rule:

^
Y (X) = arg max
g
P (Y = g|X) .

The margin of X drawn from component Y of the model is
given by

margin(X, Y ) = P ( ^
Y (X) = Y |Y ) - max
g=Y
P ( ^
Y (X) = g|Y ) .
1
2
3
4
MCg
g
1
0.998
0.002
0
0
0.002
0.162
2
0.001
0.879
0.001
0.119
0.121
0.388
3
0
0
0.908
0.092
0.092
0.191
4
0
0.060
0.105
0.835
0.165
0.259

Table 1: Misclassification matrix for the running ex-
ample.




Note that a negative margin means that X is assigned to
the wrong component, and that a small margin means that
X lies in a region where components overlap significantly.
Figure 3 shows the cumulative distribution function (cdf)
of the margin for observations drawn from the four com-
ponent mixture model of our running example.
There is
a large proportion of small margins indicating substantial
overlap between the components.




-1.0
-0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0




Margin
cdf




Figure 3: Running example: Cumulative distribu-
tion function of the margin.



2.3 Assessing separation using misclassifica-
tion probabilities
When the number of clusters is moderate, we can look at
the misclassification matrix to detect well separated as well
as overlapping components of a mixture model.
Table 1
shows the misclassification matrix for the mixture model
in our running example. Let mgg
be the probability that
the Bayes' rule assigns an observation from component g to
component g .
From the misclassification matrix we can extract informa-
tion at three different levels of detail. At the coarsest level
we can look at the overall misclassification probability given
by
g
g(1 - mgg). The lower this probability is, the better
the separation. At the next higher level of detail, we can look
at the component-wise misclassification probabilities MCg.
In our example (Table 1) the misclassification probability
for component one is very small (MC1 = 0.002), indicating
that component one is well separated.
The misclassifica-
tion probabilities for the other components are substantially
larger. On the most detailed level, the values of mgg
and
mg
g
indicate which other components overlap component
g. The pattern of entries in Table 1 shows that components
two, three and four are mutually overlapping. We could not
see this from the less detailed views.


199

3. ASSESSING SEPARATION BETWEEN
CLUSTERS
A mixture model is only an estimate for the true underly-
ing density of the data. Therefore the degree of separation
between mixture components (or lack thereof) does not al-
ways accurately reflect the actual separation between the
clusters.
We cannot compute the matrix of misclassification prob-
abilities for the observed data x1, . . . , xn, nor the margins,
(as we did in the previous section for observations simulated
from the estimated model) because those require knowing
the true labels.
However we can compute the posterior
probabilities P (Y
= g|xi), and therefore generate a plot
analogous to Figure 2, shown in Figure 4. The rootogram
for P (Y = 4|xi) (bottom panel) looks basically flat, from
which we can conclude that cluster 4 almost certainly does
not correspond to a distinct group in the data.




0.2
0.4
0.6
0.8
1.0
0
4
8
12




0.2
0.4
0.6
0.8
1.0
0
4
8
12




0.2
0.4
0.6
0.8
1.0
0
4
8
12




0.2
0.4
0.6
0.8
1.0
0
4
8
12




Figure 4: Running example: Rootograms of the pos-
terior probabilities P (Y = g|xi) for the data.



4. HYBRID CLUSTERING
Hierarchical model-based clustering generates a hierarchy
of mixture models: The model with m - 1 mixture com-
ponents is obtained by merging the two clusters of the m
component model for which the change leads to the small-
est decrease in log-likelihood.
The result of this merging
process can be represented by a binary tree T . The leaves
of the tree are the observations. Each interior node N of
the tree is assigned a generation between 1 and n - 1, in-
dicating where in the sequence of merges it was generated.
The interior node corresponding to the i-th merge in the se-
quence is assigned generation n - i; the root node therefore
has generation 1. Each node N is also associated with the
cluster formed by its descendent leaves.
The merge sequence defines a sequence of trees: Tm is ob-
tained from T by removing the offspring of all nodes with
generation greater than or equal to m. By construction, Tm
has m leaves and corresponds to a mixture model with m
mixture components. Let G be the number of mixture com-
ponents chosen by the BIC, and let TG be the corresponding
tree.
If the distinct groups in the data all have Gaussian dis-
tributions, then we expect roughly a one-to-one correspon-
dence between groups and mixture components associated
with the leaves of TG.
Also, the clusters associated with
the leaves of TG will be similar to the groups. ("Roughly"
because G, after all, is only an estimate.) If the groups are
not Gaussian, however, each group may be modeled by more
than one mixture component, and consequently will be the
union of several clusters.
The idea of hybrid clustering is to test, for each node of
TG whose daughters are leaves, whether the corresponding
clusters are well separated. If they are not, then the clusters
probably correspond to the same group, and we merge them.
The new cluster is then modeled by the sum of the mixtures
modeling the daughters that were merged.
This pruning
process is repeated until no further clusters can be merged.

4.1 Illustration of hybrid clustering
Before describing its ingredients in more detail, let us see
the pruning process in action. The upper panel of Figure 5
shows the tree T4 whose leaves correspond to the mixture
model fit to the data in our running example. The circled
node is the one being tested. The lower panel of Figure 5
shows the projection of its associated cluster onto the Fisher
discriminant direction, which is the direction that best sep-
arates the projections of the two daughter clusters [6][8,
Chapter 11.5]. The grey curve is the kernel density estimate
for the projected data with the smallest bandwidth that
yields a unimodal density [12, Chapter 6.3 and 6.4]. The
black curve is the kernel density estimate with the smallest
bandwidth that yields a bimodal density. The dot plot of
the projected data looks unimodal, and the unimodal and
bimodal distributions are almost identical, which indicates
that the daughter clusters are not well separated in feature
space. A formal test for unimodality of the projected data
(Section 4.2) would reject the null hypotheses of unimodal-
ity at level  = 0.49, meaning that the evidence against uni-
modality is weak. We therefore prune the daughters. The
new tree is the one shown in black in Figure 6. The diag-
nostic plot is qualitatively similar to the one in Figure 5; the
daughter clusters of the node being tested do not seem to
be well separated, with unimodality being rejected at level
 = 0.12. We therefore prune again and are left with the
tree shown in Figure 7. Now the picture is different: The
diagnostic plot reveals a clear separation between the clus-
ters, and a formal test rejects the hypothesis of unimodality
at level  = 0.002. We conclude that there appear to be
two distinct groups in the data, one modeled by three mix-
ture components, and the other one modeled by one mixture
component.

4.2 Testing for unimodality
In order to automate the pruning process described in Sec-
tion 4.1 we need a way of measuring the amount of evidence
against unimodality for a univariate data set (the projec-
tion of a cluster onto the Fisher discriminant direction best
separating its daughters). Even if we carry out the pruning
process interactively, by looking at diagnostic plots like the
ones in Figures 5-7, such a measure of evidence still provides
a useful guideline.
Let x1, . . . , xn be a set of (univariate) data sampled from
some density f(x), and let Fn(x) be the empirical cdf of the
sample. To test the null hypotheses that f(x) is unimodal we
use J.A. Hartigan and P.M. Hartigan's DIP test described
in [7]. The test statistic is the DIP

D = sup
x
|Fn(x) - H(x)|,

where H is the unimodal cdf closest to Fn. Bickel and Fan [1]


200

|
||
|
|||
||
||
|
||
||
| ||
|||
| ||
||
||||
||||
|
|||
||||
| ||||
|| ||
|||
|
||
|||
| ||||
|
|||
|||||
||
||
|||
||||
| |
||
|
||
||
|||
|
|||
|||||
|
||
|
||
||
||
||
|||
|
|
||
||
||
|||
|||
||
|||
||
|| ||
|
||
| |
|||
||
||
|
||
|
||
||||
||||
|||
|||
|
|| |
|
|
|||
|||
||||
|| |
|||
|
||
|||
|
||
|
|||
||
|
|||
|
||
|
|||
| | ||
||
|
||
|
|||
||
||
|||
|| |
|
|||
|
||
||||
|||
| ||
||
|
|||
|
||
||
||
||
|
||| |||
|
||
0.00
0.05
0.10
0.15
0.20




Figure 5: Running example: Tree generated by hier-
archical model-based clustering and diagnostic plot
for the circled node.




|
|
|
|
||| |
||
|||
|
|||
|
||
| | ||
||
||
|
||
|
|||||
|
|| |||
|
|||| |
||
||
|
||
|
||
||
| ||
||
|
||
|
|||||
||
||||
|
| | ||
|
||
|
|||| |
|
|
|||
|||
|
|||||
||
|
|
||
||
||
| |
|||
||
|
|||
||| |
|||
||
| |
||| |
|
|||
|
|
||||
||
||||
|
|||
||||
|
||
||
| ||
||
| ||
||
|||
|| ||
||||
|||
|
||
||
||
||
||
| |||
||
|||
|||
||
||||
|
||
||
|||
||
||
||
||
||
| |
||
||
|
||
||
|| |
|
||
|||| |
|
|
|| |
|| |
|
|||
|| || ||||
||
|
||
|||
|||
|
| ||
||
||||
||
||
||
|||
| |||| | | ||
|
||
|| |
|||
||
|||
|||||
||
|
|
|
|||
||
||
||
|||
|
|| ||
||
||
||
||||
||
|||
||
| || ||
|
||
||
||| ||
||
|
||
|
||
||||
||
|||
||
||
||| |
|| |
|
|
|||
||
|||
|||
|||
|
||
|||
|
||
| |||
||
|
|||
| ||
|
|||
||||
|| |
|
|
|||
||
||
|||
|| | |||
|
||
||||
|||
| ||
|| |
|||
|
||
|| ||
||
|
||||||
|
0.00
0.05
0.10
0.15




Figure 6: Running example: Tree generated by hi-
erarchical model-based clustering after first step of
pruning, and diagnostic plot for the circled node.



show that the nonparametric maximum likelihood estimate
of the closest unimodal cdf, given the mode location m0, is
the greatest convex minorant of Fn on (-, m0] and the
least concave majorant on [m0, ). (The greatest convex
minorant of Fn on (-, m0] is the convex function G not
exceeding Fn on (-, m0] that minimizes supx
m0
|Fn(x)-
G(x)|. The least concave majorant is defined analogously.)
Bickel and Fan [1] also show that this estimate is robust
against inaccuracy in the estimate of the mode. We could
estimate the mode location by minimizing the DIP. How-
ever, this would be computationally expensive. Instead we
estimate the mode using a kernel smoother, as suggested by
Silverman [12, Chapter 6.3 and 6.4].
Figure 8 shows the
empirical cdf of a sample (black curve), and the closest uni-
modal cdf (grey curve). The DIP is the maximum absolute
difference between the two curves, indicated by the heavy
vertical line. The estimated mode location is shown by the
grey vertical line.
The distribution of the DIP under the null hypotheses of
unimodality is not available in closed form but can be esti-
mated by Monte Carlo.
As before, let H(x) be the uni-
||
|||
||
|
||
||
|||
| ||
|||
|||||
|| ||
|
||
||
|||
||
|
|| || |
||
||
| |
|||||
||
||||
||
||
|
||
|||||
|||
|||
||| ||
|||
||
|
||
|
||
| |
||
|||
| |
| ||
|| ||
|||
|
||
|||
||
|| |
|||
|
||||||
|||
||
||||
||
||||
|
|||
||
|||
|| |
|||
||
||
|
||
|
|||
||
||||||
||
|
||||
||
||||
| |||
|
|||
|||||
||
||||
||
||
||
|
||
|||
||
|
|
||
| | |
||||
||| ||
|
| ||
|||
|
||
|||
||
||
|
||
||
|||||
||||
|
|||
||
|
| |
||
||
||| | | ||
|
|||
||
||
|||
||
||
|
|| |
| | |||
|| ||
|||
|
|| |
||
| |||
|||
|
|||
|||
|||
|
|
|||
||
|||
|||
| |||
||
||
||
| | ||
||
|||||
|| |||
||
| ||
|| ||||||
||||
|| | |
|||
|
||| | | |
|||
||
|
|
||
||
|| || || ||
||||
||
|| ||
| ||
||| || | ||
||
||
||
|||||
||
|
||
||| |
||
| ||
|
||
|||
||||
|||
|
| ||
|||
|
|||| || |
|
|||
|
||||||
|
| ||||
|||
|||
||||
|
| ||||
|
|||
||
||
|| |
|||
||
||
|
||
||
||
|
||| |
||| |
||||
|
||
|
|
||
|||||
|
|
|||
||
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35




Figure 7: Running example: Tree generated by hi-
erarchical model-based clustering after second step
of pruning, and diagnostic plot for the circled node.




cdf




0.0
0.2
0.4
0.6
0.8
1.0




Figure 8: Illustration of the DIP statistic.



modal cdf closest to Fn(x).
We generate M samples of
size n from H(x) (M = 100, say) and compute the DIPs
D1, . . . , DM . If the DIP Dorig for the original sample is the
k-th largest among {Dorig, D1, . . . , DM } then we reject the
null hypotheses of unimodality at level k/(M + 1).

4.3 Remarks
Hybrid clustering is based on the premise that groups can
correspond to collections of mixture components, not just
individual components. The purpose of our method is to
identify those collections, not to find a better fitting mixture
model. This is in contrast to the work by Sand and Moore
[10] on repairing faulty mixture models.
Automatic pruning requires specification of a significance
level for the DIP tests; the larger the level, the larger the
pruned tree. The significance level should not be taken too
literally: the total pruning procedure does not constitute
a level  test for unimodality of the multivariate feature
distribution.
First, there is the problem of multiplicity: If we are car-
rying out many tests at a given level , then the probability
of erroneously rejecting one or more of the null hypotheses
is greater than .
Second, we are choosing the projection directions to max-
imize the separation between the clusters. This becomes an


201

issue if the dimensionality of the feature space is large rela-
tive to the total number of observations in the two clusters
which are under consideration. For example, if we have a
total of p + 1 observations in a p dimensional feature space
then there will always be a direction for which the obser-
vations in the two clusters project onto exactly two points,
one for each cluster.
We deal with this problem by first
projecting the combined observations from the two clusters
onto their k largest principal components and then finding
the Fisher discriminant direction in this lower dimensional
subspace. We chose k to be one third of the total number
of observations in the two clusters.


5. EXAMPLES
We show two examples of Hybrid clustering, one with real
data and one with synthetic data simulated from a known
Gaussian mixture.

5.1 Example 1




Figure 9: Olive oil data: Original tree (all nodes)
and pruned tree (black nodes).




0
40
80




2
4
6
8
10
12
14
16
18
20
.25
.5
.75
1




Figure 10: Olive oil data: Histograms of posterior
probabilities P(Y = g|xi) for the data, before pruning


The data for our first example consist of measurements
of eight chemical concentrations on 572 samples of olive
5
10
15
20
0.00
0.02
0.04
0.06
0.08




Missclassification
Rate




Figure 11: Olive oil data: Misclassification proba-
bilities MCg for the 20 components of the mixture
model.




density




|
||
|||
||
||
| |
|||| |
||
|
| ||||||
|||
||
|
| ||
|||
||
| |
|||
0.00
0.05
0.10
0.15
0.20




Figure 12: Pruned node of olive oil tree



oils from nine different areas of Italy. Applying hierarchi-
cal model-based clustering with diagonal covariance matri-
ces and using the BIC to estimate the number of mixture
components results in a mixture model with 20 components,
corresponding to the 20 leaves of the tree shown in Figure 9.
The 20 columns of Figure 10 are histograms of P (Y = g|xi)
for g = 1, . . . , 20, with the counts encoded as grey levels;
the columns thus are a different graphical representation of
the rootograms making up the rows of Figure 4. The bars in
the upper panel of Figure 10 encode the observation counts
in the clusters. If the clusters were all well separated, then
each observation would have posterior probability one for
one of the mixture components and zero for all the others,
and the plot would have a solid black stripe at the top and
be white elsewhere.
We are obviously quite far removed
from this ideal situation. This impression is confirmed by
Figure 11. Some of the mixture components are not very
isolated; observations generated from mixture component 1,
for example, have roughly an 9% probability of being as-
signed to some other component.
Applying our pruning algorithm with significance level
 = 0.01 prunes the nodes shown in grey in Figure 9 and
results in 7 clusters, four of which are modeled by more than
one mixture component. Figure 12 shows a typical diagnos-
tic plot for a node whose daughters are pruned ( = 0.88),
and Figure 13 shows a typical plot for a node whose daugh-
ters are retained ( = 0.01). These two nodes are circled in


202

density




|| | || |
||
| |||
|||| ||
|| |
| | ||
|
||| |||
|||
| || |
||
|||
|||
||||
||||||
| ||
|||||
|||
|| |||||
| ||
| || ||
|
|
0.00
0.05
0.10
0.15
0.20
0.25




Figure 13: Non pruned node of olive oil tree


Figure 9.
Figure 14 is the post-pruning analog to Figure 10. It is
much closer to the ideal of "black stripe, white elsewhere".
The misclassification probabilities shown in Figure 15 also
have decreased significantly; the largest one is now 1.5%
instead of 9%.




50
150




1
2
3
4
5
6
7
.25
.5
.75
1




Figure 14: Olive oil data: Histograms of posterior
probabilities P(Y = g|xi) for the data, after pruning.


Figure 16 shows the cdf's of the margins for the two clus-
terings, pre-pruning in black, post-pruning in grey. If the
mixture components were perfectly separated then the cdf
of the margin would be a step function with a single step at
margin = 1. Pruning brings us closer to this ideal.

In our example we know the group labels of the observa-
tions - we know the area of origin for each olive oil and it
seems reasonable to assume that any groups in the data re-
flect the areas of origin. We therefore assess how closely the
clusters match the areas. Figure 17 shows a two way con-
tingency table of areas on the vertical axis versus clusters
on the horizontal axis, before pruning. Notice that areas
3, 8 and 9 are each broken up into several clusters and the
clustering procedure has not been able to separate out areas
1, 2 and 4. Figure 18 shows the corresponding contingency
table after pruning. Note that areas 3, 8, and 9 now corre-
spond to single clusters and 1, 2, and 4 have been combined
into one cluster. This raises the question how well areas 1,
2, and 4 are in fact separated in the 8 dimensional feature
1
2
3
4
5
6
7
0.00
0.02
0.04
0.06
0.08




Missclassification
Rate




Figure 15: Olive oil data:Misclassification probabil-
ities for the model, after pruning.




Margin
cdf




-1.0
-0.5
0.0
0.5
1.0
0
0.2
0.4
0.6
1




Figure 16: Olive oil data: Cumulative distribution
function of the margins before pruning (black line)
and after pruning (grey line).



space. Figure 19 shows a projection of the observations from
those areas onto the two linear discriminant coordinates [8,
Chapter 11.5]. There is no obvious separation of the points
into groups.
It is convenient to have a numerical measure summarizing
the degree of agreement between groups (areas) and clus-
ters. We use the Fowlkes-Mallows index [4] for this purpose.
The index is the geometric mean of two probabilities: the
probability that two randomly chosen observations are in
the same cluster given that they are in the same group, and
the probability that two randomly chosen observations are
in the same group given that they are in the same cluster.
Hence a Fowlkes-Mallows index near 1 means that the clus-
ters are a good estimate of the groups. For our example, the
Fowlkes-Mallows index before pruning is 0.52, compared to
an index of 0.81 after pruning.
This shows that pruning
substantially improved the agreement between groups and
clusters.

5.2 Example 2
In Example 1, pruning was successful in that it signifi-
cantly improved the agreement between clusters and areas.
The purpose of the second example is to illustrate how hy-
brid clustering performs on data which were in fact gener-
ated from a Gaussian mixture model. We choose a mixture
model that mimics the olive oil data: we estimate mean


203

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1
2
3
4
5
6
7
8
9




23
6
4
1




1
43
2
9




1
10
4
2
6
7
3




37 74
5



3
30
5

45 14
1 8
59
1 32
35
3
3



2
3 22 10

1
8
23



1
20 5




Figure 17: Olive oil data: Two way contingency ta-
ble of areas on the vertical axis versus clusters on
the horizontal axis




1
2
3
4
5
6
7
1
2
3
4
5
6
7
8
9




25
53
6
25
6
13
4




3
200
11
59
1
32
35
3
3



1
43

1
48




Figure 18: Olive oil data: Two way contingency ta-
ble of areas on the vertical axis versus clusters on
the horizontal axis



and covariance for each area and then generate a sample of
the same size as the olive oil data from the corresponding
mixture model.
Applying hierarchical model-based clustering with diago-
nal covariance matrices and using the BIC to estimate the
number of mixture components results in a mixture model
with 9 components, corresponding to the 9 leaves of the
tree shown in Figure 20. Pruning leads to a partition into 7
clusters corresponding to the leaves of the subtree drawn in
black, and increases the Fowlkes-Mallows index from 0.71 to
0.86. Figures 21 and 22 show the contingency tables of areas
versus clusters before and after pruning, respectively. Prun-
ing removes the split of area 3 and merges the two impure
clusters 1 and 2.


6. SUMMARY
The basic premise of model-based clustering is that each
group in the data corresponds to a single component of the
estimated mixture density. If this premise holds, then the
ability to estimate the number of mixture components (equal
to the number of groups) is a major strength of model-based
-42
-41
-40
-39
-38
-37
-36




Figure 19: Olive oil data: Projection of areas 1, 2,
and 4 on linear discriminant directions.




Figure 20: Simulated olive oil data: Original tree
(all nodes) and pruned tree (black nodes).



clustering compared to nonparametric clustering methods.
On the other hand, if the premise does not hold, the re-
sult of model-based clustering can be misleading, because
several mixture components may model the same group.
Consequently the number of mixture components will over-
estimate the number of groups, and the clusters correspond-
ing to individual mixture components will no longer be well
separated.
It is therefore important to be able to decide
whether or not the premise holds and, in case the premise
does not hold, to determine which mixture components cor-
respond to the same group.
We have introduced methods for assessing the degree of
separation between the components of a mixture model, and
between the corresponding clusters. We have also presented
an algorithm for pruning the cluster tree generated by hier-
archical model-based clustering. The algorithm starts with
the tree corresponding to the mixture model chosen by the
BIC. It then progressively merges clusters that do not ap-
pear to correspond to different modes of the data density.
We have applied model-based clustering to a simple syn-
thetic example in which the premise was violated. In this
case the method indeed exhibited the deficiencies that we
had anticipated.
We have also shown that our proposed


204

1
2
3
4
5
6
7
8
9
1
2
3
4
5
6
7
8
9




25
14
7




55
1
15

114

1
91
7
5
4

60
29
39
3
47

4
51




Figure 21: Simulated olive oil data: Two way contin-
gency table of areas versus clusters before pruning.




1
2
3
4
5
6
7
1
2
3
4
5
6
7
8
9




25
55
1
29
7




1
205
7
5
4

60
29
39
3
47

4
51




Figure 22: Simulated olive oil data: Two way con-
tingency table of areas versus clusters after pruning.


diagnostic tools reveal the true structure of the data and
lead to more accurate clustering. Application of our new
techniques to a real-world example has also been encourag-
ing. Our diagnostics have shown that most probably the
premise of model-based clustering was violated in this case
as well, and our Hybrid clustering method has significantly
improved the quality of the clustering.

7. REFERENCES
[1] P. Bickel and J. Fan. Some problems of the estimation
of unimodal densities. Statistica Sinica, 6:23­45, 1996.
[2] P. Bradley, U. Fayyad, and C. Reina. Scaling EM
(expectation-maximization) clustering to large
databases. Technical Report MSR-TR-98-35,
Microsoft Research, 1999.
[3] J.W Carmichael, G.A. George, and R.S. Julius.
Finding natural clusters. Systematic Zoology,
17:144­150, 1968.
[4] E. B. Fowlkes and C. L. Mallows. A method for
comparing two hierarchical clusterings. J. American
Statistical Association, 78:553­569, 1983.
[5] C. Fraley and A. Raftery. How many clusters? which
clustering method? answers via model-based cluster
analysis. The Computer Journal, 41(8):578­588, 1998.
[6] R. Gnanadesikan, J.R. Kettenring, and J.M.
Landwehr. Projection plots for displaying clusters. In
Statistics and Probability: Essays in Honor of C. R.
Rao, pages 269­280. Elsevier/N.Holland, 1982.
[7] J.A. Hartigan and P.M. Hartigan. The dip test of
unimodality. Annals of Statistics, 13:70­84, 1985.
[8] K. V. Mardia, J. T. Kent, and J. M. Bibby.
Multivariate Analysis. Academic Press, London, 1979.
[9] G.J. McLachlan and D. Peel. Finite Mixture Models.
John Wiley & Sons, 2000.
[10] P. Sand and A. Moore. Repairing faulty mixture
models using density estimation. In Machine
Learning: Proceedings of the eighteenth International
Conference, pages 457­464, 2001.
[11] G. Schwarz. Estimating the dimension of a model.
Annals of Statistics, 6:497­511, 1978.
[12] B W Silverman. Density Estimation for Statistics and
Data Analysis. Chapman & Hall, 1986.
[13] Jeremy Tantrum, Alejandro Murua, and Werner
Stuetzle. Hierarchical model-based clustering of large
datasets through fractionation and refractionation. In
Proc. 8th Int. Conf. on Knowledge Discovery and
Data Mining (KDD02), pages 183­190, 2002.




205

