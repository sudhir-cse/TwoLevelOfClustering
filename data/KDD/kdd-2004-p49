Clustering Time Series from ARMA Models
with Clipped Data


A. J. Bagnall
School of Computing Sciences
University of East Anglia
Norwich, England
ajb@cmp.uea.ac.uk
G. J. Janacek
School of Computing Sciences
University of East Anglia
Norwich, England
gjj@cmp.uea.ac.uk


ABSTRACT
Clustering time series is a problem that has applications in
a wide variety of fields, and has recently attracted a large
amount of research. In this paper we focus on clustering
data derived from Autoregressive Moving Average (ARMA)
models using k-means and k-medoids algorithms with the
Euclidean distance between estimated model parameters.
We justify our choice of clustering technique and distance
metric by reproducing results obtained in related research.
Our research aim is to assess the affects of discretising data
into binary sequences of above and below the median, a
process known as clipping, on the clustering of time series.
It is known that the fitted AR parameters of clipped data
tend asymptotically to the parameters for unclipped data.
We exploit this result to demonstrate that for long series the
clustering accuracy when using clipped data from the class of
ARMA models is not significantly different to that achieved
with unclipped data. Next we show that if the data contains
outliers then using clipped data produces significantly better
clusterings. We then demonstrate that using clipped series
requires much less memory and operations such as distance
calculations can be much faster. Finally, we demonstrate
these advantages on three real world data sets.

Categories and Subject Descriptors: H.2.8 [Database
Management]: Database Applications - Data Mining

General Terms: algorithms, theory, experimentation.

Keywords: time series, clustering, ARMA.


1. INTRODUCTION
The mining of time series in general and the clustering
of time series in particular has attracted the interest of
researchers from a wide range of fields, particularly from
statistics [28], signal processing [10] and data mining [26].
An excellent survey of current research in the field has re-
cently been published [24]. This growth in interest in time
series clustering has resulted in the development of a wide




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
variety of techniques designed to detect common underlying
structural similarities in time dependent data. These tech-
niques have been applied to data arising from many areas,
for example: web mining [8]; finance and economics [30];
medicine [13]; meteorology [5]; speech recognition [10]; gene
expression analysis [4] and robotics [33]. Because many of
these applications generate huge data sets, mining with the
raw data may be prohibitively costly in terms of time and
memory. For example, a current project to analyse the tem-
poral evolution of glomerular activity patterns in the an-
tennal lobe of honeybees involves mining gigabytes of neu-
ral measurements [11]. Even simple mining algorithms can
take days to run, much longer if the data cannot be stored
in main memory. Much larger data sets, up to petabytes in
size, can arise in meteorology projects such as the HiGEM
project [17]. In cases like these some form of dimensionality
reduction or discretisation is essential.
This paper examines the effect of discretising time series
on clustering accuracy. We transform real valued time series
into binary series through the process of clipping. Clipping,
or hard limiting, a time series is transforming a real valued
time series Y into a binary series C where 1 represents above
the population mean and 0 below, i.e. if µ is the population
mean of series Y and t denotes time then

C(t) =
1
if
Y (t) > µ
0
otherwise
(1)


Clipped time series retain much of the underlying struc-
ture that characterises the real valued series. We demon-
strate that clustering with clipped data provides the follow-
ing benefits:

· if the series are long enough clustering with clipped
data is not significantly less accurate than clustering
with unclipped data;

· the clusters formed are significantly better with clipped
data when there is at least a small probability of the
data containing outliers;

· significant space and time complexity improvements
can be achieved;

· algorithms developed for discrete or categorical data
can be employed;

· clustering on clipped data can serve as a diagnostic
method for outlier and model misspecification detec-
tion.



49
Research Track Paper

Our aim is to demonstrate these benefits over classes of al-
ternative underlying models and from real world data. In [3]
we assessed the effect of clipping on data originating from
polynomial models on clusters found with the k-means algo-
rithm. We found that, for data originating from a class of a
mixture of two linear models, clipping does not significantly
reduce the quality of the clusters when there are no outliers
and does significantly improve the clustering accuracy in the
presence of outliers, even when the probability of an outlier
is small. In this paper we cluster using k-means, k-medoids
and hierarchical methods with data from classes of Autore-
gressive Moving Average (ARMA) models and real world
data.
An ARMA(p,q) model has the form

Y (t) = 1 · Y (t - 1) + ...p · Y (t - p)+
(t) + 1 · (t - 1) + ... + q · (t - q)

where (t) are normally distributed random variables with
variance 2. ARMA parameters can be estimated from data
using the autocorrelations (see Section 3.2). It is shown
in [21] that there is a relationship between the autocorre-
lation function of clipped and unclipped series and hence
ARMA models fitted from clipped data tend asymptotically
to models from unclipped data. This result, discussed in
more detail in Section 3, forms the theoretical basis for us-
ing clipped data. The experimental evidence of the benefits
of clipping with clustering is presented in Sections 4 to 6.
We describe the space and time complexity improvements
that clipping provides in Section 4. In Section 5, we clus-
ter data from the simplest form of ARMA model, AR(1),
with fixed parameters taken from related research [20, 36].
We show that our method of clustering unclipped data is at
least as good as the alternative methods described in [20,
36]. This demonstrates that our choice of clustering algo-
rithms and parameterisation is not the source of the vari-
ability observed in accuracy between clipped and unclipped
data. We then demonstrate the benefits of clipping on mod-
els randomly sampled from the class of all stationary AR(1)
models and show that, if the series are long enough, clip-
ping does not decrease the accuracy of clustering and is bet-
ter with outliers. In Section 6 we cluster data from fixed
ARMA models used in [27, 28]. Results obtained are com-
parable to those published. We show that clipping produces
more accurate clusters when there are outliers in the data.
We cluster on models randomly sampled from the class of
models given by all possible ARMA(1,0) to ARMA(2,2). We
show that the benefits of clipping still exist in harder, multi-
class, multi-model situations. In Section 7 we demonstrate
the advantages of using clipped series on three real world
data sets: ECG data and population data from [20] and
reality check data set available from [23]. Finally, we sum-
marise our findings and outline the future direction of this
research in Section 8.


2. CLUSTERING TIME SERIES
The approaches to clustering time series can be categorised
as model based or model free. Model based approaches (also
called generative approaches) assume some form of the un-
derlying generating process, estimate the model from each
data then cluster based on similarity between model param-
eters. The most commonly assumed model forms are: poly-
nomial mixture models (e.g. [12, 4, 3]); ARMA (e.g. [32, 27,
35, 20, 36]); Markov Chain and Hidden Markov models (MC
and HMM) (e.g. [33, 8, 29, 34, 37]).
Reviews of methods used to cluster time series can be
found in [3, 24]. Model free approaches involve the specifica-
tion of a specific distance measure and/or a transformation
of the data. Measures based on common subsequences (used
in [9]), shape parameters [25] and correlation (e.g. [30]) have
been proposed. Transformations used in conjunction with
time series data mining include fast fourier transforms; dy-
namic time warping; wavelet transforms; and piecewise con-
stant approximation. A large number of transformations are
evaluated in [24].
One method that has similarities to our approach is the
Symbolic Aggregate Approximation (SAX) transformation
described in [26]. SAX is a transformation from real valued
series into symbolic series through a two stage process of
dimension reduction and discretisation. Clipping could be
considered a specific type of SAX transformation with two
classes and no dimension reduction. SAX may be of more
general use in time series mining than clipping. However,
the properties of clustering with clipped series are worthy of
study because, firstly, binary data can be more compactly
represented and efficiently manipulated and secondly, it is
possible to assess their theoretical behaviour.
Other approaches use a combination of transformation
and model fitting. For example, Kalpakis et al [20] fit ARMA
models to the series then transform the fitted parameters
into the associated Cepstrum. Our approach, described in
detail in Section 3, is to transform the data then fit an
ARMA model.

3. MODEL AND CLUSTERING METHODS
To test the effects of clipping we need to first specify:

1. a class of model from which the data in each cluster
may come and the parameters of the model that re-
quire estimation (Section 3.1);

2. the fitting procedure to estimate the true model (Sec-
tion 3.2);

3. the clustering method employed (Section 3.3);

4. the experimental procedure and method of evaluating
performance (Section 3.4).

3.1 Generating Process
We assume that the series in each true underlying cluster
is generated by an ARMA model. The model for cluster i
is a random variable. A model without outliers is assumed
to be of the form
Xi(t) = 1 · Xi(t - 1) + . ..p · Xi(t - p)+
i(t) + 1 · i(t - 1) + ... + q · i(t - q)
(2)

where i are N(0, ) and , p, q, j and j are constant for
a model. One of the objectives of this paper is to demon-
strate the robustness in the presence of outliers of using a
clipped time series rather than the the raw data for cluster-
ing. Hence, we add a further term to Equation 2 to model
the effect of outliers. A time series is assumed to be gener-
ated by a sequence of observations from the model

Yi(t) = Xi(t) + r
(3)

where r = s·a·b. s is a constant, a  {0, 1} and b  {-1, 1}
are observations of independent random variables, A and



50
Research Track Paper

B, where A has density f(a) = pa(1 - p)1
-a
and B has
density f(b) =
1
2
. r is a random shock effect that can occur
with probability p, and if it occurs it has the effect of either
adding or subtracting a constant s to the data (with equal
probability). s is set to 10 for the experiments described in
this paper, a value large enough to be very unusual but not
so large as to dominate the model. Some example ARMA
time series, with and without outliers, are shown in Figure 1.


ARMA(1,0)




ARMA(2,2)




ARMA(3,3)
ARMA(1,0




ARMA(2,2




ARMA(3,3




Figure 1: Example ARMA time series of length 500.
The series on the right are the same as those on
the left except that outliers have been added with
p = 0.01.

A time series y is a sequence of observations from a model
Y , y =< y(1),... y(t),...y(n) > . A binary data series
is generated by clipping above and below the sample me-
dian. If y is the sample median of the data series y(t),t =
1, ...,n, then the associated discretised time series, c, is de-
fined as

c(t) =
1 if
y(t) > y
0 otherwise
(4)


If several series are generated by the same model, we iden-
tify the generating process by a subscript i and the series
index with a superscript j. Thus series yji would be assumed
to be the jth series from model Yi.
A data set D is parameterised as follows: there are k mod-
els of the form given in Equation 3, each of which generates
l time series, thus

D = {y11,..., yl1,y12,.. .yl2,.. .,ylk,...ylk}

each of the l·k time series is of length n and is sampled at
the same points t = 1,2, ..., n;  defines the variability of
static noise, s the level of random shocks and p the proba-
bility of shocks. Given a data set D, the problem is then to
split the data into distinct sets or clusters, D1,D2,...,Dk
such that,

yji  D yji  Dm if and only if i = m.

To form these clusters, we fit an ARMA model to each
series then cluster on the basis of the similarity of the fit-
ted parameters. The method of fitting and clustering is
described in Section 3.2.

3.2 Model Estimation and Properties
Any invertible ARMA model can be represented as an
infinite AR model,


(t) =



j=0
jX(t - j).


We estimate the AR model for each series by using a stan-
dard three stage procedure. Firstly the autocorrelations
1,2,... up to a fixed maximum lag are calculated (set to 50
in experiments reported). Secondly the partial fitted models
and hence the partial autocorrelations are found by solving
the Durbin-Levinson recursion. These use the fact that the
correlation matrix is Toeplitz to create a computationally ef-
ficient iteration. If the ith autoregressive term of the model
fit to j parameters is denoted i,j and 1
,1
= 1 then the
Durbin-Levinson recursion is

k,k =
k - k
-1
1
,k-1
- k
-2
2
,k-1
- ... - 1k
-1,k-1
1 - k
-1
k
-1,k-1
- k
-2
k
-2,k-1
- ... - 11
,k-1

Finally the model that minimizes the Akaike information
criteria (AIC) is chosen [1]. The AIC for a model with k
AR terms fitted to a series of length n is defined as

AIC = ln ^
2 + 2 ·
k
n

Further information about fitting models can be found
in [18]. The choice of this methodology (DL/AIC) is made
on pragmatic grounds. The Durbin Levinson approach is
simple and efficient especially when used with AIC as a fit-
ting criterion, since the AIC can be calculated directly from
the partial autocorrelations i,j. In studies of model se-
lection such as [16] AIC performs at least as well as other
possibilities e.g. BIC [31]. The drawbacks of the procedure
are well known (see [6]) and in our view minor. DL is ef-
fectively a least squares approach. The alternative is to use
likelihood methods. These have useful asymptotic theory,
but at the cost of increased complexity. Broerson [6] among
others has pointed out that exact maximum likelihood so-
lutions may not give a real advantage in some time series
problems. Other possibilities are the Burg approach [7] and
Godolphins method [14] or to estimate via the spectrum.
For our purposes any advantages of these approaches are of
less importance than the DL/AIC benefits of simplicity and
efficiency.
In [2] we demonstrate, by repeating the experiments de-
scribed in Section 4 of [21], that using the Durbin-Levison
recursions do not adversely affect the efficiency of the esti-
mates of the model parameters.
Kedem, [21] and Kedem and Slud [22] derived several use-
ful links between the original and the clipped series. For
example, if we assume that the unclipped series y is both
Gaussian and stationary to second order, we can use the
bivariate normal distribution to compute probabilities. We
assume there are no outliers in the data (p = 0), and we de-
note the autocorrelation of lag r by Y (r) for an unclipped
model Y and C(r) the autocorrelation of lag r for a clipped
model C. It is not difficult to show that

P[C(r + s) = 1 and C(s) = 1] =
1
4
+
1
2
sin-
1
{Y (r)}.

It follows that the autocorrelations of the clipped series,
C(k), are given by

C(k) =
2
sin-
1
Y (k)
k = ... - 2, -1,0, 1,2, ... (5)



51
Research Track Paper

Since the binary series gives the runs in the time series,
we have, from equation 5, a link between the probability of
a run and the correlation structure of the original process.
Using a Taylor expansion we have

C(k) =
2

Y (k) +
1
63Y
(k) +
3
405Y
(k) + O(7Y (k))

which implies that the two series will have similar correla-
tions but that those from the clipped series will have smaller
absolute values. In fact we can go further and can show that
with the Gaussian assumptions above

E[C(t)Y (t + k)] =
1
2E[
Y (t + k)|C(t)] =
1

2 Y
(k)

Now define

e(t) = C(t) -
1

2 Y
(t)

since E[e(t)Y (t + k)] = 0 we have a process

C(t) =
1

2 Y
(t) + e(t)

where E[e(t)] = 0. This implies that if the original series
can be written in autoregressive form, say

Y (t) + 1Y (t - 1) + 2Y (t - 2) + . .. + pY (t - p) = (t)

then the clipped series has the form

C(t) + 1C(t - 1) + .. . + pC(t - p) =
(t)
2

+ e(t) - 1e(t - 1) - . .. - pe(t - p).

Here of course the errors are correlated. We can deduce
that, given the covariance matrix of the errors e(t), we can
produce a linear transformation which will give us uncor-
related errors. Hence we deduce a linear ARMA model for
original series implies a linear ARMA model for the clipped
series.

3.3 Clustering Procedure
Given a set of parameters to an ARMA model fitted by the
procedure described in Section 3.2 we cluster with k-means
and k-medoids using the Euclidean distance between the fit-
ted parameters. For any clustering experiment, we restart
k-means and k-medoids and take the clustering that mini-
mizes the within cluster distance. In Section 7.3 we also use
hierarchical clustering methods. More complex clustering
mechanisms are used for similar data in [20, 36, 28]. In [24]
it is observed that on many problems Euclidean distance
performs as well or better than more complex measures. In
Sections 5 and 6 we justify our choice of distance measure
and clustering algorithm by demonstrating that Euclidean
distance with k-means performs comparably with other pub-
lished techniques on data from ARMA models.

3.4 Experimental Procedure
An experiment consists of the following steps: set the pa-
rameters; define a model space M; randomly sample the
model space to determine the k cluster models; for each
cluster model, generate l series of length n; fit a model to
each series using the method described in 3.2; cluster the
series u times, taking as the best the clustering with the
lowest within cluster distance from the centroid; evaluate
the accuracy of the best clustering. Clustering performance
is measured by the classification accuracy, i.e. the ratio of
the percentage of the data in the final clustering that is in
the correct cluster. Note we are measuring accuracy on the
training data rather than applying the data to a separate
testing data set. We do this because wish to measure the
effects of outliers in the training data rather than assess the
algorithm's ability to solve the clustering problem. We use
this measure rather than some of the alternatives (see [15])
since we know the correct clustering.
For a given clustering we measure the accuracy by form-
ing a k × k contingency matrix. Since the clustering label
may not coincide with the actual labelling (e.g. all those
series in cluster 1 may be labelled cluster 2 by the clustering
algorithm) we evaluate the accuracy (number correctly clas-
sified divided by the total number of series) for all possible
k! permutations of the columns of the contingency table.
The achieved accuracy is the maximum accuracy over all
permutations.


4. SPACE AND TIME IMPROVEMENTS
One of the benefits of using clipped series is that the data
can be packed efficiently in integer arrays and manipulated
using bit operators. A series of doubles of length n can be
stored in an array of n/64 integers, a worthwhile reduction
particularly in applications that involve very long or very
many series. For very large data sets packing the series may
make the difference between being able to store the series
in main memory and having to access the data from disk.
Hence packing clipped series could provide a significant time
improvement for clustering algorithms that require the re-
calculation of models directly from the data at each iteration
of the clustering.
Fewer operations may also be required to perform calcu-
lations on binary series. One benefit of using clipped data
comes in the calculation of the autocorrelations. An auto-
correlation requires the calculation of the sum of the product
of lagged series with the original, i.e.


Ri =
n


t=i
C(t) · C(t - i).


The autocorrelation calculations for a clipped series repre-
sented as arrays of binaries requires fewer operations than
that of an unclipped series (see [21] for details). We can get
further speed up by packing the clipped series into integers
then using logical bit operators and shift operators to find
the multiplication. To find the sum Ri we shift a copy of
clipped C left i places, logical AND the two series together
to perform the multiplication, then sum the number of bits
in the result to find Ri. This is much faster than looping
through n times. We can also speed up the operation to sum
the bits. Any algorithm to count the bits is (n). We can
however improve the constant terms in the time complexity
function with the use of a lookup table for the number of
bits in 8 bit integers. We use shift operators to evaluate
the integer value of each 8 bit sequence then use a lookup
table to find the number of bits. Figure 2 demonstrates that
calculating the autocorrelations is then approximately three
times faster with the clipped data even when the series are
stored in main memory. Each point in Figure 2 gives the
time taken (averaged over 30 series) to find the autocorrela-
tions. The times include the procedure to clip the data. The
median of each series is found using the quick select algo-
rithm, hence clipping can be done in O(n) time on average.



52
Research Track Paper

0

0
200000
400000
600000
800000
1000000

Length of Series
Tim
e
Unclipped

Clipped




Figure 2: Average time taken to find autocorrela-
tions


Packing the data also offers the potential for improving
the performance of the clustering algorithm. For example,
the mean calculation for k-means becomes the problem of
counting bits in subseries, which can be done more efficiently
than the equivalent calculation with the unclipped data us-
ing masks and bit count lookup tables. As demonstrated in
Section 7.3 distance calculations can be made more efficient
by similar mechanisms.


5. AR(1) MODELS
The first set of experiments are on data from AR(1) mod-
els used in [20, 36]. Kalpakis et al [20] and Xiong and Ye-
ung [36] both include experiments with clustering data from
two AR(1) models with parameter in the range  = 0.3 to
 = 0.6. Kalpakis et al fit a model to the data using Lin-
ear Predictive Coding then transform the parameters into
the associated Cepstrum. They cluster using the Euclidean
distance between Cepstrum as a distance metric.
Xiong and Yeung used an EM clustering algorithm based
on the maximum likelihood estimates of the parameters of
mixture ARMA models. The majority of the experimen-
tation involved data from the same AR(1) models used by
Kalpakis et al.
To demonstrate that clustering data from AR(1) mod-
els using k-means with Euclidean distance on the fitted AR
parameters is approximately as accurate as the Cepstrum
based technique used in [20] and the mixtures of AR mod-
els (MAR) method described in [36] we reproduce the ex-
periments presented in Table 2 in [36] using the Cepstrum
method of [20] and the Euclidean distance between param-
eters. Data sets of 15 series were generated from two AR(1)
models. Data from one cluster is from a model with  =
0.3±0.01, data from the second cluster is from a model with
 as given in the first column of Table 1. It is assumed (but
not stated in [36]) that the data series were of length 256,
as in [20]. We perform two experiments to demonstrate how
the fitting procedure affects the accuracy of the clustering.
Table 1 summarises all the results. Clustering on the fit-
ted parameters and their Cepstrum was performed using
restart k-means. The results for the unclipped data with no
outliers and no AIC were found through fitting an AR model
with a maximum of 20 terms without any check to mea-
sure the significance of the included variables. Clustering on
Euclidean distance between the parameters is significantly
worse than with Cepstrum at each treatment level. This is
Table 1: Clustering accuracy of k-means on AR(1),
averaged over 30 runs


Accuracy (min/av/max/st dev)
AR Coeff
CEP
Euclidean Dist
without AIC, unclipped data, no outliers
0.60±0.01
(0.93/0.99/1.00/0.02)
(0.73/0.97/1.00/0.05)
0.50±0.01
(0.73/0.94/1.00/0.04)
(0.50/0.78/1.00/0.12)
0.40±0.01
(0.63/0.76/0.97/0.07)
(0.50/0.58/0.70/0.07)
with AIC, unclipped data, no outliers
0.60±0.01
(0.93/1.00/1.00/0.01)
(0.90/0.99/1.00/0.02)
0.50±0.01
(0.73/0.95/1.00/0.05)
(0.87/0.95/1.00/0.04)
0.40±0.01
(0.60/0.79/0.97/0.07)
(0.53/0.78/0.93/0.08)
with AIC, clipped data, no outliers
0.60±0.01
(0.73/0.94/1.00/0.05)
(0.80/0.92/1.00/0.05)
0.50±0.01
(0.53/0.84/0.97/0.08)
(0.53/0.81/0.97/0.09)
0.40±0.01
(0.50/0.70/0.90/0.08)
(0.50/0.66/0.87/0.09)
with AIC, unclipped data with outliers, p = 0.01
0.60±0.01
(0.57/0.83/1.00/0.09)
(0.63/0.85/0.97/0.08)
0.50±0.01
(0.57/0.73/0.87/0.07)
(0.53/0.76/0.93/0.08)
0.40±0.01
(0.50/0.63/0.77/0.07)
(0.50/0.61/0.77/0.07)
with AIC, clipped data with outliers, p = 0.01
0.60±0.01
(0.77/0.94/1.00/0.05)
(0.80/0.95/1.00/0.05)
0.50±0.01
(0.67/0.84/0.97/0.07)
(0.57/0.81/0.97/0.08)
0.40±0.01
(0.50/0.69/0.87/0.09)
(0.50/0.67/0.87/0.08)




because of the noise introduced by the insignificant param-
eter estimates. The Cepstrum reduce the effect of this noise
by smoothing and hence produce more accurate clusterings.
We can achieve equivalent accuracy with Euclidean distance
by introducing the standard model selection method of min-
imizing AIC described in Section 3.2. AIC nearly always
reduces the model to 1 or 2 parameters. The second set of
results in Table 1 show that the results are comparable to
those given in Table 2 of [36]. This suggests that for this type
of data k-means is as good as the MAR algorithm presented
in [36]. Secondly, there is no significant difference between
the mean accuracy at any treatment level between the Cep-
strum clustering and the AIC clustering. This demonstrates
that the LPC Cepstrum based clustering described in [20] is
not better than the Euclidean based clustering if a standard
method is used to reduce the noise by removing redundant
parameter estimates.
The third part of Table 1 shows that clustering with clipped
data does seem to degrade clustering performance for this
model with n = 256. The averages are lower for all mod-
els. However, the maximum of each run is approximately
the same. The poor average figure is caused by occasional
very bad run (as shown by the wide differences in minimum
values) and the short data length.
Obviously the LPC Cepstrum and MAR techniques may
be superior for data from other classes of model. However,
our interest focuses on the benefits of clipping the data with
possible outliers. There is no evidence to suggest that more
complex algorithms provide significantly more accurate clus-
tering for data from AR(1) models. Hence we use the sim-
plest clustering methods available (k-means and k-medoids)
and suggest that it is reasonable to assume that the benefits
of clipping observed would also be seen if the more complex
methods described in [36, 20] were used instead.



53
Research Track Paper

To demonstrate the benefits of clipping we repeated the
experiments with a small probability of an outlier, as defined
in Section 3.1. The results in the fourth and fifth part of
Table 1 clearly show that the extra noise in the data means
that the clustering on the clipped data produces more accu-
rate clusterings. The mean difference between the clipped
average and the unclipped average is significantly greater
than zero at every level.

Table 2: Accuracy using k-means and k-medoids on
clipped data with 2 = 0.50

k-means
k-medoids
unclipped, p=0
0.95
0.93
clipped, p=0
0.81
0.80
unclipped, p=0.01
0.57
0.58
clipped, p=0.01
0.81
0.81


Table 2 gives the accuracy of the k-means and k-medoids
for the situations considered in previous experiments. The
results in Table 2 demonstrate that k-means clusters as ac-
curately as k-medoids. There is no significant difference be-
tween the means of each combination. We use k-means for
the majority of experimentation because it is faster.
To reinforce the observation that clustering with clipped
data is better when the data contains outliers, we experi-
ment with the parameter  chosen randomly on an interval
[-0.9, 0.9] for each cluster. The wider the range the easier,
on average, the clustering task. If the series are of a rea-
sonable length the clustering algorithms should be able to
find the correct clusterings in the majority of cases. Fig-
ure 3 shows the mean and median accuracy difference aver-
aged over 30 runs for random models. Two observations can
be made about these graphs. Firstly, the mean accuracy is
lower with the clipped data when there are no outliers. This
is caused by the occasional very bad clustering, a point il-
lustrated by the fact that the median values are the same.
This effect is reduced as the size of the series increases: with
n = 2000 and p = 0 the observed mean difference in accu-
racy was -0.0013; and with n = 10000 we observed a positive
mean difference (0.0007). In both cases there was no signifi-
cant difference between the medians (using a Wilcoxon test
for paired samples).
Secondly, for low probability of outliers, the average clus-
tering accuracy using unclipped data is significantly worse.
As the probability of a data being an outlier increases the
accuracy on the unclipped data decreases more rapidly than
the accuracy on the clipped data, demonstrated by the pos-
itive slope of both graphs in Figure 3.
Thus for two classes of data from AR(1) models, k-means
clustering based on the Euclidean distance between fitted
parameters and n = 256, clipping the data does not signifi-
cantly reduce the median accuracy of clustering when there
are no outliers and significantly increases the accuracy even
when the probability of an outlier is small. To demonstrate
that this result is not an artifact of the clustering algorithm
we repeat the experiments using the k-medoids algorithm.
Table 3 shows the results for clipped and unclipped data.
The pattern of performance is the same: when there are no
outliers the mean for unclipped data is higher but the me-
dians are approximately the same; when the probability of
a data being an outlier is 0.01 the average (both mean and
median) clustering is significantly better with the clipped
data.
Mean difference accuracy
(clipped-unclipped)




-0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16




0
0.02 0.04 0.06 0.08
Probability of an outlier
Median difference accuracy
(clipped-unclipped)




0
0.05
0.1
0.15
0.2
0.25




0
0.02 0.04 0.06 0.08
Probability of an outlier


Figure 3: Difference in clustering accuracy between
the clipped and unclipped series for varying values
of p


Table
3:
Accuracy
using
k-medoids
to
cluster
clipped
data
from
random
AR(1).
(mean/median/standard deviation)

Unclipped
Clipped
No Outliers
(0.91/1.00/0.164)
(0.87/0.97/0.176)
Outlier p=0.01
(0.80/0.83/0.21)
(0.86/0.97/0.182)




To demonstrate that the result is observable in harder
clustering problems, the experiments were repeated with
larger values of k. Figure 4 shows the median accuracy for
clipped and unclipped data for p = 0 on the left hand graph
and p = 0.01 on the right. For each cluster we are generat-
ing 15 series of length 256 (i.e. l = 15 and n = 256). Since
we keep l and n constant, the accuracy of clustering de-
creases for both clipped and unclipped as k increases. This
achieves the desired result of presenting progressively harder
clustering problems. Figure 4 demonstrates that clustering
on the clipped data improves the accuracy in the presence
of outliers. With no outliers, clustering with clipped data
is less accurate, but this gap in accuracy reduces as n in-
creases. From these experiments we can conclude that if


No Outliers, p=0




0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95




2
3
4
5
Number of Clusters, k
Unclipped

Clipped
Outliers, p=0.01




0.5
0.6
0.7
0.8
0.9
1




2
3
4
5
Number of Clusters, k
Unclipped

Clipped




Figure 4: Clustering accuracy for alternative values
of k, with no outliers (left) and a small probability
of outlier (right)


a set of time series are derived from a mixture model of
AR(1) processes, and if each series is of adequate length,
then clustering (using k-means or k-medoids) with clipped



54
Research Track Paper

data rather than unclipped data: significantly speeds up the
clustering process; does not significantly decrease the accu-
racy of clustering; and significantly increases the clustering
accuracy when the data contains undetected outliers.

6. ARMA MODELS
Maharaj has investigated clustering time series from ARMA
models in [27, 28]. She fits AR series to data from a vari-
ety of ARMA models, then clusters using a greedy algo-
rithm with a distance measure derived from the p-value of
a significance test. The first experiment involved five gen-
erating models (k = 5) each generating four series (l = 4)
of length 200 (n = 200). The models used were AR(1),
MA(1), AR(2), MA(2), ARMA(1,1). The second experi-
ment was designed to be easier. The parameters were the
same except k was changed to four and the models used
were AR(0), AR(1), MA(1), ARMA(1,1). The results for k-


Table 4: Clustering data from fixed ARMA models
with no outliers
Experiment 1 (k=5, l=4, n=200)
Mean
Median
Min
Max
SD
Unclipped
0.79
0.8
0.5
1
0.11
Clipped
0.61
0.6
0.35
0.95
0.084

Experiment 2 (k=4,l=4,n=200)
Unclipped
0.98
1
0.63
1
0.048
Clipped
0.75
0.75
0.5
1
0.117


means clustering using clipped and unclipped data with no
outliers is shown in Table 4. Maharaj assesses the quality of
a clustering by a measure based on the number of clusters
exactly correct. This measure is not directly comparable to
the accuracy measure we use. A simple conversion can be
performed to estimate the accuracy by our definition from
the measure used in [27]. If m clusters are exactly correct
then m·l data are correct. Of the remaining (k-m)·l data, if
we assume each is equally likely to be in any of the remaining
clusters, and we assume that there are k+1 clusters, then we
can estimate the overall accuracy. The estimated accuracy
of the results reported in [27] is 0.60 for Experiment 1 and
0.95 for Experiment 2. Unclipped clustering with k-means
performs better, although of course we have the advantage
of knowing the number of clusters, k, a priori. The clipped
series do not cluster as well on the easier problem (Experi-
ment 2). The relatively large number of clusters means the
data is not long enough to mitigate against the amount of
information that is lost through clipping. However, if we
add some outliers to the data, the situation is reversed. Ta-
ble 5 shows that the outliers effect the accuracy of clusters
on the clipped series far less than that on the unclipped.
For the class of mixture ARMA generating models we wish
to demonstrate that if there are no outliers in the data and
the series are long enough then clustering with clipped data
does not significantly decrease the accuracy and if there are
a small number of outliers, then clipping the data gives a sig-
nificant increase in accuracy. To test these hypotheses we
generated mixture models for two classes randomly struc-
tured from AR(1), MA(1) to ARMA(2,2) and with random
parameters. We are primarily interested in clustering sta-
tionary ARMA models. This is because for long series non
stationary models quickly increase or decrease to a point
Table 5: Clustering data from fixed ARMA models
with probability of an outlier equal to 0.01

Experiment 1 (k=5, l=4, n=200)
Mean
Median
Min
Max
SD
Unclipped
0 .57
0.55
0.4
0.85
0.08
Clipped
0 .60
0.6
0.35
0.85
0.09
Experiment 2 (k=4, l=4, n=200)
Unclipped
0.67
0.69
0.44
1.00
0.11
Clipped
0.74
0.75
0.44
1.00
0.12




where numeric errors can occur. To make sure the ran-
dom model generated is stationary we perform a simple test
where a series is discarded if a value above or below a certain
threshold is observed.
10 series of length 1000 were generated from each class.
The mean classification accuracy for clipped and unclipped
data with and without outliers is shown in Table 6, aver-
aged over 200 experiments. For the data with no outliers,
we are unable to reject the null hypothesis that the pop-
ulation mean difference is zero using a paired two sample
t-test on the mean difference, hence we conclude there is no
evidence to suggest that clustering accuracy is worse with
clipped data for data from this model. When performing the
test with the data containing outliers we are able to reject
the null hypothesis at the 1% level and conclude that there
is evidence to suggest the average accuracy is higher with
clipped data when the probability of a data being an outlier
is 0.01.


Table 6: Clustering accuracy on random ARMA
models, k = 2

Unclipped
Clipped
No Outliers
0.9675
0.9668
Outlier p = 0.01
0.94225
0.9570



On the majority of cases both methods find the correct
clustering. This is because of the wide range of possible
models sampled make this a fairly easy problem. The num-
ber of times one algorithm outperforms another is given in
Table 7. It shows that when there are no outliers, un-
clipped outperformed clipped on 25 models and clipped out-
performed unclipped on 21. With outliers, clipped did bet-
ter on 29 occasions, unclipped on only 12.


Table 7: Number of times one method outperformed
the other on random ARMA models, k = 2

Unclipped
Clipped
No Outliers
25
21
Outlier p = 0.01
12
29



Figure 5 illustrates how the accuracy on the unclipped
data decreases as the frequency of outlier increases when
the series are of length n = 200.
A more complex clustering problem involves random data
from ARMA models with more clusters. To test whether
the benefits of clipping are still present on harder problems



55
Research Track Paper

0.8
0.85
0.9
0.95
1




0
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09

probability of an outlier
Acc
uracy



Clipped

Unclipped




Figure 5: Accuracy for increasing probability of an
outlier


we generated data as before but with k from 2 to 9. Figure 6
shows the accuracy for clipped and unclipped data with n =
400 and no outliers. Figure 7 shows the results from the
same experiment with the probability of an outliers equal to
0.01.




0.5
0.6
0.7
0.8
0.9
1




2
3
4
5
6
7
8
9
number of clusters, k
accu
racy




Clipped

Unclipped




Figure 6: Mean accuracy for data with no outliers




0.5
0.6
0.7
0.8
0.9
1




2
3
4
5
6
7
8
9
number of clusters, k
ac
curacy



Clipped

Unclipped




Figure 7: Mean accuracy for data with probability
of an outlier 0.01

Figures 6 and 7 demonstrate that: the clustering problem
becomes harder with the number of clusters; with no out-
liers clipping does not decrease the accuracy compared to
unclipped data even for harder problems up to six clusters;
and if outliers are present, clipping increases the accuracy.
Clipping is less accurate for 7, 8 and 9 clusters. It is consis-
tent with previous observations to suppose that the clipped
series would be as accurate for larger number of clusters if
the series were longer and that the unclipped data would not
reach the level of accuracy of clipped data if outliers were
present, however long the series.


7. OTHER DATA SETS
Kalpakis et al [20] and Xioung and Yeung [36] also test
their algorithms on four real data sets, available from [19].
We use two of the data sets, an ElectroCardioGram (ECG)
data set and a population data set. We also evaluate the
technique of clipping on a data set from [23], the Reality
Check data set.

7.1 ECG data set
The ECG data consists of 70 series of 1000 ECG measure-
ments. 22 of these patients suffered from malignant ventric-
ular arrhythmia and are denoted Group 1 or venarh. Group
2, or normal, is made up of measurements of 13 healthy peo-
ple. Group 3 (suprav) contains data on 35 people with the
condition superventricular arrhythmia. The ECG data is
characterised by large peaks and troughs and a high degree
of periodicity which would normally indicate that autore-
gressive models are unsuitable [18]. However, both [20, 36]
fitted ARMA models. Kalpakis et al normalise the data,
smooth each series with a period 3 moving average, apply
the difference operator three times and fit AR(2) models to
the data (i.e. fit an ARIMA(2,3,0) to normalised, smoothed
data). They then perform two clustering experiments. Ex-
periment 1 involves clustering data groups 1 and 2 (venarh
and normal). Experiment 2 cluster involves clustering data
groups 2 and 3 (normal and suprav). The results for restart



Table 8: Results for clustering ECG data

Unclipped
Clipped
Experiment 1
0.625
0.604
Experiment 2
0.625
0.825



k-means shown in Table 8 are within the range of values
for the techniques used in [20]. Clipping the data actually
leads to a higher level of accuracy in Experiment 2 than
the best reported in [20], even without outliers in the data.
This improvement can be explained by the unsuitability of
an ARMA model. The peaks in the data set have a simi-
lar effect to the estimators as outliers and clipping removes
their unwanted influence. This highlights another advantage
of clustering with clipped data. A large difference in accu-
racy between clipped and unclipped data could indicate the
presence of outliers, but it could also serve as a means of de-
tecting model misspecification. A model based on estimates
of frequency and amplitude may be more suitable for ECG
data.

7.2 Population data set
The population data set consists of annual population
data for 20 states of the US for the period 1900-1999. The
states are grouped into two clusters, the first with an expo-
nential increasing trend, the second with a stabilizing trend.
Following Kalpakis, we fit ARMA(1,1,0) to each series and
cluster on the parameters. We also added noise to the data
(probability of an outlier p = 0.02). The results are given in
Table 9. The results for k-means with no noise are within
the range of those reported by Kalpakis for autocorellation



56
Research Track Paper

Table 9: Results for clustering population data

unclipped
clipped
No Noise
0.684
0.58
Noise p=0.02
0.53
0.58




based techniques. Clipping results in a significant degrada-
tion in performance. The most obvious explanation for this
is that the series are too short (100 data). The benefits of
clipping can still be observed when noise is added. The out-
liers do not effect the accuracy of the clusters formed with
clipped data, but they significantly degrade the accuracy of
the unclipped data.

7.3 Reality Check
The Reality Check data is a dataset for which the intuitive
clustering can be found with Euclidean distance metrics.
It consists of data from Space Shuttle telemetry, Exchange
Rates and artificial sequences and serves a useful purpose of
providing a basis for alternative distance metrics. Previous
clusterings have been based on clustering model parameters
with k-means. We use the reality check data to demonstrate
that, firstly, clipping does not decrease the quality of clus-
ters formed when distance is based on Euclidean distance
between data rather than parameters of fitted models and
secondly, that clipping does not massively alter the struc-
ture of the dendrogram that results from clustering using
hierarchical clustering methods.
Figures 8 and 9 show the dendrograms with clipped and
unclipped data using average linkage hierarchical clustering.
The only difference in the dendrograms is the order groups
(9,10) and (6,7,8) are linked into largest clusters. The fact
that this is not a major difference is demonstrated by the
observation that the dendrograms for unclipped data using
nearest and furthest linkage show a similar amount of devia-
tion from the average linkage graph to the difference between
Figures 8 and 9.



12

11




6
8
7




9
10
1
4
13
14




2
5
3
13
14




Figure 8: Average linkage dendrogram for unclipped
data

Hierarchical algorithms require the calculation of all dis-
tances prior to the clustering process. Figure 10 demon-
strates that the distance calculation with clipped data is
approximately 5 times faster than with unclipped data. This
speed up, coupled with the reduced space requirements of
12




9
11

10




1
4




6
8
7
13
14




2
5
3




Figure 9: Average linkage dendrogram for clipped
data


clipped data, could provide a justification for clipping in its
own right.




0
5000
10000
15000
20000
25000




100
300
500
700
900
Number of Series
Ti
m
e



NonClipped

Clipped




Figure 10: Average time taken to find distances be-
tween series length 1000



8. CONCLUSIONS
This paper discusses the benefits of clipping data when
clustering time series. For data from the class of ARMA
models we show that, theoretically, the models fitted to
clipped data asymptotically approach the model fitted from
the unclipped data. We demonstrate the application of this
result in clustering through a series of clustering experiments
with k-means and k-medoids using Euclidean distance on
the fitted parameters as a distance metric. We justify our
choice of clustering algorithm and distance metric by re-
peating published experiments and reproducing comparable
results. We then demonstrate how the property of clipped
series can be used in clustering by randomly sampling AR(1)
and ARMA models. Over this class of model we show that:
calculating the autocorrelations is faster with clipped data;
if the data series are long enough clustering accuracy on
clipped data is not significantly less than clustering accu-
racy on unclipped data; and if the data contains outliers,
the clustering accuracy on clipped data is significantly bet-
ter.
Clustering clipped data may provide a sufficiently good
clustering in its own right, particularly when data sets are



57
Research Track Paper

massive and the clustering algorithm is slow. It can also
serve as a means of outlier detection or identification of
the use of an inappropriate model, as demonstrated in Sec-
tion 7.1.
Our advice to researchers wishing to cluster time series
would be to start with clipped data, then examine any re-
sults from more sophisticated transformations in relation to
the results obtained after clipping, particularly if the series
are long and time and space are an important considera-
tions.
The next step in this research will be to evaluate the ef-
fects of clipping on data originating from Hidden Markov
Models and on more real data sets, such as those derived
from biological [11] and meteorological projects [17], using
a wider range of clustering algorithms.


9. REFERENCES

[1] H. Akaike. Likelihood of a model and information criteria.
Journal of Econometrics, 16:3­14, 1981.
[2] A. J. Bagnall and G. Janacek. Clustering time series from
ARMA models with clipped data. Technical Report
CMP-C04-01, School of Computing Sciences, University of
East Anglia, 2004.
[3] A. J. Bagnall, G. Janacek, B. d. Iglesia, and M. Zhang.
Clustering time series from mixture polynomial models
with discretised data. In Proceedings of the second
Australasian Data Mining Workshop, pages 105­120, 2003.
[4] Z. Bar-Joseph, G. Gerber, D. Gifford, T. Jaakkola, and
I. Simon. A new approach to analyzing gene expression
time series data. In Proceedings of The Sixth Annual
International Conference on Research in Computational
Molecular Biology (RECOMB), pages 39­48, 2002.
[5] R. Blender, K. Fraedrich, and F. Lunkeit. Identification of
cyclone-track regimes in the north atlantic. Quart J. Royal
Meteor. Soc., (123):727­741, 1997.
[6] P. Broerson and S. de Waele. Empirical time series and
maximum likelihood estimation. In Proc 2nd IEEE Benelux
Signal Processing Symposium, 2000.
[7] J. P. Burg. Maximum entropy spectral analysis. presented
at 37th meeting of the Society of Exploration
Geophysicists, Oklahoma City, 1967.
[8] I. V. Cadez, D. Heckerman, C. Meek, P. Smyth, and
S. White. Visualization of navigation patterns on a web site
using model-based clustering. In Knowledge Discovery and
Data Mining, pages 280­284, 2000.
[9] G. Das, D. Gunopulos, and H. Mannila. Finding similar
time series. In Principles of Data Mining and Knowledge
Discovery, pages 88­100, 1997.
[10] E. Dermatas and G. Kokkinakis. Algorithm for clustering
continuous density HMM by recognition error. IEEE Tr.
On Speech and Audio Processing, 4(3):231­234, 1996.
[11] R. F. Galan, S. Sachse, C.G. Galizia and A. V.M. Herz.
Odor-driven attractor dynamics in the antennal lobe allow
for simple and rapid olfactory pattern classification. Neural
Computation, 16(5):999­1012, 2004.
[12] S. Gaffney and P. Smyth. Curve clustering with random
effects regression mixtures. In C. M. Bishop and B. J. Frey,
editors, Proceedings of the Ninth International Workshop
on Artificial Intelligence and Statistics, 2003.
[13] A. B. Geva and D. H. Kerem. Fuzzy and Neuro-Fuzzy
Systems in Medicine, chapter 3. Brain state identification
and forecasting of acute pathology using unsupervised fuzzy
clustering of EEG temporal patterns. CRC Press, 1998.
[14] E. J. Godolphin. A direct representation for the
large-sample maximum likelihood estimator of a gaussian
autoregressive-moving average process. Biometrika,
71(2):281­289, 1984.
[15] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. On
clustering validation techniques. Journal of Intelligent
Information Systems, 17(2-3):107­145, 2001.
[16] T. Hastie, R. Tibshirani, and J. Friedman. The elements of
statistical learning: Data mining, inference, and prediction.
Springer-Verlag, 2001.
[17] HiGEM. High Resolution Global Environment and
Modelling. http://www.higem.nerc.ac.uk/index.php.
[18] G. J. Janacek. Practical Time Series. Ellis Horwood, 2001.
[19] K. Kalpakis. Distance measures for clustering time series.
http://www.csee.umbc.edu/ kalpakis.
[20] K. Kalpakis, D. Gada, and V. Puttagunta. Distance
measures for effective clustering of ARIMA time-series. In
Proceedings of the 2001 IEEE International Conference on
Data Mining (ICDM'01), pages 273­280, 2001.
[21] B. Kedem. Estimation of the parameters in stationary
autoregressive processes after hard limiting. Journal of the
American Statistical Association, 75:146­153, 1980.
[22] B. Kedem and E. Slud. On goodness of fit of time series
models: An application of higher order crossings.
Biometrika, 68:551­556, 1991.
[23] E. Keogh and T. Folias. The ucr time series data mining
archive. http://www.cs.ucr.edu/ eamonn/TSDMA/.
[24] E. Keogh and S. Kasetty. On the need for time series data
mining benchmarks: A survey and empirical demonstration.
Data Mining and Knowledge Discovery, 7(4):349­371, 2003.
[25] K. Kosmelj and V. Batagelj. Cross-sectional approach for
clustering time varying data. Journal of Classification,
7:99­109, 1990.
[26] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A symbolic
representation of time series, with implications for
streaming algorithms. In Proceedings of the 8th ACM
SIGMOD workshop on Research issues in data mining and
knowledge discovery, pages 2­11. ACM Press, 2003.
[27] E. A. Maharaj. A significance test for classifying ARMA
models. Journal of Statistical Computation and
Simulation, 54:305­331,1996.
[28] E. A. Maharaj. Clusters of time series. Journal of
Classification, 17:297­314, 2000.
[29] T. Oates, L. Firoiu, and P. R. Cohen. Using dynamic time
warping to bootstrap HMM-based clustering of time series.
Lecture Notes in Computer Science, 1828:35­52, 2001.
[30] P. Ormerod and C. Mounfield. Localised structures in the
temporal evolution of asset prices. In New Approaches to
Financial Economics. Santa Fe Conference, 2000.
[31] D. K. Pauler. The Schwarz criterion and related methods
for normal linear models. Biometrika,85(1):13­27,1998.
[32] D. Piccolo. A distance measure for classifying ARIMA
models. Journal of Time Series Analysis, 11(2):153­164,
1990.
[33] M. Ramoni, P. Sebastiani, and P. Cohen. Bayesian
clustering by dynamics. Machine Learning, 47(1):91­121,
2002.
[34] P. Smyth. Clustering sequences with hidden markov
models. In M. C. Mozer, M. I. Jordan, and T. Petsche,
editors, Advances in Neural Information Processing
Systems, volume 9, page 648. The MIT Press, 1997.
[35] P. Tong and H. Dabas. Cluster of time series models: An
example. Journal of Applied Statistics, 17:187­198, 1990.
[36] Y. Xiong and D.-Y. Yeung. Mixtures of ARMA models for
model-based time series clustering. In Proceedings of the
IEEE International Conference on Data Mining
(ICDM'02), 2002.
[37] S. Zhong and J. Ghosh. Scalable, balanced model-based
clustering. In Proceedings of SIAM Int. Conf. on Data
Mining, 2003.




58
Research Track Paper

