A Data Mining Approach to Modeling Relationships Among
Categories in Image Collection


Ruofei Zhang, Zhongfei (Mark) Zhang
Department of Computer Science
SUNY at Binghamton
Binghamton, NY 13902
USA
{rzhang, zhongfei}@cs.binghamton.edu
Sandeep Khanzode
Polaris Software Lab
Andheri East, Mumbai 400 096
Maharashtra
India
sandeepkhanzode@hotmail.com

ABSTRACT
This paper proposes a data mining approach to modeling
relationships among categories in image collection. In our
approach, with image feature grouping, a visual dictionary
is created for color, texture, and shape feature attributes re-
spectively. Labeling each training image with the keywords
in the visual dictionary, a classification tree is built. Based
on the statistical properties of the feature space we define
a structure, called -Semantics Graph, to discover the hid-
den semantic relationships among the semantic categories
embodied in the image collection. With the -Semantics
Graph, each semantic category is modeled as a unique fuzzy
set to explicitly address the semantic uncertainty and se-
mantic overlap among the categories in the feature space.
The model is utilized in the semantics-intensive image re-
trieval application. An algorithm using the classification
accuracy measures is developed to combine the built classi-
fication tree with the fuzzy set modeling method to deliver
semantically relevant image retrieval for a given query im-
age. The experimental evaluations have demonstrated that
the proposed approach models the semantic relationships
effectively and the image retrieval prototype system utiliz-
ing the derived model is promising both in effectiveness and
efficiency.


Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications--
data mining, image databases


General Terms
Algorithms, Measurement, Design


Keywords
Relationships, semantic category, fuzzy model, image collec-
tion




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
1. INTRODUCTION
Large collections of images have become popular in many
applications, from photo collections to Web pages or even
video databases. To classify or retrieve them is a challenge
which is the focus of many research projects (for instance
IBM's QBIC [6]). Almost all of these systems generate low-
level image features such as color, texture, shape, and mo-
tion, for image index and retrieval. This is partly because
low-level features can be computed automatically and effi-
ciently. The semantics of the images, which users are mostly
interested in, however, is seldom captured by the low-level
features. On the other hand, there is no effective method
yet to automatically generate good semantic features of an
image. One common compromise is to obtain some seman-
tic information through manual annotation. Since visual
data contain rich information and the manual annotation is
subjective and ambiguous, it is difficult to capture the se-
mantic content of an image using words precisely and com-
pletely, not to mention the tedious and labor-intensive work
involved.
It is desirable to organize an image collection in a mean-
ingful manner using image classification. Image classifica-
tion is the task of classifying images into (semantic) cat-
egories based on the available training data. A common
approach to image classification involves addressing the fol-
lowing four issues: (i) image features ­ how to represent the
image; (ii) organization of the feature data ­ how to orga-
nize the data; (iii) classifier ­ how to classify an image; and
(iv) semantics modeling ­ how to address the relationships
between the semantic classes.
In this paper, we propose a data mining approach to mod-
eling relationships among categories in image collection. We
assume that a set of training images with known class labels
is available. Multiple features (color, texture, and shape) are
extracted for each image in the collection and are grouped
to create visual dictionaries. Using the visual dictionaries
for the training images, a classification tree is constructed.
Once the classification tree is obtained, any new image can
be classified easily. On the other hand, to model the se-
mantic relationships between the image categories, a repre-
sentation, called -Semantics Graph, is generated based on
the defined semantics correlations for each semantic cate-
gory pairs. Based on the -Semantics Graph each semantic
category is modeled as a unique fuzzy set to explicitly ad-
dress the semantic uncertainty and the semantic overlap be-
tween the semantic categories in the feature space. For the
image retrieval application, a retrieval algorithm is devel-
oped based on the classification tree and the fuzzy semantics
model for the semantics-relevant image retrieval.




749
Research Track Poster

2. RELATED WORK
Very few studies have considered data classification on the
basis of image features in the context of image indexing and
retrieval. In the general context of information retrieval,
the majority of the related work has been concerned with
handling textual information [2]. Not much work has been
done on how to represent imagery (i.e., image features) and
how to organize the features. In the following, we review
some of the previous work on automatic classification based
image retrieval.
Yu and Wolf presented a one-dimensional Hidden Markov
Model (HMM) for indoor/outdoor scene classification [15].
An image is first divided into horizontal (or vertical) seg-
ments and each segment is further divided into blocks. Color
histograms of blocks are used to train HMM's for a preset
standard set of clusters, such as a cluster of sky, tree, river,
a cluster of sky, tree, grass, etc. Maximum likelihood classi-
fiers are then used to classify an image as indoor or outdoor.
In general, it is difficult to enumerate a set to cover a gen-
eral case such as indoor/outdoor. The configural recognition
scheme proposed by Lipson et al [10] is also a knowledge-
based scene classification method. A model template, which
encodes the common global scene configuration structure us-
ing qualitative measurements, is hand-crafted for each cat-
egory. An image is then classified to the category whose
model template best matches the image by deformable tem-
plate matching (which requires intensive computation, de-
spite that the images are subsampled to low resolutions) --
the nearest neighbor classification.
One early work for resource selection in distributed visual
information systems was reported by Chang et al [3]. The
method proposed was based on a metadata base at a query
distribution server. The metadata base records a summary
of the visual content of the images in each category through
image templates and statistical features. The selection of
the databases is driven by searching the metadata base us-
ing a nearest-neighbor ranking algorithm that uses query
similarity to a template and the features of the database as-
sociated with the template. Another approach [8] proposes
a new scheme for automatic hierarchical image classifica-
tion. Using banded color correlograms, the approach mod-
els the features using singular value decomposition (SVD)
[4] and constructs a classification tree. The technique used
extracts a certain form of knowledge to classify images. Us-
ing a noise-tolerant SVD description, the image is classified
in the training data using the nearest neighbor with the first
neighbor dropped. Based on the performance of this clas-
sification, the categories are partitioned into subcategories,
and the interclass dissociation is minimized through using
normalized cuts. In this scheme, the content representation
is weak (only using color and some kind of spatial informa-
tion) and the semantic overlap among semantic categories
in the feature space is not addressed.


3. IMAGE FEATURES AND VISUAL
DICTIONARIES
To capture as much content as possible to describe and
distinguish images, we extract multiple semantics-related
features as image signatures. Specifically, our framework
incorporates color, texture, and shape features to form a
feature vector for each image in the collection. Since image
features f  Rn, it is necessary to perform regularization
on the feature set such that the visual data can be indexed
efficiently. In our approach, we create a visual dictionary
for each feature attribute to achieve this objective.
3.1 Image Features
The color feature is represented as a color histogram based
on the CIELab space [1] due to its desired property of the
perceptual color difference proportional to the numerical dif-
ference in the CIELab space. The CIELab space is quantized
into 96 bins (6 for L, 4 for a, and 4 for b) to reduce the com-
putational intensity. Thus, a 96-dimensional feature vector
C is obtained for each image as a color feature representa-
tion.
To extract texture information of an image, we apply a
set of Gabor filters [12], which are shown to be effective
for CBIR [11], to the image to measure the response. The
Gabor filters are one kind of two-dimensional wavelets. The
discretization of a two-dimensional wavelet applied to an
image is given by

Wmlpq =
I(x,y)ml(x - p x,y - q y)dxdy
(1)

using Gabor function ml, where I denotes the processed
image,
x,
y denotes the spatial sampling rectangle; p, q
are image positions, and m, l specify the scale and orienta-
tion of the wavelets, respectively.
Applying the Gabor filter bank to an image results, for
every image pixel (p,q), in an M (the number of scales in
the filter bank) by L array of responses to the filter bank.
We only need to retain the magnitudes of the responses:
Fmlpq = |Wmlpq| m = 0, ...,M - 1, l = 0,. ..L - 1 (2)

Hence, a texture feature is represented by a vector with
each element of the vector corresponding to the energy in
a specified scale and orientation sub-band w.r.t. a Gabor
filter. In the implementation, a Gabor filter bank of 6 ori-
entations and 4 scales is performed for each image in the
collection, resulting in a 48-dimensional feature vector T (24
means and 24 standard deviations for |Wml|) for the texture
representation.
The edge map is used with water filling algorithm [17]
to describe the shape information for each image due to
its effectiveness and efficiency for CBIR. A 18-dimensional
shape feature vector, S, is obtained by generating edge maps
for each image in the collection.
Fig. 1 shows visualized illustrations of the extracted color,
texture, and shape features for an example image. These
features describe the content of images and are used to index
the images.

3.2 Visual Dictionary
The creation of the visual dictionary is a fundamental pre-
processing step necessary to index features. It is not possible
to build a valid classification tree without the preprocessing
step in which similar features are grouped. The centers of
the feature groups constitute the visual dictionary. Without
the visual dictionary, we would have to consider all feature
values of all images, resulting in a situation where very few
feature values shared by images, which makes it impossible
to discriminate categories.
For each feature attribute(color, texture, and shape) we
create a visual dictionary, respectively, using Self Organiza-
tion Map (SOM) [9] approach. SOM is ideal for our prob-
lem, as it can project high-dimensional feature vectors to
2-dimensional plane with mapping similar features together
while separating different features apart at the same time.
The procedure to create "keywords" in the dictionary is
similar to the one developed in [16]. By the procedure,
the number of "keywords" is adaptively determined and the
similarity-based feature grouping is achieved. Applying this
procedure to each feature attribute, a visual dictionary is
created for each one.



750
Research Track Poster

(a)
(b)
(c)
(d)

Figure 1: An example image and its corresponding color, texture and shape feature maps. (a)The original
image. (b)The CIELab color histogram. (c)The texture map. (d) The edge map.


4. -SEMANTICS GRAPH AND FUZZY
MODEL FOR CATEGORIES
Although we can take advantage of the semantics-oriented
classification information from the training set, there are
still some issues not addressed yet. One is the semantic over-
lap between the classes. For example, one category named
"river" has some affinities with the category named "lake".
For some users, the images in the category "lake" are also
interesting although they pose a query image of "river". An-
other issue is the semantic uncertainty, which means that
an image in one category may also contain semantic objects
inquired by the user although the category is not for the
semantics in which the user is interested. For instance, an
image containing peoples in an "beach" category is also rel-
evant to users inquiring the retrieval of "people" images. To
address these issues, we need to construct a model to explic-
itly describe the semantic relationships among images and
the semantics representation for each category.

4.1 -Semantics Graph
To describe the uncertainty and overlap of semantic cat-
egories quantitatively, we propose a metric to measure the
scale, called semantics correlation, which reflects the rela-
tionships between two semantic categories in the feature
space. The semantics correlation is based on statistical mea-
sures on the shape of the category distributions.
Perplexity. The perplexity of feature distributions of a
category reflects the uncertainty of the category; it can be
represented based on the entropy measurement [13]. Sup-
pose there are k elements s1,s2,... ,sk in a set with probabil-
ity distribution P = {p(s1),p(s2),..., p(sk)}. The entropy
of the set is defined as

En(P) = -
k


i=1
p(si)log p(si)


By Shannon's theorem [13], this is the lower bound on the
average number of bits per element (bpe) required to en-
code a state of the set. For a particular semantics repre-
sented in the images, it is difficult to precisely determine
the probability of an image feature p(si). Consequently we
use the statistics in the training semantic category to esti-
mate the probabilities. Since each image is represented as a
3-component vector [C, T,S], the entropy of each category,
ri, is defined as


H(ri) = -
1
Ni
Nj


j=1
P(Cj,Tj,Sj)log P(Cj,Tj,Sj)
(3)


where P(Ci,Ti,Si) is the joint occurrence probability of an
image feature in the category and Ni is the number of im-
ages in the category. Assuming that color, texture and
shape properties are independent in image representation,
i.e., P(Cj,Tj,Sj) = P(Cj)P(Tj)P(Cj) where P(Cj), P(Tj),
and P(Sj) are the occurrence probabilities of the single fea-
ture attribute in the category, respectively, it follows that


H(ri) = -
1
Ni
Ni


j=1
P(Cj)P(Tj)P(Sj)log{P(Cj)P(Tj)P(Sj)}

(4)
As an analogy to the concept of perplexity [14] for a text
corpus, we define the perplexity of a semantic category ri in
the image collection as

(ri) = 2H
(ri)
(5)

which is an approximate measure of the homogeneity of the
feature distributions in the category ri. The more perplex
in the category, the bigger ; and vice versa.
Distortion. The distortion is a statistical measure to
estimate the compactness degree of the category. For each
category, ri, it is defined as


D(ri) =
1
Ni
Ni


j=1
fj - ci
2
(6)


where fj is the feature point j in this category and ci is
the centroid of the category. The distortion describes the
distribution shape of categories, i.e., the looser the category,
the bigger D defined.
Based on these statistical measures on the categories, we
propose a metric to describe the relationship between any
two different categories ri and rj, i = j, in the category set
Re. The metric, called semantics correlation, is a mapping
corr : Re × Re - R. For any category pair {ri,rj}, i = j,
it is defined as


Li,j =
(D2(ri) + D2(rj))(ri)(rj)
ci - cj
(7)


corri,j = Li,j/Lmax
(8)

where Lmax is the maximal Li,j between any two differ-
ent semantic categories, and Lmax = maxrk,rtRe,
k=t
(Lk,t).
This definition of semantics correlation has following prop-
erties:

· If the perplexity of a category is large, which means
that the homogeneity degree of the category is weak,
it has a larger correlation with other categories.

· If the distortion of a category is large, which means
that the category is looser, it has a larger correlation
with other categories.

· If the inter-category distance between two categories
is larger, the category-pair has a smaller correlation.

· The range of the semantics correlation is [0,1].



751
Research Track Poster

For convenience, the supplement of the semantics correlation
for each semantic category pair is defined as

disci,j = 1 - corri,j
(9)

and is called semantics discrepancy between the two differ-
ent semantic categories, resulting in an quantitative measure
of the relationship between any two different semantic cate-
gories based on their distributions in the feature space.
With semantics correlations defined above, a graph is con-
structed in the category space. We call the graph -Semantics
Graph. It is defined as follows:

Definition 4.1. Given a semantic category set D = {r1,
r2,...,rm}, the semantics correlation function corri,j de-
fined on the set D, and a constant   R, a weighted undi-
rected graph is called -Semantics Graph if it is constructed
abiding to the following rules:

· The node set of the graph is the symbolic category set.

· There is an edge between any nodes i,j  D if and
only if corri,j  .

· The weight of the edge (i,j) is corri,j.

The -Semantics Graph uniquely describes the relation-
ships between semantic categories for an arbitrary . With
a tuned , we can model a semantic category based on its
connected neighbors and corresponding edge weights in the
-Semantics Graph.

4.2 Fuzzy Model for Categories
To address the semantic uncertainty and the semantic
overlap problems, we propose a fuzzy model for each cat-
egory based on the constructed -Semantics Graph. In this
model , each semantics category is defined as a fuzzy set
while one particular image may belong to several semantic
categories.
A fuzzy set F on the feature space Rn is defined by a
mapping µF : Rn  [0, 1] named the membership function.
For any feature vector f  Rn, the value of µF (f) is called
the degree of membership of f to the fuzzy set F (or, in
short, the degree of membership to F). For a fuzzy set F,
there is a smooth transition for the degree of membership
to F besides the hard cases f  F (µF(f) = 1) and f /
 F
(µF(f) = 0).
The most commonly used prototype membership func-
tions are cone, trapezoidal, B-splines, exponential, Cauchy,
and paired sigmoid functions [7]. Since we could not think
of any intrinsic reason why a particular one should be pre-
ferred to any other, we tested the cone, trapezoidal, expo-
nential, and Cauchy functions in our system. In general, the
performance of the exponential and the Cauchy functions is
better than that of the cone and trapezoidal functions. Con-
sidering the computational complexity, we use the Cauchy
function because it requires much less computation. The
Cauchy function is defined as

F(x) =
1

1 + (
x-v
d
)

where d and   R, d > 0,  > 0, v is the center loca-
tion (point) of the fuzzy set, d represents the width of the
function, and determines the shape (or smoothness) of the
function. Collectively, d and  portray the grade of fuzzi-
ness of the corresponding fuzzy set. For fixed d, the grade
of fuzziness increases as  decreases. If  is fixed, the grade
of fuzziness increases with the increase of d.
For each category, the parameters v and d are determined
based on the constructed -Semantics Graph. For the center
point of each semantic category ri, it can be conveniently
estimated by the mean vector, ci, of the feature vectors in
the category. For the width di, it is determined as follows:


di =
w


k=1
ci - cw corri,w
(10)


where {c1,c2,...,cw} is the set of the centroids of all con-
nected nodes to the node ri in the -Semantics Graph and
is the Euclidean distance in Rn. In other words, the
width of the membership function for each category is a
semantics correlation weighted combination of the distance
to its connected nodes in the -Semantics Graph. Conse-
quently, each category ri in the training set is modeled as a
unique fuzzy set

Fi(f) =
1

1 + (
f-ci
di
)
(11)


Denoting the distance between a feature f and ci as dist,
the above equation can be equally presented as

Fi(dist) =
1
1 + (dist
di
)
(12)


The experiments show that the performance changes in-
significantly when  is in the interval [0.7, 1.5], but degrades
rapidly outside the interval. Thus, we set  = 1 in Eq. 11
to simplify the computation.


5. CLASSIFICATION BASED RETRIEVAL
ALGORITHM
With the three visual dictionaries ready, an order for the
"keywords" in the visual dictionaries is determined and an
index to each "keyword" is assigned. Given an image, for
each feature attribute, replace it by the index of the "key-
word" to which it is assigned in the corresponding visual dic-
tionary. Hence each image in the training set is represented
by a tuple Img[Color,Texture,Shape] while each attribute
has discrete value type in a limited domain.
To build a classification tree, C4.5 algorithm [5] is applied
on the training tuple sets transformed. We assume that
each image in the training set belongs to only one semantic
category. The splitting attribute selection for each branch
is based on information gain ratio [5]. Associated with each
leaf node of the classification tree is a ratio m/n, where
m is the number of images classified to this node and n is
the number of incorrectly classified images. This ratio is a
measure of the classification accuracy of the classification
tree for each class in the training image set.
A retrieval algorithm is developed based on the classifica-
tion and its accuracy with the fuzzy model for the category
to which the query image is classified. In this algorithm,
the category is predicted by the classification tree for the
query image. At the same time, a reference feature is deter-
mined by inverse analysis from the classification accuracy;
the reference feature's membership values of the semantic
categories of the neighborhood to the predicted category in
the -Semantics Graph are determined. The intuition is il-
lustrated in Fig. 2. In this figure, two categories modeled
with the fuzzy set are shown. Every vector in the feature
space is associated with the two categories by the obtained
membership values. These membership values are used to
guide the sampling percentage in the corresponding seman-
tic categories. In addition, since the algorithm is orthogonal
with the distance metric DM, different distance metric DM
can be used for different applications. In our evaluation ex-



752
Research Track Poster

(a)
(b)

Figure 2: Illustration of two semantic category models in
the feature space. (a) Sideview. (b) Topview; the dark (red)

curve represents part of the intersection curve.



periment, we use Euclidian distance as DM for its simplicity
and effectiveness.
With this algorithm the images are retrieved not only
based on the category the query image is classified to (which
is called primary category) but also based on the semantics
correlations between this primary category and other neigh-
boring categories in the -Semantics Graph constructed.
The percentage of images sampled in each potential rele-
vant categories is determined by the corresponding classi-
fication accuracy and its fuzzy model. Intuitively, we give
most share to the primary category; the shares to the con-
nected categories of the primary category in the -Semantics
Graph are based on their semantics correlations with the pri-
mary category. In other words, more shares are given to the
highly semantics-correlated categories while fewer shares to
the lowly semantics-correlated categories. Consequently, we
have solved the semantic uncertainty and semantic overlap
problems explicitly.


6. EXPERIMENT RESULTS
We have implemented the approach in a prototype sys-
tem on a platform of Pentium IV 2.0 GHZ CPU with 256M
memory. The image retrieval evaluations were performed on
a general-purpose color image collection containing 10,000
images from COREL collection of 96 semantic categories.
Each semantic category has 85­120 images. Images in the
same category are often not all visually similar. For this im-
age collection we randomly shuffle the images in each cat-
egory and take 50% of them as the training set to train
the image classifier. To evaluate the image retrieval perfor-
mance, 1,500 images were randomly selected from all cate-
gories of the remaining 50% of the COREL collection as the
query set. The relevancy of the retrieved images is subjec-
tively examined by the users and the retrieval accuracy is
the average values across all query sessions.
Before we evaluate the prototype system, an appropriate
 must be decided. For the extreme case  = 0, each node is
connected to all other nodes in the 0-Semantics Graph (all
categories are treated as semantics-related to each other);
for  = 1, each node is isolated (with no edges connected
to other nodes), the 1-Semantics Graph degraded to a cat-
egory set. In the experiment we have calculated pair-wise
semantics correlation corri,j for all the category pairs in the
training set; the third quartile, which is obtained as 0.649
for the training set, was used as the  in the prototype.
Fig. 3 shows an excerpted -Semantics Graph example
for the categories in the training set. The annotation of
each category is labeled on its node. The length of each
Figure 3: An example of -semantics graph.




(a)
(b)
(c)

Figure 4: Three test images. (a)This image is associated
with a single category by -semantics graph. (b)This image
is associated with 3 categories. (c)This image is associated

with 7 categories.




edge between two nodes in the figure is proportional to the
semantics discrepancy between the two corresponding cat-
egories. It is noticeable that the semantic uncertainty and
the semantic overlap among categories described in Section
4.1 are measured explicitly. For example, for the "out-
door scene" category, category "castle" is more semantics-
correlated than "beach" category; category "waterfall" has
strong semantics correlations with "fishing", "rafting", and
"beach" categories; "peasant life" category is connected to
"outdoor scene" and "fashion model" categories. These se-
mantics correlations measured in the feature space among
categories agree well to the subjective perceptions of the
image contents.
Fig. 4 shows three test images with 0, 2 and 6 cate-
gories connected in the constructed -semantics graph re-
spectively. The primary category assigned to Fig. 4(a) is
category "china", which is correct, without any edges con-
nected. Fig. 4(b) is assigned primary category as "people"
and two categories, "building" and "outdoor scene", are con-
nected to the primary category with the corresponding se-
mantics correlations 0.652 and 0.723, respectively. Based on
the subjective observation, "building" is not relevant while
the primary category "people" and the other connected cate-
gory "outdoor scene" are. The primary category of Fig. 4(c)
is "winter season" with connections to "building", "beach",
"European town ", "mountain", "sea shore", and "vacation
resort" categories in the -semantics graph. Although the
primary category "winter season" assigned to this image by
the classification tree is not semantically relevant, there are
4 semantically relevant categories ("building", "European
town", "sea shore", and "vacation resort") connected with
the primary category ("winter season") this image is classi-
fied to. Thus, the retrieval accuracy is significantly improved
by incorporating these categories into the fuzzy model de-
scribed in Section 5.



753
Research Track Poster

Figure 5: Average precision comparison with / with-
out -Semantics Graph.


Table 1: The classification and retrieval precision statistics.
Average
classifi-
cation
accuracy
Average re-
trieval pre-
cision

Correctly classified images
0.902
0.672
Incorrectly classified images
0.815
0.343



To evaluate the effectiveness of the semantics correlation
measurement and the fuzzy model for categories, we have
compared the retrieval precision with and without -Semantics
Graph. Fig. 5 shows the results. From the figure, it is ev-
ident that the -Semantics Graph and the derived fuzzy
model for categories improve the retrieval precision signifi-
cantly. These results substantiate our motivations: by ex-
plicitly addressing the semantic uncertainty and the seman-
tic overlap, the classification errors can be substantially re-
duced for image retrieval.
To evaluate the influence of the error rate of the classifica-
tion tree, we have recorded the statistics of the correspond-
ing retrieval precisions for the testing and training sets. Two
evaluation statistics are recorded. They are:

Average classification accuracy: The average value of the clas-
sification accuracy for training images in all categories.

Average retrieval precision: The average ratio of relevant im-
ages in top 50 retrieved images for every query image.

The results are shown in Table 1.
Another advantage of our method is its high online query
efficiency. In most state-of-the-art CBIR systems, the search
is performed linearly. In other words, the computation com-
plexity is O(n) for an image collection with n images. In our
method, the average computation complexity is O(log m) for
image classification and O(w) for image similarity compu-
tation, where m is the number of image categories and w
is the average number of images in a category. Thus, the
overall complexity is O(log m+w). Hence, with image clas-
sification the computation complexity of our method is much
more tractable than that of the linear search methods. This
conclusion is also validated in the experiment. The observed
average query time for returning top 30 images is less than
0.5 second.
7. CONCLUSIONS
In this paper, we have proposed a data mining approach
to modeling relationships among categories in image col-
lections. A semantics correlation based structure, called
-Semantics Graph, is proposed to represent the semantic
uncertainty and the semantic overlap explicitly. Founded
on the -Semantics Graph, each semantic category is mod-
eled as a fuzzy set which captures the statistical distribu-
tion in the feature space. With the generation of a mul-
tiple feature (color, texture, and shape) supported visual
dictionary, a classification tree is trained using a provided
training set. The model derived is utilized in the image re-
trieval application. A unique image retrieval algorithm is
developed through integrating the classification results and
the fuzzy model for each category. With the effective su-
pervised learning applied to the image collection and the
precise modeling of image semantic categories, the proposed
methodology inaugurates a new generation of content-based
image retrieval approaches, which aims at achieving more
semantics-relevant performance.


8. REFERENCES
[1] K. R. Castleman. Digital Image Processing. Prentice Hall,
Upper Saddle River, NJ, 1996.
[2] S. F. Chang, J. R. Smith, M. Beigi, and A. Benitez. Visual
information retrieval from large distributed online repositories.
Comm. ACM, 40(2):63­67, 1997.
[3] W. Chang, G. Sheikholeslami, J. Wang, and A. Zhang. Data
resource selection in distributed visual information systems.
IEEE Trans. on Knowledge and Data Engineering,
10(6):926­946, Nov./Dec. 1998.
[4] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. Indexing by latent semantic analysis. Journal of
American Sociation of Information Science, 41:391­407, 1990.
[5] M. H. Dunham. Data Mining, Introductory and Advanced
Topics. Prentice Hall, Upper Saddle River, NJ, 2002.
[6] M. F. et al. Query by image and video content: The qbic
system. IEEE Computer, 28(9):23­32, September 1995.
[7] F. Hoppner, F. Klawonn, R. Kruse, and T. Runkler. Fuzzy
Cluster Analysis: Methods for Classification, Data Analysis
and Image Recognition. John Wiley & Sons, New York, 1999.
[8] J. Huang, R. Kumar, and R. Zabih. An automatic hierarchical
image classification scheme. In The Sixth ACM Int'l Conf.
Multimedia Proceedings, 1998.
[9] T. Kohonen, S. Kaski, K. Lagus, J. Saloj¨arvi, J. Honkela,
V. Paatero, and A. Saarela. Self organization of a massive
document collection. IEEE Trans. on Neural Networks,
11(3):1025­1048, May 2000.
[10] P. Lipson, E. Grimson, and P. Sinha. Configuration based scene
classification and image indexing. In The 16th IEEE Conf. on
Compuer Vision and Pattern Recognition Proceedings, pages
1007­1013, 1997.
[11] W. Y. Ma and B. S. Manjunath. A comparison of wavelet
transform features for texture image annotation. In Internation
Conference on Image Processing, pages 2256­2259, 1995.
[12] B. S. Manjunath and W. Y. Ma. Texture features for browsing
and retrieval of image data. IEEE Trans. on Pattern Analysis
and Machine Intelligence, 18(8), August 1996.
[13] C. Shannon. Prediction and entropy of printed english. Bell
Sys. Tech. Journal, 30:50­64, 1951.
[14] G. Taubin and D. B. Cooper. Recognition and positioning of
rigid objects using algerbraic moment invariants. In SPIE
Geometric Methods in Computer Vision Proceedings, volume
1570, pages 175­186, 1991.
[15] H. Yu and W. Wolf. Scenic classification methods for image
and video databases. In SPIE International Conference on
Digital Image Storage and Archiving Systems, volume 2606,
pages 363­371, 1995.
[16] R. Zhang and Z. Zhang. Hidden semantic concept discovery in
region based image retrieval. In IEEE International
Conference on Computer Vision and Pattern Recogntion
(CVPR) 2004, Washington, DC, June 2004.
[17] X. S. Zhou, Y. Rui, and T. S. Huang. Water filling: A novel
way for image structural feature. In IEEE Conf. on Image
Processing Proceedings, 1999.




754
Research Track Poster

