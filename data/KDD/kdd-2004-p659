Estimating the Size of the Telephone Universe:
A Bayesian Mark-Recapture Approach


David Poole
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932
poole@research.att.com


ABSTRACT
Mark-recapture models have for many years been used to es-
timate the unknown sizes of animal and bird populations. In
this article we adapt a finite mixture mark-recapture model
in order to estimate the number of active telephone lines
in the USA. The idea is to use the calling patterns of lines
that are observed on the long distance network to estimate
the number of lines that do not appear on the network. We
present a Bayesian approach and use Markov chain Monte
Carlo methods to obtain inference from the posterior dis-
tributions of the model parameters. At the state level, our
results are in fairly good agreement with recent published
reports on line counts. For lines that are easily classified
as business or residence, the estimates have low variance.
When the classification is unknown, the variability increases
considerably. Results are insensitive to changes in the prior
distributions. We discuss the significant computational and
data mining challenges caused by the scale of the data, ap-
proximately 350 million call-detail records per day observed
over a number of weeks.


Categories and Subject Descriptors
G.3 [Probability and Statistics]: Probabilistic algorithms
(including Monte Carlo), statistical computing


General Terms
Algorithms


Keywords
Bayesian inference; massive datasets; population size esti-
mation; telecommunications


1. INTRODUCTION
On a typical weekday, the AT&T network carries approx-
imately 350 million telephone calls. This traffic includes




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
standard long distance calls within the US, international
calls, calls to toll-free numbers, collect calls, and calls made
using pre-paid cards. Both the originating and dialed num-
bers may belong to wireless telephones. A fundamental
question is the following: how much of the overall total does
this traffic represent? More specifically, how many active
telephone wirelines (i.e. fixed landlines as opposed to wire-
less) do not appear on the AT&T network in a given day,
week, or some other fixed period of time? An answer to this
question is of significant interest because it can be used to
determine wireline share in a current market, or potential
line share in a new market.
There are a number of simple approaches to the problem.
Perhaps the simplest of these is to look at existing published
reports of wireline counts. The Federal Communications
Commission (FCC), publishes reports on many aspects of
the telecommunications industry. The reports on statistical
trends in telephony [2] cover wireline and wireless counts,
payphone data, local and long distance revenues, minutes of
use, call durations, and so on. These reports rely, for the
most part, on data supplied by the various companies in the
industry. For a number of reasons, these data may be incom-
plete. As an example, small carriers with less than 10,000
lines in a state are not required to file their line counts with
the FCC (page 7-1 of [2]). Additionally, it takes a consider-
able amount of time to gather and process all the data for
these reports, so the reports will typically not contain any
information pertaining to the most recent months.
The motivation behind the work in this article is to esti-
mate the whole universe based on a partial, current, view of
that universe. Expressed as a question: is it possible to infer
information about the whole space using only one view (our
own) of that space, without relying on any external data?
Mark-recapture models have a long history in the statis-
tics literature, and particularly in biometry, where they have
been used successfully for estimation of animal population
sizes and survival rates. A number of comprehensive reviews
of work in this area [19, 17, 1] are available. For estimating a
fixed population size, the idea is to sample the population a
number of times and record the degree to which the same in-
dividuals are observed. This information is used to infer the
number of individuals that were unseen, and hence provides
an estimate of the total population size. Mark-recapture has
also been used in applications outside of biometry. For ex-
ample a recapture model, different to the model described
here, was recently used to estimate the size of the World
Wide Web [5].




659
Research Track Poster

Section 2 is a brief outline of mark-recapture models for
population size, and it describes how the technique can be
applied to the telephone line problem. Section 3 is a de-
scription of the call detail data used in the analysis, with
particular emphasis on the software tools required for ma-
nipulating massive datasets of this type. Section 4 discusses
the model fitting process and Section 5 presents the results.
Section 6 describes possible extensions and ideas for further
work.


2. MARK-RECAPTURE MODELS
Suppose that a population consists of N (unknown) in-
dividuals. A sample of n1 individuals is captured, marked
and released. Some time later a second sample of size n2 is
caught, of which m have been marked from the first sample.
An intuitive estimate of N can be obtained by assuming
that the ratio m/n2 should equal n1/N, and the resulting
estimator is:

^
N =
n1n2
m
.
(1)

This is the simplest example of a recapture estimate of a
population size, and is known as the Lincoln-Petersen es-
timator [12, 15]. It was used in the early 20th century
for wildlife populations, although a recapture ratio was also
used earlier by Laplace to estimate the population of France.
If n1 and n2 are regarded as fixed, then m has a hypergeo-
metric distribution and it can be shown that (1) is a maxi-
mum likelihood estimate (MLE) of N. The Lincoln-Petersen
model can be extended to allow for more than two sampling
occasions, as described for example in [19], and multiple re-
captures typically lead to greater precision in the estimate
of N.
Two important assumptions of this model are: (1) N is
closed (no additions or removals) over the period in which
samples are taken, and (2) all individuals are equally likely
to be captured in each sample. Estimates are fairly robust to
mild violations of the first assumption, but variance in cap-
ture probability among individuals (capture heterogeneity)
can be a serious problem because it causes negative bias in
the estimates of N. Various solutions to heterogeneity have
been proposed [14, 16].
Maximum likelihood methods have typically been used to
fit most of these models and to obtain parameter estimates,
but advances in computing power within the last decade
have brought a number of Bayesian methods to the fore [13,
10, 18].

2.1 Estimating active telephone lines
We cast the telephony application in the recapture frame-
work as follows: every telephone line (represented by a 10
digit telephone number) corresponds to an individual that
may or may not be observed at each sampling occasion. For
this exercise a sampling occasion will be a week of usage
on the network.1 Thus the first week in which a number
is observed is the initial "capture and marking", and any
subsequent weeks in which the number has usage will cor-
respond to the "recaptures". As long as the time period
between the first and last weeks of the experiment is not
too long, it is reasonable to assume that the population is

1
Day-to-day variability can be quite large, particularly at
weekends, so we prefer to use a full week rather than a single
day as our sampling unit.
closed. However, the assumption of equal catchability in
this application is violated. For example, it is clear that a
busy business line has a greater probability of generating
network usage in a given week than a seldom-used residen-
tial line. In order to deal with this heterogeneity we employ
a finite mixture model, developed by Pledger [16], with the
following properties:

1. Each individual belongs to one of A groups with prob-
ability a,a = 1,...,A. (Pa = 1).

2. Individuals in group a are captured with probability
a at each sampling occasion, so there is homogeneity
within groups.

Instead of assuming that every line is equally likely to gener-
ate usage in one week, we let each line belong to a group in
which the probability of usage is the same for each member,
but the probability varies among the groups. The value of
A needs to be selected, and this will be discussed later.
If we have k sampling occasions (weeks in this case), then
the probability of a line being observed j times (j  k) is

A
X

a=1
aja(1 - a)k
-j
.


Let fj be the number of lines that are observed j times (j =
1,...,k). The overall likelihood (omitting the constant) for
this model can then be written as


L(N,,) 
N!
(N - r)!
k
Y

j=0
A
X

a=1
aja(1 - a)k
-j
!fj
, (2)


where r is the total number of distinct lines observed over
the k samples, and so f0 = N - r. The model has 2A
independent parameters and k minimal sufficient statistics
f1,...,fk, so we require 2A  k in order to fit the model.
Pledger [16] maximized the log-likelihood using the EM
algorithm. In this article, we employ a Bayesian approach
using standard Markov chain Monte Carlo (MCMC) meth-
ods to obtain samples from the joint posterior distribution
of all the model parameters.


3. DATA COLLECTION AND STORAGE
Unlike many population estimation problems, where data
are often scarce, the volume of data in this application is
very large. Collection and management of the call detail is
itself a computational challenge. Fortunately a number of
tools have been constructed specifically for efficient manip-
ulation and analysis of these data.
The Hancock programming language [3] was developed
for computing "signature" information from large streams
of transactional data. Hancock is C-based, and the code
is highly optimized for trawling through the vast volumes
of traffic on the long distance network. For this study, a
small Hancock program was custom-written to return a list
of the distinct telephone numbers observed on the network
in each of the weeks of the study. Excluding toll-free and
international numbers, the network currently observes ap-
proximately 180 million distinct telephone numbers (both
originating and dialed) each week. Storage of these numbers
in flat ASCII files is time consuming and highly inefficient in
terms of I/O, so the Sfio compression tool [11, 6] was used to
store these weekly lists. Use of Sfio plus the standard Unix



660
Research Track Poster

tool gzip allows an ASCII file of 180 million numbers to be
reduced in size by a factor of 26. gzip alone compresses the
same file by a factor of 4.5.
For this study we chose to use k = 5 weeks, specifically
05/03/2003 through 06/06/2003, since it was felt that 5
weeks was short enough not to violate the assumption of
a fixed N, but was also long enough to yield a viable sam-
ple of data. Once the lists of distinct telephone numbers
for each week were extracted, the next task was to remove
all numbers that belong to cellular, or wireless, telephones.
Since the object was to estimate the number of active wire-
lines in the US, wireless traffic needed to be excluded. The
growth in cellular traffic over the last couple of years has
been dramatic, so it could not be ignored. The Local Ex-
change Routing Guide (LERG), published monthly by Tel-
cordia Corporation, lists the owners of line ranges, so this
was used to identify and exclude all the numbers that be-
longed to wireless companies.2
After the wireless numbers were removed, we were left
with approximately 130 million distinct wirelines each week.
These lines were first divided up by state, based on area
code, and then further divided into 3 categories within each
state: residential, business, or unknown. Classification of
lines into these categories is based on a number of criteria,
including the time of day when most usage occurs. Clearly
a business line is more likely to peak during office hours.
Lines for which the criteria are highly indecisive are placed
in the unknown category. This classification is, of course,
not 100% accurate, but it serves as a useful way to split the
data a priori.
Once the lines were divided up by state and category,
the next task was to determine the capture history of each
telephone line. For each observed line i, there exists a k-
vector i of zeroes and ones which represents the recapture
history of that line over the k weeks of the study. So for
k = 5, the vector

i = (0,1,1,0,1), i = 1,...,r,

indicates that telephone line i was not observed (i.e. had
no usage on the network) in week 1, was first observed in
week 2, was seen again in week 3, went unseen in week 4,
and was observed again in the final week of the study. Each
line can possess exactly one of 2k - 1 possible vectors, since
the all-zero case is unobserved.
Calculation of the i was computationally intense because
it required simultaneous scanning of all k = 5 lists, each of
which contained approximately 130 million telephone num-
bers. Another software tool, VennPivot, was useful for this
purpose. VennPivot was written to construct Venn diagrams
for lists of telephone numbers stored in the Sfio format. For
example, given two lists of numbers, it determines which
numbers are common to both and which appear in only one
or the other. Therefore, given 5 lists corresponding to the
5 weeks of the study, VennPivot returned the counts for
each possible capture history vector . In other words, for
each of the 25 - 1 = 31 possible capture histories, a count
of the number of telephone lines with that specific history
was obtained. Once these counts were known, it was trivial

2
In November 2003, the FCC introduced wireless portabil-
ity, whereby a wireless number can be "ported" to a landline
telephone, and vice versa. This complicates the identifica-
tion of wireless numbers, but is not an issue here because
the period under study was prior to November 2003.
to calculate the fj, the number of lines observed exactly j
times (j = 1,...,5). The fj, in turn, are the data points
which enter the likelihood (2).


4. FITTING THE MODEL
Once the sufficient statistics fj were extracted from the
data, we were ready to proceed with the Bayesian analysis.
Note that the choice of k = 5 weeks restricted us to set A =
2, a simple dichotomy of the lines into two usage groups. To
have A > 2, we would need more than 5 weeks of data, or else
some highly informative prior information. This restriction
was mitigated, however, by our subdivision of the lines into
the residential, business and unknown categories. We fit
a model with A = 2 to each of these three groups, so in
actuality we allowed for six different homogeneous capture
probability groups. We did this separately for each state.
As a result, we obtained an estimate of N for every state
and category combination; these were then summed over
category to determine the line estimate at the state level,
and, in turn, summed over state to calculate the nationwide
estimate.
We chose to place Uniform[0,1] (flat) prior distributions on
all the capture (i) and group membership (i) probabilities.
For N, we chose the uninformative Jeffrey's prior [9]

p(N) 
1
N
,

which was also used, for example, in [7]. This choice means
that the posterior full conditional distribution of N, p(N |
,,r), is a standard Negative Binomial distribution


p(N | ,,r)  NegBin r,1 -
A
X

a=1
a(1 - a)k
!

,
(3)


with N >= r, and we therefore used a Gibbs sampling algo-
rithm to generate samples from it. The posterior conditional
distributions of the i and i do not have standard forms, so
direct sampling from them is not possible. For these cases
we adopted the more general Metropolis-Hastings approach,
similar to that in [18], to obtain the requisite samples from
the posterior distributions.
The MCMC simulations were performed using a C pro-
gram custom-written for the purpose. In each case we ran
the chain for 200,000 iterations with an initial "burn-in" of
10,000, storing every 20th point from the output stream.
This resulted in a sample of size 10,000 from the joint pos-
terior distribution of N,  and . We used standard diag-
nostic tools for MCMC (e.g. [8]) as well as time series plots
of the componentwise posterior samples to verify adequate
convergence and mixing of the Markov chain. Once all the
posterior samples for each state and classification pair were
obtained, they were aggregated first to the state and then
to the national level.


5. RESULTS
Figure 1 is a graphical representation of the raw data fj,
the number of lines seen j times over the 5 weeks, split by
the classification of residence, business, or unknown. The
total number of distinct lines, r, observed over the 5 weeks
was just over 181 million.
It is clear from Figure 1 that the usage pattern of lines
in the unknown group is markedly different from the other
two groups. In this group, more lines were seen in only one



661
Research Track Poster

·
·
·
·
·




Number of weeks with observed usage
Count
of
telephone
lines
(millions)




1
2
3
4
5
0
10
20
30
40




·

·
·
·
·




·


·
·
·
·
Residence
Business
Unknown




Figure 1: Counts, fj, of wirelines observed in the 5
weeks from 05/03/2003 through 06/06/2003. The
lines are split into the residence, business and un-
known categories.



week (out of five) than were seen in two or more weeks. For
residence and business, the number of lines seen every week
was more than double the number seen in any lesser number
of weeks. This is not surprising since the residence/business
classification is in part a function of the observed usage, so
for lines with less usage, the classification is harder and these
lines are more likely to end up in the unknown bucket.
Figure 2 shows the posterior distribution of N summed
across all states and classification groups. It represents the
distribution of the total number of active wireline telephones
in the US in May 2003. The posterior mean, a point esti-
mate, is 200,597,000 and 99% of the mass falls between 199
million and 203 million lines. The distribution is slightly
skewed to the right. Since 181 million distinct lines were ac-
tually seen over the five weeks of observation, we conclude
that roughly 10% of the universe went unobserved during
that time.
Table 1 shows detailed results for the state of New Jer-
sey. The estimates of N are given in thousands, and in each
case the number in parentheses is the number of distinct
lines actually observed over the five weeks (denoted by r in
equation 2). For residence, the posterior mean of N is only
33,000 lines larger than r. In other words, repeated observa-
tion of the same lines in this case leads us to an estimate of
total size that is only about 1% larger than the number that
were observed. This explains why the standard deviation of
this estimate is so small. Since r must be a lower bound for
N, and r is so close to N, there is very little uncertainty in
the lower quantiles of the posterior distribution for N. For
business lines, a similar result is evident, although there is
slightly more variability in the distribution.
In the case of the unknowns, the results are very differ-
ent. Here, the point estimate of N is more than double
the number of lines that were observed, and the posterior
standard deviation is ten times larger than that for known
198
199
200
201
202
203
204
0.0
0.1
0.2
0.3
0.4
0.5
0.6




Active telephone lines (millions)
Density




Figure 2: Posterior distribution of N, the total num-
ber of active wirelines in the US in May 2003. The
mean of the distribution is 200,597,000, just over
200 million lines.


residential lines. The inference is that a large number of
lines that would be classified as unknowns were unobserved
during the five weeks. This is intuitively sensible because
unknowns are typically low usage lines. Although the un-
knowns form a small part of the total number of lines in New
Jersey, it is clear that most of the variability is associated
with them.
Table 1 also illustrates how the two homogeneous capture
groups vary according to the residence/business classifica-
tion. For residential numbers, there is one group whose es-
timated probability of usage in a given week is 1 = 0.461.
The second group has a much higher probability 2 = 0.962.
The latter group can be thought of as the almost "everyday"
users. Approximately one quarter of the lines belong to the
lower usage group (1 = 0.246), so by implication nearly
three quarters belong to the higher usage group. This helps
explain why the estimate of N is only marginally larger than
r. For business, the results are similar, although in this case
roughly 40% of the lines are assigned to the lower usage
group, which has 1 = 0.339. For the unknowns, the cap-
ture group results are very different, as we would expect.
Here, nearly 95% of all the lines are assigned to a group
with very low probability of usage in a week (1 = 0.089).
For the purposes of illustration, we focused on New Jer-
sey in Table 1. The trends are similar in other states, al-
though obviously the aggregated point estimates
^
N vary
widely among the states. Table 2 shows the point estimates
from the model (the posterior means) at the state level and
the corresponding line counts from an FCC report [2] for
the beginning of 2002.3. Considering that the data used in
our model were from May 2003, more than one year later,
the agreement is good. In most cases our point estimates

3
Line counts for the beginning of 2002 were the most recent
data at the state level that we were able to find from this
source. See Table 7.2 of the FCC report.



662
Research Track Poster

Table 1: Results from the model for New Jersey.
The point estimates of N (posterior means) are given
in thousands, as are their standard deviations. In
each case the number in parentheses is the number
of distinct lines (r) observed over the five weeks. 2
is not listed since it is equal to 1 - 1.
NJ
Param.
Estimate
Std. dev.
Residence
N
3,019 (2,986)
7.24
1
0.461
0.015
2
0.962
0.003
1
0.243
0.011
Business
N
3,447 (3,266)
20.01
1
0.339
0.010
2
0.952
0.003
1
0.416
0.010
Unknown
N
553 (218)
97.89
1
0.089
0.018
2
0.851
0.046
1
0.943
0.013



are a few percentage points higher than the counts from the
FCC report. It should also be noted that the FCC counts
used here exclude lines belonging to some competitive local
exchange carriers (page 7-1 of [2]).


6. DISCUSSION
As a result of this analysis, we have obtained point es-
timates of the number of fixed wirelines in each US state
in May 2003. Recall that we chose uniform prior distribu-
tions for all the i and i. We repeated the analysis us-
ing Beta(0.5,0.5) priors, which place higher a priori mass
on high and low values. The results were almost identical
to before. The sheer volume of data behind the likelihood
appears to wash away the effects of the priors in this appli-
cation.
We chose to use k = 5 weeks of data and set A = 2
homogeneous capture groups in the model. With k = 5
we cannot have A > 2. As discussed earlier, however, our
a priori division of lines into the residence, business and
unknown categories actually provides us with six groups,
two within each of the three classifications. Our choice to
use A = 2 was also influenced in part by our own initial
experiments and the observations of other authors [16], who
found that a dichotomy of individuals was often sufficient to
correct for bias induced by heterogeneity. Additional groups
tended to have little effect on the estimate of N.
Use of a longer time period, say 10 weeks, would nonethe-
less still be worthy of investigation. In this case a number of
models with different values of A could be fit, and a model
selection criterion (e.g. a likelihood ratio test) could be used
to pick the best fitting model, and hence the "best" number
of capture groups for the data. This is the approach used
in [16]. The scale of the data in this application needs to be
borne in mind; the addition of each new week creates signifi-
cant computational overhead, particularly in the calculation
of the i, the individual capture histories. Each new week
doubles the number of possible capture histories.
The mixture model we chose allows for capture hetero-
geneity across individuals but not across time. We assume
that a given individual has the same probability of obser-
vation at each sampling occasion (ie. in each week), which
is a reasonable assumption here, particularly over a small
number of weeks. However, the model could be extended
to allow for capture probability to vary with time, as well
as across capture groups. This would, however, add extra
parameters to the model and we do not view it as necessary
for these data.
There is one rather subtle difference between this applica-
tion and the wildlife applications. Here, the sampling occa-
sions are contiguous weeks of telephone usage. The end of
one sampling occasion and the beginning of the next is some-
what artificial, decided simply by date. In the wildlife con-
text, the population is visited, animals are caught, marked
and released back into the population to mingle. The ex-
perimenter leaves and returns at some later time for the
next visit. In this case the sampling occasions are clearly
defined as the visits by the experimenter. The use of the
model, however, remains justified as long as the associated
assumptions regarding capture probability at each sampling
occasion (regardless of how it is defined) remain valid.
Lastly, the question of undetectable lines should be ad-
dressed. Any line which never makes or receives a long dis-
tance call (including toll-free calls) would have probability
zero of being observed on the long distance network. Such
lines have to be excluded as part of the total population
size N which is estimable using the current model. Lines
with a very small, but non-zero, probability of usage are
still legitimately included, provided the mixture model has
enough groups. For this analysis, we have assumed that the
size of the true zero probability group is too small to cause
any real concern. There has, however, been some recent
progress [4] on ways to account for an undetectable segment
of the population, and such methods may be useful in future
applications.


7. CONCLUSION
In this article we presented a mark-recapture model, typ-
ically used for wildlife population estimation, and applied
it to usage data in order to estimate the number of fixed
telephone lines in the US. With the aid of some fast com-
putational software and efficient storage mechanisms, we
were able to sort through approximately 9 billion call de-
tail records, construct capture histories, and apply a Markov
chain Monte Carlo Bayesian procedure to the resulting data.
The results are not inconsistent with existing published re-
ports. There is potential for future work in this and related
areas using these methods.


8. ACKNOWLEDGMENTS
The author would like to thank Deepak Agarwal, Corinna
Cortes, Kathleen Fisher, Anne Rogers, and Daryl Pregibon
for their helpful suggestions and comments on this work.


9. REFERENCES
[1] S. T. Buckland, I. B. J. Goudie, and D. L. Borchers.
Wildlife population assessment: Past developments
and future directions. Biometrics, 56:1­12, 2000.
[2] Federal Communications Commission. Report of the
Industry Analysis and Technology division: Wireline
Competition Bureau. In Trends in Telephone Service.
Available at http://www.fcc.gov/wcb/stats, August
2003.



663
Research Track Poster

Table 2: Point estimates of N (posterior means) for each state, based on usage from May 2003, and state-level
line counts reported by the FCC for early 2002. These counts are in thousands.
State
Est.
^
N (5/2003)
FCC (1/2002)
State
Est.
^
N (5/2003)
FCC (1/2002)
Alabama
2,798
2,498
Montana
573
554
Alaska
466
461
Nebraska
1,119
956
Arizona
3,418
3,095
Nevada
2,010
1,349
Arkansas
1,813
1,509
New Hampshire
976
855
California
25,247
23,386
New Jersey
7,019
6,923
Colorado
3,255
2,949
New Mexico
1,129
1,004
Connecticut
2,704
2,407
New York
13,896
13,077
Delaware
642
590
North Carolina
5,659
5,107
D. C.
1,005
920
North Dakota
412
392
Florida
11,980
11,318
Ohio
7,418
7,054
Georgia
5,878
5,148
Oklahoma
2,321
2,036
Hawaii
740
721
Oregon
2,441
2,171
Idaho
837
763
Pennsylvania
8,534
8,301
Illinois
8,838
8,013
Rhode Island
723
642
Indiana
3,929
3,804
South Carolina
2,615
2,367
Iowa
1,855
1,705
South Dakota
472
410
Kansas
1,933
1,667
Tennessee
3,714
3,386
Kentucky
2,502
2,207
Texas
14,572
13,192
Louisiana
2,734
2,575
Utah
1,382
1,172
Maine
913
884
Vermont
457
426
Maryland
4,150
3,941
Virginia
5,384
4,760
Massachusetts
5,239
4410
Washington
4,370
3,743
Michigan
6,565
6,149
West Virginia
1,081
1,036
Minnesota
3,503
3,136
Wisconsin
3,674
3,525
Mississippi
1,610
1,443
Wyoming
321
313
Missouri
3,771
3,630
Total
200,597
184,080


[3] C. Cortes, K. Fisher, D. Pregibon, A. Rogers, and
F. Smith. Hancock: A language for extracting
signatures from data streams. In Proceedings of the
Sixth International Conference on Knowledge,
Discovery and Data Mining, pages 84­89. ACM
SIGKDD, August 2000.
[4] C. Q. Da-Silva, J. Rodrigues, J. G. Leite, and L. A.
Milan. Bayesian estimation of the size of a closed
population using photo-id data with part of the
population uncatchable. Communications in Statistics
- Simulation and Computation, 32:677­696, 2003.
[5] A. Dobra and S. E. Fienberg. How large is the World
Wide Web? In M. Levene and A. Poulovassilis,
editors, Web Dynamics. Springer-Verlag, New York,
2003.
[6] G. S. Fowler, D. G. Korn, and K. P. Vo. Extended
data formatting using SFIO. In Proceedings of the
2000 Summer Conference, pages 107­116, San Diego,
CA, June 2000. USENIX.
[7] E. I. George and C. P. Robert. Capture-recapture
estimation via Gibbs sampling. Biometrika,
79:677­683, 1992.
[8] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter,
editors. Markov Chain Monte Carlo in Practice.
Chapman and Hall, London, 1996.
[9] H. S. Jeffreys. Theory of Probability. Oxford
University Press, Oxford, 3 edition, 1961.
[10] R. King and S. P. Brooks. On the Bayesian analysis of
population size. Biometrika, 88:317­336, 2001.
[11] D. G. Korn and K. P. Vo. SFIO - a Safe/Fast
String/File I/O. In Proceedings of the 1991 Summer
Conference, pages 235­255. USENIX, 1991.
[12] F. C. Lincoln. Calculating waterfowl abundance on
the basis of banding returns. U.S. Department of
Agriculture Circular, 118, 1930.
[13] D. Madigan and J. C. York. Bayesian methods for
estimation of the size of a closed population.
Biometrika, 84:19­31, 1997.
[14] J. L. Norris and K. H. Pollock. Nonparametric MLE
under two closed capture-recapture models with
heterogeneity. Biometrics, 52:639­649, 1996.
[15] C. G. J. Petersen. The yearly immigration of young
plaice into the Limfjord from the German sea. Report
of the Danish Biological Station, 6:1­48, 1896.
[16] S. Pledger. Unified maximum likelihood estimates for
closed capture-recapture models using mixtures.
Biometrics, 56:434­442, 2000.
[17] K. H. Pollock. Modeling capture, recapture, and
removal statistics for estimation of demographic
parameters for fish and wildlife populations: Past,
present, and future. Journal of the American
Statistical Association, 86:225­238, 1991.
[18] D. Poole. Bayesian estimation of survival from
mark-recapture data. Journal of Agricultural,
Biological and Environmental Statistics, 7:264­276,
2002.
[19] G. A. F. Seber. The estimation of animal abundance
and related parameters. Macmillan, New York, 2nd
edition, 1982.




664
Research Track Poster

