A Study of Support
Vectors on Model Independent
Example
Selection


Nadeem
Ahmed
Syed
Huan
Liu
Kah Kay Sung
Program for Researchin Intelligent Systems (PRIS)
School of Computing
National University of Singapore
Singapore 119260
{nadeem, huh, sungkk}@comp.nus.edu.sg


Abstract

As databases for real-world
problems increase in size, there is
a need in many situations
to select and keep relevant training
data for efficient storage and processing reasons.
Support
vector machines (SVMs) reportedly
exhibit
certain desirable
properties
in selecting
and preserving
useful training
data
as support
vectors.
This
paper
attempts
to quantify
the extent
to which SVM training
behaves like a model
independent
example
selection
procedure.
Using several
common machine-learning
training
databases, we compare
the
prediction
results
obtained
by different
classifiers,
trained
with
data selected by SVMs
and by two other
example
selection
methods
(IB2
and random
sampling).
Some interesting
observations
are made with explanations.


1
Introduction

Example-based
learning
is an attractive
framework
for
extracting
knowledge
from empirical
data, with the goal
of generalizing
well on new input
patterns.
Many
real-
world
processes
can be formulated
as a Z-way classi-
fication task that may be solved using example-based
learning
methods.
When
developing
classifiers
using
learning
methods,
though
more training
data
can re-
duce risk,
the learning process can itself get computa-
tionally
intractable.
This
issue is becoming
more ev-
ident today, because there are complex classification
problems waiting to be solved in many domains, where
large amounts of training
data are already available [9].
Researchers
in the machine
learning
and data mining
community
[6, 5, ll] have therefore been trying to scale
up classical inductive learning algorithms to handle ex-
tremely large data sets.
One popular
approach
for dealing
with
the in-
tractability
problem of learning from huge databases

bmission
to make digital or hard topics ofall or Part ofthis work fb,
Personal or classroom USCis granted without fee provided
t]lar
e-pies
are not made or distributed fbr profit or commercial advantage and that
copies hear this mxicc and the full citation on the tirst page. To copy
otllcr\visc, 10quhlish,
to post on servers or to rcdisrribute to lists.
requires prior specific permission and!or a f&,
KDD-99
San Diego CA lJSA
%wkht
ACM 1999 l-581 13-143-7/99/08...$5.00
is to select small data subsets for training.
Training
with a small data sample from a large database requires
less computation
and memory resource. On the other
hand, a small training
set can omit important
informa-
tion captured in the original large data set, and hence
lead to significantly
poorer prediction
results.
Vari-
ous data sampling methods have been studied [3] such
as windowing
[13], random sampling,
stratified
sam-
pling, peepholing
[5], and information-theoretic
peep-
holing [lo]. Catlett's study [S] showed that certain sam-
pling strategies helped in speed-ups, and improving
the
accuracy of classifiers in noise free domains.
However,
he also acknowledged that naive sampling methods are
often not suitable for real world domains with noisy
training
data, where prediction
results can degrade un-
predictably
and significantly.
Boundary
hunting
methods
like IB2 [l] seek out
training
examples near class boundaries because these
examples tend to be useful for locating class boundaries
precisely.
Assuming only a small fraction
of training
examples in a large database lie along class boundaries,
these boundary
hunting
methods
can be desirable
for
selecting small but highly
informative
training
data
subsets.
However, examples selected by a particular
boundary hunting method may be specifically tuned for
use with an implicit
classifier architecture,
and hence
may not work well for training
other classifier types.
An interesting
theoretical
issue to address is the
extent
to which
one can select small
but
highly
informative
training
data subsets from large databases
in a model independent fashion.
Model independence
means selecting
data that
trains
well with
a wide
range of different classifier types, such that prediction
accuracy for each type is comparable to results obtained
from training
with the entire data set. From a practical
standpoint,
model independent
example selection can
be extremely
useful for efficient data storage, because
one can treat omitted data points as being redundant
in a very broad sense, and can hence be safely and
permanently
discard from the database.
Support Vector Machines (SVMs) are a recently de-
veloped class of statistical
learning
architectures
de-




272

rived from the structural
risk minimization
principle
[16, 41. Over the past few years, researchers have re-
ported experimental
results suggesting that the SVM
training
procedure
exhibits
certain
promising
model
independent
example selection characteristics
for pat-
tern classification.
Specifically:
1) SVM training
in-
volves selecting a small subset of critical
data points
(known as support vectors) from the original training
database. This critical data subset fully and succinctly
defines the classification
task at hand.
2) SVMs can
be implemented
with different kernel functions to sim-
ulate a variety of standard classifier architectures
(e.g.,
polynomial,
Gaussian RBF and Neural Network clas-
sifiers).
SVMs trained with different kernel functions
on the same database have been observed to select al-
most identical data subsets as support vectors [15]. 3)
SVMs trained
with different
kernel functions
on the
same database have given rise to similar high predic-
tion rates on novel data [7].
It is thus tempting to treat SVM training as a form of
model independent example selection procedure.
This
paper attempts to verify the model independent char-
acteristics of support vectors, obtained via batch mode
SVM training over the entire training database. Notice
that in order for SVM training to be useful as an exam-
ple selection procedure for extremely
large databases,
one also has to evaluate the model independent nature
of data selected by incrementally
applying and combin-
ing SVM training methods on parts of the full database
(refer to our other paper in these proceedings).
Tjsing several common machine learning databases
as benchmarks, we conducted the following two sets of
experiments.
For a few representative
classifier types,
compare the difference in prediction
results of:

training
each classifier type with a full database
versus training the same classifier with SVM selected
examples.
This study reveals how well support
vectors preserve relevant
class information
in a
model independent fashion.

training
each classifier
type with
SVM selected
support vectors versus training
the same classifier
with data selected by two other selection methods:
random sampling and IB2. This study compares the
information-retaining
quality of SVM selected data
against data selected by other methods.

Section 2 describes SVM theory and properties rel-
evant to example selection. The detailed experimental
procedure is given in Section 3, with observations and
explanations
discussed in Section 4.

2
SVMs
and Example
Selection
Support Vector Machines (SVMs) are a general class of
statistical learning architectures that perform structural
risk minimization
on a nested set structure of separating
hyper-planes
[16]. For a classification
problem, given
1 data points
{(xl, yr), . . . , (xl, yl)},
SVM
training
involves solving a quadratic programming
problem and
the optimal solution gives rise to a decision function of
the following form:


f(X) = sgn

1
k
yiCYi(X.
Xi)
+
b

i=l
I

To allow for decision surfaces other than hyper-
planes, one can first non-linearly
transform
the set
of input
training
vectors x1,. . . , xl
into
a higher-
dimensional
feature space using a map @(xi)
++ zi,
and then do a linear separation there.
Often, only a
small fraction of the ai coefficients are non-zero.
The
corresponding
pairs of xi entries (known as support
vectors) and yi output
labels fully define the decision
function.
Together with the Eli coefficients,
they are
preserved for use in the classification
procedure.
All
remaining training
examples may be redundant.
We now discuss the SVM example selection proce-
dure. Given a large input database, run the SV learn-
ing algorithm
using an appropriate
kernel function
I(,
and keep the resulting support vectors as selected exam-
ples. It is tempting to expect support vectors to exhibit
the following characteristics
desirable of sampled data:
1) High
information
content.
Because the optimal
decision function
from SVM training
depends only on
examples that are support vectors, one may view the
support vector set as a comprehensive data sample that
fully defines the classification
task at hand.
2) Rep-
resentative
of available
and
unseen
data.
This
follows from the argument that SVM training
is based
on the structural
risk minimization
principle
which at-
tempts to minimize
an actual risk error measure.
3)
Model/platform
independence.
Fairly platform in-
dependent, since different kernels have been reported to
give similar support vectors. Examples selected should
be good training
data for different
classifier architec-
tures.
Also implies choice of K may not be critical.
The experiments in the next section attempt to quan-
tify the extent to which all this is true.
We are interested in studying
the effect of using a
subset of examples to train a classifier on its predictive
accuracy.
If the change in performance
of various
classifiers,
when trained
with
a selected subset of
examples is comparable,
i.e.
the increase or drop
in performance
of various classifiers is similar,
then
we will have empirical
evidence to confirm
that the
selected subset is indeed model independent.
This
will in turn mean that the selection strategy used to
obtained the subset is model independent.
It might be
that the selection strategy is model independent,
but
on using the selected subset, the predictive
accuracy of
various classifiers drops drastically,
equally among all




2'73

Dataset
1 # of Examples
1 No. of Attr.
Australian
I
670
I
14
Diabetes
760
8
German
1000
24
Heart
270
13
Ionosphere
351
34
Liver-Disorders
345
6
Monks-l
432
6
Monks-2
432
6
Monks-3
432
6
Mushroom
8124
22
Promoter-Gene
106
57
208
60
-

Table 1: The datasets used in experiments.
Figure 1: Diagrammatic
representation
of the experi-
mental procedure



the classifiers. In such a situation,
the selection strategy
is of not much use either.
So to provide evidence for
model independence of any example selection strategy,
we consider the following two criteria:
[Cl]:
The change in performance of classifiers should
be comparable
when trained
with
the the selected
examples against when the classifiers are trained with
all the examples for a given selection strategy 5'.
[C2]: If the predictive
accuracy of any classifier drops
upon using only the selected examples for training, such
a drop in accuracy should not be significant.
If we can obtain
such a method,
then it will
be
very valuable for scaling up classification
algorithms:
any classifier will work just as well with the selected
examples, and the subset can be used to train various
inductive
learning algorithms
efficiently
and quickly.
To verify the model independence of the selected ex-
amples, and to study the effect on the accuracy of the
classifiers trained
with these examples, the following
procedure was adopted (Figure 1 shows the steps):
1. A representative
set of classifiers is selected, repre-
senting various learning architectures.
2. These classifiers are trained with all the data using
lo-fold cross validation
method, and the average classi-
fication accuracy on the test set, PA,ll is recorded.
3. Next, within the lo-fold cross validation
method ex-
ample selection is carried out on the the training
set,
separately using SVM, IB2, and random sampling, and
the selected subsets are stored.
4. Finally,
the classifiers were again trained with the
selected examples, and the average predictive accuracy
on the test set PA,,l is recorded.
We chose to use the following classifiers as a representa-
tive set (a) Multi-Layer
Perceptron
(MLP)
[14, 21
- connectionist
learning architecture.
Simple backprop-
agation algorithm
was used to train the Neural Net-
work.(b)
Nearest
Neighbor
(l-NN)
[8] - instance-
based learning architecture.
(c) C4.5 [12] - tree based
classifier. (d) SVM
[16] - optimal margin classifier.
The datasets used for carrying
out the empirical
studies are listed in Table 1. The datasets are standard
among the machine learning community,
and obtained
from the UCI-machine
learning repository
l. We also
used SVM"ght 2 and IBL 3 in our experiment.
Table
2 provides
predictive
accuracy
of various
classifiers (including
SVM) when trained
with all the
training
vectors. The column labeled "kernel function"
shows the kernel used for the SVM training:
"g" for
gaussian, and L for linear. The numbers in the brackets
show the number of examples saved by the classifier for
classification
stage. For SVM, it means the number of
support vectors, for l-NN and IB2 the number of saved
prototypes.
Since l-NN
stores all the examples from
the training
set, the number in the brackets under this
column shows the average number of examples in the
training dataset. In the case of C4.5 and MLP, they do
not store the training
vectors, and so there are no such
brackets under their column.
The support vectors selected by SVMs after training
were stored as the selected subset.
Similarly
the
prototypes
by IB2 training
were saved after training.
The example selection process was also carried out using
random sampling.
For selecting examples by random
sampling, the number of examples sampled was equal
to the maximum
of the number of examples selected
by SVM and IB2. After doing example selection using
the three strategies, the classifiers were again trained
using only the selected examples,
with
lo-fold
cross
validation,
and the average predictive
accuracy on the
test set PA,,1 was recorded.
This predictive
accuracy
is shown in Table 3 for the classifiers.
Since we are
interested in the change in performance, it will be more
instructive
to look at difference in predictive
accuracy
(PA,,1 - P&d.
Table 4 shows these values, the
negative numbers show a drop in performance on using
just the selected examples for training,
and the positive
numbers would mean an increase in predictive accuracy.


`http://www.ics.uci.edu/Nmlearn/MLRepository.html
2http://www-ai.informatik.uni-dortmund.de/FORSCHUNG/
VERFAHREN/SVMLIGHT/svmlight.eng.html
3http://www.aic.nrl.navy.mil/Naha/software/list.html




274

1 Kernel
Function
1 SVM
1 l-NN
1 IB2
I C4.5
1 MLP
r
Austr.
I ~-0.0005
1 84.63
,-0.0005
(203.9)
81.50
73.58
85.1
97.76
Diab.
77.08(401.7j
1
(603.0)
(691.2j
1
(151.5)
69.52
(253.5)
(
1
64.95
74.4
German
g-o.005
75.10 (487.0)
66.90
(900.0)
61.20
(338.7)
73.7
I;g;
Heart
g-o.0005
84.45
(84.8)
75.58
(243.0)
66.33
(76.1)
77.8
97178
Ionos.
g-1.6
94.57
(167.1)
84.57
(315.9)
86.86
(54.7)
90
99.43
Liver.
g-o. 1
68.68
(209.7)
62.35
(310.5)
57.99
(121.3)
68.4
74.75
Monk-l
g-o.1
100.0 (413.2)
84.91
(500.4)
81.64
(70.3)
100
100
Monk-2
g-o.1
99.17 (425.8)
76.07
(540.9)
68.39
(191.4)
83.7
100
Monk-3
EO.O1
98.91 (116.6)
95.85
(498.6)
88.25
(71.7)
98.9
98.73
Mush.
100.0 (437.3)
100.0 (7311)
99.59
(25.6)
100
91.7
Prom.
L
93.55
(65.7)
81.90
) 87.38
(125.8)
(95.4)
84.90
85.8
100
Sonar
g-o.4
85.47
(187.i)
1
(28.4)
80.20
(46.9)
1 72.6
99.5
Avg.
1 88.87
(261.64)
80.36
(1016.35)
1 76.16
(119.18)
1 84.2
94.71
a




Aus.
Dia.
Ger.
Hrt.
IIlS.
LW.
Mkl
Mk2
Mk3
Msh.
PIYII.
SIW
Avg.
Table 2: Predictive
Accuracy (PA,n)
of various algorithms trained using all the examples.



-
SVM
MLP
98.81
67.71
97.19
100
99.14
63.82
100
100
94.05
99.47
100
99.5
93.31
?G
l-
ection
imv-l-
54.78
52.84
51.59
49.26
85.43
57.70
86.16
71.08
73.15
96.28
80.99
85.41
70.39
c4.5
69.7
67.8
65.0
66.7
88.6
36.6
100
84.8
93.8
95.3
94.4
87
79.25
IB2 SI
MLP
98.36
69.00
98.1
97.04
100
76.81
98
100
94.96
61.32
100
100
91.14
elecnon
I l-NN
I
72.83
65.87
61.5
65.96
87.15
58.04
78.99
65.06
87.16
99.71
79.26
83.06
75.36
1
SVM
85.37
74.48
74.8
81.11
58.00
68.41
96.58
98.5
93.49
91.32
79.37
68.24
81.05
-
c4.5
84.2
70.0
70.6
79.6
69.2
67.5
98
63.2
98.9
86
81.0
69.7
78.16
Randc
Samp
MLP
l-NN
98.81
81.05
82.56
69.37
98.40
66.0
99.26
70.77
99.43
86.01
88.69
56.23
99.82
78.98
100
67.57
99.46
78.19
26.55
98.00
100
78.99
99.52
79.28
91.0
75.85
ZZ
lin
-r
L
SVM
84.18
73.57
74.4
78.88
66.57
67.23
100
98.00
93.17
98.14
87.00
76.33
83.37
c4.5
84.8
77.6
76.6
83.0
94.0
69.5
100
79.0
95.5
98.7
88.5
85.1
86.03
T




Table 3: Predictive
Accuracy (PA,,l)
of various algorithms trained with selected examples.



3
Findings
and Discussion

The SVM selection strategy performs poorly as a model
independent
example
selector,
whereas the random
sampling strategy appears to provide best results - the
average decrease in predictive accuracy is the least. The
following can be observed from Table 4.
1.
Which selection strategy
is the best?
(a) The
random
sampling
selection
strategy
is the best in
terms of model independence.
(b) In spite of better
performance in general, because of its random nature,
random
sampling
can sometimes cause catastrophic
decrease in accuracy, as can be seen the performance
of MLP on the mushroom data when it is trained with
randomly sampled subset. (c) Overall, among the three
example selection strategies,
SVM selected examples
perform the poorest, and random sampling still holds
the edge over other techniques.
2. Which learning algorithm is the most robust in using
the selected examples? (a) MLP in general is the most
robust of all the classifiers, irrespective of the selection
strategy used to select its training
subset.
(b) C4.5
actually
performs very well with training
set selected
using random sampling.
In many cases, its predictive
accuracy improves when trained
with
the randomly
sampled training
subset.
However, its performance
drops in large number of cases on using SVM, and IB2
selected examples for training.
(c) For the majority
of
cases l-NN performs reasonably well when trained with
IB2 selected, and randomly
sampled examples, but its
predictive accuracy drops drastically,
when it is trained
using SVM selected examples.
(d) Conversely, SVM
also performs very badly when trained with IB2 selected
examples. Further it performs badly even when trained
with randomly sampled examples.
Valuable lessons are learned from these observations:
(1) More often than not the random sampling strategy
is a good choice for model independent
example selec-
tion. It is the most robust among the three strategies,
yet simplest and fastest.
(2) In spite of nice claimed
properties of SVM, the use of SVMs for model indepen-
dent example selection is not a feasible idea in general.
(3) Although
the instance based learning
algorithms
and SVMs hunt boundary data points, they vary vastly
in their learning bias. As such using one as an example
selection method to get a training
set for training
the
other will often not work. (4) The observation
(l.(b))
suggests a need for caution when using random sam-
pling as a data reduction method. Sometimes, it could
cause an adverse effect as was also found in [6].
The above evidence suggests that the model indepen-




215

0.59
2.22
-0.29
-10.93
0.0
0.0
-4.68
7.77
0.0
0.0
-1.40
-10.68
-15.31
-20.32
0.86
-4.65
1.25
-4.99
-22.7
-3.72
-0.91
-0.06
-10.0
IB2 Selection
MLP
l-NN
0.6
-8.67
c4.5
-15.4
-6.6
-8.7
-11.1
-1.4
-31.8
0.0
1.1
-5.1
4.7
8.6
14.4
-4.275
-11.21
1.5
-0.74
0.57
2.06
-2.0
0.0
-3.77
-30.38
0.0
0.5
-3.57
-3.65
-5.4
-9.62
2.58
-4.31
-5.92
11.01
-8.69
-0.29
-2.64
L
L
SVM
Selection
SVM
0.74
-2.6
-0.3
-3.34
-36.67
-0.27
-3.42
-0.67
-5.42
-8.68
-14.18
L
c4.5
-0.9
-4.4
-3.1
1.8
-20.8
-0.9
-2.0
-20.5
0.0
-14.0
-4.8
-2.9
-6.04
E
2.35
1.8
1.48
0.0
13.94
-0.18
0.0
0.73
-65.15
0.0
0.0
-3.71
Random
Sampling
n
MLP
l-NN
SVM
1.05
-0.45
-0.45
-0.15
-3.51
-0.9
-0.7
-4.81
-5.57
1.44
-28.0
-6.12
-1.45
-5.93
0.0
1.44
-28.0
-17.66
-5.74
-2.0
-1.86
-2.91
-6.55
-6.lY
-11.05
-4.51
-5.50
L
Table 4: Changes (PAan - PA,,l)
in predictive
accuracy of various classifiers.
The bold cases are those where
accuracy changes by 10% or more. Under each selection strategy, the average change in bold means the most stable.



dence of the support vectors is limited to SVM formu-
lation.
In fact, the bias of the SVM formulation
seems
to be very different from the bias of the algorithms cor-
responding to the SVM kernels. For random sampling,
its very randomness lands it with advantage in gain-
ing model independence.
Its lack of bias means that
in many cases, randomly sampled subsets can still give
us a reasonable performance.
On the other hand, this
randomness also lends itself to being unreliable because
there could be cases when the randomly selected exam-
ples cause catastrophic
loss in predictive accuracy.



4
Conclusion

Starting from the verified claims about SVMs, we set
to search for model independent
example selection via
SVMs. In a nut shell, our endeavor leads us to many
valuable findings
but we fail to achieve the original
objective
- a selection
strategy
that
is better
than
random sampling. We ask where the discrepancy occurs
between the previous findings
about SVMs and our
findings reported here.
One sensible explanation
lies
in what is minimized
in different learning algorithms.
SVMs
minimize
the structural
risk and the other
algorithms
only minimize
the empirical
risk.
When
one tries to minimize
the structural
risk, it is more
likely that different
SVMs end up having the similar
support vector set for the same data set. When one
only minimizes
the empirical
risk, many bias coming
with various algorithms, influence the induction
results.
As such, examples selected by some other learning
algorithms
as the most suitable, may be different from
the support vector set.
In conclusion,
the salient finding of this study is to
realize the limits of SVMs and therefore we are one step
closer towards our goal of model independent
example
selection, and able to suggest a general guideline for
example selection facing a suite of selection strategies.
References
PI

PI

131


[41


[51



[61


[71


181


PI


PO1



[Ill



PI

[131




[141



[151



[161
D. W. Aha, D. Kilber,
and M. K. Albert.
Instance-based
learning
algorithms.
Machine
Learning,
6:37-66,
1991.

I.
Aleksander
and
H.
Morton.
An
Introduction
to
Neural
Computing.
Chapman
and Hall,
1990.

A.L.
Blum
and P. Langley.
Selection
of relevant
features
and
examples
in machine
learning.
AI,
97:245-271,
1997.

C.J.C.
Burges.
A tutorial
on support
vector
machines.
Data
Mining
and Knowledge
Discovery,
2, 1998.

J. Catlett.
Megainduction
:
A
Machine
Learning
on
Very
Large
Databases.
PhD thesis,
Department
of Computer
Science,
University
of Sydney,
Australia,
1991.

J. Catlett.
Megainduction
: A test
flight.
In Proceedings
of
Eighth
International
Workshop
on Machine
Learning,
pages
596-599.
Morgan
Kaufmann,
1991.

C. Cortes
and V. Vapnik.
Support
vector
networks.
Machine
Learning,
20:273-297,
1995.

T.
M.
Cover
and
P. E.
Hart.
Nearest
neighbour
pattern
classification.
Institute
of Electrical
and Electronics
Engineers
Transactions
on Information
Theory,
13:21-27,
1967.

U. M. Fayyad,
G. Piatetsky-Shapiro,
and P. Smyth.
From
data
mining
to knowledge
discovery:an
overview.
In
Advances
in
Knowledge
Discovery
and Data
Mining.
MIT
Press,
1996.

R.
Musick.,
J.
Catlett,
and
S. Russel.
Decision
theoretic
subsampling
for induction
on large
databases.
In Proceeding8
of The
Tenth
International
Conference
on Machine
Learning,
pages 212-219,
Mateo,
CA.,
1993. Morgan
Kaufmann.

F. J. Provost
and V. Kolluri.
A survey
of methods
for scaling
up
inductive
learning
algorithms.
Technical
Report
ISL-97-
3, Intelligent
Systems
Lab.,
Department
of Computer
Science,
University
of Pittsburgh,
1997.

J. R. Quinaln.
C4.5:
Programs
For Machine
Learning.
Morgan
Kaufmann,
San Mateo,
CA.,
1993.

J.
R.
Quinlan.
Learning
efficient
classification
procedures
and
their
appplication
to chess endgames.
In
R. Michalski,
J. Carbonell,
and T. Mitchell,
editors,
Machine
Learning
: An
AI
Approach.
Morgan
Kaufmann,
Los Altos,
CA.,
1983.

D. E. Rumelhart,
G. E. Hinton,
and R. J. Williams.
Learning
internal
representations
by error
propagation.
In PDP:
Explo-
rations
in the Microstructure
of Cognition.
MIT
Press,
1986.

B Scholkopf,
D. Burges,
and V. Vapnik.
Extracting
support
data
for a given task.
In Proceedings
of 1st Intl.
Conf.
on Knowledge
Discovery
d Data
Mining,
pp 252 - 257. AAAI,
1995.

V. Vapnik.
The Nature
of Statistical
Learning
Theory.
Springer
Verlag,
New York,
1995.




276

