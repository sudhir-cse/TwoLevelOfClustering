Online Novelty Detection on Temporal Sequences

Junshui Ma
Aureon Biosciences Corp
Yonkers, NY 10701 USA
614-288-3593, +1
junshuima@yahoo.com
Simon Perkins
NIS-2, Los Alamos National Lab MS D436, NIS-2, LANL
Los Alamos, NM 87544 USA
505-667-9558, +1
s.perkins@lanl.gov

Abstract:
In this paper, we present a new framework for online novelty
detection on temporal sequences. This framework includes a
mechanism for associating each detection result with a confidence
value. Based on this framework, we develop a concrete online
detection algorithm, by modeling the temporal sequence using an
online support vector regression algorithm. Experiments on both
synthetic and real world data are performed to demonstrate the
promising performance of our proposed detection algorithm.

Categories and Subject Descriptors
I.5.2 [Pattern Recognition]: Design Methodology

General Terms
Algorithms

Keywords:
Novelty detection, Anomaly Detection, Online algorithm, Support
vector regression.

1. INTRODUCTION
Novelty detection, or anomaly detection, refers to the
automatic identification of unforeseen or abnormal phenomena
embedded in a large amount of normal data. [4, 8, 17] This paper
targets the novelty detecting in temporal sequences [2, 4, 5, 8],
which has many immediate applications. For example, in a safety-
critical environment, it is helpful to have an automatic supervising
system, which can screen the time series generated by monitoring
sensors, and report any abnormal observations. Another promising
application is to help scientists in different areas by liberating
them from exhaustive examination of data, and drawing their
attention only to unusual and "interesting" phenomena.
Novelty detection is a challenging topic, mainly because it is
hard to obtain sufficient knowledge and an accurate representative
of "novelty" given a problem [17], which rules out the use of
many supervised techniques. Despite its challenge, in the past
over ten years it has been a topic acquiring increasing attention,
and quite a few techniques have been proposed and investigated.
These techniques were experimentally proven to be effective in
some cases, while they can fail in other cases. For example, some
methods were designed based on the assumption of possessing
precise models of the underlying problem [9], or knowing the
novelty conditions [1, 6, 13], which are generally not true in real
world. In some other studies [3, 7, 10, 14], novelty detection was
simply
interpreted
as
outlier
detection.
However,
this
simplification produces methods that cannot discover novel
patterns formed by several continuous instances. In particular, the
novelty detection method proposed in [14] and [3] is based on a
technique called one-class support vector machine, whose
formulation forces it to "identify" some novel individuals no
matter how normal the whole data set is. A wavelet-based signal
trend shift detection method is proposed in [15]. Nevertheless, this
method cannot detect short novel patterns embedded in normal
signals. An interesting idea for novelty detection, inspired by the
negative-selection mechanism in the immune system, was
proposed in [4]. However, this method can fail when the negative
set goes to null with the increasing diversity of the normal set.
Another method for target time-series novelty detection, called
TARZAN [8], is based on converting the time series into a
symbolic string. However, the procedure for discretizing and
symbolizing real values in time series, as well as the various
choices of string-representation parameters, can cause the loss of
meaningful patterns in the original time series.
Furthermore, the ability to conduct online novelty detection
is especially desirable for temporal sequences. However, few of
the existing algorithms explicitly address this issue. The most
relevant result is an incremental algorithm designed to detect
normal events, instead of novel events. [5]
In this paper, we propose a direction to detect novelty in
temporal sequences. As with other detection algorithms, it is
impossible for our proposed direction to succeed in all scenarios.
However, it can at least provide an alternative and complementary
solution to some problems in which other available techniques
may fail. The remainder of this paper is organized as follows. A
general online novelty detection framework is proposed in Section
2. Based on the framework, a concrete online detection algorithm
is proposed. Because the new algorithm utilizes a technique called
support vector regression (SVR), Section 3 is devoted to a brief
introduction to SVR. The new algorithm, as well as issues
involved in its implementation, is presented in Section 4. Two
variants of the original detection algorithms are proposed in
Section 5 to adapt the original algorithm to different application
scenarios. Experiments are proposed in Section 6.
Copyright
2003
Association
for
Computing
Machinery.
ACM
acknowledges that this contribution was authored or co-authored by a
contractor or affiliate of the U.S. Government. As such, the Government
retains a nonexclusive, royalty-free right to publish or reproduce this
article, or to allow others to do so, for Government purposes only.
SIGKDD '03, August 24-27, 2003, Washington, DC, USA. Copyright 2003
ACM 1-58113-737-0/03/0008...$5.00.
The main contributions of this paper are (a) It proposes an
online novelty detection framework for temporal sequences. This
online framework is capable of associating a confidence level
with each detection result. (b) It proposes a concrete online
novelty detection algorithm based on the framework, and
describes experiments to test the new algorithm.



612
Copyright 2003 Association for Computing Machinery. ACM acknowledges
that this contribution was authored or co-authored by a contractor or affiliate
of the U.S. Government. As such, the Government retains a nonexclusive,
royalty-free right to publish or reproduce this article, or to allow others to
do so, for Government purposes only.
SIGKDD03, August 24-27, 2003, Washington, DC, USA
Copyright 2003 ACM 1-58113-737-0/03/0008$5.00



613

2. A FRAMEWORK FOR ONLINE
NOVELTY DETECTION WITH
CONFIDENCE
We first reflect on the concept of novelty, as well as that of
novelty detection, philosophically. First of all, novelty is always a
relative concept with regard to our current knowledge. Therefore,
novelty should be defined in the context of a representation of our
current knowledge. Such representation can be a database, a
knowledge base, or a model. In our framework, we prefer
representing our knowledge with a model, simply because of its
mathematical neatness. Moreover, in online applications, it is
desirable that the representation of our knowledge can also be
updated with the acquisition of new data. Second, there is
generally no clear-cut line between novel events and normal
events in real world application. Therefore, it is attractive to
associate with each novel event a value to characterize how
confident we are about our judgment. Finally, in a real world
environment with noise the novel events in a temporal sequence
are generally associated with segments, instead of individual time
points. These understandings lay the foundation of a formal
formulation of an online novel detection framework, which is
defined in the remaining part of this section.

In this paper, a temporal sequences is represented as
,
where
. They can be time series, video sequences, or
some other objects that can be indexed by time t.
is a
stochastic process, and
x
is employed to represent one of its
realizations.
( )
t
X


)
1
t =
N




0
)
(t
X
( )
t


As we mentioned previously, we utilize a model
to
represent our knowledge about the underlying temporal sequence
up to
t
. This model can be a physics-based model provided by
domain experts, or a model constructed from available datax
,
where
t
t
.
0
( )
t
x
M



(t
0



=
)

0
1

Surely, it is impossible for us to utilize a model with finite
number of parameters to completely represent the information
embedded in a random temporal sequence. Also, each model may
only be good at representing certain types of information, because
different models have different sensitivity to different type of
information. For example, some models are more sensitive to
extreme values, while some other models are more sensitive to
unusual patterns. However, this selective property of model-based
knowledge representation, when applied to novelty detection, can
become an advantage. In a particular application, we may only be
interested in some types of novelties among all of the different
possible novelties. Thus, utilizing different models provides us
with the opportunities of selecting different types of interested
novelties.
Definition 1 (Matching Value and Matching Function)
The matching function, denoted as
, is a
function that can quantify how well the model
M
matches the
temporal sequence. The match value
V t
, where
V t
0
(
(
1), ( )
F
t
t
-
x
M
x

0
( )
t
x


0
( )
0
( )R
, is
defined as
V t
.
0
0
( )
(
(
1), ( )
F
t
t
=
-
x
M
x
0
)

In other words, the matching value
V t
is employed to measure
how well our knowledge about the time series at
t
0
( )

0
1
-
,
represented by
M
, can describe the instance
x
.
0
(
1
t -
x
)
0




0
( )
t
Definition 2 (Occurrence)
Denoted by
O t
, occurrence at
t
is defined as
0
( )
0


0
0
0
( )
{ ( ) (
( ), ( ))}
O t
I V t
t
t



 -
(1)

where
is the indicator function, and
2 (
{ }
I i
0
) 0
t

>
is a
predefined tolerance width.

0
( )
t



0
(
O t
can be determined based on the noise level, as well as the
precision requirement of the underlying problem. Note that
is a random variable.
)

Definition 3 (Surprise)
A surprise is observed if
O t
.
0
( ) 1
=

That is, a surprise happens when a new instance
x
falls
outside of the tolerance range
(
(
0
( )
t

0
0
), ( )
t
t )


-
.

Definition 4 (Event and Event Duration)

Denoted by
, an event at time t is defined as
0
( )
n
E t
0

[
]
0
0
0
0
( )
( )
(
1)
(
1)
T
n
E t
O t
O t
O t
n
=
+
+ -
(2a)

where n is called the event duration. The 1-norm of
is
denoted as
0
( )
n
E t

0
( )
n
E t
. That is

1

0
0
0
( )
(
)
n

n
i
E t
O t
-


=
i
=
+

(2b)



0
( )
n
E t
is the number of surprises happened in the event
.
0
( )
n
E t

Event duration n is a predetermined algorithmic parameter. We
omit the absolute operation on
O t
in (2b) because
O t
0
(
i
+ )
0
(
)
i
+
is nonnegative according to (1).
is a random binary vector

with at most
different realizations, while
0
( )
n
E t
2n
0
( )
n
E t
is a random

variable with at most n+1 different realizations. A realization of
(or
0
( )
n
E t
0
(
n
E t )
) is denoted as
e t
(or
0
( )
n
0
( )
n
e t
). The discrete

density function of
0
( )
n
E t
is represented as
0
( ( ) )
E
n
p
e t
n
,

where
0
( )
n
e t
n
0
=
. The formulation of
0
) )
t

1
( (
n
E
n
p
e

0
n -
1}
n -
can be

determined by the occurrences
O t
. For example,
if the occurrences in
{ (
are identical
independent Bernoulli variables,
becomes a binomial
random variable.
0
(
)
i i

0
),
i i
+
=

0
(
n
E t
,
+
=
0
O t
)

0
)
t
( (
n
E
n
p
e

0
1}
n
)
will be different if the occurrences

in
{ (
0
)
i ,iO t +
=
-
are interdependent, such as following a
Markov chain distribution.
Definition 5 (Novel Event with Confidence)
Given a confidence level
, where
c t
, Event
e t
is defined as a novel event with confidence
c t
, if
satisfies
0
( )
c t
0
( ) (0,1)

(
0
( )
n


0
( )
n
e t
0
)


(a)
0
( )
max( , {
( )})
n
n
e t
h
E t
>
E
0
, where
E
is the mean, h is a

fixed lower bound of
{ }
i

0
( )
n
E t
with h
, and
(3a)
N

(b)
0
( ( ) ) 1
( )
n
E
n
p
e t
c t
< -
0
.
(3b)




613
614

The condition (3a) makes sure that at least certain number of
surprises happen in the event
e t
, while the condition (3b)
ensures that the probability for the number of surprises to happen
in the event
e t
is small enough to satisfy our confidence level
. The h is an algorithmic parameter to define the lower
bound of the number of surprises.
0
( )
n




0
( )
n


0
( )
c t


Four items in this framework need to be instantiated before it
becomes a concrete algorithm. (a) The model
M
to represent
the temporal sequence
X
; (b) The matching function
to quantify the disagreement between the
model output and the temporal sequence
X
observation at
;
(c) The tolerance width
2 (
0
( )
t
x




)
( )
t




0
)
t
0
(
(
1), ( )
F
t
t
-
x
M
x
0
)
(t
0
t

at
; (d) The discrete density
function of
0
t

0
( )
n
E t
,
0
( ( )
E
n
t )
n
p
e
, where
. In Section 4,

a concrete algorithm is derived by instantiating all these four
items in the framework.
0
m
n
=




3. A BRIEF INTRODUCTION TO
SUPPORT VECTOR REGRESSION
Because the algorithm proposed in Section 4 is extensively
based on a regression technique, called Support Vector
Regression (SVR), we briefly introduce the basic concepts of
SVR in this section. A more detailed presentation of SVR can be
found in [16].

Given a training set
T
y
, where
{( , ),
1... }
i
i
i
=
=
x
l
D
i
Rx
,
and
, we construct a linear regression function with regard
to W and
i
y R
( )
x

( )
( )
T
f
b
=
 +
x
W
x
(4)

where W and

are vectors in a huge dimensional feature
space F. Meanwhile,
can also be considered as a mapping
function, which maps
( )
x
( )
 x
D
Rx
to a vector in F. The
and b in
(4) are obtained by solving an optimization problem:
W


*

,
1



*


*
1
min
(
)
2
. .
(
( )
)

(
( )
)

,
0,
1
l
T
i
i
b
i

T
i
i

T
i
i


i
i
P
C

s t
y
b

b
y

i
l
 

 
 
 
=
=
+
+

-
 +  +
 + -  +

=

W
W W

W
x

W
x
(5)




The optimization criterion penalizes data points whose y-
values differ from f (x) by more than . The slack variables  and

*
represent the size of this excess deviation for positive and
negative deviations respectively. Introducing Lagrange multipliers
 and *, we get:


*
*
*

,
1
1


*
*

1
1


*
*

1
1
min
(
)(
)
2

(
)
(
)


. .
0
,
1
,
(
) 0
l
l

ij
i
i
j
j
i
j

l
l

i
i
i
i
i
i
i

l

i
i
i
i
i
D
Q


y


s t
C
i
l
   



 
 


 
 
=
=




=
=




=
=
-
-


+
+
-
-




=
-
=






 




( )
( )
( ,
)
T
ij
i
j
i
j
Q
K
=

=
x
x
x x
( ,
)
i
j
x x
(6)




where
. Here
K
is a kernel
function [16]. Given the solution of (6), the regression function
(4) can be written:



1
( )
( , )
l

i
i
i
f
K

=
b
=
+

x
x x
(7)


where the coefficient
*
i
i
i
  
= -
. (7) is a nonlinear function with

regard to
D

x R


i
if the kernel function
is chosen as a

nonlinear function. Normally, only a small fraction of the
coefficients
( ,
)
i
j
K x x


in (7) are finally nonzero. Those samples
with
nonzero
i
x

i

are called support vectors of the regression function,
because it is these critical samples in the training set T that solely
determine the formulation of (7).


4. AN SVR-BASED ONLINE NOVELTY
DETECTION ALGORITHM
In this section, we instantiate the four items in the framework
listed in Section 2 to devise a novelty detection algorithm.

First, SVR is employed to model a temporal sequence
X
.
SVR has quite a few attractive features. For example, (a) the
regression function
( )
t

( )
f x
obtained by SVR can approximate any
nonlinear relationship between the input vector x and its
associated target value y if a proper kernel function
is

chosen, (b) SVR has good generalization properties, and (c) SVR
can handle high dimensional data efficiently.
( ,
K
)
i
j
x x



More specifically, given a realization of a temporal sequence,
or
, where
t
( )
t
x
0
1 t
=
( )
t
x
, we can construct a set of training
samples
T t
from
0
( )
D


0
( ) {(
, ),
1}
t
t
D
D
T t
t
D
t0
=
=
-
x y
,
(8)

where
,
, and D is
called the embedding dimension of the training set
T t
.
According to Section 3, from the training set
T t
SVR training
algorithm can construct a regression function
[ (
1)
( )]
t
D
t D
t
=
- +
x
x
x
T
(
1
t
t
=
+
y
x



0
( )
D

0
(
)
t
)

0
( )
D




D
f x

0
0
0
1

0
1
(
1)
(
)
(
,
)
t
t
t
t
D
i
D
D
t
t
f
K

-


=
0
t
b
=
+ =
=
+

y
x
x
x x
(9)


Naturally, the model
M
representing the temporal
sequence
X
can be defined as
0
( )
t
x
( )
t

0
0
1

0
( )
(
)
(
,
)
t
t
t
D
i
D
D
i E
t
f
K

-


=
0
t
b
=
=

x
M
x
x x




0
1
t
t
+
(10)

Equation
(10)
suggests
that
model
essentially
incorporates all the points of the temporal sequence
x
, where
0
( )
t
x
M
( )
t
=
( )
t
X
, and thus is a good candidate for representing our
acquired knowledge of
.

Second, the matching function and the matching value are
defined as

0
0
0


0
0
0
( )
(
(
1), ( ))
( )
(
1)
( )
( )
V t
F
t
t
t
t
t
0
t
=
-
=
-
- =
-
x


x
M
x
x
M
x
x
(11)




614
615

Equation (11) indeed suggests that the matching value
V t
is
the residual of the regression function (9) at
t
.
0
( )

0

Third, we need to define the tolerance width
2 (
0
)
t

at
.
Fortunately, SVR formulation generally adopts an
0
t
 -insensitive
loss function [16], which possesses exactly the same flavor as our
tolerant range defined in Definition 2. Thus, we just simply merge
the concepts of two tolerant ranges together, and define the
tolerant width in Definition 2 as 2 for any
t
, where
0
 is the
insensitive parameter in the  -insensitive loss function of SVR. A
direct consequence of this definition is that any sample
(
,
in
that turns to be a surprise to model
M
will be a
support vector in the updated model
M
[16].
0
0
t
)
t
x y
D


0
( )
t
0
( )
D
T t
x


0
(t +
x
1)

Finally, in order to make our algorithm theoretically
tractable, we simply define
as a sequence of independent
Bernoulli random variables with the same parameter, and the
discrete density function of
0
( )
n
E t


0
(
n
E t )
,
0
( ( ) )
n
E
n
p
e t
, can thus be

formulated as [12]



0
0


( )
( )
0
0
0


0
( ( ) )


( )
(1
( ))
,
( )

( )
0


0,
n



n
E
n


e t
n e t

n


n
p
e t

n
q t
q t
e t

where
e t
n


otherwise
-
=



-





=




0
n
(12)




where
is the probability of an occurrence in the event
to be a surprise. The
can be approximately estimated
as
0
( )
q t

0
( )
n
E t
0
( )
q t



0
0
0
( )
( )
SV
N
t
q t
t
D
=
-
,
(13)


where
is the number of support vectors in model
,
and
is the number of training samples in
T t
according
to (8).
0
( )
SV
N
t
t
D
-
0
( )
t
x
M

0
0
( )
D



This definition is valid under the following two assumptions
(a) The occurrences in an event are independent; (b) All
occurrences in an event have approximately the same probability
of being a surprise. Intuitively, the first assumption is reasonable
if the regression function (9) can sufficiently capture the
dependent relationship in a temporal sequence. This can be
achieved by an adequate training stage to fully build the model
before it is utilized for detection. The second assumption
is sensible if the event duration n is not too large. More discussion
of the role that event duration n plays in the algorithm will be
presented in Subsection 5.1. Failure of meeting either assumption
deteriorates the accuracy of the confidence level associated with
the detection output.
0
( )
t
x
M




We now have a complete online novelty detection algorithm
has been proposed based the framework in Section 2. This
algorithm requires a set of algorithmic parameters, which include
(a) Embedding dimension D; (b) Event duration n; (c) Tolerant
width 2 ; (d) Kernel function
; (e) Confidence level c ;
and (f) Fixed lower bound of the number of surprises h. The
algorithmic procedures can be summarized as follows:




( ,
)
i
j
K x x
1)
At training stage,

i) When a new point in the temporal sequence
x
becomes
available, the training set
T t
will be updated to
T t
following (8). Accordingly, the model
0
( )
t


1)
0
(
1
D
- )
0
( )
D


0
(t -
x

)
M
will also be
updated to
based on the
T t
.
0
( )
t
x
M
0
(
D

ii) If training stage does not finish, go to step i). Otherwise,
move to the detection stage.
2)
At detection stage,

i) When a new point in the temporal sequence
x
becomes
available, calculate the matching value
V t
following (11).
0
( )
t

0
( )

ii) Based on
V t
and the tolerant width 2
0
( )
 , determine the
value of occurrence
O t
following (1).
0
( )

iii) Based on the values of occurrence
O t0
(
i)
-
, where

0
i
n 1
=
- , determine the value of event E t0
n
(
1
n
)
- +
following (2). If
0
(
n
E t
n 1)
- +
meets the conditions in (3), a
novel event, which starts at
t
n
and ends at
t
, is
detected with a confidence of c.
0
1
- +
0



iv) The training set
T t
is updated to
T t
by
considering the new point
x
following (8). Accordingly,
the model
0
(
1
D
-

0
(t
1)
)
0
( )
D
)

0
(t -
x
M
is then updated to
M
based on
.
0
(t
x
)

0
( )
D
T t

v) If the detection stage does not finish, go to step i). Otherwise,
quit.
Finally, it is worth mentioning that the SVR training
algorithm introduced in Section 3 is a batch algorithm. That is,
whenever a new sample is added into the training set, the existing
regression function can only be updated by retraining the whole
training set, which is not an efficient way to implement our
detection algorithm. Fortunately, we have recently derived an
incremental SVR training algorithm [11], which can efficiently
update the regression function whenever a sample is added to or
removed from the training set.


5. TWO VARIANTS OF THE NOVELTY
DETECTION ALGORITHM
When we apply this algorithm to particular real world
scenarios, it is sometimes necessary to fine-tune some parts of the
algorithms to meet the special requirements of an application. In
this section, we propose two variants of the original algorithm.


5.1 Robust Online Novelty Detection
The event duration n is a critical parameter in the detection
algorithm. In some cases where the novel events are outstanding,
the original algorithm is not sensitive to event duration n. This can
be illustrated by experiments in Section 6. However, in the other
cases, an improper choice of this parameter does lead to the
deterioration of the algorithmic performance. Meanwhile, it is



615
616

hard to know in advance what kind of novel events will be
detected in a temporal sequence, which is indeed the nature of
novelty detection.
3
1
40
( ) sin(
)
( )
( )
( )
t
t
n t
e t
2
e t
N

=
+
+
+
X
(14c)


where
1
t
N
=
,
1200
N =
, and
n t
is an additive Gaussian
noise with zero-mean and a SDT of 0.1.
and
e t
are two
novel events,
( )

1
( )
e t
2
( )
Although, given a particular problem, how to select an
optimal event duration n for the original detection algorithm is
still an open topic, in some applications it is possible for us to
know the range in which an optimal event duration n may fall in.
Based on this assumption, a robust version of the original
detection algorithm can be intuitively devised. In this variant, we
evenly pick up r different event duration n's from the available
range, and apply each of them to the original detection algorithm,
and thus obtain r detection outputs. The final robust detection
output is obtained by a voting procedure among all the generated
outputs. When implementing this idea, it is not necessary to
literally repeat the original detection algorithm for each event
duration n. Repeating merely the step iii) at the detection stage for
each event duration n is adequate.
1
1
( ),
[600,620]
( )
0,
n t
t
e t
otheriwse


=


where
n t
follows a normal distribution of
.
1
( )
(0,0.5)
N



2
40
0.4*sin(
),
[820,870]
( )
0,
t
t
e t
N
otheriwse




=



We use the first 400 points in the three time series to train
our models, and conduct the novelty detection on the remaining
800 points. The algorithmic parameters are arbitrarily set as (a)
Embedding dimension D=8; (b) Event duration n=6; (c) Tolerant
width 2 =0.2; (d) Kernel function
2
( ,
) exp{
}
i
j
i
i
K
=
- -
x x
x
x
;
(e) Confidence level c = 95%; (f) Fixed lower bound of number
of surprises h =n/2. The experimental results are demonstrated in
Figure 1.
5.2 Fixed-Resource Online Novelty Detection
One problem with our original detection algorithm is the
longer the prediction goes on, the bigger the training set
T t
will become, and the more SVs will be involved in the SVR
regression function (9). In some environments with limited
memory and computational power, it is possible to stress out the
system resources with the complexity of the SVR model (9)
growing in this way. One way to deal with this problem is to
impose a "forgetting" time W. When training set
T t
grows to
this maximum W, then the SVR model (9) will first be trained to
remove the oldest sample before the next new sample is used to
update the model. Accordingly, the
in (13) becomes
0
( )
D




0
( )
D




0
( )
q t

0
0
( )
( )
SV
N
t
q t
W
=
, where
is still the number of support

vectors in model
M
.
0
( )
SV
N
t

)
0
(t
x
The blue curves in Figure 1 are the synthetic time series, and
the peaks on the red lines indicate the positions and durations of
the detected novel events. Because the first 400 points of each
time series are taken out for training the support vector regression
functions, no detection outputs are produced at that segment.




This variant is intrinsically suitable for non-stationary
temporal sequences, as it can be updated in real-time to fit the
most recent behavior of a temporal sequence.


6. EXPERIMENTS
Experiments based on both synthetic and measured data are
presented to demonstrate the performance of our algorithm.
According to our knowledge, a comparable automatic online
novelty detection algorithm supported by confidence is still not
available in other literatures when we prepare this paper.
Therefore, it is difficult for us to implement comparative
experiments to justify our performance. Thus, we here do it by
comparing our detection results with visual detection by humans.
Figure 1. Experimental Results on Synthetic Time Series


The plots in Figure 1 show that, our detection algorithm
successfully detects all the novel events in
x
and
.
Meanwhile, our algorithm properly figures out that no any part of
the
can be considered as a novel event. As suggested in
Subsection 5.1, because the novel events in these synthetic time
series are fairly distinguishable, the detection output of the
original detection algorithm is not sensitive to the choice of event
duration n. It can be shown that the same detection output can be
produced when event duration n is set to 8 or 10.
2
( )
t
3
( )
t
x


1
( )
t
x


6.1 Experiments Based on Synthetic Data
Three synthetic time series are generated from the following
stochastic processes respectively.


1
40
( ) sin(
)
( )
t
t
n t
N

=
+
X
(14a)

6.2 An Experiment Based on Measured Data

2
1
40
( ) sin(
)
( )
( )
t
t
n t
e t
N

=
+
+
X
(14b)
The experiment is to apply the original detection algorithm to
the famous Santa Fe Institute Competition data, which is 1000-
point time series. We define the first 200 points as the train



616
617

[3] Campbell, Colin, Kristin P. Bennett, A Linear Programming
Approach to Novelty Detection, in Advances in Neural
Information Processing Systems, vol 14, 2001.
segment, and the remaining 800 points as the detection segment.
The algorithmic parameters are set exactly the same as the
experiments done in Subsection 6.1. The time series, along with
the detection results, are plotted in Figure 2.
[4] Dasgupta, Dipanker, and Stephanie Forrest, Novelty
Detection in Time Series Data Using Ideas from
Immunology, In Proceedings of the 5th International
Conference on Intelligent Systems, Reno, Nevada, June 19-
21, 1996.
[5] Guralnik, Valery, Jaideep Srivastava, Event Detection from
Time Series Data. In Proceedings of the International
Conference Knowledge Discovery and Data Mining, San
Diego, California, 1999.
[6] Isermann, Rolf, Process Fault Detection Based on Modeling
and Estimation Method - A Survey, Automatica, vol. 20,
pp.387-404, 1984.
Figure 2. Experimental Results on Santa Fe Institute Time
Series
[7] Jagadish, H. V., N. Kouda, and S. Muthukrishnan, Mining
deviates in a time series database, in Proceedings of 25th
International Conference on Very Large Data Bases, pp 102-
113, 1999.
In this experiment our algorithm claims that two novel events
happen in the detection segment of the time series. It is easy to
visually examine its validity. Also, similar to the experiments
done in Subsection 6.1, this Santa Fe Institute Time Series is also
insensitive to different choice of event duration n. The same result
can be obtained if we set the event duration n to 8 or 10.
[8] Keogh, E., S Lonardi, and W Chiu, Finding Surprising
Patterns in a Time Series Database In Linear Time and
Space, In the 8th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp 550-556,
Edmonton, Alberta, Canada, July 23 - 26, 2002.
[9] Kozma, R., M. Kitamura, M. Sakuma, and Y. Yokoyama,
Anomaly Detection by Neural Network Models and
Statistical Time Series Analysis, in Proceedings of IEEE
International Conference on Neural Networks, Orlando,
Florida, June 27-29, 1994.
7. CONCLUSIONS
This paper proposes a new direction for online novelty
detection on temporal sequences. Primitive experimental results
demonstrate the promising performance of this algorithm.
Meanwhile, many topics brought up by this paper are still
open. For example, we notice that some relationship exists among
the algorithm parameters, such as event duration n, tolerant width
2 , and confidence level c. However, we still cannot figure out a
method to make use of this relationship to define a set of optimal
algorithmic parameters. Also, intuitively, for some temporal
sequences, Markov chain can be a better model than Binomial
distribution to describe the relationship among the occurrences
. Thus, how to implement this intuition is another direction
worth future investigation. Surely, new detection algorithms can
also be devised by employing different temporal sequence
models, such as data clustering and one-class support vector
machine.
0
)(
O t
[10] Lauer, Martin, A Mixture Approach to Novelty Detection
Using Training Data With Outliers, Lecture Notes in
Computer Science, vol. 2167, pp. 300-310, 2001.
[11] Ma, Junshui, James Theiler, and Simon Perkins, "Accurate
Online Support Vector Regression," to appear in Neural
Computation, 2003.
[12] Mood, A. M., F. A. Graybill, and D. C. Boes, Introduction to
The Thoery of Statistics, 3rd Edition, McGraw-Hill, Inc,
1974.
[13] Roberts, S., and L. Tarassenko. A Probabilistic Resource
Allocating
Network
for
Novelty
Detection,
Neural
Computation, vol. 6, pp. 270-284, 1994.
[14] Schölkopf, B., R.C. Williamson, A.J. Smola, J. Shawe-
Taylor, and J. Platt. Support vector method for novelty
detection. In Neural Information Processing Systems, 2000.
8. ACKNOWLEDGEMENTS
Most of this work was supported by the NASA project NRA-
00-01-AISR-088. The final part of it was supported by Aureon
Bioscience Corp.
[15] Shahabi. C., X. Tian, and W. Zhao, Tsa-tree: A Wavelet-
based Approach to Improve the Efficiency of Multi-level
Surprise and Trend Queries. In Proceedings of 12th
International Conference on Scientific and Statistical
Database Management, 2000.
9. REFERENCES
[16] Smola, A. J., and B. Scholkopf (1998). A Tutorial on Support
Vector Regression, NeuroCOLT Technical Report NC-TR-
98-030, Royal Holloway College, University of London, UK.
[1] Bishop, C. M., Novelty Detection and Neural Network
Validation, IEE Proceedings - Vision, Image and Signal
Processing, vol. 141, no. 4, pp. 217-222, August, 1994.
[17] Ypma, Alexander, and Rober P. Duin, Novelty Detection
Using Self-Organizing Maps, in Progress in Connectionist-
Based Information Systems, pp 1322-1325, London:
Springer, 1997.
[2] Brotherton, Tom, Tom Johnson, and George Chadderdon,
Classification and Novelty Detection Using Linear Models
and a Class Dependent-Elliptical Basis Function Neural
Network, in Proceedings of the International Conference on
Neural Networks, Anchorage, May 1998.


617
618

