Towards Systematic Design of Distance Functions for Data
Mining Applications

Charu C. Aggarwal
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532
charu@us.ibm.com


ABSTRACT
Distance function computation is a key subtask in many
data mining algorithms and applications. The most effec-
tive form of the distance function can only be expressed in
the context of a particular data domain. It is also often a
challenging and non-trivial task to find the most effective
form of the distance function. For example, in the text do-
main, distance function design has been considered such an
important and complex issue that it has been the focus of
intensive research over three decades. The final design of
distance functions in this domain has been reached only by
detailed empirical testing and consensus over the quality of
results provided by the different variations. With the in-
creasing ability to collect data in an automated way, the
number of new kinds of data continues to increase rapidly.
This makes it increasingly difficult to undertake such efforts
for each and every new data type. The most important as-
pect of distance function design is that since a human is
the end-user for any application, the design must satisfy the
user requirements with regard to effectiveness.
This cre-
ates the need for a systematic framework to design distance
functions which are sensitive to the particular characteris-
tics of the data domain. In this paper, we discuss such a
framework. The goal is to create distance functions in an
automated way while minimizing the work required from
the user. We will show that this framework creates distance
functions which are significantly more effective than popu-
larly used functions such as the Euclidean metric.


Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications--
Data Mining


Keywords
Data mining, distance functions, user interaction




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGKDD '03, August 24-27, 2003, Washington, DC, USA
Copyright 2003 ACM 1-58113-737-0/03/0008 ...$5.00.
1. INTRODUCTION
Distance function design remains at the core of many im-
portant data mining applications. Most applications such as
clustering, classification and nearest neighbor search use dis-
tance functions as a key subroutine in their implementation.
Clearly, the quality of the resulting distance function signifi-
cantly affects the success of the corresponding application in
finding meaningful results. For many data mining applica-
tions, the choice of the distance function is not pre-defined,
but is chosen heuristically. There is not much literature on
how distance functions should be designed for arbitrary ap-
plications. Many data mining algorithms and applications
use the Euclidean distance metric as a natural extension of
its use for spatial applications. The widespread use of dis-
tance functions such as the L2-norm is perhaps rooted in
the initial development of index structures for spatial ap-
plications. In such cases, the special interpretability of the
L2-norm assumed great importance.
However, this inter-
pretability is not really relevant for arbitrary domains of
data containing many dimensions.
The issue of distance function design becomes even more
critical in the context of high dimensional applications. Re-
cent work [13] has shown that in high dimensional space,
the sparsity of the data may affect the quality of the dis-
tance function. In particular, it has been indicated in [4, 7]
that distance functions such as the Lp-norm are often not
very meaningful for high dimensional data because of the
fact that the pairwise distances between the points are very
similar to one another. In such cases, the poor contrast in
the measurement of distances reduces the effectiveness of the
measurements. The overall effect of high dimensionality on
nearest neighbor search has been examined in some detail
in [2, 11]. In [2], it has been shown that nearest neighbor
search by visual interaction can often provide good qual-
ity results. However, this approach is only designed for the
specific problem of nearest neighbor search and is not fo-
cussed on determining the most effective distance functions
in closed form. In fact, the merits of user feedback for near-
est neighbor search have been well documented for a number
of data domains such as text and image retrieval [19, 20, 22].
These procedures cannot easily be used in arbitrary data
mining applications. For most real applications, an efficient
computation of a closed form distance function is critical to
the success of the underlying algorithm.
Some recent work [3] has discussed the effects of different
kinds of closed-form distance functions in high dimension-
ality, but this method is not user centered in its approach.




9

This technique only aims to increase the contrast in the
distances of the different points from one another. It is im-
portant to understand that since the quality of a distance
function may often be determined by the perceptual sig-
nificance to an end-user, the user needs to be kept in the
loop during distance function design.
This would ensure
that the final distance function can model what the user
wants, rather than simply satisfy a particular criterion such
as maximizing the contrast. Consider the following cases for
distance function design:

 It has been shown in [8], that in categorical databases,
the quality of similarity can be best measured in the
context of the overall behavior of the data. Thus, a
simple distance function such as the hamming distance
does not work very well for this case. Instead, it is
necessary to determine the statistical behavior of the
interactions among different attributes in order to find
the most effective form of the distance function. This
fact cannot be determined easily a-priori without con-
siderable human understanding and experience of the
nature of categorical databases.

 In domains such as time series databases, similarity
can be effectively measured while taking into account
noise, scaling and translation of the data [5].
Such
normalizations are usually heavily dependent in an in-
direct way upon the perceived user intent in measuring
similarity.

 In an image database the similarity between two im-
ages may be quite subjective. For example, only par-
ticular visual aspects of the image may contribute to
the definition of similarity in that domain. What makes
the problem even more difficult is that the nature of
the best possible distance function may vary consider-
ably even across different image domains. It is often
difficult to predict the best distance function in such
cases a-priori without some kind of human interven-
tion.

 The similarity between two different digital music data
sets may be defined by commonalities in certain seg-
ments. These features can often be perceived by au-
dio, but are difficult to predict manually using ad-hoc
techniques. Again, the particular aspects of the stream
which define similarity cannot be easily identified man-
ually because of the lack of interpretability of the rel-
evant features.

Which features of the data provide the greatest similar-
ity? For example, in the image database, should the color
histograms be given greater weight or do the texture and
coarseness provide greater similarity? In most cases, these
specific combination of weights one must assign to the dif-
ferent features cannot be easily specified only with domain
knowledge. This makes distance function design by man-
ual and ad-hoc techniques a somewhat impractical solution.
In this paper, we will concentrate on the design of distance
functions which are sensitive to the data behavior and serve
the needs of the user.
We note that the process of designing distance functions
even for a specific domain such as Information Retrieval (IR)
has intrigued researchers over three decades [19].
Such a
design in the IR community has been achieved by consider-
able testing and understanding of the particular character-
istics of the data which are the most meaningful indicators
of similarity. Unlike IR applications, we cannot always use
such domain specific information about the "typical" nature
of the data for arbitrary applications. Therefore, for these
cases, designing distance functions is an even more difficult
task. Furthermore, since advances in hardware technologies
continue to increase the number of data collection domains
at a rapid rate, it is impractical to undertake such an inten-
sive research effort for each newly discovered data domain.
On the other hand, given the importance and reusability of
distance functions as a tool for many different algorithms
in knowledge discovery, it may be acceptable to allow a few
hours of human labor in modelling the distance function,
provided that a systematic way of constructing the distance
function is available. In this paper, we will provide just such
a framework.
This paper is organized as follows.
The remainder of
this section will discuss important aspects and motivations
of distance function design.
In section 2, we will discuss
how distance functions can be modelled and designed in a
systematic way so as to model an end-user's perceptions.
This section will primarily concentrate on the use of popu-
lar canonical forms such as the cosine model or Minkowski
model for constructing distance functions. In section 3, we
will discuss the use of purely learning strategies for distance
function design. Techniques for combining different strate-
gies in order to create the most effective distance function
are discussed in section 4. The empirical results are pre-
sented in section 5. Finally, the conclusions and summary
are contained in section 6.

1.1 Desiderata for Effective Distance Function
Design
The design of distance functions for well studied data do-
mains such as Information Retrieval provides some useful
hints for the high dimensional case.
We list some of the
practical desiderata for effective design of distance functions
below.
(1) User-Centered: Since the success of the data min-
ing application ultimately rests with the perception of the
user, it is clear that the distance function should model the
corresponding requirements as closely as possible.
While
incorporation of domain knowledge in the design of the dis-
tance function is often effective, it lacks the direct systematic
approach that is necessary in order to handle arbitrary ap-
plications. In many cases, for a new data domain, we have
little experience or understanding of the features which are
most indicative of similarity. Therefore, a necessary require-
ment is to find a way to solicit the feedback from the user
which can be used to construct the distance function by a
systematic modelling process.
(2) Contrasting: Since many data mining applications
are inherently high dimensional, it is important to ensure
that the distance functions provide sufficient discriminatory
power. The results of [7] indicate that standard distance
functions such as the Lp-norm are not very effective in this
respect. This non-contrasting behavior is because even the
most similar records are likely to have a few features which
are well separated. For distance functions such as the Lp-
norm, the results of [7] show that the averaging effects of
the different dimensions start to dominate with increasing



10

dimensionality.
The utilization of user-feedback serves as
a useful tool in resolving this problem of high dimensional
distance functions. This is because it is easier for the user
to guide the ultimate form of the distance function in such
a way that the level of contrast is considerably increased.
(3) Statistically Sensitive: It is very rare that the data
is uniformly distributed along any given dimension. Some
of the values may be more sparsely populated than others.
This behavior is commonly noted in Information Retrieval
applications in which a given document usually contains a
small fraction of the available vocabulary. Distance func-
tions such as dice, cosine or jaccard coefficients [19] do not
treat the attribute values uniformly but normalize using the
frequency of occurrence of the words in the documents. Em-
pirical testing on large collections has established that the
normalization process significantly affects the quality of the
distance function. In this paper, we will try to achieve statis-
tical sensitivity by analyzing the user requirements directly,
rather than try to guess the importance of frequency distri-
butions on the quality of similarity. In other words, we wish
to develop generic models for distance functions which can
directly incorporate user requirements.
(4) Compactness: In many applications, the computa-
tion of the distance function itself can be the most expensive
operation because of its repeated use over a large number
of iterations. Therefore, a distance function should be ef-
ficiently computable. In the event that the distance func-
tion uses a pre-stored model or statistical information, the
storage requirements should be modest. We refer to this re-
quirement as that of compactness. We note that a distance
function can be represented either in closed form, or algo-
rithmically on the feature sets of the two objects. In most
cases, closed form distance functions have the advantage of
being more efficiently computable.
(5) Interpretability: The interpretability of a distance
function is very valuable for effective use in many applica-
tions. For example, one would like to know which features
of the data are most influential in affecting the distance
function. This can provide considerable insight and under-
standing in a large number of applications. In this respect,
closed-form distance functions are usually more desirable.
In the next section, we will discuss some useful models for
distance function computation. Specifically we will discuss
two generalizations of widely used distance functions such as
the Lp-norm and the cosine distance function. Specifically,
we will propose the Parametric Minkowski model and the
Parametric Cosine model for distance function computation.
We will show how to use these models in conjunction with
human interaction in order to create techniques which are
significantly more friendly to the needs of a given user.


2. PARAMETRICMODELLINGOFDISTANCE
FUNCTIONS
The essential idea behind a parametric model for distance
function computation is that we do not specify the distance
function exactly, but specify it incompletely along with some
parameters. Then, we utilize user interaction and feedback
to fill in these incomplete parameters. In order to formalize
this concept, we will introduce some additional notations
and definitions.
We assume that we have a data set D containing a set of
N records, each with d dimensions. Let us assume that the
records X and Y are drawn from the database D. We de-
fine the parametric distance function f(X, Y , 1, . . . k) as
the function of its arguments X and Y and the k param-
eters 1 . . . k. We will denote the vector (1 . . . k) by .
Thus, we note that this definition of the distance function is
incomplete since it only assumes a basic form of the distance
function and leaves several parameters unspecified. These
parameters are then learned by a series of user interactions
in which the user feedback is utilized in order to solicit the
perceptual similarity between different pairs of objects. If
desired, multiple users can be used in order to increase the
robustness of the data collection process. These similarity
values are then used in conjunction with a canonical form
in order to train the data for a distance function which is
most reflective of user behavior. We note that the use of a
canonical form has the advantage that it is able to incorpo-
rate some degree of domain knowledge about the data, while
leaving the function sufficiently incomplete, so that its final
form is sufficiently reflective of user behavior.
Let us assume that the data pairs for which the distance
function needs to be defined are given by (X1, Y1) . . . (XN , YN ).
The interactive process assumes that for each pair of objects
(Xi, Yi), the user provides the perceptual similarity value i.
We assume that the user-defined similarity values for these
different data pairs are given by 1 . . . N . We note that the
nature of each of these objects and the user feedback may
depend upon the particular domain at hand. For example,
in an image database, this perceptual similarity could corre-
spond to the similarity in the patterns between the different
images. In this case, the visual perception of the user may
be leveraged in order to define the final similarity values.
Thus, at the end of the process, the values 1 . . . N are
the user responses to the similarities between the corre-
sponding pairs of objects (X1, Y1) . . . (XN , YN ). We would
like to choose values of 1 . . . k, so that the error in predic-
tion is as low as possible. This error can be defined in several
ways; some of which are algorithmically more desirable. For
example, one way of defining the error E(1 . . . k) is the
square prediction error:


E(1 . . . k) =
N



i=1
(f(Xi, Yi) - i)2
(1)


A slightly different way of defining the error is the mean
p-norm-squared error.


E(1 . . . k) =
N



i=1
(f(Xi, Yi)p - pi )2
(2)


We note that the use of the p-norm squared error is a matter
of algorithmic convenience, since the final objective function
often takes on a simpler form for some of the models that
we will propose.
In order to determine the value of this objective func-
tion we use the gradient search method.
In the gradient
search method, we start off with an initial value of the vec-
tor  = 0 and successively update to 1, 2, . . ., m using
the gradient vector E. This gradient vector is defined as
follows:

E() = (E(1 . . . d)/1 . . . E(1 . . . d)/d)
(3)

For each value of  = m, we substitute its corresponding
value in the above equation in order to determine the gra-
dient. The value of m
+1
is obtained from m by utilizing



11

the steepest descent method. In this method, the greatest
reduction of the objective function value of E(m) oc-
curs along the gradient direction given by dm = -E(m).
Therefore, the value of m is updated as follows:

m
+1
= m + m  dm
(4)

Here, the choice of m determines the convergence behavior
of this iterative mechanism. Picking m effectively ensures
a rapid convergence rate for the algorithm. To this effect,
we use the line search method [6] in order to perform the
iterations.
In the line search technique we would like to
pick m such that the value of (m) = E(m + m  dm)
is minimized.
In order to achieve this, we should have
 (m) = 0. This value of m =  can be determined using
a bracket bisection technique. It is elementary to show that
 (m) = -E(m + m  dm)t  dm. We note that since the
direction dm is a descent gradient at m, we have  (0) < 0.
In the event that we are able to determine at least one value
of m such that  (m)  0, we know that the required value
of m must lie in this range (0, ). Finding the value of m
exactly can often be time consuming; therefore we estimate
this value within a factor of two. To do so, we first pick an
arbitrary value of m = 1. If  (1) < 0, we keep doubling
m until we obtain the largest value for which  (m) < 0.
Otherwise, if  (1) > 0, we keep halving m until we obtain
the first value of m for which  (m) < 0. The final value
of m at the end of this two-pronged search method is used
for the update step of Equation 3. We note that this tech-
nique is somewhat similar to the Armijo rule [6] used for
line search in gradient methods.

2.1 The Parametric Minkowski Model
The parametric minkowski model used in this paper is
basically a generalization of the Lp norm. In this model,
the distance function is defined by a weighted version of the
Lp norm. The weights on the different dimensions are the
parameters for this model. Let us assume that the weights
for the d dimensions are given by 1 . . . d. Therefore, for a
pair of objects X = (x1 . . . xd) and Y = (y1 . . . yd), the value
of the parametric distance is defined as follows:


f(X, Y , 1 . . . d) =
d



i=1
i||(xi - yi)||p
1/p

(5)


Note that this distance function is easily interpretable in
terms of the different dimensions. A higher value of i in-
dicates a greater significance for a particular dimension. In
this case, the use of the p-norm mean square error results
in a linear set of equations for 1 . . . k. Let us assume that
the coordinates for each individual pair of points Xi and Y
i

are given by (xi1 . . . xiN ) and (yi1 . . . yiN ) respectively. The
error function in this case is of the following form:


E(1 . . . k) =
N



i=1
(
d



j=1
aijj) - pi
2

(6)


Here the value of aij is given by ||xij - yij||p. On using the
derivative with respect to each j, we obtain the following:


E(1 . . . d)/j =
N



i=1
aij  (
d



k=1
aikk) - pi
(7)
Now the gradient vector E can be defined directly using
Equation 3. This gradient vector is used in order to update
the vector  as indicated in Equation 4.

2.1.1
Generalizations
In real applications, many attributes in the data are cor-
related with one another. Inter-attribute correlations have
often been used for designing distance functions in categor-
ical domains where there is no natural ordering of attribute
values. In such cases, the use of inter-attribute summary in-
formation provides the only possible insight into the similar-
ity of objects by examining whether commonly co-occurring
inter-attribute values are present in the two objects [8]. This
insight is equally relevant even for quantitative domains of
data where a natural ordering of attribute values exists. The
use of aggregate data behavior in order to measure similarity
becomes more important for high dimensional data, where
there may be considerable redundancies, dependencies, and
relationships among the large number of attributes. Since a
lot of the proximity information may be hidden in the aggre-
gate summary behavior of the data, the use of the linearly
separable parametric model may miss many of the critical
characteristics. We also note that some data domains such
as text factor the correlation behavior indirectly into the
distance function by using data transformation techniques
such as Latent Semantic Indexing [12].
Fortunately, the parametric Minkowski model can be di-
rectly generalized in a way so as to make it more sensitive
to the correlation behavior of the data. In order to do so,
we use the generalized parametric Minkowski model in order
to define distances. In this case, we use a symmetric d  d
matrix  containing the parametric values. In this case, the
distance between the point vectors X and Y is given by:

f(X, Y , ) = (X - Y )T    (X - Y )
(8)

We note that when the matrix  has non-zero entries only
on the diagonal, the model reduces to the simple Minkowski
case. Thus, the non-zero values on the non-diagonal entries
provide the magnitudes of the correlations between the cor-
responding dimensions. As in the previous case, the gradi-
ent descent method can be used to determine the parameter
values.

2.2 The Parametric Cosine Model
The parametric cosine model is actually a similarity func-
tion rather than a distance function, since higher values im-
ply greater similarity. The basic idea behind the parametric
cosine model is drawn from the cosine function used in text
databases [19, 21]. We will propose this function for the
case of boolean data sets, though it can be directly extended
to quantitative data sets by applying a simple preprocess-
ing phase of discretization.
For two boolean data points
X = (x1 . . . xd) and Y = (y1 . . . yd), the cosine distance be-
tween these points is given by:


Cosine(X, Y ) =
d



i=1
xi  yi

d
i=1
x2i 
d
i=1
y2i
(9)


In many cases, the attributes are appropriately weighted and
normalized in order to improve the quality of the distance
function. An example of such normalization is the simple
inverse document frequency normalization used in Informa-
tion Retrieval applications.
We note that such weighting



12

or normalization is usually done purely on the basis of the
statistical properties of the data set, but may often devi-
ate significantly from how similarity may be perceived by a
given user. In order to achieve this, we introduce a paramet-
ric cosine function which is more sensitive to the process of
actually learning similarity from user behavior. The para-
metric cosine similarity of X and Y is defined in terms of
the d parameters  = 1 . . . d:


f(X, Y, 1 . . . d) =
d



i=1
i  xi  yi
(10)


We note that this is a similarity function as opposed to a
distance function, since higher numbers imply greater sim-
ilarity. As before, let us assume that the coordinates for
each individual pair of points Xi and Y
i
are denoted by
(xi1 . . . xiN ) and (yi1 . . . yiN ) respectively. In this case, we
find that by using the mean square error function, we again
obtain a similar form of the error function:


E(1 . . . k) =
N



i=1
(
d



j=1
aijj) - i
2

(11)


In this case, the value of aij is equal to xij  yij. The re-
maining analysis is exactly similar to that of the Minkowski
model.

2.3 Interpretability Issues
We note that the above techniques for distance function
design naturally lead to a high degree of interpretability of
the distance function in a given data domain. For example,
unlike simple Euclidean functions which treat all dimensions
equally, the parametric Minkowski model provides a clear
idea of which features are the most important based on the
underlying data behavior. It is clear that any feature i for
which the weight i is small is less relevant in determining
the similarity value.
Similarly, in the case of the parametric cosine function, the
importance of each attribute is decided by the correspond-
ing weights. We note that in the case of the text domain, in
which similarity functions have been extensively researched,
such weights are determined by using term/inverse docu-
ment frequency normalization [19]. While it is intuitively
clear that such normalization gives greater weight to statis-
tically infrequently occurring events, it does not incorporate
the characteristics of the particular kind of data at hand.
The ability to capture such relationships lies at the heart
of a technique which incorporates such feedback into the
learning process.

3. ADISCUSSIONONTHEUSEOFPURELY
LEARNING STRATEGIES
In the previous section, we discussed a method which used
purely parametric forms for learning a distance function
with a particular canonical form. While such a technique
has the advantage of providing a closed form representa-
tion to the distance function, a natural alternative is to
transform the problem of distance function design to that of
pure regression modelling. In order to illustrate this point,
consider the case, when we have a set of N object pairs
(X1, Y1) . . . (XN , YN ) along with the corresponding user re-
sponses 1 . . . N . In this case, we can recreate a new com-
posite object containing 2톎 attributes, by concatenating the
attributes of Xi and Yi for each value of i. Therefore, we can
create the new object Zi, by concatenating the d attributes
each of Xi and Yi. Now, we can use the set Z1 . . . ZN along
with 1 . . . N as the training data in order to model the
distance function.
While the simplicity of the above model is tempting, it
has some drawbacks as well. In most data domains, dis-
tance functions have some naturally desirable qualities in
terms of the relationship of the quantitative class variable
with the feature variables. Parametric techniques such as
the Cosine or Minkowski models precisely try to encode such
information within the training process. Just as the use of
a fixed form such as the Lp-norm is inflexible, the absence
of any canonical form ignores any kind of understanding of
the inherent nature of distance functions. In order to create
accurate models in cases where there are no guiding canoni-
cal forms, the amount of data required is likely to consider-
able. This is a significant drawback in a system in which the
generation of each data point requires human intervention.
However, it is possible to make some compromises in order
to incorporate feature specific knowledge into the training
process without at the same time using a fixed and inflexible
canonical form. To do so, we propose to use feature transfor-
mations which capture the natural functional properties of
many distance functions. Thus, for the set of N object pairs
(X1, Y1) . . . (XN , YN ) along with the corresponding user re-
sponses 1 . . . N , one could create the set of functional fea-
tures g1(X1 . . . XN , Y1, . . . YN ), . . . gq(X1 . . . XN , Y1, . . . YN ).
Thus, a set of new features are created by the transforma-
tion. These new features are created by the user in order to
help the learning process to some extent, while not design-
ing precise functional forms for the distance function. While
a regression model built on the original feature-response tu-
ples (Xi, Yi, i) ought to be the most flexible, the reality is
that the constraints on training data availability make such
a system difficult to implement. The use of functional fea-
tures indirectly incorporates knowledge about the natural
behavior of distance functions, which would otherwise have
to be learned. In the next subsections, we will discuss some
functional transformations which derive their motivations
from the Minkowski and Cosine models respectively.

3.1 TheDifferenceFunctionalTransformation
One example of such a system is the difference functional
transformation.
In this case, we have a total of q = N
functional features g1(. . .) . . . gN (. . .) which are defined by
gi(. . .) = ||Xi -Yi||. In this model, smaller values of i indi-
cate greater similarity. We note that this model is influenced
by the Minkowski model, since the latter is also determined
completely by the values of ||Xi - Yi||. We note that such a
transformation directly reduces the number of features by a
factor of two, while adding additional knowledge in order to
improve the efficiency of the knowledge discovery process.

3.2 The Product Functional Transformation
This model assume that each of the values of (Xi, Yi) are
boolean.
However, quantitative values can easily be con-
verted to boolean by the use of data discretization. As is
the case of the cosine functional representation, this model
uses only the products of corresponding values of (Xi, Yi)
in order to define the new features on which the distance
function model is constructed. In this case q = N new fea-
tures g1(. . .) . . . gN (. . .) are defined.
Specifically, we have



13

gi(. . .) = Xi  Yi. In this model, larger values of i indi-
cate greater similarity. This model derives its motivation
from the cosine function in which the (normalized) product
sum defines the value of the similarity function. However, a
product functional transformation is somewhat more flexible
than a parametric cosine model because it recognizes that
the actual similarity value may be a more complex combina-
tion of feature interactions than is allowed by the parametric
closed form.

3.3 Use of Functional Features for Distance
Function Learning
Once the functional features have been created, we use
them directly for distance function learning. Specifically, a
standard classification model is constructed on this new set
of features in order to relate them to the user-defined similar-
ity values. We note that the classification model often works
much more effectively on this new set of features, because
they often incorporate the inherent characteristics of natural
similarity (or distance) functions. For example, the differ-
ence functional transformation automatically stores the rela-
tionship between two of the columns which is often likely to
be very relevant to the final distance value. While a purely
learning strategy may also try to learn this relationship, it
is likely to be significantly more inaccurate when insufficient
amounts of data are available. In fact,as our empirical re-
sults will show, even the feature transformational models do
not work as well as the more restricted parametric forms
when small amounts of training data are available. In gen-
eral, a direct learning strategy often finds it more difficult to
model the natural characteristics of similarity calculations.


4. COMBINING STRATEGIES BY HOLD-
OUTS
One of the interesting points to be noted is that no partic-
ular strategy may provide an optimum response. This is be-
cause a given feature transformation or canonical form may
incorporate certain advantages for a particular data domain,
while it may not be quite as effective for other domains. Un-
fortunately, without additional domain knowledge, it is not
possible to know the most effective transformation a-priori.
In order to handle this problem, it is possible to use a hold-
out strategy in picking the most effective canonical form or
transformation.
In order to compare different strategies, we divided the
data set D of size N into two subsets D1 and D2 of size
N1 and N2 respectively. The set D2 is defined as the hold-
out set. Once these two data sets have been constructed,
we build each model separately on the data set D1. Next,
we determine the estimate of the distance function value for
each record in the data set D2, using the model constructed
on the data set D1. The average error in the distance func-
tion using each canonical form or feature transformation is
calculated and used for the determining which of the var-
ious methods is most appropriate for that particular data
domain.
We note that a normalization issue needs to be taken into
account while combining different strategies.
Since some
of the methods discussed above correspond to maximization
objective functions, whereas others correspond to minimiza-
tion objective functions, we need to utilize the user feedback
in a consistent way. In order to achieve this, we assume that
1 . . . N are generated using the minimization convention,
whereas the corresponding maximization counterparts are
generated as M -1 . . . M -N . Here M is an upper bound
on any value of i and may be chosen as M = maxi{i}. We
note that while using this convention the absolute inaccu-
racy in modelling the maximization objective function value
is directly comparable to that of modelling the minimization
value. This makes it possible to compare different distance
function models in order to test the appropriateness of that
approach for the given data set.


5. EMPIRICAL RESULTS
The aim of this section is to show that the models dis-
cussed in this paper can effectively capture the distance
functions quite accurately for a number of real data sets. All
results were implemented on an AIX4.1.4 system at 233MHz
and 200MB of main memory.

5.1 Data Generation
In order to test the performance of our methodology, we
used a number of data sets from the UCI machine learn-
ing1 repository. While all data sets were based on real do-
mains, the user responses were synthetically generated from
within the data. We used one of the feature variables in or-
der to synthetically generate the distance function responses
1 . . . N . This feature was stripped from the other records.
Let the records in the original databases D be denoted by
W1 . . . Wr.
We will denote the values of the special col-
umn from which the objective function was generated by
z1 . . . zr. In order to actually generate the objective func-
tion values, we randomly picked N pairs of objects from the
records W1 . . . Wr. Let us assume that the ith pair of objects
picked is denoted by (Xi, Yi) = (Wni, Wmi). Thus, ni and
mi are indices of the object pairs which are sampled. The
value of the objective function i was given by ||zmi - zni||
while testing for minimization objective functions2 such as
the parametric Minkowski distance function or the difference
functional transformation. On the other hand, while dealing
with maximization objective functions such as the paramet-
ric cosine model or the product functional transformation,
this value was set at M -||zmi - zni||, where M is an upper
bound on the value of ||zmi -zni||. The value of M is chosen
to be maxi{zi}-mini{zi}. We note that this method of cre-
ating the similarity value is synthetic as opposed to manual.
Since a difference function was used on a particular column
to consistently generate the similarity values, this generates
a similarity function with a particular bias, as is the case
with most user defined similarity perceptions. For example,
when the column for "Age" is used in order to define the
similarity function, the difference function on this column
creates a function which will make two objects similar only
when they are similar in age. Some of the features are likely
to be more important than others while making such a de-
termination. However, the aim of this section is to show
the effectiveness of these techniques in modelling any kind
of similarity, whether they be manually defined or synthetic.
The advantage of using synthetic data sets is to be able to
provide objective measures of accuracy of the method.

1
http://www.cs.uci.edu/~mlearn.
2
Smaller values are better for the case of minimization ob-
jective functions.



14

5.2 Accuracy Measurements
In addition to the N data pairs which are used for distance
function modelling, we also generate a separate set of N
data pairs for testing the effectiveness of the distance func-
tion. Let us assume that these N data pairs are denoted by
(X1, Y1) . . . (XN , YN ) along with corresponding test simi-
larity values of 1 . . . N . Once the appropriate model has
been constructed, we use it to predict the similarity values
1 . . . N
for each of the N test cases. The absolute differ-
ence between each value of i and i (denoted by ||i - i||)
is the inaccuracy on that particular test example. The over-
all inaccuracy I is then determined by averaging the same
calculation over all the different test examples. Therefore,
we have:


I =
N



i=1
||i - i||)/N
(12)


We calculated this error value for each individual model as
well as the composite model by obtaining the best canonical
form in the holdout model.
While the above measures give insight into the absolute
errors of the different strategies, the real effectiveness of a
distance function is determined by how well the distance
function orders the different objects in terms of their rela-
tive distance values. Thus, for example, if objects A and
B are closer to one another than the user perceives objects
C and D, then we would like that the model is able to ac-
curately predict this fact. To this effect, we sampled two
pairs of objects at a time and calculated the fraction of time
that the correct ordering was maintained. We will refer to
this value as the ordering accuracy. Therefore, if n1 and
n2 be the respective number of times that the correct and
incorrect order was maintained between randomly chosen
pairs of objects, then the ordering accuracy was given by
n1/(n1 + n2). Such a measure is independent of the abso-
lute values of the similarity and measures quality in terms
of percentage of ordering correctness. This makes it possible
to compare distance functions with widely varying absolute
values.
5.3 Results
In Table 1, we have illustrated the statistics of the differ-
ent data sets. For each data set, the column which was used
to generate the distance function value is denoted by colum-
nid. This column was always chosen to be numeric in order
to define the distance function effectively. This column was
also removed from the data set, so that the distance func-
tion was modelled using only the remaining features. The
resulting dimensionality (after removal of this column) is
indicated in Table 1. In the table we have also illustrated
the maximum and minimum values of the target objective
function once they were synthetically generated. These val-
ues provide an idea of the range of the absolute error in the
estimation of the final objective function.
The absolute error for each data set is illustrated in Table
2. Each of the different methods proposed in this paper were
tested together with a pure learning methodology as a base-
line. The learning method used was a decision (regression)
tree classifier [17]. It is interesting to see that the pure learn-
ing method was never as effective as either the parametric
functional methods or the transformational learning meth-
ods in any case. The reason for this is that the parametric
distance functions and the transformational features encode
100
200
300
400
500
600
700
800
900
1000
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.28
0.3
0.32




NUMBER OF DATA POINTS
ABSOLUTE
ERROR
PARAMETRIC MINKOWSKI
PARAMETRIC COSINE
DIFFERENCE TRANSFORM LEARNING
PRODUCT TRANSFORM LEARNING
PURE LEARNING




Figure 1: Accuracy versus Data Set Size (Ecoli Data
Set)



critical information about the underlying data. The other
salient observation is that the effectiveness of the different
distance functions varies from one data set to another. For
example, the parametric distance functions are more effec-
tive than the transform learning functions for the Ecoli and
Machine data sets, whereas the transformational features
are more effective for the other cases. Even among the dif-
ferent kinds of parametric forms, there were slight variations
for different data sets. For example, for the Ecoli data set
the parametric Minkowski function had a lower error than
the parametric cosine function, whereas for the glass data
set, the reverse was true. The same observation was true
about the transformational features. In terms of the abso-
lute values of the error rates, the error values for the pure
learning strategy are significantly higher, whereas the differ-
ences between the other four methods are relatively small.
This tends to indicate that any of the latter methods would
be significantly robust for most data sets. We also note that
by using the hold out strategy discussed earlier, it is possible
to isolate and use the most effective distance function for a
given data set.
In Table 3, we have illustrated the ordering accuracy of
the different methods for each of the data sets. In this case,
we have omitted the pure learning strategy, because our re-
sults in Table 2 show that it is conclusively worse than any of
the other techniques in each case. Instead, we have used the
ordering error on the simple Euclidean function as a base-
line in order to show the advantages of this method over
the traditional Euclidean metric. The results in Table 2 in-
dicate that the relative ordering accuracies of the different
methods closely mirror the absolute inaccuracies of the dif-
ferent methods. Furthermore, in each case, the techniques
turn out to be more effective than the traditional Euclidean
distance function. The reason for this is that the Euclidean
function is not geared to provide the particular notion of
nearness which is application dependent and data driven.
Thus, the combined results of Table 2 and 3 show that the
careful method of designing the distance function is able to
achieve results that either traditional distance functions or
pure learning strategies cannot achieve.

5.4 Data Size Requirements


15

Table 1: Statistics of Different Data Sets
Data
Dimensionality
Column Id
Minimum
Maximum
Set
Target Similarity
Target Similarity
Ecoli
6
2
0
0.89
Glass
8
6
0
5.29
Machine
7
10
0
1213
Auto-Mpg
7
7
0
12
Housing
12
14
0
45




Table 2: Absolute Estimation Error with Different Methods
Data
Parametric
Parametric
Difference
Product
Pure
Set
Minkowski
Cosine
Transform
Transform
Learning
Function
Function
Learning
Learning
Ecoli
0.101
0.110
0.121
0.127
0.153
Glass
0.443
0.412
0.4000
0.353
0.561
Machine
41.3
40.8
43.3
41.5
92.6
Auto-Mpg
2.7
2.9
2.5
2.6
4.6
Housing
7.6
7.1
5.4
5.3
9.7




Table 3: Ordering Accuracy Comparisons
Data
Parametric
Parametric
Difference
Product
Euclidean
Set
Minkowski
Cosine
Transform
Transform
Function
Function
Function
Learning
Learning
Ecoli
0.81
0.75
0.72
0.71
0.69
Glass
0.80
0.81
0.82
0.78
0.71
Machine
0.89
0.90
0.86
0.88
0.74
Auto-Mpg
0.78
0.76
0.79
0.78
0.65
Housing
0.71
0.73
0.77
0.78
0.62




16

100
200
300
400
500
600
700
800
900
1000
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9




NUMBER OF DATA POINTS
ABSOLUTE
ERROR
PARAMETRIC MINKOWSKI
PARAMETRIC COSINE
DIFFERENCE TRANSFORM LEARNING
PRODUCT TRANSFORM LEARNING
PURE LEARNING




Figure 2: Accuracy versus Data Set Size (Glass Data
Set)




100
200
300
400
500
600
700
800
900
1000
40
60
80
100
120
140
160
180
200
220
240




NUMBER OF DATA POINTS
ABSOLUTE
ERROR
PARAMETRIC MINKOWSKI
PARAMETRIC COSINE
DIFFERENCE TRANSFORM LEARNING
PRODUCT TRANSFORM LEARNING
PURE LEARNING




Figure 3: Accuracy versus Data Set Size (Machine
Data Set)
100
200
300
400
500
600
700
800
900
1000
2.5
3
3.5
4
4.5
5
5.5




NUMBER OF DATA POINTS
ABSOLUTE
ERROR
PARAMETRIC MINKOWSKI
PARAMETRIC COSINE
DIFFERENCE TRANSFORM LEARNING
PRODUCT TRANSFORM LEARNING
PURE LEARNING




Figure 4: Accuracy versus Data Set Size (Auto-Mpg
Data Set)




100
200
300
400
500
600
700
800
900
1000
5.5
6
6.5
7
7.5
8
8.5
9
9.5
10
10.5




NUMBER OF DATA POINTS
ABSOLUTE
ERROR
PARAMETRIC MINKOWSKI
PARAMETRIC COSINE
DIFFERENCE TRANSFORM LEARNING
PRODUCT TRANSFORM LEARNING
PURE LEARNING




Figure 5: Accuracy versus Data Set Size (Housing
Data Set)



The previous section showed that while the quality of re-
sults obtained are approximately comparable, they provide
little guidance about the comparative data size requirements
in order to achieve effective results. We note that since the
focus of this paper is to learn distance functions from do-
main specific data, the amount of data available may often
be limited. Therefore, it is useful to have an idea of how
well the different methods compare in terms of the amount
of data required in order to achieve accurate modelling of
the distance function.
The results for the Ecoli, Glass,
Machine, Glass, Auto-Mpg and Housing data sets are illus-
trated in Figures 1, 2, 3, 4 and 5 respectively. On the X-axis,
we have illustrated the number of data pairs used for train-
ing, whereas on the Y -axis, we have illustrated the absolute
error resulting from the training procedure. In each case,
the error reduces with increasing number of training pairs,
but stabilizes at some point. We note that the results in Ta-
ble 2 are each based on 10,000 data pairs which is this stable
region. An interesting common characteristic of all these re-
sults is that in each case, the parametric forms require much
fewer number of records to reach the region of stability than



17

the functional feature transformations. In most cases only
about 500 to 1000 data pairs were sufficient to reach the
region of stability. In fact, in some of the cases such as the
Glass, Auto-Mpg and Housing data sets (Figures 2, 4 and
5), the error rate of the parametric forms is lower for smaller
number of records, even though the steady state value for
larger number of records is higher. This behavior is particu-
larly evident in the housing data set of Figure 5. The reason
for this is that the parametric models are somewhat more
restrictive in performing distance function modeling. Thus,
fewer number of data pairs are required in order to reach
the optimal error value. At the same time, Table 2 shows
that the qualitative results are not very different between
the transformational feature techniques and the parametric
forms. This indicates an overall advantage for the paramet-
ric method since the same overall qualitative results can be
obtained with much less data. Furthermore, the paramet-
ric forms have the advantages of being compact, since they
can be easily computed and expressed in closed form. When
combined with the advantages of low data size requirements,
it is clear that parametric modeling is a promising technique
for systematic design of distance functions.

6. CONCLUSIONS AND SUMMARY
While distance functions continue to be one of the most
important and widely used element of many data mining
problems, they are often implemented using naive meth-
ods for many data domains.
In this paper, we discussed
a user-centered and systematic method for modeling dis-
tance functions effectively. We illustrated the advantages of
a parametric approach over both a fixed distance function
and a purely learning methodology. We also discussed the
usefulness of transformational features in distance function
modeling. Given the importance and application specificity
of distance functions, the results in this paper can be valu-
able for a large number of data mining problems.

7. REFERENCES
[1] C. C. Aggarwal. Re-designing distance functions and
distance based applications for high dimensional data.
ACM SIGMOD Record, March 2001.
[2] C. C. Aggarwal. Towards Meaningful High Dimensional
Nearest Neighbor Search by Human-Computer
Interaction. ICDE Conference, 2001.
[3] C. C. Aggarwal, P. S. Yu. The IGrid Index: Reversing
the Dimensionality Curse for Similarity Indexing in High
Dimensional Space. KDD Conference, 2001.
[4] C. C. Aggarwal, A. Hinneburg, D. A. Keim. On the
Surprising Behavior of Distance Metrics in High
Dimensional Space. ICDT Conference, 2001.
[5] R. Agrawal, K.-I. Lin, H. S. Sawhney, K. Shim. Fast
similarity search in the presence of noise, scaling, and
translation in time-series databases. VLDB Conference,
pages 490-501, 1995.
[6] D. Bertsekas. Nonlinear Programming. Athena
Scientific, 2nd Edition, 1999.
[7] K. Beyer, J. Goldstein, R. Ramakrishnan, U. Shaft.
When is Nearest Neighbors Meaningful? ICDT
Conference, 1999.
[8] G. Das, H. Mannila. Context-Based Similarity
Measures for Categorical Databases. PDKK Conference,
2000.
[9] J. Foote. A Similarity Measure for Automatic Audio
Classification. AAI 1997 Spring Symposium on
Intelligent Integration and Use of Text, Image, Video,
and Audio Corpora, 1997.
[10] D. Gunopulos, G. Das. Time Series Similarity
Measures and Time Series Indexing. ACM SIGMOD
Conference, 2001.
[11] A. Hinneburg, C. C. Aggarwal, D. Keim. What is the
nearest neighbor in high dimensional spaces? VLDB
Conference, 2000.
[12] S. Deerwester et al. Indexing by Latent Semantic
Analysis. Journal of the American Society for
Information Science, 41(6): 391-407, 1990.
[13] N. Katayama, S. Satoh. Distinctiveness Sensitive
Nearest Neighbor Search for Efficient Similarity
Retrieval of Multimedia Information. ICDE Conference,
2001.
[14] E. Keogh, M. Pazzini. Scaling up Dynamic Time
Warping to Massive Data Sets. Principles of Data
Mining and Knowledge Discovery, pages 1-11, 1999.
[15] P. Moen. Attribute, Event Sequence, and Event Type
Similarity Notions for Data Mining. ining. PhD Thesis,
Report A-2000-1, Department of Computer Science,
University of Helsinki, February 2000.
http://www.cs.helsinki.fi/TR/A-2000/1/.
[16] A. Hinneburg, D. A. Keim, M. Wawryniuk. HD-Eye:
Visual Mining of High Dimensional Data. IEEE Comp.
Graphics and Applications, 19(5), pp. 22-31, 1999.
[17] M. James. Classification Algorithms, Wiley, 1985.
[18] M. M. Richter. On the notion of similarity in
case-based reasoning. Mathematical and Statistical
Methods in Artificial Intelligence (ed. G. della Riccia et
al). Springer Verlag, 1995, p. 171-184.
[19] G. Salton, M. J. McGill. Introduction to Modern
Information Retrieval. Mc Graw Hill, New York, 1983.
[20] T. Seidl, H.-P. Kriegel: Efficient User-Adaptable
Similarity Search in Large Multimedia Databases. VLDB
Conference, 1997.
[21] A. Singhal, G. Salton, M. Mitra, C. Buckley. Pivoted
Document Length Normalization. ACM SIGIR
Conference, 1996.
[22] L. Wu, C. Faloutsos, K. Sycara, T. Payne. FALCON:
Feedback Adaptive Loop for Content-Based Retrieval.
VLDB Conference, 2000.




18

