Mining Concept-Drifting Data Streams Using
Ensemble Classifiers


Haixun Wang
Wei Fan
Philip S. Yu
1
Jiawei Han
IBM T. J. Watson Research, Hawthorne, NY 10532, {haixun,weifan,psyu}@us.ibm.com
1
Dept. of Computer Science, Univ. of Illinois, Urbana, IL 61801, hanj@cs.uiuc.edu




ABSTRACT
Recently, mining data streams with concept drifts for actionable
insights has become an important and challenging task for a wide
range of applications including credit card fraud protection, target
marketing, network intrusion detection, etc. Conventional knowl-
edge discovery tools are facing two challenges, the overwhelming
volume of the streaming data, and the concept drifts. In this paper,
we propose a general framework for mining concept-drifting data
streams using weighted ensemble classifiers. We train an ensemble
of classification models, such as C4.5, RIPPER, naive Bayesian,
etc., from sequential chunks of the data stream. The classifiers in
the ensemble are judiciously weighted based on their expected clas-
sification accuracy on the test data under the time-evolving environ-
ment. Thus, the ensemble approach improves both the efficiency in
learning the model and the accuracy in performing classification.
Our empirical study shows that the proposed methods have sub-
stantial advantage over single-classifier approaches in prediction
accuracy, and the ensemble framework is effective for a variety of
classification models.


Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications--data min-
ing; I.2.6 [Artificial Intelligence]: Learning--concept learning;
I.5.2 [Pattern Recognition]: Design Methodology--classifier de-
sign and evaluation


Keywords
classifier, classifier ensemble, data streams, concept drift


1. INTRODUCTION
The scalability of data mining methods is constantly being chal-
lenged by real-time production systems that generate tremendous
amount of data at unprecedented rates. Examples of such data
streams include network event logs, telephone call records, credit
card transactional flows, sensoring and surveillance video streams,
etc. Other than the huge data volume, streaming data are also char-
acterized by their drifting concepts. In other words, the underlying




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGKDD '03, August 24-27, 2003, Washington, DC, USA
Copyright 2003 ACM 1-58113-737-0/03/0008 ...$5.00.
data generating mechanism, or the concept that we try to learn from
the data, is constantly evolving. Knowledge discovery on streaming
data is a research topic of growing interest [1, 4, 7, 19]. The funda-
mental problem we need to solve is the following: given an infinite
amount of continuous measurements, how do we model them in or-
der to capture time-evolving trends and patterns in the stream, and
make time-critical predictions?
Huge data volume and drifting concepts are not unfamiliar to the
data mining community. One of the goals of traditional data mining
algorithms is to learn models from large databases with bounded-
memory. It has been achieved by several classification methods,
including Sprint [21], BOAT [14], etc. Nevertheless, the fact that
these algorithms require multiple scans of the training data makes
them inappropriate in the streaming environment where examples
are coming in at a higher rate than they can be repeatedly analyzed.
Incremental or online data mining methods [25, 14] are another
option for mining data streams. These methods continuously revise
and refine a model by incorporating new data as they arrive. How-
ever, in order to guarantee that the model trained incrementally is
identical to the model trained in the batch mode, most online algo-
rithms rely on a costly model updating procedure, which sometimes
makes the learning even slower than it is in batch mode. Recently,
an efficient incremental decision tree algorithm called VFDT is in-
troduced by Domingos et al [7]. For streams made up of discrete
type of data, Hoeffding bounds guarantee that the output model of
VFDT is asymptotically nearly identical to that of a batch learner.
The above mentioned algorithms, including incremental and on-
line methods such as VFDT, all produce a single model that repre-
sents the entire data stream. It suffers in prediction accuracy in the
presence of concept drifts. This is because the streaming data are
not generated by a stationary stochastic process, indeed, the future
examples we need to classify may have a very different distribution
from the historical data.
In order to make time-critical predictions, the model learned
from the streaming data must be able to capture transient patterns in
the stream. To do this, as we revise the model by incorporating new
examples, we must also eliminate the effects of examples represent-
ing outdated concepts. This is a non-trivial task. The challenge of
maintaining an accurate and up-to-date classifier for infinite data
streams with concept drifts including the following:

· ACCURACY. It is difficult to decide what are the exam-
ples that represent outdated concepts, and hence their effects
should be excluded from the model. A commonly used ap-
proach is to `forget' examples at a constant rate. However,
a higher rate would lower the accuracy of the `up-to-date'
model as it is supported by a less amount of training data and
a lower rate would make the model less sensitive to the cur-
rent trend and prevent it from discovering transient patterns.


226

· EFFICIENCY. Decisiontreesareconstructed inagreedy divide-
and-conquer manner, and they are non-stable. Even a slight
drift of the underlying concepts may trigger substantial changes
(e.g., replacing old branches with new branches, re-growing
or building alternative subbranches) in the tree, and severely
compromise learning efficiency.

· EASE OF USE. Substantial implementation efforts are re-
quired to adapt classification methods such as decision trees
to handle data streams with drifting concepts in an incremen-
tal manner [19]. The usability of this approach is limited as
state-of-the-art learning methods cannot be applied directly.

In light of these challenges, we propose using weighted classi-
fier ensembles to mine streaming data with concept drifts. Instead
of continuously revising a single model, we train an ensemble of
classifiers from sequential data chunks in the stream. Maintaining
a most up-to-date classifier is not necessarily the ideal choice, be-
cause potentially valuable information may be wasted by discard-
ing results of previously-trained less-accurate classifiers. We show
that, in order to avoid overfitting and the problems of conflicting
concepts, the expiration of old data must rely on data's distribution
instead of only their arrival time. The ensemble approach offers
this capability by giving each classifier a weight based on its ex-
pected prediction accuracy on the current test examples. Another
benefit of the ensemble approach is its efficiency and ease-of-use.
In this paper, we also consider issues in cost-sensitive learning, and
present an instance-based ensemble pruning method that shows in
a cost-sensitive scenario a pruned ensemble delivers the same level
of benefits as the entire set of classifiers.

Paper Organization. The rest of the paper is organized as fol-
lows. In Section 2 we discuss the data expiration problem in mining
concept-drifting data streams. Section 3 proves the error reduction
property of the classifier ensemble in the presence of concept drifts.
Section 4 outlines an algorithm framework for solving the problem.
In Section 5, we present a method that allows us to greatly reduce
the number of classifiers in an ensemble with little loss. Experi-
ments and related work are shown in Section 6 and Section 7.


2. THE DATA EXPIRATION PROBLEM
The fundamental problem in learning drifting concepts is how to
identify in a timely manner those data in the training set that are
no longer consistent with the current concepts. These data must be
discarded. A straightforward solution, which is used in many cur-
rent approaches, discards data indiscriminately after they become
old, that is, after a fixed period of time T has passed since their
arrival. Although this solution is conceptually simple, it tends to
complicate the logic of the learning algorithm. More importantly, it
creates the following dilemma which makes it vulnerable to unpre-
dictable conceptual changes in the data: if T is large, the training
set is likely to contain outdated concepts, which reduces classifica-
tion accuracy; if T is small, the training set may not have enough
data, and as a result, the learned model will likely carry a large
variance due to overfitting.
We use a simple example to illustrate the problem. Assume a
stream of 2-dimensional data is partitioned into sequential chunks
based on their arrival time. Let Si be the data that came in between
time ti and ti
+1
. Figure 1 shows the distribution of the data and the
optimum decision boundary during each time interval.
The problem is: after the arrival of S2 at time t3, what part of the
training data should still remain influential in the current model so
that the data arriving after t3 can be most accurately classified?
optimum boundary:
overfitting:




(a) S0,arrived
during [t0,t1)
(b) S1,arrived
during [t1,t2)
(c) S2,arrived
during [t2,t3)
positive:
negative:




Figure 1: data distributions and optimum boundaries



On one hand, in order to reduce the influence of old data that
may represent a different concept, we shall use nothing but the most
recent data in the stream as the training set. For instance, use the
training set consisting of S2 only (i.e., T = t3 - t2, data S1, S0 are
discarded). However, as shown in Figure 1(c), the learned model
may carry a significant variance since S2's insufficient amount of
data are very likely to be overfitted.


optimum boundary:




(a) S2+S1
(b) S2+S1+S0
(c) S2+S0


Figure 2: Which training dataset to use?


The inclusion of more historical data in training, on the other
hand, may also reduce classification accuracy. In Figure 2(a), where
S2  S1 (i.e., T = t3 - t1) is used as the training set, we can see
that the discrepancy between the underlying concepts of S1 and S2
becomes the cause of the problem. Using a training set consisting
of S2 S1 S0 (i.e., T = t3 -t0) will not solve the problem either.
Thus, there may not exists an optimum T to avoid problems arising
from overfitting and conflicting concepts.
We should not discard data that may still provide useful infor-
mation to classify the current test examples. Figure 2(c) shows that
the combination of S2 and S0 creates a classifier with less over-
fitting or conflicting-concept concerns. The reason is that S2 and
S0 have similar class distribution. Thus, instead of discarding data
using the criteria based solely on their arrival time, we shall make
decisions based on their class distribution. Historical data whose
class distributions are similar to that of current data can reduce the
variance of the current model and increase classification accuracy.
However, it is a non-trivial task to select training examples based
on their class distribution. In this paper, we show that a weighted
classifier ensemble enables us to achieve this goal. We first prove,
in the next section, a carefully weighted classifier ensemble built
on a set of data partitions S1, S2, ··· , Sn is more accurate than a
single classifier built on S1  S2 ··· Sn. Then, we discuss how
the classifiers are weighted.


3. ERROR REDUCTION ANALYSIS


227

Given a test example y, a classifier outputs fc(y), the probability
of y being an instance of class c. A classifier ensemble pools the
outputs of several classifiers before a decision is made. The most
popular way of combining multiple classifiers is via averaging [24],
in which case the probability output of the ensemble is given by:


fE
c
(y) =
1
k
k



i=1
fic(y)


where fic(y) is the probability output of the i-th classifier in the
ensemble.
The outputs of a well trained classifier are expected to approx-
imate the a posterior class distribution. In addition to the Bayes
error, the remaining error of the classifier can be decomposed into
bias and variance [15, 6]. More specifically, given a test example
y, the probability output of classifier Ci can be expressed as:

fic(y) = p(c|y) +
ic + ic(y)

added error for
y
(1)



where p(c|y) is the a posterior probability distribution of class c
given input y, ic is the bias of Ci, and ic(y) is the variance of Ci
given input y. In the following discussion, we assume the error
consists of variance only, as our major goal is to reduce the error
caused by the discrepancies among the classifiers trained on differ-
ent data chunks.
Assume an incoming data stream is partitioned into sequential
chunks of fixed size, S1, S2, ··· , Sn, with Sn being the most recent
chunk. Let Ci, Gk, and Ek denote the following models.

Ci :
classifier learned from training set Si;
Gk :
classifier learned from the training set consisting of
the last k chunks Sn
-k+1
 ···  Sn;
Ek :
classifier ensemble consisting of the last k classifiers
Cn
-k+1
, ··· , Cn.




classification
error on y




...

...
...

...
streamSn
Sn-1

test
example
y
Sn-k
Sn-k-1




Figure 3: Models' classification error on test example y.


In the concept-drifting environment, models learned up-stream
may carry significant variances when they are applied to the cur-
rent test cases (Figure 3). Thus, instead of averaging the outputs
of classifiers in the ensemble, we use the weighted approach. We
assign each classifier Ci a weight wi, such that wi is reversely pro-
portional to Ci's expected error (when applied to the current test
cases). In Section 4, we introduce a method of generating such
weights based on estimated classification errors. Here, assuming
each classifier is so weighted, we prove the following property.
Ek produces a smaller classification error than Gk, if classifiers
in Ek are weighted by their expected classification accuracy on the
test data.




Figure 4: Error regions associated with approximating the a
posteriori probabilities [24].


We prove this property through bias-variance decomposition based
on Tumer's work [24]. The Bayes optimum decision assigns x to
class i if p(ci|x) > p(ck|x), k = i. Therefore, as shown in Fig-
ure 4, the Bayes optimum boundary is the loci of all points x such
that p(ci|x) = p(cj|x), where j = argmaxk p(ck|x) [24].
The decision boundary of our classifier may vary from the optimum
boundary. In Figure 4, b = xb - x denotes the amount by which
the boundary of the classifier differs from the optimum boundary.
In other words, patterns corresponding to the darkly shaded region
are erroneously classified by the classifier. The classifier thus intro-
duces an expected error Err in addition to the error of the Bayes
optimum decision:


Err =



-
A(b)fb(b)db


where A(b) is the area of the darkly shaded region, and fb is the
density function for b. Tumer et al [24] proves that the expected
added error can be expressed by:


Err =
2c
s
(2)

where s = p (cj|x) - p (ci|x) is independent of the trained
model1, and 2c denotes the variances of c(x).
Thus, given a test example y, the probability output of the single
classifier Gk can be expressed as:

fg
c
(y) = p(c|y) + gc (y)

Assuming each partition Si is of the same size, we study 2
g
c
,
the variance of gc (y). If each Si has identical class distribution,
that is, there is no conceptual drift, then the single classifier Gk,
which is learned from k partitions, can reduce the average variance
by a factor of k. With the presence of conceptual drifts, we have:


2
g
c

1
k2
n



i=n-k+1
2ic
(3)


1
Here, p (cj|·) denotes the derivative of p(cj|·).


228

For the ensemble approach, we use weighted averaging to com-
bine outputs of the classifiers in the ensemble. The probability out-
put of the ensemble is given by:


fE
c
(y) =
n



i=n-k+1
wific(y)/
n



i=n-k+1
wi
(4)


where wi is the weight of the i-th classifier, which is assumed to be
reversely proportional to Erri (c is a constant):

wi =
c
2ic
(5)


The probability output Ek (4) can also be expressed as:

fE
c
(y) = p(c|y) + E
c
(y)

where

E
c
(y) =
n



i=n-k+1
wiic(y)/
n



i=n-k+1
wi


Assuming the variances of different classifiers are independent, we
derive the variance of E
c
(y):


2E
c
=
n



i=n-k+1
w2i 2ic /(
n



i=n-k+1
wi)2
(6)


We use the reverse proportional assumption of (5) to simplify (6)
to the following:


2E
c
=
1/
n



i=n-k+1
1
2ic
(7)


It is easy to prove:

n



i=n-k+1
2ic
n



i=n-k+1
1
2ic
 k2


or equivalently:


1/
n



i=n-k+1
1
2ic

1
k2
n



i=n-k+1
2ic


which based on (3) and (7) means:

2E
c
 2
g
c


and thus, we have proved:

ErrE  ErrG

This means, compared with the single classifier Gk, which is
learned from the examples in the entire window of k chunks, the
classifier ensemble approach is capable of reducing classification
error through a weighting scheme where a classifier's weight is re-
versely proportional to its expected error.
Note that the above property does not guarantee that Ek has
higher accuracy than classifier Gj if j < k. For instance, if the con-
cept drifts between the partitions are so dramatic that Sn
-1
and Sn
represent totally conflicting concepts, then adding Cn
-1
in decision
making will only raise classification error. A weighting scheme
should assign classifiers representing totally conflicting concepts
near-zero weights. We discuss how to tune weights in detail in
Section 4.
4. CLASSIFIER ENSEMBLE FOR DRIFT-
ING CONCEPTS
The proof of the error reduction property in Section 3 showed
that a classifier ensemble can outperform a single classifier in the
presence of concept drifts. To apply it to real-world problems we
need to assign an actual weight to each classifier that reflects its
predictive accuracy on the current testing data.

4.1 Accuracy-Weighted Ensembles
The incoming data stream is partitioned into sequential chunks,
S1, S2, ··· , Sn, with Sn being the most up-to-date chunk, and each
chunk is of the same size, or ChunkSize. We learn a classifier Ci
for each Si, i  1.
According to the error reduction property, given test examples T ,
we should give each classifier Ci a weight reversely proportional to
the expected error of Ci in classifying T . To do this, we need to
know the actual function being learned, which is unavailable.
We derive the weight of classifier Ci by estimating its expected
prediction error on the test examples. We assume the class distri-
bution of Sn, the most recent training data, is closest to the class
distribution of the current test data. Thus, the weights of the clas-
sifiers can be approximated by computing their classification error
on Sn.
More specifically, assume that Sn consists of records in the form
of (x, c), where c is the true label of the record. Ci's classification
error of example (x, c) is 1 - fic(x), where fic(x) is the probability
given by Ci that x is an instance of class c. Thus, the mean square
error of classifier Ci can be expressed by:


MSEi =
1
|Sn|
(x,c)Sn
(1 - f ic(x))2


The weight of classifier Ci should be reversely proportional to MSEi.
On the other hand, a classifier predicts randomly (that is, the proba-
bility of x being classified as class c equals to c's class distributions
p(c)) will have mean square error:

MSEr =
c
p(c)(1 - p(c))2


For instance, if c  {0, 1} and the class distribution is uniform, we
have MSEr = .25. Since a random model does not contain useful
knowledge about the data, we use MSEr, the error rate of the ran-
dom classifier as a threshold in weighting the classifiers. That is, we
discard classifiers whose error is equal to or larger than MSEr. Fur-
thermore, to make computation easy, we use the following weight
wi for classifier Ci:

wi = MSEr - MSEi
(8)

For cost-sensitive applications such as credit card fraud detection,
we use the benefits (e.g., total fraud amount detected) achieved by
classifier Ci on the most recent training data Sn as its weight.


predict fraud
predict ¬fraud
actual fraud
t(x) - cost
0
actual ¬fraud
-cost
0

Table 1: Benefit matrix bc,c


Assume the benefit of classifying transaction x of actual class c
as a case of class c is bc,c (x). Based on the benefit matrix shown
in Table 1 (where t(x) is the transaction amount, and cost is the


229

fraud investigation cost), the total benefits achieved by Ci is:

bi =
(x,c)Sn
c
bc,c (x) · fic (x)


and we assign the following weight to Ci:

wi = bi - br
(9)

where br is the benefits achieved by a classifier that predicts ran-
domly. Also, we discard classifiers with 0 or negative weights.
Since we are handling infinite incoming data flows, we will learn
an infinite number of classifiers over the time. It is impossible and
unnecessary to keep and use all the classifiers for prediction. In-
stead, we only keep the top K classifiers with the highest predic-
tion accuracy on the current training data. In Section 5, we dis-
cuss ensemble pruning in more detail and present a technique for
instance-based pruning.
Algorithm 1 gives an outline of the classifier ensemble approach
for mining concept-drifting data streams. Whenever a new chunk
of data has arrived, we build a classifier from the data, and use
the data to tune the weights of the previous classifiers. Usually,
ChunkSize is small (our experiments use chunks of size ranging
from 1,000 to 25,000 records), and the entire chunk can be held in
memory with ease.
The algorithm for classification is straightforward, and it is omit-
ted here. Basically, given a test case y, each of the K classifiers is
applied on y, and their outputs are combined through weighted av-
eraging.



Input: S: a dataset of ChunkSize from the incoming stream
K: the total number of classifiers
C: a set of K previously trained classifiers
Output: C: a set of K classifiers with updated weights

train classifier C from S;
compute error rate / benefits of C via cross validation on S;
derive weight w for C using (8) or (9);
for each classifier Ci  C do
apply Ci on S to derive MSEi or bi;
compute wi based on (8) and (9);

C  K of the top weighted classifiers in C  {C };
return C;


Algorithm 1:
A classifier ensemble approach for mining
concept-drifting data streams


4.2 Complexity
Assume the complexity for building a classifier on a data set of
size s is f(s). The complexity to classify a test data set in order
to tune its weight is linear in the size of the test data set. Suppose
the entire data stream is divided into a set of n partitions, then the
complexity of Algorithm 1 is O(n · f(s/n) + Ks), where n
K. On the other hand, building a single classifier on s requires
O(f(s)). For most classifier algorithms, f(·) is super-linear, which
means the ensemble approach is more efficient.


5. ENSEMBLE PRUNING
A classifier ensemble combines the probability or the benefit out-
put of a set of classifiers. Given a test example y, we need to consult
every classifier in the ensemble, which is often time consuming in
an online streaming environment.
5.1 Overview
In many applications, the combined result of the classifiers usu-
ally converges to the final value well before all classifiers are con-
sulted. The goal of pruning is to identify a subset of classifiers that
achieves the same level of total benefits as the entire ensemble.
Traditional pruning is based on classifiers' overall performances
(e.g., average error rate, average benefits, etc.). Several criteria can
be used in pruning. The first criterion is mean square error. The
goal is to find a set of n classifiers that has the minimum mean
square error. The second approach favors classifier diversity, as
diversity is the major factor in error reduction. KL-distance, or
relative entropy, is a widely used measure for difference between
two distributions. The KL-distance between two distributions p and
q is defined as D(p||q) =
x
p(x) log p(x)/q(x). In our case, p
and q are the class distributions given by two classifiers. The goal
is then to find the set of classifiers S that maximizes mutual KL-
distances.
It is, however, impractical to search for the optimal set of clas-
sifiers based on the MSE criterion or the KL-distance criterion.
Even greedy methods are time consuming: the complexities of the
greedy methods of the two approaches are O(|T | · N · K) and
O(|T |· N · K2) respectively, where N is the total number of avail-
able classifiers.
Besides the complexity issue, the above two approaches do not
apply to cost-sensitive applications. Moreover, the applicability of
the KL-distance criterion is limited to streams with no or mild con-
cept drifting only, since concept drifts also enlarge the KL-distance.
In this paper, we apply the instance-based pruning technique [11]
to data streams with conceptual drifts.

5.2 Instance Based Pruning
Cost-sensitive applications usually provide higher error toler-
ance. For instance, in credit card fraud detection, the decision
threshold of whether to launch an investigation or not is:

p(fraud|y) · t(y) > cost

where t(y) is the amount of transaction y. In other words, as long
as p(fraud|y) > cost/t(y), transaction y will be classified as fraud
no matter what the exact value of p(fraud|y) is. For example, as-
suming t(y) = $900, cost = $90, both p(fraud|y) = 0.2 and
p(fraud|y) = 0.4 result in the same prediction. This property helps
reduce the "expected" number of classifiers needed in prediction.
We use the following approach for instance based ensemble prun-
ing [11]. For a given ensemble S consisting of K classifiers, we
first order the K classifiers by their decreasing weight into a "pipeline".
(The weights are tuned for the most-recent training set.) To classify
a test example y, the classifier with the highest weight is consulted
first, followed by other classifiers in the pipeline. This pipeline
procedure stops as soon as a "confident prediction" can be made or
there are no more classifiers in the pipeline.
More specifically, assume that C1, ··· , CK are the classifiers in
the pipeline, with C1 having the highest weight. After consulting
the first k classifiers C1, ··· , Ck, we derive the current weighted
probability as:


Fk(x) =
k
i=1
wi · pi(fraud|x)
k
i=1
wi

The final weighted probability, derived after all K classifiers are
consulted, is FK(x). Let
k
(x) = Fk(x) - FK(x) be the error
at stage k. The question is, if we ignore
k
(x) and use Fk(x) to
decide whether to launch a fraud investigation or not, how much
confidence do we have that using FK(x) would have reached the
same decision?


230

Algorithm 2 estimates the confidence. We compute the mean and
the variance of
k
(x), assuming that
k
(x) follows normal distri-
bution. The mean and variance statistics can be obtained by eval-
uating Fk(.) on the current training set for every classifier Ck. To
reduce the possible error range, we study the distribution under a
finer grain. We divide [0, 1], the range of Fk(·), into  bins. An
example x is put into bin i if Fk(x)  [
i

,
i+1

). We then com-
pute µk,i and 2k,i, the mean and the variance of the error of those
training examples in bin i at stage k.
Algorithm 3 outlines the procedure of classifying an unknown
instance y. We use the following decision rules after we have ap-
plied the first k classifiers in the pipeline on instance y:



Fk(y) - µk,i - t · k,i > cost/t(y),
fraud
Fk(y) + µk,i + t · k,i  cost/t(y),
non-fraud
otherwise,
uncertain
(10)


where i is the bin y belongs to, and t is a confidence interval param-
eter. Under the assumption of normal distribution, t = 3 delivers
a confidence of 99.7%, and t = 2 of 95%. When the prediction is
uncertain, that is, the instance falls out of the t sigma region, the
next classifier in the pipeline, Ck
+1
, is employed, and the rules are
applied again. If there are no classifiers left in the pipeline, the cur-
rent prediction is returned. As a result, an example does not need to
use all classifiers in the pipeline to compute a confident prediction.
The "expected" number of classifiers can be reduced.



Input: S: a dataset of ChunkSize from the incoming stream
K: the total number of classifiers
: number of bins
C: a set of K previously trained classifiers
Output: C: a set of K classifiers with updated weights
µ, : mean and variance for each stage and each bin

train classifier C from S;
compute error rate / benefits of C via cross validation on S;
derive weight w for C using (8) or (9);
for each classifier Ck  C do
apply Ck on S to derive MSEk or bk;
compute wk based on (8) and (9);

C  K of the top weighted classifiers in C  {C };
for each y  S do
compute Fk(y) for k = 1, ··· , K;
y belongs to bin (i, k) if Fk(y)  [
i

,
i+1

);
incrementally updates µi,k and i,k for bin (i, k);


Algorithm 2: Obtaining µk,i and k,i during ensemble con-
struction


5.3 Complexity
Algorithm 3 outlines instance based pruning. To classify a dataset
of size s, the worst case complexity is O(Ks). In the experiments,
we show that the actual number of classifiers can be reduced dra-
matically without affecting the classification performance.
The cost of instance based pruning mainly comes from updating
µk,i and 2k,i for each k and i. These statistics are obtained during
training time. The procedure shown in Algorithm 2 is an improved
version of Algorithm 1. The complexity of Algorithm 2 remains
O(n·f(s/n)+Ks) (updating of the statistics costs O(Ks)), where
s is the size of the data stream, and n is the number of partitions of
the data stream.
Input: y: a test example
t: confidence level
C: a set of K previously trained classifiers
Output: prediction of y's class

Let C = {C1, ··· , Cn} with wi  wj for i < j;
F0(y)  0;
w  0;
for k = {1, ··· , K} do
Fk(y) 
Fk-
1
·w+wk·pk(fraud|x)
w+wk
;
w  w + wk;
let i be the bin y belongs to;
apply rules in (10) to check if y is in t- region;
return fraud/non-fraud if t- confidence is reached;
if FK(y) > cost/t(y) then
return fraud;

else
return non-fraud;


Algorithm 3: Classification with Instance Based Pruning


6. EXPERIMENTS
We conducted extensive experiments on both synthetic and real
life data streams. Our goals are to demonstrate the error reduction
effects of weighted classifier ensembles, to evaluate the impact of
the frequency and magnitude of the concept drifts on prediction
accuracy, and to analyze the advantage of our approach over al-
ternative methods such as incremental learning. The base models
used in our tests are C4.5 [20], the RIPPER rule learner [5], and
the Naive Bayesian method. The tests are conducted on a Linux
machine with a 770 MHz CPU and 256 MB main memory.

6.1 Algorithms used in Comparison
We denote a classifier ensemble with a capacity of K classifiers
as EK. Each classifier is trained by a data set of size ChunkSize.
We compare with algorithms that rely on a single classifier for min-
ing streaming data. We assume the classifier is continuously being
revised by the data that have just arrived and the data being faded
out. We call it a window classifier, since only the data in the most
recent window have influence on the model. We denote such a clas-
sifier by GK, where K is the number of data chunks in the window,
and the total number of the records in the window is K·ChunkSize.
Thus, ensemble EK and GK are trained from the same amount of
data. Particularly, we have E1 = G1. We also use G0 to denote
the classifier built on the entire historical data starting from the be-
ginning of the data stream up to now. For instance, BOAT [14] and
VFDT [7] are G0 classifiers, while CVFDT [19] is a GK classifier.

6.2 Streaming Data

Synthetic Data. We create synthetic data with drifting concepts
based on a moving hyperplane. A hyperplane in d-dimensional
space is denoted by equation:

d



i=1
aixi = a0
(11)


We label examples satisfying
d
i=1
aixi  a0 as positive, and ex-
amples satisfying
d
i=1
aixi < a0 as negative. Hyperplanes have
been used to simulate time-changing concepts because the orienta-


231

tion and the position of the hyperplane can be changed in a smooth
manner by changing the magnitude of the weights [19].
We generate random examples uniformly distributed in multi-
dimensional space [0, 1]d. Weights ai (1  i  d) in (11) are
initialized randomly in the range of [0, 1]. We choose the value of
a0 so that the hyperplane cuts the multi-dimensional space in two
parts of the same volume, that is, a0 =
1
2
d
i=1
ai. Thus, roughly
half of the examples are positive, and the other half negative. Noise
is introduced by randomly switching the labels of p% of the exam-
ples. In our experiments, the noise level p% is set to 5%.
We simulate concept drifts by a series of parameters. Parame-
ter k specifies the total number of dimensions whose weights are
changing. Parameter t  R specifies the magnitude of the change
(every N examples) for weights a1, ··· , ak, and si  {-1, 1}
specifies the direction of change for each weight ai, 1  i  k.
Weights change continuously, i.e., ai is adjusted by si · t/N af-
ter each example is generated. Furthermore, there is a possibility
of 10% that the change would reverse direction after every N ex-
amples are generated, that is, si is replaced by -si with probabil-
ity 10%. Also, each time the weights are updated, we recompute
a0 =
1
2
d
i=1
ai so that the class distribution is not disturbed.

Credit Card Fraud Data. We use real life credit card transac-
tion flows for cost-sensitive mining. The data set is sampled from
credit card transaction records within a one year period and con-
tains a total of 5 million transactions. Features of the data include
the time of the transaction, the merchant type, the merchant loca-
tion, past payments, the summary of transaction history, etc. A
detailed description of this data set can be found in [22]. We use
the benefit matrix shown in Table 1 with the cost of disputing and
investigating a fraud transaction fixed at cost = $90.
The total benefit is the sum of recovered amount of fraudulent
transactions less the investigation cost. To study the impact of con-
cept drifts on the benefits, we derive two streams from the dataset.
Records in the 1st stream are ordered by transaction time, and
records in the 2nd stream by transaction amount.

6.3 Experimental Results

Time Analysis. We study the time complexity of the ensemble
approach. We generate synthetic data streams and train single deci-
sion tree classifiers and ensembles with varied ChunkSize. Con-
sider a window of K = 100 chunks in the data stream. Figure 5
shows that the ensemble approach EK is much more efficient than
the corresponding single-classifier GK in training.
Smaller ChunkSize offers better training performance. How-
ever, ChunkSize also affects classification error. Figure 5 shows
the relationship between error rate (of E10, e.g.) and ChunkSize.
The dataset is generated with certain concept drifts (weights of 20%
of the dimensions change t = 0.1 per N = 1000 records), large
chunks produce higher error rates because the ensemble cannot de-
tect the concept drifts occurring inside the chunk. Small chunks
can also drive up error rates if the number of classifiers in an en-
semble is not large enough. This is because when ChunkSize is
small, each individual classifier in the ensemble is not supported by
enough amount of training data.

Pruning Effects. Pruning improves classification efficiency. We
examine the effects of instance based pruning using the credit card
fraud data. In Figure 6(a), we show the total benefits achieved by
ensemble classifiers with and without instance-based pruning. The
X-axis represents the number of the classifiers in the ensemble, K,
which ranges from 1 to 32. When instance-based pruning is in ef-
50
100
150
200
250
300
350
400
450




500
1000
1500
2000
13
13.5
14
14.5
15
15.5
16
16.5
17
17.5
18




Training
Time
(s)




Error
Rate
(%)




ChunkSize
Training Time of G100
Training Time of E100
Ensemble Error Rate




Figure 5: Training Time, ChunkSize, and Error Rate




fect, the actual number of classifiers to be consulted is reduced. In
the figure, we overload the meaning of the X-axis to represent the
average number of classifiers used under instance-based pruning.
For E32, pruning reduces the average number of classifiers to 6.79,
a reduction of 79%. Still, it achieves a benefit of $811,838, which
is just a 0.1% drop from $812,732 ­ the benefit achieved by E32
which uses all 32 classifiers.
Figure 6(b) studies the same phenomena using 256 classifiers
(K = 256). Instead of dynamic pruning, we use the top K classi-
fiers, and the Y-axis shows the benefits improvement ratio. The top
ranked classifiers in the pipeline outperform E256 in almost all the
cases except if only the 1st classifier in the pipeline is used.

Error Analysis. We use C4.5 as our base model, and compare
the error rates of the single classifier approach and the ensemble
approach. The results are shown in Figure 7 and Table 2. The syn-
thetic datasets used in this study have 10 dimensions (d = 10).
Figure 7 shows the averaged outcome of tests on data streams gen-
erated with varied concept drifts (the number of dimensions with
changing weights ranges from 2 to 8, and the magnitude of the
change t ranges from 0.10 to 1.00 for every 1000 records).
First, we study the impact of ensemble size (total number of clas-
sifiers in the ensemble) on classification accuracy. Each classifier
is trained from a dataset of size ranging from 250 records to 1000
records, and their averaged error rates are shown in Figure 7(a).
Apparently, when the number of classifiers increases, due to the in-
crease of diversity of the ensemble, the error rate of Ek drops sig-
nificantly. The single classifier, Gk, trained from the same amount
of the data, has a much higher error rate due to the changing con-
cepts in the data stream. In Figure 7(b), we vary the chunk size and
average the error rates on different K ranging from 2 to 8. It shows
that the error rate of the ensemble approach is about 20% lower than
the single-classifier approach in all the cases. A detailed compar-
ison between single- and ensemble-classifiers is given in Table 2,
where G0 represents the global classifier trained by the entire his-
tory data, and we use bold font to indicate the better result of Gk
and Ek for K = 2, 4, 6, 8.
We also tested the Naive Bayesian and the RIPPER classifier un-
der the same setting. The results are shown in Table 3 and Table 4.
Although C4.5, Naive Bayesian, and RIPPER deliver different ac-
curacy rates, they confirmed that, with a reasonable amount of clas-
sifiers (K) in the ensemble, the ensemble approach outperforms the
single classifier approach.



232

750000
760000
770000
780000
790000
800000
810000
820000
830000




5
10
15
20
25
30
Benefits
($)




# of classifiers in the ensemble (K)
Instance-based Pruning
Classifier ensemble EK

0.98
0.99
1
1.01
1.02
1.03
1.04
1.05
1.06




0
50
100
150
200
250
Improvement
ratio




# of classifiers used out of 256 (K)
Top K Classifiers
Classifier ensemble E256




(a) Reduction of ensemble size
(b) Benefit improvement of pruned
by instance-based pruning
ensemble for credit card dataset

Figure 6: Effects of Instance-Based Pruning




11
12
13
14
15
16
17




2
3
4
5
6
7
8
Error
Rate
(%)




K
Single GK
Ensemble EK




12
12.5
13
13.5
14
14.5
15
15.5
16
16.5
17
17.5




200
300
400
500
600
700
800
900
1000
Error
Rate
(%)




ChunkSize
Single GK
Ensemble EK




(a) Varying window size/ensemble size
(b) Varying ChunkSize

Figure 7: Average Error Rate of Single and Ensemble Decision Tree Classifiers




ChunkSize
G0
G1 = E1
G2
E2
G4
E4
G6
E6
G8
E8
250
18.09
18.76
18.00
18.37
16.70
14.02
16.72
12.82
16.76
12.19
500
17.65
17.59
16.39
17.16
16.19
12.91
15.32
11.74
14.97
11.25
750
17.18
16.47
16.29
15.77
15.07
12.09
14.97
11.19
14.86
10.84
1000
16.49
16.00
15.89
15.62
14.40
11.82
14.41
10.92
14.68
10.54


Table 2: Error Rate (%) of Single and Ensemble Decision Tree Classifiers




ChunkSize
G0
G1=E1
G2
E2
G4
E4
G6
E6
G8
E8
250
11.94
8.09
7.91
7.48
8.04
7.35
8.42
7.49
8.70
7.55
500
12.11
7.51
7.61
7.14
7.94
7.17
8.34
7.33
8.69
7.50
750
12.07
7.22
7.52
6.99
7.87
7.09
8.41
7.28
8.69
7.45
1000
15.26
7.02
7.79
6.84
8.62
6.98
9.57
7.16
10.53
7.35


Table 3: Error Rate (%) of Single and Ensemble Naive Bayesian Classifiers




ChunkSize
G0
G1=E1
G2
E2
G4
E4
G6
E6
G8
E8
50
27.05
24.05
22.85
22.51
21.55
19.34
24.05
22.51
19.34
17.84
100
25.09
21.97
19.85
20.66
17.48
17.50
21.97
20.66
17.50
15.91
150
24.19
20.39
18.28
19.11
17.22
16.39
20.39
19.11
16.39
15.03


Table 4: Error Rate (%) of Single and Ensemble RIPPER Classifiers




233

10
15
20
25
30
35
40
45
50




0
5
10
15
20
25
30
35
Error
Rate




Dimension
Single
Ensemble




15
20
25
30
35
40
45




10
15
20
25
30
35
40
Error
Rate




Dimension
Single
Ensemble




21
22
23
24
25
26
27
28




0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
Error
Rate




Weight Change (monotonic)
Single
Ensemble




(a) # of changing dimensions
(b) total dimensionality
(c) (monotonic) weight change per dimension

Figure 8: Magnitude of Concept Drifts




100000
105000
110000
115000
120000
125000
130000
135000
140000
145000
150000
155000




2
3
4
5
6
7
8
Total
Benefits
($)




K
Ensemble EK
Single GK




0
50000
100000
150000
200000
250000
300000




3000 4000 5000 6000 7000 8000 9000
10000110001200013000
Total
Benefits
($)




ChunkSize
Ensemble EK
Single GK




110000
120000
130000
140000
150000
160000
170000
180000




2
3
4
5
6
7
8
Total
Benefits
($)




K
Ensemble EK
Single GK




50000
100000
150000
200000
250000
300000
350000




3000 4000 5000 6000 7000 8000 9000
10000110001200013000
Total
Benefits
($)




ChunkSize
Ensemble EK
Single GK




(a) Varying K
(b) Varying ChunkSize
(c) Varying K
(d) Varying ChunkSize
(original stream)
(original stream)
(simulated stream)
(simulated stream)

Figure 9: Averaged Benefits using Single Classifiers and Classifier Ensembles


Concept Drifts. Figure 8 studies the impact of the magnitude
of the concept drifts on classification error. Concept drifts are con-
trolled by two parameters in the synthetic data: i) the number of
dimensions whose weights are changing, and ii) the magnitude of
weight change per dimension. Figure 8 shows that the ensemble
approach outperform the single-classifier approach under all cir-
cumstances. Figure 8(a) shows the classification error of Gk and
Ek (averaged over different K) when 4, 8, 16, and 32 dimen-
sions' weights are changing (the change per dimension is fixed at
t = 0.10). Figure 8(b) shows the increase of classification error
when the dimensionality of dataset increases. In the datasets, 40%
dimensions' weights are changing at ±0.10 per 1000 records. An
interesting phenomenon arises when the weights change monoton-
ically (weights of some dimensions are constantly increasing, and
others constantly decreasing). In Figure 8(c), classification error
drops when the change rate increases. This is because of the follow-
ing. Initially, all the weights are in the range of [0, 1]. Monotonic
changes cause some attributes to become more and more 'impor-
tant', which makes the model easier to learn.

Cost-sensitive Learning. For cost-sensitive applications, we
aim at maximizing benefits. In Figure 9(a), we compare the sin-
gle classifier approach with the ensemble approach using the credit
card transaction stream. The benefits are averaged from multiple
runs with different chunk size (ranging from 3000 to 12000 trans-
actions per chunk). Starting from K = 2, the advantage of the
ensemble approach becomes obvious.
In Figure 9(b), we average the benefits of Ek and Gk (K =
2, ··· , 8) for each fixed chunk size. The benefits increase as the
chunk size does, as more fraudulent transactions are discovered in
the chunk. Again, the ensemble approach outperforms the single
classifier approach.
To study the impact of concept drifts of different magnitude, we
derive data streams from the credit card transactions. The simulated
stream is obtained by sorting the original 5 million transactions by
their transaction amount. We perform the same test on the simu-
lated stream, and the results are shown in Figure 9(c) and 9(d).
Detailed results of the above tests are given in Table 6 and 5.


7. DISCUSSION AND RELATED WORK
Data stream processing has recently become a very important re-
search domain. Much work has been done on modeling [1], query-
ing [2, 13, 16], and mining data streams, for instance, several pa-
pers have been published on classification [7, 19, 23], regression
analysis [4], and clustering [17].
Traditional data mining algorithms are challenged by two char-
acteristic features of data streams: the infinite data flow and the
drifting concepts. As methods that require multiple scans of the
datasets [21] can not handle infinite data flows, several incremental
algorithms [14, 7] that refine models by continuously incorporating
new data from the stream have been proposed. In order to handle
drifting concepts, these methods are revised again to achieve the
goal that effects of old examples are eliminated at a certain rate. In
terms of an incremental decision tree classifier, this means we have
to discard, re-grow sub trees, or build alternative subtrees under a
node [19]. The resulting algorithm is often complicated, which
indicates substantial efforts are required to adapt state-of-the-art
learning methods to the infinite, concept-drifting streaming envi-
ronment. Aside from this undesirable aspect, incremental methods
are also hindered by their prediction accuracy. Since old examples
are discarded at a fixed rate (no matter if they represent the changed
concept or not), the learned model is supported only by the current
snapshot ­ a relatively small amount of data. This usually results
in larger prediction variances.
Classifier ensembles are increasingly gaining acceptance in the
data mining community. The popular approaches to creating en-


234

ChunkSize
G0
G1=E1
G2
E2
G4
E4
G6
E6
G8
E8
12000
296144
207392
233098
268838
248783
313936
263400
327331
275707
360486
6000
146848
102099
102330
129917
113810
148818
118915
155814
123170
162381
4000
96879
62181
66581
82663
72402
95792
74589
101930
76079
103501
3000
65470
51943
55788
61793
59344
70403
62344
74661
66184
77735


Table 5: Benefits (US $) using Single Classifiers and Classifier Ensembles (simulated stream)

ChunkSize
G0
G1=E1
G2
E2
G4
E4
G6
E6
G8
E8
12000
201717
203211
197946
253473
211768
269290
211644
282070
215692
289129
6000
103763
98777
101176
121057
102447
138565
103011
143644
106576
143620
4000
69447
65024
68081
80996
69346
90815
69984
94400
70325
96153
3000
43312
41212
42917
59293
44977
67222
45130
70802
46139
71660


Table 6: Benefits (US $) using Single Classifiers and Classifier Ensembles (original stream)


sembles include changing the instances used for training through
techniques such as Bagging [3] and Boosting [12]. The classifier
ensembles have several advantages over single model classifiers.
First, classifier ensembles offer a significant improvement in pre-
diction accuracy [12, 24]. Second, building a classifier ensemble is
more efficient than building a single model, since most model con-
struction algorithms have super-linear complexity. Third, the na-
ture of classifier ensembles lend themselves to scalable paralleliza-
tion [18] and on-line classification of large databases. Previously,
we used averaging ensemble for scalable learning over very-large
datasets [10]. We show that a model's performance can be esti-
mated before it is completely learned [8, 9]. In this work, we use
weighted ensemble classifiers on concept-drifting data streams. It
combines multiple classifiers weighted by their expected predic-
tion accuracy on the current test data. Compared with incremental
models trained by data in the most recent window, our approach
combines talents of set of experts based on their credibility and
adjusts much nicely to the underlying concept drifts. Also, we in-
troduced the dynamic classification technique [11] to the concept-
drifting streaming environment, and our results show that it enables
us to dynamically select a subset of classifiers in the ensemble for
prediction without loss in accuracy.


8. REFERENCES
[1] B. Babcock, S. Babu, M. Datar, R. Motawani, and J. Widom.
Models and issues in data stream systems. In ACM
Symposium on Principles of Database Systems (PODS),
2002.
[2] S. Babu and J. Widom. Continuous queries over data
streams. SIGMOD Record, 30:109­120, 2001.
[3] Eric Bauer and Ron Kohavi. An empirical comparison of
voting classification algorithms: Bagging, boosting, and
variants. Machine Learning, 36(1-2):105­139, 1999.
[4] Y. Chen, G. Dong, J. Han, B. W. Wah, and J. Wang.
Multi-dimensional regression analysis of time-series data
streams. In Proc. of Very Large Database (VLDB),
Hongkong, China, 2002.
[5] William Cohen. Fast effective rule induction. In Int'l Conf.
on Machine Learning (ICML), pages 115­123, 1995.
[6] P. Domingos. A unified bias-variance decomposition and its
applications. In Int'l Conf. on Machine Learning (ICML),
pages 231­238, 2000.
[7] P. Domingos and G. Hulten. Mining high-speed data streams.
In Int'l Conf. on Knowledge Discovery and Data Mining
(SIGKDD), pages 71­80, Boston, MA, 2000. ACM Press.
[8] W. Fan, H. Wang, P. Yu, and S. Lo. Progressive modeling. In
Int'l Conf. Data Mining (ICDM), 2002.
[9] W. Fan, H. Wang, P. Yu, and S. Lo. Inductive learning in less
than one sequential scan. In Int'l Joint Conf. on Artificial
Intelligence, 2003.
[10] W. Fan, H. Wang, P. Yu, and S. Stolfo. A framework for
scalable cost-sensitive learning based on combining
probabilities and benefits. In SIAM Int'l Conf. on Data
Mining (SDM), 2002.
[11] Wei Fan, Fang Chu, Haixun Wang, and Philip S. Yu. Pruning
and dynamic scheduling of cost-sensitive ensembles. In
Proceedings of the 18th National Conference on Artificial
Intelligence (AAAI), 2002.
[12] Yoav Freund and Robert E. Schapire. Experiments with a
new boosting algorithm. In Int'l Conf. on Machine Learning
(ICML), pages 148­156, 1996.
[13] L. Gao and X. Wang. Continually evaluating similarity-based
pattern queries on a streaming time series. In Int'l Conf.
Management of Data (SIGMOD), Madison, Wisconsin, June
2002.
[14] J. Gehrke, V. Ganti, R. Ramakrishnan, and W. Loh. BOAT­
optimistic decision tree construction. In Int'l Conf.
Management of Data (SIGMOD), 1999.
[15] S. Geman, E. Bienenstock, and R. Doursat. Neural networks
and the bias/variance dilemma. Neural Computation,
4(1):1­58, 1992.
[16] M. Greenwald and S. Khanna. Space-efficient online
computation of quantile summaries. In Int'l Conf.
Management of Data (SIGMOD), pages 58­66, Santa
Barbara, CA, May 2001.
[17] S. Guha, N. Milshra, R. Motwani, and L. O'Callaghan.
Clustering data streams. In IEEE Symposium on Foundations
of Computer Science (FOCS), pages 359­366, 2000.
[18] L. Hall, K. Bowyer, W. Kegelmeyer, T. Moore, and C. Chao.
Distributed learning on very large data sets. In Workshop on
Distributed and Parallel Knowledge Discover, 2000.
[19] G. Hulten, L. Spencer, and P. Domingos. Mining
time-changing data streams. In Int'l Conf. on Knowledge
Discovery and Data Mining (SIGKDD), pages 97­106, San
Francisco, CA, 2001. ACM Press.
[20] J. Ross Quinlan. C4.5: Programs for Machine Learning.
Morgan Kaufmann, 1993.
[21] C. Shafer, R. Agrawal, and M. Mehta. Sprint: A scalable
parallel classifier for data mining. In Proc. of Very Large
Database (VLDB), 1996.
[22] S. Stolfo, W. Fan, W. Lee, A. Prodromidis, and P. Chan.
Credit card fraud detection using meta-learning: Issues and
initial results. In AAAI-97 Workshop on Fraud Detection and
Risk Management, 1997.
[23] W. Nick Street and YongSeog Kim. A streaming ensemble
algorithm (SEA) for large-scale classification. In Int'l Conf.
on Knowledge Discovery and Data Mining (SIGKDD), 2001.
[24] Kagan Tumer and Joydeep Ghosh. Error correlation and
error reduction in ensemble classifiers. Connection Science,
8(3-4):385­403, 1996.
[25] P. E. Utgoff. Incremental induction of decision trees.
Machine Learning, 4:161­186, 1989.



235

