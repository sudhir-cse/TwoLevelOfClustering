A Generative Probabilistic Approach to Visualizing Sets of
Symbolic Sequences


Peter Tino
School of Computer Science
The University of Birmingham
Birmingham B15 2TT, UK
P.Tino@cs.bham.ac.uk
Ata Kab´an
School of Computer Science
The University of Birmingham
Birmingham B15 2TT, UK
A.Kaban@cs.bham.ac.uk
Yi Sun
Faculty of Eng & Info Sciences
University of Hertfordshire
Hatfield, AL10 9AB, UK
Y.2.Sun@herts.ac.uk


ABSTRACT
There is a notable interest in extending probabilistic gener-
ative modeling principles to accommodate for more complex
structured data types. In this paper we develop a generative
probabilistic model for visualizing sets of discrete symbolic
sequences. The model, a constrained mixture of discrete
hidden Markov models, is a generalization of density-based
visualization methods previously developed for static data
sets. We illustrate our approach on sequences representing
web-log data and chorals by J.S. Bach.

Categories and Subject Descriptors: H.3.3 [Informa-
tion Storage and Retrieval]: Information Search and Re-
trieval

General Terms: Algorithms, Design, Theory

Keywords: Hidden Markov model, latent space models,
topographic mapping, EM algorithm


1. INTRODUCTION
Topographic visualisation techniques have been an im-
portant tool in multi-variate data analysis and data min-
ing. Generative probabilistic approaches [2, 3, 7] have been
developed and demonstrated to offer numerous advantages
over non-probabilistic alternatives in terms of a flexible and
technically sound framework that makes various extensions
possible in a principled manner.
However, in their current form, most of these methods
make an assumption that observation data can be repre-
sented in the form of a set of i.i.d. unstructured numerical
vectors. While this may be a reasonable assumption in some
cases, many of the most recent practical problems face us
with the notion of structured observation types. This cre-
ates new challenges for data analysis research. Algorithms
that are able to discover structure in sample sets of such
structured entities need to be developed. Practical examples
include various user profiling tasks, where, in the simplest
case, one observation consists of a log trace left by a user as




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
a result of interacting with an electronic environment.
There has been a notable interest in extending probabilis-
tic generative modeling principles to accommodate for more
complex structured data types [4, 6, 13, 14]. The work
in [4] essentially provides a method of visualisation of user
navigation sequences based on clustering of Markov chains.
Probabilistic clustering of hidden Markov models have also
been developed [13] and applied to user navigation mod-
elling in [14]. In addition to clustering models, the work in
[6] proposes a computationally efficient convex distributed
dynamic model for profiling and prediction of dynamic user
activity.
However, none of these methods provide features for a
topographic organization of these more complex data types.
Topographic mappings require nonlinear distributed models
in order to preserve the core of the information contained
in the possibly complex and heterogeneous set of structured
observations in a 2D visualisation plane.
The necessity of visualising non-vectorial structured types
of data has been recognised in the literature and there are
non-probabilistic, SOM-based approaches to e.g. visualising
time series [5, 9]. In [5], a self organising map of first order
Markov chains is developed and it is pointed out that the
Euclidean distance measures initially employed in the non-
probabilistic SOM method are not appropriate for clustering
or visualisation based on probabilistic models. A Kullback-
Leibler divergence is then employed within the SOM, how-
ever, this heuristic does not follow from a consistent model
formulation.
In this paper we develop a consistent generative proba-
bilistic model for visualizing sets of discrete symbolic se-
quences, where an appropriate divergence measure is defined
by the noise model1. The model is a constrained mixture
of discrete hidden Markov models (HMM) [11]), where the
constraint, introduced in the spirit of [2, 7], provides the
model with topographic organisation capabilities.
The remainder of the paper is organised as follows: Sec-
tion 2 introduces the model and the training algorithm. Sec-
tion 3 provides experimental illustration of the method on
two real world data sets: melodic lines of chorales by J.S.
Bach and web navigation sequences. We conclude the paper
with a brief summary of the key ideas and findings in section
4.



1
Empirical Kullback-Leibler divergence between the un-
known true distribution that generated the data item (se-
quence) and the reference hidden Markov model.



701
Research Track Poster

2. A LATENT TRAIT MODEL FOR
DISCRETE HMMS
Consider a set of symbolic sequences over the alphabet of
S symbols, S = {1, 2, ..., S}. The sequences can represent
e.g. melodic lines, or traces of web users requested by a
population of users. The n-th sequence will be denoted by
×(
n)
= (s(
n)
t
)t
=1:Tn
where n = 1 : N and Tn denotes the
length of the n-th sequence. The lengths of the sequences
may vary. Consider further an (L=2)-dimensional latent
space [-1, 1]2. The aim is to represent each sequence as
using the latent space, such that the important overall char-
acteristics of the set o sequences are revealed. A natural way
of achieving this is to impose a maximum entropy (uniform)
distribution over the latent space. For tractability reasons
it is convenient to discretize the latent space into a regu-
lar grid of C points
Ü1,
...,
ÜC.
These sample (grid) points
are analogous to the nodes of a Self Organising Map. With
each grid point
Üc,
we associate a generative distribution
over sequences p(
×|Üc).
Assuming the sequences
×(
n)
, n = 1 : N, were indepen-
dently generated, the data likelihood of our model is


L =
N


n=1
p(
×(
n)
) =
N


n=1
1
C
C


c=1
p(
×(
n)
|
Üc).
(1)


In order account for temporal dependency the sequences,
we let the noise terms p(
×|Üc)
to take the form of hidden
Markov models with K hidden states [11]:


p(
×(
n)
|
Üc)
=
p(h1|
Üc)
Tn


t=2
p(ht|ht
-1
,
Üc)
Tn


t=1
p(s(
n)
t
|ht,
Üc),

(2)
where
is the set of all Tn-tuples over the K hidden states.
The (logarithm of the) data likelihood needs now to be
maximised. As there are hidden variables in the model,
an EM-type solution will be adopted. According to the
EM methodology, the expectation of the complete log like-
lihood needs to be maximised. The complete latent-centre-
conditional data distribution factorises into several multino-
mials:

p(
×(
n)
,
(n)
|
Üc)
=
K


k=1
p(h1 = k|
Üc)
(h1=k|
×
(n)
,
Ü
c
)



Tn


t=2
K


l=1
K


k=1
p(ht = k|ht
-1
= l,
Üc)
(ht=k,ht
-1
=l|
×
(n)
,
Ü
c
)



Tn


t=1
K


k=1
p(s(
n)
t
|ht = k,
Üc)
(ht=k|
×
(n)
,
Ü
c
)
.


In order to have the HMM components topologically organ-
ised -- e.g. on a two-dimensional equidistant grid -- we
will, in the spirit of [2, 7], constrain the mixture of HMMs,


p(
×)
=
1
C
C


c=1
p(
×|Üc),

by requiring that the HMM parameters be generated through
a parameterised smooth nonlinear mapping from the latent
space into the HMM parameter space. In particular

c
= {p(h1 = k|
Üc)
}k
=1:K

= {gk(
(
)
(
Üc))
}k
=1:K
Ìc
= {p(ht = k|ht
-1
= l,
Üc)
}k,l
=1:K

= {gk(
(
Ì
l
)
(
Üc))
}k,l
=1:K



c
= {p(s(
n)
t
= s|ht = k,
Üc)
}s
=1:S,k=1:K

= {gs(
(
k
)
(
Üc))
}s
=1:S,k=1:K

where

· the function g(.) is the softmax function, which is the
canonical inverse link function of multinomial distribu-
tions and gk(.) denotes the k-th component returned
by the softmax, i.e.

gk (a1, a2, ..., aq)T =
eak
Èq
i=1
eai
, k = 1, 2, ..., q,


·
Üc
 R2 is the c-th grid point (representing the c-
th sample from a uniformly distributed latent variable
over the continuous visualisation space2 [-1, 1]2),c =
1 : C,

· (.) = (1(.),...,M(.))T,m(.) : R2  R is an or-
dered set of M non-parametric nonlinear smooth basis
functions (typically RBFs),

· the matrices
(
)
 RK
×M
,
(
Ì
l
)
 RK
×M
and
(
k
)
 RS
×M
are free parameters of the model.

The expectation (with respect to the posterior distribu-
tion over the hidden variables, given the observed data) of
the complete data log likelihood (relative likelihood [2]) is


Q =
N


n=1
C


c=1
p(
Üc|×(
n)
)


¢
K


k=1
p(h1 = k|
×(
n)
,
Üc)
log p(h1 = k|
Üc)


+
Tn


t=2
K


l=1
K


k=1
p(ht = k, ht
-1
= l|
×(
n)
,
Üc)

log p(ht = k|ht
-1
= l,
Üc)

+
Tn


t=1
K


k=1
p(ht = k|
×(
n)
,
Üc)
log p(s(
n)
t
|ht = k,
Üc)
£



Substituting the above quantities and solving stationary
equations w.r.t. all parameters, we obtain the following al-
gorithm:

2.1 Algorithm

· E step:

­ For each sequence n = 1 : N and for each grid
point
Üc,
c = 1 : C, compute
 (
c)
nkt
= p(ht = k|
×(
n)
,
Üc),
t = 1 : Tn
 (
c)
nklt
= p(ht = k,ht
-1
= l|
×(
n)
,
Üc),
t = 1 : Tn.
Both quantities are determined by the forward-
backward algorithm [11], using c = {
c
,
Ìc,
c
},
parameters of the c-th HMM.
2
uniform prior distribution of latent classes encourages full
use of the available visualization space



702
Research Track Poster

­ Compute `responsibilities' of HMMs correspond-
ing to grid points
Üc,
c = 1 : C, for sequences
×(
n)
, n = 1 : N,

rcn = p(
Üc|×(
n)
) =
p(
×n|Üc)
È
c
p(
×n|Üc
)
,

where p(
×n|Üc)
is the likelihood of the n-th se-
quence under the c-th HMM.

· M step: Solve the following non-linear equations

Q

(
)
k
=
N


n=1
C


c=1
rcn((
c)
nk1
- gk(
(
)
c
))
T
c


= 0
Q


(
Ì
l
)
k
=
N


n=1
C


c=1
rcn

Tn


t=2
(
c)
nklt
- gk(
(
Ì
l
)
c
)
Tn


t=2
(
c)
n,l,t-1
T
c

= 0, l = 1 : K

Q


(
k
)
s
=
N


n=1
C


c=1
rcn

¾
Tn


t=1s(
n)
t
=s
(
c)
nkt
- gs(
(
k
)
c
)
Tn


t=1
(
c)
n,k,t
¿




T
c
= 0, k = 1 : K,

where
(.)
i
denotes the i-th row of the parameter ma-
trix
(.)
.

2.2 Visualizing symbolic sequences
Having trained the model on a set of sequences
×(1), ×(2),
...,
×(
N)
, each sequence can now be represented by a point
in the latent space ­ the mean of the posterior distribution
over the latent space, given that sequence:


Proj(
×(
n)
) =
C


c=1
Üc
p(
Üc|×(
n)
) =
C


c=1
Üc
rcn.


This way, each sequence is mapped to one point in the 2D
visualisation space. Because dynamic models have been em-
ployed as the noise models of the generative topographic
mapping, dynamic structure of the sequences is the main
feature that determines the notion of `closeness' of sequence
representations in the latent space.

3. EXPERIMENTS
In the experiments reported below the latent space centres
Üc
were positioned on a regular 10×10 square grid (C = 100)
and there were M = 16 basis functions i. The basis func-
tions were spherical Gaussian functions of the same width
 = 1.0. The basis functions were centred on a regular
4×4 square grid, reflecting uniform distribution of the latent
classes. We account for a bias term by using an additional
constant basis function 17(
Ü)
= 1.
Free parameters of the model were randomly initialized
in the interval [-1,1]. Training consisted of repeating EM
integrations. Typically, the likelihood levelled up after 30-50
EM cycles.
3.1 Visualisation of Melodic Lines of
Bach Chorals
In this experiment we visualize a set of 100 chorales by
J.S. Bach [10]. We extracted the melodic lines ­ pitches are
represented in the space of one octave, i.e. the observation
symbol space consists of 12 different pitch values. Tempo-
ral structure of the sequences is the essential feature to be
considered when organizing the data items in any sensible
manner.
Figure 1 shows the posterior mean mapping obtained with
our model. The method has essentially discovered the nat-
ural topography of the key signatures, corroborated with
similarities of melodic motives. The upper right region con-
tains the mapping of melodic lines that utilise keys with
sharps. Coming towards the center region of the visualisa-
tion space we gradually find keys with less and less sharps to
natural keys. There are no sharps or flats in the center and
upper right regions of the plot, while the lower region of the
plot is concerned with flats. The number of flats increases
from left to right. The melodies can contain sharps and flats
other than those included in their key signature due to both
modulation and ornaments.
More interesting is to observe sub-groupings created ac-
cording to melodic motives (patterns). The three melodic
lines closest to the left boundary of the plot all contain
a very characteristic and tense expressive melodic pattern
g-f#-bb-a (that has interval of 4-), motive that can only
be found in minor keys and is in general employed rather
infrequently and with good musical reason only. Interest-
ingly, the two closest points to those three do also contain
an alleviated version of the same pattern (g-f#-g-a-bb-a),
where the interval of 4- (f#-bb) is not explicit. Moreover,
the closest point of the cluster from the upper region of the
plot has (after a key modulation) a reversed form of this
motive (g-f#-e-d#-e) that is far not as tensive as the ones
previously mentioned. The (this time descending) 4- (g-d#)
is now `resolved' (to e) within the motive.
To conclude, the benefit of the topographical representa-
tion is twofold: (1) More generally, it offers a means of or-
ganising an otherwise possibly tedious set of data structures
in an intuitively understandable compact form and (2) for
this specific example it also has the potential of providing a
way of automatically addressing the phenomenon known as
enharmony [1]. Enharmony refers to the situation when the
same physical pitch value may have different musical inter-
pretations as a function of the context (key and temporal
structure). For example, a c# and a db are both encoded
with the same number in a MIDI file, and are indeed phys-
ically the same frequency in a tempered intonation system.
This is achieved by featuring the context in which the pitches
appear. Correctly recognising enharmonincs is essential e.g.
in automatic chord annotation.

3.2 VisualisationofWebNavigationSequences
The data considered in this experiment is a subset of the
msnbc.com user navigation collection initially employed in
[4] and also used in [6]. There are 1,480 browsing sessions
totalling 119,667 page requests in this data set. The ses-
sions consists of navigation patterns by users who visited at
least 9 of the 17 page categories (frontpage, news, tech, lo-
cal, opinion, on-air, misc, weather, msn-news, health, living,
business, msn-sports, sports, summary, bbs, travel). From
the analysis in [6] using Markov chains, where the state



703
Research Track Poster

-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Latent space visualization
no sharps
no flats




sharps




flats
g-f#-bb-a




g-f#-g-a-bb-a
g-f#-e-d#-e




Figure 1: Visualization of melodic lines of 100 chorals by J.S. Bach.


space consisted of the above page categories, it transpires,
that state repetition is a common feature of all browsing
behaviours. Therefore, in this experiment, rather than con-
centrating on individual page categories, we consider a dif-
ferent alphabet. It will allow us to visually detect browsing
behaviours in terms of the user's `hunger for novelty'. To
this end, we have defined three symbols: '1': repeat the last
category request '2': return to category requested two moves
before, '3': all the other cases.
The browsing sessions are visualised in figure 2. To un-
derstand the plot better we also show the state transition
probabilities and the emission probabilities of each of the
10 × 10 = 100 hidden Markov models underlying the visu-
alization system. State transition structures are shown in
figure 3(a) as a grid of maps of K × K = 2 × 2 state tran-
sition matrices p(ht = k|ht
-1
= l,
Üc).
Figure 3(b) presents
emission probabilities in a grid of S × K = 3 × 2 matrices
p(s|k,
Üc).
Strong structure of emission probabilities is clearly visible
in figure 3(b). The second hidden state is devoted almost
exclusively to symbol 1 - `repeat the last category request'.
Indeed, as mentioned earlier, category repetition is a com-
mon feature of all browsing behaviours (see [6]). In gen-
eral, the first hidden state takes care of symbol 3 that en-
compasses all browsing behaviours different from `repeat the
last category request' and `return to category requested two
moves before'. Such patterns can potentially include non-
trivial navigation histories. Symbol 2 (`return to category
requested two moves before') is less frequent than symbols
1 and 3 and is usually explained by the first state.
While the emission structure in figure 3(b) tells us about
the marginal frequencies of symbols in sequences captured
by the underlying HMMs, it is the state-transition struc-
ture in figure 3(a) that determines the temporal correla-
tions between the symbols, i.e. the lengths of continuous
blocks of symbols generated from one state. The stronger
is the self-loop p(ht = k|ht
-1
= k,
Üc)
in state k of HMM
c, the longer blocks of symbols favoured by that state (i.e.
with higher emission probabilities p(s|k,
Üc))
are admissible.
Transition probabilities close to 1/K = 0.5 indicate a possi-
bility for symbol production arising from highly oscillating
hidden states.
Moving from top to bottom of the latent space, we ob-
serve almost independent hidden states, with little chance
of mutual transitions. Then the first state loses mass in its
self-transition in favour of the second state, which becomes
almost a trap state. Towards the bottom of the latent space,
the first state recovers its power through a band of mixing
patterns over the two hidden states. The mixing is strongest
in the vicinity of the lower-left corner of the latent square.
Top of the plot is reserved for sequences containing long
blocks of consecutive 1s and subsequences of 2s and 3s.
Right part of the top cluster contains sequences with poten-
tially long blocks of 1s and 2s. Sequences mapped in the left
part of the top cluster contain potentially long blocks of 1s
and 3s. Sequences corresponding to browsing within a single
category are represented in the center of the plot. Lower-left
corner is devoted to sequences of broad exploratory brows-
ing behaviour, where all kinds of moves are possible, without
longer persistent blocks of the same type of navigation be-
haviour. Dense cluster of sequences in the middle of the bot-
tom part of the plot represents navigation patterns contain-
ing long periods of staying within the same category (con-
secutive 1s), interleaved with long periods of possibly non-
trivial inter-topic search (consecutive 3s). Such sequences
can also be found in the left part of the top cluster. In this
sense, there is a hint of cylindrical organization of the latent
space.


4. DISCUSSION
We have presented a generative probabilistic model for
visualizing sets of discrete symbolic sequences. The model
is essentially a constrained mixture of hidden Markov mod-
els that allows us to represent non-Markovian dynamical
structures in a two-dimensional visualisation plane. For-
mally the model is a generalization of density-based visual-
ization methods previously developed for static data sets [7].
We illustrated the model on sequences representing web-log
data and chorals by J.S. Bach. We experimented with hid-
den Markov noise models with more than two hidden states,



704
Research Track Poster

-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Latent space visualization
...332232221323111111111111111...


...22222223232222322211311321111111111...




11131313311111111111111111




...33333333333333333333111111311...
......33333333333332311321113231311111111111111...
...323222132113211321132132...
...3213213213213221...
...133333113331111113131111313...




Figure 2: Visualization of web navigation sequences. For selected sequence representations we show a typical
subsequence contained in the projected sequence.




State-transition probabilities




0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9




(a)
Emission probabilities




0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9




(b)


Figure 3: State transition (a) and emission (b) probabilities of HMMs underlying the visualization system in
the web navigation experiment.




705
Research Track Poster

0
5
10
15
20
25
30
35
2.05
2.1
2.15
2.2
2.25
2.3
2.35
2.4
2.45
2.5
2.55




epoch
negative
log
likelihood
train
test




Figure 4: Evolution of negative log-likelihood per symbol measured on the training (o) and test (*) sets in
the Bach chorals experiment.


however, the structure extracted by two-state models was
already rich and interesting enough.
Our model constitutes a principled approach to visualiza-
tion of sets of symbolic sequences. Probabilistic formulation
enables the model to deal with e.g missing data, hierarchy
building, or model selection in a consistent manner. Vi-
sualization plots can be naturally interpreted by plotting
the state-transition and emission structures of the hidden
Markov noise models corresponding to local regions of the
latent space. In this sense, hidden Markov models are a
much more viable option than standard fixed-order Markov
noise models ­ going beyond the first-order Markov chain
structure can prohibitively increase the number of states.
When the task we are facing is, for example, building a
good probabilistic model for a given data set of sequences,
without any concern for data visualization, then a suit-
able approach may be to use e.g. mixtures of HMMs, with
appropriately chosen number of mixture components us-
ing a model selection technique. On the other hand, for
model based visualization of sequential data, we may use
many HMM components, but constrain them with a tight
two-dimensional grid neighborhood structure. Such a con-
strained mixture of HMM may not be able to compete with
appropriately constructed (probably smaller) unconstrained
mixture of HMM on the grounds of density modeling, but it
is suitable for data visualization and importantly, the tight
grid topology prevents constrained models with many com-
ponents (suitable for high-quality visualization) from exces-
sively overfitting the data. The issue of data explanation vs.
data prediction is covered e.g. in [12]. To evaluate tendency
of our model to overfit the training data, we split the 100
Bach chorals into training and test sets containing 80 and
20 sequences, respectively. The model is trained solely on
training sequences. Figure 4 shows the evolution of negative
log-likelihood (NLL) per symbol measured on the training
and test sets. After the 12th training epoch, the test set
NLL stops decreasing, but crucially, due to the constrained
nature of our model, it does not tend to increase at the
expense of better modeling of the training data.
Scaling is, however, an issue. The E-step complexity is
O(NCTK2). Obviously, a generative topographic formula-
tion with fixed-order Markov chains would be much cheaper,
since the noise models would not involve hidden variables.
On the other hand, as argued above, the flexibility and in-
terpretability of such formulations would be compromised.
Also, the model can be trained on a subsample of the avail-
able data and then used to visualize the whole data set.
We are currently working on speeding up computations in
our model through a variety of cheaper approximation tech-
niques.


5. REFERENCES
[1] J. Barnes, Bach's Keyboard Temperament: Internal Evidence
from the Well-Tempered Clavier, Early Music, 7(2), pp.
236-249, 1979.
[2] C. Bishop, M. Svens´en, and C. Williams, Generative
Topographic Mapping, Neural Computation, 10(1), pp.
215-235, 1998.
[3] C. Bishop, M. Svens´en, and C. Williams, Developments of the
Generative Topographic Mapping, Neurocomputing, 23, pp.
203-224, 1998.
[4] I. Cadez, D. Heckerman, C.Meek, P. Smyth and White, S,
Model-Based Clustering and Visualisation of Navigation
Patterns on a Web Site, Journal of Data Mining and
Knowledge Discovery, 7(4), pp.399-424 2003.
[5] J. Hollm´en, V. Tresp and O. Simula, A self-organizing map
algorithm for clustering probabilistic models, Proc. of the
Ninth International Conference on Artificial Neural Networks
(ICANN'99), vol.2, pp. 946­951. IEE, 1999.
[6] M. Girolami and A. Kab´an. Simplicial Mixtures of Markov
Chains: Distributed Modelling of Dynamic User Profiles.
Advances in Neural Information Processing Systems
(NIPS'03), in press.
[7] A. Kab´an and M. Girolami, A combined latent class and trait
model for the analysis and visualization of discrete data, IEEE
Transactions on Pattern Analysis and Machine Intelligence,
23(8), pp. 859 -872, 2001.
[8] A. Kab´an and M. Girolami, A Dynamic Probabilistic Model to
Visualise Topic Evolution in Text Streams. Journal of
Intelligent Information Systems, 18(2­3), pp. 107­125,.2002.
[9] J. Lampinen and E. Oja. Self-organising maps for spatial and
temporal AR models. Proc. 6 SCIA, Scand. Conf. on Image
Analysis, pp. 120­127, 1989.
[10] Merz, C. and Murphy, P. UCI repository of Machine Learning
Databases, 1998.
[11] R.L. Rabiner and B.H. Juang. An introduction to hidden
Markov models, IEEE ASSP Magazine, 3, pp. 4­16, 1986.
[12] B. Ripley, Statistical Theories of Model Fitting. In Ch. Bishop,
editor, Neural Networks and Machine Learning (NATO ASI
series III, Computer and System Sciences), pp 3­26. Springer
Verlag, 1998.
[13] P. Smyth, Mixtures of Hidden Markov Models, Advances in
Neural Information Processing 9, M. C. Mozer, M. I. Jordan
and T.Petsche (eds) Cambridge, MA: MIT Press, pp. 648­654,
1997.
[14] A. Ypma and T. Heskes, Categorisation of web pages and user
clustering with mixtures of hidden markov models. WEBKDD
(pp. 31­43), 2002.




706
Research Track Poster

