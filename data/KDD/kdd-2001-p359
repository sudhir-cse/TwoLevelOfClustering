Experimental Comparisons of Online and Batch Versions
of Bagging and Boosting

Nikunj C. Oza
Stuart Russell
oza@cs.berkeley.edu
russell@cs.berkeley.edu

ComputerScience Division
Universityof California
Berkeley, CA 94720-1776


ABSTRACT
Bagging and boosting are well-known ensemble learning meth-
ods. They combine multiple learned base models with the
aim of improving generalization performance. To date, they
have been used primarily in batch mode, i.e., they require
multiple passes through the training data. In previous work,
we presented online bagging and boosting algorithms that
only require one pass through the training data and pre-
sented experimental results on some relatively small datasets.
Through additional experiments on a variety of larger syn-
thetic and real datasets, this paper demonstrates that our
online versions perform comparably to their batch counter-
parts in terms of classification accuracy. We also demon-
strate the substantial reduction in running time we obtain
with our online algorithms because they require fewer passes
through the training data.


1.
INTRODUCTION
Traditional supervised learning algorithms generate a sin-
gle model such as a decision tree or neural network and use
it to classify examples. 1 Ensemble learning algorithms com-
bine the predictions of multiple base models, each of which
is learned using a traditional algorithm.
Bagging [3] and
Boosting [4] are well-known ensemble learning algorithms
that have been shown to be very effective in improving
generalization performance compared to the individual base
models. Theoretical analysis of boosting's performance sup-
ports these results [4].
In previous work [7], we developed online versions of these
algorithms. Online learning algorithms process each train-
ing instance once "on arrival" without the need for storage
and reprocessing, and maintain a current hypothesis that re-
flects all the training instances seen so far. Such algorithms
have advantages over typical batch algorithms in situations
where data arrive continuously. They are also useful with

1In this paper, we only deal with the classification problem.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed lbr profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, to republish, to post on servers or to redistribute to lists.
requires prior specific permission and/or a Ice.
KDD 01 San Francisco CA USA
Copyright ACM 2001 1-58113-391-x/01/08...$5.00
very large data sets on secondary storage, for which the
multiple passes required by most batch algorithms are pro-
hibitively expensive. In Sections 2 and 3, we describe our
online bagging and online boosting algorithms, respectively.
Specifically, we describe how we mirror the methods that
the batch bagging and boosting algorithms use to gener-
ate distinct base models, which are known to help ensemble
performance.
In our previous work, we also discussed our theoretical
results and some empirical comparisons of the classification
accuracies of our online algorithms and the corresponding
batch algorithms on some relatively small datasets. In Sec-
tion 4, we review the previous experiments and further ex-
plore the behavior of our online algorithms through experi-
ments with larger datasets--both synthetic and real. Con-
sistent with previous work, we run our online bagging and
boosting algorithms with lossless online algorithms for de-
cision trees and Naive Bayes classifiers--for a given training
set, a lossless online learning algorithm returns a hypoth-
esis identical to that returned by the corresponding batch
algorithm. Overall, our online bagging and boosting algo-
rithms perform comparably to their batch counterparts in
terms of classification accuracy. We also compare their run-
ning times. If the online base model learning algorithm is
not significantly slower than the corresponding batch algo-
rithm, then the bagging and online bagging algorithms do
not have a large difference in their running time in our tests.
On the other hand, our online boosting algorithm runs sig-
nificantly faster than batch boosting. For example, on our
largest dataset, batch boosting ran four times longer than
online boosting to achieve comparable classification accu-
racy.
Sometimes our online boosting algorithm significantly un-
derperforms batch boosting for a small number of training
examples. Even when the training set is large, our online
algorithm may underperform initially before finally catching
up to the batch algorithm. This characteristic is common
with online algorithms because they do not have the luxury
of viewing the training set as a whole the way batch algo-
rithms do. We experiment with "priming" online boosting
by running it in batch mode for some initial subset of the
training set and running in online mode for the remainder
of the training set. In most of the experiments that we dis-
cuss in this paper, priming leads to improved classification
performance.
We also compare the base models' error rates on the train-




359

OnlineBagging(h, d)
For each base model hm, (m 6 {1,2,... ,M}) in h,
Set k according to Poisson(1).
Do k times
h,m = Lo(hm, d)



Figure
1: Online Bagging
Algorithm:
h is the set of M
base models learned so far, d is the latest training exam-
ple to arrive, and Lo is the online base model
learning
algorithm.



ing set under batch and online boosting. This is an impor-
tant comparison because these errors axe used to calculate
the weights of the training examples supplied to the differ-
ent base models. They are also used to assign weights to
the base models for use when classifying a new example.
The closer these errors are, the more similar the training
example weights and base model weights axe in the two algo-
rithms, leading to more similar classification performances.
The base model error rates exhibit similar trends in batch
and online boosting, which partly explains the similar clas-
sification accuracies we have obtained so far.
Our experiments with synthetic datasets are meant to
compare batch and online boosting with base models hav-
ing small, medium, and large errors.
Our three synthetic
datasets are of varying difficulty for learning Naive Bayes
classifiers.
We show that boosting behaves differently on
these datasets and that online boosting mirrors these be-
haviors.


2.
ONLINE BAGGING
Given a training dataset of size N, standard batch bagging
creates M base models, each trained on a bootstrap sam-
ple of size N created by drawing random samples with re-
placement from the original training set. Each base model's
training set contains K copies of each of the original training
examples where




which is the Binomial distribution.
As N ~
co, the dis-
tribution of K tends to a Poisson(1) distribution: P(K
=

k) -~ e~l).
As discussed in [7], we can perform bag-
ging online as follows: as each training example is presented
to our algorithm, for each base model, choose the example
K ~ Poisson(1) times and update the base model accord-
ingly (see figure 1). New instances are classified the same
way in online and batch bagging--by unweighted voting of
the M base models.
Online bagging is a good approximation to batch bag-
ging to the extent that their base model learning algorithms
produce similar hypotheses when trained with similar dis-
tributions of training examples. In past work [7], we proved
that if the same original training set is supplied to the two
bagging algorithms, then the distributions over the training
sets supplied to the base models in batch and online bag-
ging converge as the size of that original training set grows
to infinity. We further proved, for some very simple learn-
ing algorithms (K-Nearest Neighbor and contingency-table
learning), that the convergence of the distributions over the
Initial conditions: A~ = 0, A~nw = 0.
OnlineBoosting(h3 Lo , d)
Set the example's "weight" *d = 1.
For each base model hm, (m E {1, 2,..., M}) in h,
Set k according to Poisson(Ad).
Do k times
hm = Lo(hm, d)
If hm(d) is the correct label,
then


em e--- ~,~+)~g.~


else
,~
e--- ,~
+ ~d

sc
8a~
Am"b,~rn



To classify new examples:
Return h(x)
= argmaxcec )"~'m:h,n(~)=~log cm "


Figure
2: Online Boosting
Algorithm:
h is the set of M
base models learned so far, d is the latest training exam-
ple to arrive, and Lo is the online base model
learning
algorithm.



training sets leads to convergence of the classification per-
formance of online bagging to that of batch bagging. We are
working on tightly characterizing the learning algorithms for
which we obtain this type of convergence.


3.
ONLINE BOOSTING
Our online boosting algorithm is designed to correspond
to the batch boosting algorithm, AdaBoost.M1 [4]. Ad-
aBoost generates a sequence of base models hi,.. ·, hM us-
ing weighted training sets such that the training examples
misclassified by model h,~_ 1 are given half the total weight
when generating model hm and the correctly classified ex-
amples are given the remaining half of the weight.
Our online boosting algorithm (Figure 2) is similar to our
online bagging algorithm except that when a base model
misclassifies a training example, the Poisson distribution pa-
rameter (A) associated with that example is increased when
presented to the next base model; otherwise it is decreased.
Just as in AdaBoost, our algorithm gives the examples re;s-
classified by one stage half the total weight in the next stage;
the correctly classified examples are given the remaining half
of the weight.
One area of concern is that, in AdaBoost, an example's
weight is adjusted based on the performance of a base model
on the entire training set while in online boosting, the weight
adjustment is based on the base model's performance only
on the examples seen earlier. To see why this may be an
issue, consider running AdaBoost and online boosting on a
training set of size 10000. In AdaBoost, the first base model
hi is generated from all 10000 examples before being tested
on, say, the tenth training example. 2 In online boosting, hi
is generated from only the first ten examples before being

2Recall that we test base model h,~ on the training exam-
ples in order to adjust their weights before using them to
generate base model hm+l.




360

Table 1: The datasets used in our experiments. For
the Soybean and Census Income datasets, we have
given the sizes of the supplied training and test sets.
For the remaining datasets, we have given the sizes
of the training and test sets in our five-fold cross-
validation runs.
Table 2:
P(Aa = OIAa+x,C) for a E {1,2,...,19}
in
Synthetic Datasets

P(Aa =O)
A~,+I =O
Aa+l=l
C = 0
0.8
0.2
C= 1
0.9
0.1


Data Set


Promoters
Balance
Soybean-Large
Breast Cancer [5]
84
500
307
559
German Credit
800
Car Evaluation
1382
Chess
2556
6499Mushroom
Nursery
10368
Connect4
54045
Synthetic-1
80000
Synthetic-2
80000
Synthetic-3
80000
Census Income
199523
Forest Covertype
464809
22
57
2
125
4
3
376
35
19
140
9
2
200
20
2
346
6
4
640
36
2
1625
22
2
2592
8
5
13512
42
3
20000
20
2
20000
20
2
20000
20
2
99762
39
2
116203
54
7




tested on the tenth example. Clearly, at the moment when
the tenth training example is being tested, we may expect
the two hl's to be very different; therefore, h2 in AdaBoost
and h2 in online boosting may be presented with different
weights for the tenth training example. This may, in turn,
lead to different weights for the tenth example when gener-
ating h3 in each algorithm, and so on. Intuitively, we want
online boosting to get a good mix of training examples so
that the base models and their normalized errors in online
boosting quickly converge to what they are in AdaBoost.
The more rapidly this convergence occurs, the more similar
the training examples' weight adjustments will be and the
more similar their performances will be. In the next section,
we demonstrate, for some of our larger datasets, that this
appears to happen.


4.
EXPERIMENTAL RESULTS
In this section, we discuss some experiments that demon-
strate the performance of our online algorithms relative to
their batch counterparts. For decision trees, we have reim-
plemented the lossless ITI online algorithm [8] in C++;
batch and online Naive Bayes algorithms are essentially iden-
tical.
We ran these experiments on Dell 6350 computers
having 600MHz Pentium III processors and 2GB of mem-
ory.

4.1
The Data
We tested our algorithms on several UCI datasets [2], two
datasets (Census Income and Forest Covertype) from the
UCI KDD archive [1], and three synthetic datasets. We give
their sizes and numbers of attributes and classes in Table 1.
All three of our synthetic datasets have two classes. The
prior probability of each class is 0.5, and every attribute ex-
cept the last one is conditionally dependent upon the class
and the next attribute. We set up the attributes this way
because the Naive Bayes model only represents the prob-
abilities of each attribute given the class, and we wanted
data that is not realizable by a single Naive Bayes classifier
so that boosting is more likely to yield improvement. The
probabilities of each attribute except the last one (A~ for
a E {1, 2,..., 19}) are as shown in Table 2.
The only difference between the three synthetic datasets
is P(A~o]C). In Synthetic-l, P(A2o -= OIC = O) = 0.495 and
P(A2o = OIC = 1) = 0.505. In Synthetic-2, these probabili-
ties are 0.1 and 0.8, while in Synthetic-3, these are 0.01 and
0.975, respectively.

4.2
General Results
Figures 3 and 4 are scatterplots comparing the errors of
the batch and online versions of bagging and boosting. The
full paper [6] contains a table with all the results. Each point
in the figures represents one dataset. To reduce clutter, we
do not show error bars in our figures, however we performed
significance tests (t-test, a = 0.05) and discuss the results
later in this paper. The batch algorithm accuracies are aver-
ages over ten runs of five-fold cross-validation for a total of
50 runs, except for the Soybean and Census Income datasets
where we performed ten runs with the supplied training and
test set. We tested our online algorithms with five random
orders of each training set generated for the batch algorithms
(order matters for online boosting, even with a lossless learn-
ing algorithm) for a total of 250 runs (50 runs on the Soybean
and Census Income datasets). We tested bagging and boost-
ing with decision trees only on some of the smaller datasets
(Promoters, Balance, Breast Cancer, Car Evaluation) be-
cause the lossless decision tree algorithm is too expensive
with larger datasets in online mode.
Bagging and online
bagging perform comparably in all our tests. Boosting and
online boosting perform comparably on all except the very
small Promoters data.set.
The largest dataset for which we ran the bagging and
boosting algorithms with decision trees was the Car Evalu-
ation dataset from the UCI Repository. Figure 5 shows the
learning curve. Batch and online bagging with decision trees
perform almost identically (and always significantly better
than a single decision tree).
AdaBoost also performs sig-
nificantly better than a single decision tree for all numbers
of examples. Online boosting struggles at first but performs
comparably to AdaBoost and significantly better than single
decision trees for the maximum number of examples. Note
that online boosting's performance steadily becomes closer
to that of AdaBoost as the number of examples grows, as
one expects from an online algorithm when compared to its
batch version.
Figure 6 shows the learning curves for the Census Income
dataset.
Batch and online boosting perform comparably
to each other and significantly outperform a single model
for all numbers of examples. On the other hand, bagging




361

and online bagging do not improve significantlyupon a sin-
gle Naive Bayes classifier. Bagging does not improve upon
Naive Bayes on any of the datasets, which we expected be-
cause of the stability of Naive Bayes [3], i.e., small changes
in the dataset do not significantlychange each Naive Bayes
classifier, so that almost all of the base models tend to vote
the same way for a given example. Online bagging always
performs comparably to batch bagging in our experiments;
therefore, online bagging also does not improve upon Naive
Bayes.

4.3
Priming the Online Boosting Algorithm
Figure 7 gives a scatterplot similar to Figure 4 except
that the onlineboosting algorithm trains in batch mode with
some initial portion of the training set and online mode with
the remainder. In primed mode, batch training was done
with the lesser of the first 20% of the dataset or the first
10000 training examples. Overall, primed online boosting
improves upon the unprimed version. Only in case of the
Promoters dataset with Naive Bayes classifiers did priming
yield significant improvement over unprimed online boost-
ing. Nevertheless, we did achieve some improvementthrough
priming in all cases except Promoters and Breast Cancer
with decision trees, and Soybean, Car Evaluation, and For-
est Covertype with Naive Bayes.
As we discussed earlier, in the Car Evaluation dataset's
learning curves (Figure 5), online boosting significantlyun-
derperforms batch boosting for all but the maximumnumber
of examples. Figure 8 displays the original batch boosting
and online boosting learning curves along with primed on-
line boosting with the first 200 training examples learned
in batch mode. Primed online boosting with decision trees
performs comparably to batch boosting for all numbers of
examples, i.e., its performance gets close to batch boosting's
performance much quicker.

4.4
Base Model Errors
Figures 9 and 10 show the average errors on the train-
ing sets of the consecutive base models in batch and online
boosting with Naive Bayes for the second synthetic dataset
and Census Income dataset, respectively (see the full pa-
per [6] for more such graphs). As mentioned earlier, the
closer these errors axe in batch and online boosting, the
closer the behavior of these two algorithms. We depict the
average errors for the maximum number of base models gen-
erated by the batch boosting algorithm. For example, on the
Census Income dataset, no run of batch boosting ever gen-
erated more than 22 base models. This happens because
if the next base model that is generated has error greater
than 0.5, then the algorithm stops. Our online boosting al-
gorithm always generates the full set of 100 base models be-
cause, during training, we do not know how the base model
errors will fluctuate; however, to classify a new example, we
only use the first L base models such that model L + 1 has
error greater than 0.5.
The base model errors of online and batch boosting are
quite similar for Synthetic-2: the first base model performs
quite well in both batch and online boosting. Both algo-
rithms then follow the pattern of having subsequent base
models perform worse, which is typical because subsequent
base models are presented with previously misclassified ex-
amples having higher weight, which makes their learning
problems more difficult. In the Census Income dataset, the
performances of the base models also follow this general
trend, although more loosely.

4.5
Running Times
Figures 11 and 12 contain the average running times of
Naive Bayes and the ensemble algorithms with Naive Bayes
base models for the Census Income dataset and Forest Cover-
type dataset, respectively. Both the onlineand batch ensem-
ble algorithms use a learning algorithm for the Naive Bayes
base models that requires one pass through the training set.
As the number of training examples increases, we expect the
rate of growth of the running time to be less for our online
ensemble algorithms than for the batch algorithms. Our on-
line algorithms require only one pass through the training
set whereas batch bagging requires one pass per base model
(to generate its training set and perform the training) and
batch boosting requires two passes per base model (once
to generate the Naive Bayes classifier and once to test the
newly-generated classifier on the training examples to up-
date their weights). However, for small numbers of training
examples, the running time may be greater for online learn-
ing because the greater number of passes required through
the data structures that represent the base models may out-
weigh the greater number of passes required through the
training set. Also, in case of base models for which online
learning takes much more time than batch learning, the to-
tal execution time for the online ensemble algorithm would
be much greater than for the batch algorithm. Additionally,
our online boosting algorithm always generates and updates
100 base models, whereas boosting often generates fewer
base models as discussed above.
The running time for online boosting is substantially less
than for batch boosting on both Census Income (20 minutes
vs. 7.1 hours on the entire dataset) and Forest Covertype
(4.3 hours vs. 18.8 hours). Relative to the boosting al-
gorithms, the running times of the bagging algorithms are
negligible.


5.
CONCLUSIONS
This paper discusses online versions of the popular bag-
ging and boosting algorithms. We have demonstrated that
they mostly perform comparably to their batch counterparts
in terms of classification accuracy. We experimented with
priming our algorithm by running an initial subset of the
training set in batch mode and then processing the remain-
ing examples online and achieved improvement by doing so.
We also demonstrated the comparable performance of on-
line boosting and batch boosting in more detail by examin-
ing the errors of the base models on the training set, which
directly affect the weights given to the training examples
in the different stages of boosting. We have also shown
that, if the online base model learning algorithm has a run-
ning time comparable to the corresponding batch algorithm,
then the runningtime of online boosting can be much lower
than batch boosting, demonstrating the significant savings
obtained by processing the training set just once.
In addition to continuingempirical work with large datasets
and different base model learning algorithms, we are work-
ing on several theoretical tasks including tightly character-
izing the class of learning algorithms for which convergence
between online and batch bagging can be proved and de-
veloping an analytical frarnework for online boosting. We
axe also investigating the case of lossy online base model




362

5O
45
40
~ 35
'~ 30
m 25
ID
~- 20
6 15
10
5
0
0
....'



.o'


,,, .o" '

O"
o

.~..'

· )3' o'~



.o'



I',',
I
I
I
I
I
I
I
I
I

5
10
15
20
25
30
35
40
45
50

Batch Bagging


Figure
3:
Test Error Rates:
Batch Bagging vs.
Online Bagging. A star indicates that the two al-
gorithms used decision tree base models while a
square indicates Naive Babes base models.
50
45
40
~ 35

8
30
0
m 25
~ 20
~ 15
10
5
0
0
o
o

%
o
·




P
~o


I
I
i
I
I
L
I
I

5
10
15
20
25
30
35
40
45

Batch Boosting
50



Figure 4: Test Error Rates:
Batch Boosting vs.
Online Boosting, A star indicates that the two al-
gorithms used decision tree base models while a
square indicates Naive Bayes base models.




1

0.95

0.9
I-
o
0.85
0
,-
0.8
O
0.75

,,
0.7

0.65

0.6
0



Figure
5:
dataset.
Car Evaluation with Decision Trees




/
~
f
e-l-
"---'m


!'//=
Decision Tree -. ,
.,'IF
Bagging
×
I/
Online Bagging
i/~
AdaBoost
o
,,'//,;'
Online Boosting
·

i/,,,
.
.
.
.
.

200
400
600
800 1000 1200 1400
Number of Training Examples


Learning curves for Car-Evaluatlon
,~
0.9

o
0.8
O
t-
O
0.7

u_ 0.6

0.5
0



Figure
6:
dataset.
Census Income with Naive Bayes


·
o
o
o
....
I




Naive Bayes -
Bagging
Online Bagging
Boosting
Online Boosting

50000
100000
150000
Number of Training Examples
200000



Learning curves for Census Income




50
45
40
~ 35

8 30
o
m 25

~ 20
~ 15
10
5
0
~o


5
o

.%
x



o

o
o


o
o




I
I
I
I

10
15 20
25
30
35

Batch Boosting
I
I

40 45
50



Figure
7:
Test Error Rates:
Batch Boosting vs.
Primed Online Boosting.
o
L)
t-
O
1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6
Car Evaluation with Decision Trees




Boosting
[
.
Online Boosting
*
/
S,
Online Boosting(200)
·



Ill
I
I
I
I
I

200
400
600
800 1000 1200 1400

Number of Training Examples


Figure
8:
Learning curves for Car Evaluation
dataset.
Online Boostlng(200) is primed online
boosting with the first 200 examples learned in
batch
mode--it
performs
comparably
to
batch
boosting.




363

1
,g
co
0.8

:e_ 0.6
I-
t-
0.4
O


0.2




Figure 9:
Dataset.
Synthetic-2 with Naive Bayes

' Boosting
Online Boosting




-x*
/ .~.....~.,...z~L.~ -',---~'~'~-~-"~--'~ ..._~_.~_--,,,---




i
¢
i
i
i
i
i
i

2
4
6
8
10
12
14
16

Base Model Number


Base
Model
Errors for Synthetic-2
1
-$
0.8

r-

:_E 0.6

t-
'-
0.4
O


0.2
u.l

0
Census Income with Naive Bayes

Boosting
'
Online Boosting




,i
i
~
i

5
10
15
20

Base Model Number


Figure 10: Base Model Errors forCensus
Income
Dataset.



30000

c
25000
O
O
v 20000

E
15000

10000

5000

0
Census Income with Naive Bayes

Naive Bayes
Bagging
×
Online Bagging
·
Boosting
o
Online Boosting
·




0
o



50000
100000
150000
200000
NumberofTraining Examples

Figure 11:
Running Times for Census Income
Dataset.
70000

60000
O
50000

® 40000
E
30000

20000
'E
10000

0
Forest Covertype with Naive Bayes

Naive Bayes' -
Bagging
×
Online Bagging
-
Boosting
o
Online Boosting
·
o
I
D

Q




am-
-
-
am



0
50 100150200250300350400450500

Number of Training Examples (x 1000)

Figure 12: Running Times for Forest Covertype
Dataset.



learning and its effect on ensemble performance.


6.
ACKNOWLEDGEMENTS
The Wisconsin Breast Cancer dataset was obtained from
the University of Wisconsin Hospitals, Madison from Dr.
William H. Wolberg. The Forest Covertype is Copyrighted
1998 by Jock A. Blackard and Colorado State University.


7.
REFERENCES

[1] S.D. Bay. The UCI KDD archive, 1999. (URL:
http://kdd.ics.uci.edu).
[2] C. Blake, E. Keogh, and C.J. Merz. UCI repository of
machine learning databases, 1999. (URL:
http: / /www.ics.uci.edu /.~mlearn/MLRepository.htmI ).
[3] L. Breiman. Bagging predictors. Machine Learning,
24(2):123-140, 1996.
[4] Yoav Freund and Robert E. Schapire. A
decision-theoretic generalization of on-line learning and
an application to boosting. Journal of Computer and
System Sciences, 55(1):119-139, 1997.
[5] O. L. Mangasarian, R. Setiono, and W. H. Wolberg.
Pattern recognition via linear programming: Theory
and application to medical diagnosis. In Thomas F.
Coleman and Yuying Li, editors, Large-Scale Numerical
Optimization, pages 22-30. SIAM Publications, 1990.
[6] Nikunj C. Oza and Stuart Russell. Experimental
comparisons of online and batch versions of bagging
and boosting. Technical report, Electrical Engineering
and Computer Science Department, University of
California, Berkeley, CA. In prepaxation.
[7] Nikunj C. Oza and Stuart Russell. Online bagging and
boosting. In Artificial Intelligence and Statistics 2001,
pages 105-112. Morgan Kanfmann, 2001.
[8] P.E. Utgoff, N.C. Berkman, and J.A. Clouse. Decision
tree induction based on efficient tree restructuring.
Machine Learning, 29(1):5-44, 1997.




364

