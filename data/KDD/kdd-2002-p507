SyMP: An Efficient Clustering Approach to Identify
Clusters of Arbitrary Shapes in Large Data Sets


Dept.
Hichem Frigui
Electrical and Computer Engineering
Universityof Memphis
hfrigui@memphis.edu


ABSTRACT
We propose a new clustering algorithm, called SyMP, which
is based on synchronization of pulse-coupled oscillators. SyMP
represents each data point by an Integrate-and-Fire oscilla-
tor and uses the relative similarity between the points to
model the interaction between the oscillators. SyMP
is ro-
bust to noise and outliers,determines the number of clusters
in an unsupervised manner, identifies clusters of arbitrary
shapes, and can handle very large data sets. The robust-
hesS of SyMP
is an intrinsicproperty of the synchronization
mechanism. To determine the optimum number of clusters,
SyMP
uses a dynamic resolution parameter.
To identify
clusters of various shapes, SyMP
models each cluster by
multiple Gaussian components. The number of components
is automatically determined using a dynamic intra-cluster
resolution parameter. Clusters with simple shapes would be
modeled by few components while clusters with more com-
plex shapes would require a larger number of components.
The scalable version of SyMP
uses an efficientincremental
approach that requires a simple pass through the data set.
The proposed clustering approach is empirically evaluated
with several synthetic and real data sets, and its perfor-
mance is compared with CURE.


Keywords
Clustering, Gaussian Mixture Models, large datasets.


1.
INTRODUCTION
Clustering is an effective technique for exploratory data
analysis, and has found applications in a wide variety of
areas. Most existing methods can be categorized into three
categories: partitioning, hierarchical, and locality-based meth-
ods. Partitional clustering generate a partition of the data
such that objects in a cluster are more similar to each other
than they are to objects in other clusters. The k-Means[17],
EM[4], and k-medoids[12] are examples of partitional meth-
ods. Partitional algorithms have the advantage of being able




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Tocopy otherwise,to
republish, to post on serversor to redistributeto lists, requires prior specific
permission and/or a fee.
SIGRDD '02 Edmonton, Alberta,Canada
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00.
to incorporate knowledge about the global shape or size of
clusters by using appropriate prototypes and distance mea-
sures in the objective function [17,4, 13]. Hierarchical clus-
tering procedures[17] yield a nested sequence of partitions
that corresponds to a graphical representation known as the
dendrogram. Hierarchical procedures can be either agglom-
erative or divisive. Locality-based methods group objects
based on local relationships. Some locality-based algorithms
are density based, while others assume a random distribu-
tion. Density-based algorithms are based on the notion that
within each cluster, the density of the points is significantly
higher than the density of points outside the cluster. Re-
cently, the advent of World Wide Web
search engines, the
problem of organizing massive multimedia databases, and
the concept of "data mining" large databases has lead to
renewal of interest in clustering and the development of new
algorithms[9]. Some of these methods are evolutionary and
introduce enhancements of traditional methods, others are
revolutionary and introduce new concepts.
In many applications, clustering is used as an intermedi-
ate compression tool. First, the data is clustered, then only
the clusters' representatives are used for the analysis part.
For instance, in image database and text document catego-
rization, clustering is used to select few representatives to
reduce the number of comparisons during the retrieval pro-
cess. In data mining applications, clustering can be used
to discover groups of data that have common
features. The
few clusters'representatives can be used to interpret the dis-
covered groups. Hierarchical and density-based methods are
not suitable for these applications. Partitional methods, on
the other hand, are ideal candidates for these applications
since they represent each cluster by a prototype as part of
the clustering process. Unfortunately, existing partitional
methods use a single prototype to represent each cluster,
and can be used to identify only clusters with simple and
identical shapes.
In this paper, we present a new algorithm, called Syn-
chronization with Multiple Prototypes (SyMP), that can ef-
ficientlyidentify clusters of various shapes in very large data
sets while operating within a limited memory (RAM) buffer.
The organization of the rest of the paper is as follows. In
section 2, we brieflyreview related work on scalable cluster-
ing algorithms. In section 3, we briefly describe SOON,
and
in section 4, we introduce SyMP, which is an extension of
SOON
that uses multiple prototypes to represent each clus-
ter. In section 5, we present the scalable SyMP. In section 6,
we evaluate the performance of SyMP
and compare it with
CURE.
Finally, section 7 contains the conclusions.




507

2.
RELATED WORK
There exist a plethora of clustering algorithms which have
been formulated differently depending on the research area
where they have originated such as statistics, pattern recog-
nition, optimization, neural networks, and graph theory.
Unfortunately, only few of these algorithms can cluster large
data sets. In Ill], Kaufman and Rousseeuw proposed CLARA,
which is based on finding k medoids. Ng and Han [14] pro-
posed a variation of CLARA called CLARANS, that makes
the search for the k medoids more efficient. The Scalable K-
Means[2, 6] and Scalable EM[3] are two other scalable parti-
tional algorithms. DBSCAN [5] is a density-based algorithm
that was developed to identify arbitrarily shaped clusters.
DBSCAN is not suitable for high dimensionaldata since the
intrinsic structure of all clusters cannot be characterized by
global density parameters. Other density-based algorithms
include DBCLASD[20], a density-based algorithm that as-
sumes that points within a cluster are uniformly distributed,
STING[19], an enhancement of DBSCAN, WaveCluster[18],
a method based on wavelets, and DENCLUE[10] which uses
influence functions to model the points density. CLIQUE[l]
is a region-grouping algorithm that can find clusters em-
bedded in subspaces of the original high dimensional data
space. BIRCH[21] and CURE[8] are two hierarchical algo-
rithms that use region grouping techniques. BIRCH first
performs a pre-clustering phase in which dense regions are
identified and represented by compact summaries. Second,
it treats each of the sub-cluster summaries as representative
points and applies an agglomerative hierarchical clustering.
CURE represents each cluster by a given number of "well-
scattered" points to capture its shape and extent. During
the merging step, the chosen scattered points are "shrunk"
towards the centroid of the cluster, and the clusters with the
closest pair of representative points are merged. The mul-
tiple representative points allow CURE to recognize non-
spherical clusters, and the shrinking process reduces the ef-
fect of outliers. To handle large databases, CURE employs
a combination of random sampling and partitioning.
In [15, 7], we introduced a model that combines synchro-
nization and clustering concepts and resulted in a clustering
approach that is efficient, robust, and can cluster very large
data sets. This approach is described in the next section.


3.
SELF-ORGANIZATION
OF OSCILLA-
TORS NETWORK
Let ~ = {YsIJ = 1,.-. , N} be a set of N objects, where
each object, yl E Rp, is characterized by p attributes. We
represent each yi by an Integrate-and-Fire (IF) oscillator
(Oi) which is characterized by a phase ¢i and a state variable
xi, given by

1
[1 + (eb
I)¢,].
(1)
xl = f(¢i) = ~ In

The function f : [0, 1] ~
[0, 1] is smooth, monotonically
increasing and satisfies f(0)=0 and f(1)=l, b is a constant
(usually=2) that measures the extent to which f is concave
down. Whenever xi reaches a threshold at xi = 1, the ith
oscillator fires and ¢i and xl are instantaneously reset to
zero, after which the cycle repeats. As a consequence, the
phases of all other oscillators Oj (j ~ i) will change by an
amount e~(¢s), i.e.,

· s(t+) = B(~s(t) + ,,(¢s)),
(2)
where B is a clipping function that is used to guarantee that
xs(t ) is confined to [0,1], and ei(¢j) is a coupling function
defined as
{
CE [1 -- d~j/50]
if d,5 _<5o

ei(¢S) ='
-C, [~2~-8o°]
if 50 < d~s < 51
(3)
-C~
otherwise

In (3), dis is the relative dissimilarity measure between Oi
and Os, CE and Cl are the maximum excitatory and in-
hibitory coupling. If dis < 60, then oscillators Oi and Oj
are considered similar, and the coupling would be excita-
tory. On the other hand, if dis > 50, then oscillators Oi
and Os are considered dissimilar, and the coupling would be
inhibitory. The parameter 60 can be regarded as a resolu-
tion parameter. 61 (typically 61=2x50) is a constant that is
used to indicate that if an oscillator is too far, then it should
simply be maximally inhibited.
The Self-Organization of Oscillators Network (SOON) is
summarized at a high level as follows:
1 Whenever a given oscillator reaches the threshold, it ex-
cites similar oscillators and inhibits dissimilar ones.
2 Initially, oscillators begin to clump together in small groups.
Within each group, oscillators fire simultaneously.
As the
system evolves, new groups keep forming, and existing groups
keep getting bigger.
3 The system will reach a stable state where the N oscillators
are organized into C stable groups. Oscillators belonging to
different groups are phase-locked, and oscillators belonging
to the same group are synchronized and form a cluster.
In [7], we integrated the Mahalanobis distance into SOON
to identify hyper-ellipsoidal clusters. The distance between
two oscillators (i.e., clusters), Ok and Ol, is computed using

2
·
t
-I
--I
dM=mzn{(ck--cl) Ct (Ck--Cl),(C~--Cl)tCk (Ck--Cl)}, (4)

where (cl¢,Ck) and (c~,C~) are the center and covariance
matrix of oscillators (or group of oscillators) Ok and Ol. If
the clusters are assumed to come from a multivariate Gaus-
sian distribution, then d~ has a X2 probability distribution
with p degrees of freedom. This desirable feature allows
us to (i) automate the choice of the resolution parameter 60;
(ii) make the neighborhood of the excitatory region dynamic
and cluster dependent. We use

5g = X~(~,P),
(5)

that is, the a tu percentile of the X~ probability distribution,
Initially, each data point is represented by one oscillator
and constitute a cluster by itself. The initial center of the
oscillator (i.e., cluster) is the point itself, and the covariance
matrix is initialized to ff ·Ip×p, where Ip×p is the identity
matrix, and ~ is a constant that depends on the dynamic
range of the data.
Whenever a group of oscillators syn-
chronizes and result in a group k, the center and covariance
matrix of this group are updated using the features of the
synchronized oscillators.
During the evolution of SOON, once a group of oscillators
synchronize they will share the same center and covariance
matrix until the algorithm reaches a stable state. Thus, a
group of synchronized oscillators can be treated as a single
oscillator. ScaleSOON[7] was designed to exploit this fact
and efficiently cluster huge data sets while operating within
a limited memory (RAM) buffer in one scan of the data set.
ScaleSOON, is outlined as follows:



508

1 Get a sample from the data, and fill memory buffer.
2 Apply SOON to the data contents in the buffer.
3 Identify synchronized groups, summarize each group by a
single oscillator, and purge synchronized oscillators.
4 If there are any points that have not been previously loaded,
go to step 1.
Let G be a group of synchronized oscillators. If the dis-
tance defined in (4) is used, then the sufficient statistics for
G are the triplet (N~, Sc, SSG), where Nc is the number of
oscillators in group G, Sa=~yec y, and SSck =~yeC yyT.
Initially, each feature vector (or oscillator) yj has its initial
sufficient statistics, i.e., (Nu~,Su~,SSu~) = (1,yj,y~y~).
After performing SOON on the data set resident in the mem-
ory buffer, each group of synchronized oscillators, G~, will
be summarized by an oscillator that has the following suffi-
cient statistics:

Nak = Z
Nu; Sak = Z
Sy; SSak = Z
SS~. (6)
YqOk
yqG k
yqGk

Next, oscillators that belong to Gk are purged from the
memory buffer. In (6), y can represent a single oscillator or
a group of oscillators that have been synchronized and sum-
marized in previous iterations. Finally, the memory buffer
is filled with feature vectors that have not been previously
loaded. The new oscillators that summarize groups of syn-
chronized oscillators will inherit the phases of the groups
they are summarizing. This is one important feature of our
incremental approach since information accumulated in the
phases will be propagated as new data gets loaded.


4.
SYNCHRONIZATION WITH MULTIPLE
PROTOTYPES
To identify clusters of arbitrary shapes, we propose mod-
eling each cluster by multiple prototypes, where each pro-
totype (center and covariance matrix) represents one Gaus-
sian component. Our choice for Gaussian components, as
opposed to multiple point representatives (as in [8]) is mo-
tivated by two factors. First, the number of prototypes re-
quired to model each cluster can he automatically deter-
mined using the X2 probability distribution of the distances
within each Gaussian component. Second, fewer Gaussian
components than spherical components (single point) are
needed to model complex shapes. The Synchronizationwith
Multiple Prototypes (SyMP) algorithm is an extension of
SOON and involves the following modifications.

4.1
Distance computation
Let ,k' = {yl,'-. ,YM} be the set of oscillators that are
being processed by SyMP at a given iteration. Each yi can
represent a single oscillator or a group of oscillators that
have been synchronized and merged in previous iterations.
We assume that each cluster yi is represented by ki Gaus-
sian components {yi1
k~
,'" , Yi }, where ki is unknown. Each
Gaussian component y~ has a center c~ and a covariance
matrix C~. The Multiple Prototype distance, d~tp, between
two clusters y~ and Ym is computed using

d~p(yt,ym) =
·
,2,
i
j,
l_<,<km~n<j<k~ aE(yt, Ym),
(7)

i
j
where d~(yt, Ym) is a function that measures the distance
between two hyper-ellipsoidal components. The distance
in (7) is similar to the single linkage hierarchical[17] and
CURE [8]. The difference is that in traditional hierarchi-
cal algorithms, the distance is computed between pairs of
data points, in CURE, the distance is computed between
few selected representative points, and in (7) the distance is
computed between few Gaussian representatives.
There are several measures that can be used to assess the
similarity between two hyper-ellipsoidalcomponents. In this
paper, we report the results using a variation of the Maha-
lanobis distance. This choice is motivated by the computa-
tional simplicity and the ability to find the optimal number
of components needed to model each cluster in an unsuper-
vised manner. We use a distance that is similar to the one
used in [7] (see equation (4)). Instead of taking the mini-
mum of the Mahalanobis distance between cluster yi and the
center of cluster yj and the Mahalanobis distance between
cluster yj and the center of cluster yi, we take a convex
combination of the two distances. We use

d~(y~,y~) =~dE,,,
+(1-
d2
8) ~....
(8)

where

d~m~n
.
z
i j
2
j
=
mzn{dM(Yl,Cra),dM(Ym,C/)},i

=
max{dM(yl,c~),
M(Ym,Ct)}, and

d2 , i
j,
MtY,, cm) = (¢~ - c~)~(C~)
-1 (c~ - c/~)
(9)

is the Mahalanobis distance between the i th Gaussian com-
ponent of cluster l and the center of the jth component of
cluster m. In all experiments we fix/~ to 0.98.

4.2
Prototype Selection and Updating
Let y = {Yl,..' ,yT} C ,~' be the subset of T oscilla-
tors that have synchronized in the current iteration. Each
yl has kl prototypes
1 .
{Yi," " ,Y~}. The T oscillators are
then merged into one cluster, say y,~, with km prototypes
computed using the prototypes of the synchronized oscilla-
tors. These km prototypes are selected to be well scattered
and to reflect the shape of the merged clusters. We use the
following procedure. The first component is selected as the
center of all synchronized components. Then, we repeatedly
select the furthest (using the distance in (8)) component
from the already selected components. This process is ter-
minated when the maximum number of allowed components
(CMAx) is reached or when the furthest component has a
distance less than 6~c. This condition ensures that all se-
lected components have a pair-wise distance larger than 6we.
We refer to 5~c as the within-cluster (or intra-cluster) reso-
lution, and we set it using the X2 probability distribution of
the distances within each component. That is,

6~c
2 o~
= x~(,oo,p).
(lo)

In other words, if the probability that the furthest compo-
nent belongs to an already selected component is greater
than c~,
then there is no need for an additional com-
ponent. This idea is similar to the Minimum Description
Length (MDL) principle I161.
We should note here that 6w~ is different from 60 used in
the coupling function in (3). 60 is needed to decide if two
clusters (as opposed to two components within a cluster) are
similar and if they should excite or inhibit each other when
one fires. We will refer to 60 as the inter-cluster resolution
or 6i,~t~,.and rewrite equation (5) as

6ijnter
20~
= Xj(,n*,~,P)
(11)



509

to make this distinction clear. We require that 5~o~<6i~t~
(or o~,¢<ai,~t,,~). When ~o~=~int,,-, then SyMP is equiva-
lent to SOON and only one component would be selected to
model any cluster.
The SyMP algorithm is summarized below


Initialize phases ¢i randomly for i = 1,... , N;
Each point is a cluster with I Gaussian components;
Repeat
Identify next oscillator to fire={Oi : ¢~=maxJY=l~bj};
Compute d~p(y,,y~) for j=l... N, j#i using (7);
Bring ¢~ to threshold, and adjust other phases:
Cj=¢j + (1 - 61) forj = 1,... ,N;
For all oscillators Oj (j y£ i) Do
Compute state variable xj = f(¢j) using (1);
Compute coupling ¢i(¢j) using (3);
Adjust state variables using (2);
Compute new phases using Cj = f-l(xj);
End For
Identify synchronized oscillators (¢j=l) and select
the optimal k components to model them;
Compute center ~4 covariance of each component;
Reset phases of all synchronized oscillators;
Until (Synchronized groups stabilize);


5.
SCALABLE SYMP
SyMP can be extended to handle very large data sets using
an incremental approach similar to the one used to extend
SOON. That is we proceed by loading only a sample from
the data set, apply SyMP, summarize and purge synchro-
nized groups. In ScaleSOON, each cluster C was modeled
by one Gausian component and its sufficient statistics are
the triplet (NG,Sc,SSc). ScaleSyMP on the other hand
uses k Gaussian components to model each cluster. Thus,
the sufficient statistics of each synchronized group would be
the set (Nb,S~,SSb,...
~
k
, NS, So, SSc).


6. EMPIRICAL EVALUATION
We evaluate the performance of SyMP with several syn-
theticMly generated datasets. We demonstrate the ability
of SyMP to identify clusters of arbitrary shapes in a com-
pletely unsupervised fashion. Since CURE was shown to
outperfom BIRCH and MST when the clusters do not have
spherical shapes [8], we only compare the performance of
SyMP to CURE. We use the basic algorithm without the
random sampling. The random sampling scales-up CURE
but introduces other parameters and can degrade the perfor-
mance when the clusters' sizes vary significantly. Moreover,
the random sampling technique is generic and can be used
by SyMP as well. For all experiments shown in this section,
we use the incremental version, i.e. ScaleSyMP. However,
to provide a fair comparison, when we compare the exe-
cution time with CURE, we use smaller data sets and the
non-scalable SyMP. Unless stated differently, the parame-
ters are set to their default values specified in Table 1. All
experiments were performed on an Ultra Sparc IIi 300 Mhz
workstation with 256 MB RAM.
The data sets used in this experiment are shown in Fig. 1.
For illustrative purpose, we use 2-dimensional data sets so
that we can display the clusters' representation. The image
containing DS1 is generated at four different resolutions and
Symbol
CE (3)
CI (3)
~inter (11)



CMAX
Buffer
~"able 1: Parameters
Meaning
Max."Excitatory coupling
Max. Inhibitory coupling
Inter-cluster resolution
X2(c~int~,p)
Within-cluster resolution
X2(Otwe,p)
Maximum No. Components
Number of samples loaded
Default value
0.1
CE/NG
c~inter = 99%
2D --~inter=9.0
c~wc= 97%
2D --~6wc=7.0
20
1000




DS2
DS4

Figure
1: Data Sets



resulted in data sets that have 10454 (DSI-a), 41843 (DS1-
b), 107343 (DSI-c), and 199003 (DSI-d) data points. These
data will be used to illustrate the scalabilityof our approach.
DSI-a will also be used to compare the execution time of
SyMP and CURE. DS2, DS3, and DS4 have 46028, 10007,
and 32850 data points respectively.

6.1
Clustering Quality
The results of clustering DS1 is shown in Fig. 2, where
SyMP identifiesthe 5 clusters correctly. Each cluster is mod-
eled by a different number of Gaussian components depend-
ing on its complexity. For instance, the spherical and ellip-
soidal clusters are modeled by a single component, while the
"W" shaped cluster required 10 components. Fig. 2(b) dis-
plays the results using CURE when the number of clusters is
specified as 5 and 10 representatives per cluster were used.
As can be seen, the partition is not correct, as the "crescent"
shaped cluster grabs part of the "W" shaped cluster. This
is because 10 representative points are not sufficient for the
"W" shaped cluster. Using 20 representative per cluster,
CURE partitions the data correctly. However, the represen-
tation is not efficient. Obviously 20 representatives are not
needed for the spherical and ellipsoidal clusters.
Fig.
3 illustrates the robustness of ScaleSyMP. Noise
points do not affect the partition or the components that
model the clusters. This is because noise points are located
in non-dense regions and they get inhibited by most of the
other points. Even if these points reach the threshold, they
will excite a few if any other oscillators and thus, do not




(a)
(b)

Figure 2: Clustering Data set DSI: (a) using SyMP,
(b) using CURE
with I0 Rap per cluster



510

Figure 3: Clustering Data set DS2 using ScaleSyMP




/
\J
(2./
......




(a)
(b)

Figure 4: Clustering DS3 and DS4 using ScaleSyMP



synchronize. In some cases, few of these noise points syn-
chronize and form tiny clusters. The results using CURE
depends on the shrinking factor (c~)even when a large num-
ber of representatives is specified. If o~is small, then CURE
becomes sensitive to noise and cannot partition the data
correctly. On the other hand, if o~is large, CURE splits the
non-spherical clusters[8].
Data set DS3 is similar to the one used in CURE to il-
lustrate the need for multiple representatives to avoid the
"chaining effect". Using MST, where every points is con-
sidered representative, would group all the data into one
cluster. On the other hand, using a single representative
(centroid), would not allow the detection of ellipsoidal clus-
ters. CURE overcomes this problem by using few represen-
tatives and shrinking them towards the centroid. When the
shrinking factor is too small (close to 0), CURE degener-
ates to the MST, and when the shrinking factor is too large
(close to 1), CURE degenerates to traditional centroid-based
algorithms (we do not show these results here as similar ex-
periments were provided in [8]). SyMP on the other hand,
is robust and does not require fixing the number of clusters
a priori. Thus, some of the "bridging points" end up in tiny
clusters while others do not synchronize at all. In any case,
these points can be easily classified as noise. The clusters
identified by ScaleSyMP are shown in Fig. 4(a).
DS4 contains 2 spherical clusters with large differences
in their sizes. Centroid-based methods would cause the big
cluster to be fragmented. Moreover, since these methods are
biased towards clusters with smaller variances, some points
that belong to the big cluster will be assigned to the small
cluster if they are close enough to its center. The multiple
representative approach adopted by CURE solves this prob-
lem to a certain degree. If enough representatives (compared
to the ratio of the sizes) are used, then the data can be par-
titioned correctly. However, since the size of the clusters is
not known a priori, CURE may not be reliable for this case.
Fig. 4(b) shows the results using ScaleSyMP.
The example provided in DS4 is also not trivial for scal-
able algorithms that are based on random sampling. This
is because, due to the size contrast of the two clusters, not
enough data points may be selected from the small cluster
at any given step to form an initial cluster. As a result, it
is possible for this type of algorithms to miss the small clus-
Table 2: CPU time

DSl(a)
139 sec.
600 see.



ter. ScaleSyMP is also based on loading a random sample
into memory. However, the few points (if any) that belong
to the small cluster will remain in the buffer if they do not
synchronize. Eventually, if these points are not noise, they
will synchronize with other points in the subsequent buffers
(the example in Fig. 5 will illustrate this point).
Table 2 compares the CPU time of SyMP and CURE with
20 representatives. DS1 (a) was used in this experiment.
As can be Seen, SyMP is much more efficient than CURE.
One reason for the disparity in performance is that CURE
maintains 20 representatives for each cluster, and comput-
ing the distances for all pairs of representatives can be very
expensive. SyMP on the other hand, starts with one rep-
resentative per cluster and representatives are added only
when needed.

6.2
Scalability
In [7], the scalability of ScaleSOON has been investigated.
It has been shown that ScaleSOON scales linearly with re-
spect to the number of data points and the number of at-
tributes. It has also been shown that SealeSOON is more
efficient with smaller buffer size. However, the buffer should
include enough samples to maintain a diverse population
and allow smooth agglomeration of the clusters. The be-
havior of ScaleSyMP is almost identical to ScaleSOON, and
due to lack of space, we cannot show all the results in this
paper. We only illustrate two experiments.
ScaleSyMP is based on loading and processing a random
sample at a time. However, unlike random sampling, it is
not sensitive to the size and distribution of the random sam-
ple. If at a given step, only few scattered points from a given
cluster are sampled, these points would remain in the buffer
if they do not synchronize. Eventually, if these points are
not noise, they will synchronize with other points when sub-
sequent samples get loaded. This behavior is illustrated in
Fig. 5 where 2 intermediate steps of ScaleSyMP are shown.
In this experiment, for illustrative purposes, DSi-b is sam-
pled in a raster order. Fig. 5(a) shows the results after
processing 10 samples (10×1000 points). The processed and
unprocessed data are shown with different gray values. Since
the loaded samples of the "W" shaped cluster are scattered,
these samples are assigned to 3 independent clusters. Af-
ter loading and processing 20 more samples, these clusters
are still disconnected. However, they have expanded and
used more Gaussian components. Other clusters have also
expanded, and new clusters were created. These results are
shown in Fig. 5(b). Eventually, after loading all points, the
sub-clusters of the "W" shaped cluster get merged and be-
come components of the same cluster. The final result is
the same as the one shown in Fig. 2.
In the second scalability experiment, we used the different
sizes of DS1, and recorded the CPU time for each data set.
The results are shown in Fig. 6, where ScaleSyMP scales
linearly with respect to the number of data points.


7.
CONCLUSION
We introduced a new clustering algorithm that can iden-




511

(a)
(b)

Figure 5: Intermediate steps of ScaleSyMP. Results
after processing (a)10, (b)30 samples




Figure 6: Scalability of ScaleSyMP


tify clusters of arbitrary shapes in a completely unsuper-
vised fashion. Unlike CURE, which uses a specified number
of point representatives to model each cluster, SyMP uses
Gaussian components and requires fewer representatives to
model complex clusters.
Moreover, SyMP can determine
the optimal number of components to model each cluster
using a dynamic within-cluster resolution that depends on
the X2 probability distribution of each component. As a re-
sult, simple clusters would be modeled by few components
while more complex clusters would be modeled by a larger
number of components. Another advantage of SyMP is that
it does not require a priori knowledge about the number
of clusters present in the data. This number is determined
using an adaptive inter-cluster resolution that depends on
the distribution of the components that model each clus-
ter. The robustness of SyMP is an intrinsic property of the
synchronization mechanism. Noise points will either syn-
chronize and form tiny clusters or will not synchronize at all.
Our proposed approach is very efficient even when compared
with clusters that can detect only spherical or ellipsoidal
clusters and require the specification of the number of clus-
ters. To cluster very large data set with a limited memory
buffer, we proposed the ScaleSyMP algorithm. ScaleSyMP
is based on loading only a sample of the data set at a time,
and using SyMP to cluster it. Our incremental approach
is more efficient and less sensitive than random sampling.
This is because the random samples are not treated inde-
pendently. Information that is accumulated in the phases of
the oscillators is propagated as new data gets loaded.

Acknowledgments
This material is based upon work supported by the National
Science Foundation under Grant No. IIS-0133415.

8.
REFERENCES
[1] R. Agrawal, J. Gehrke, D. Gunopulos, and
P. Raghavan. Automatic subspace clustering of high
dimensional data for data mining aplications. In Proc.
of the ACM SIGMOD, 1999.
[2] P. Bradley, U. Fayyad, and C. Reina. Scaling
clustering algorithms to large databases. In Proc. of
the ACM SIGKDD, 1998.
[3] P. Bradley, U. Fayyad, and C. Reina. Scaling EM
clustering to large databases. Technical Report
MSR-TR-98-35, Microsoft Research, 1998.
[4] A. P. Dempster, N. M. Laird, and D. B. Rubin.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society
Series B, 39(1):1-38, 1977.
[5] M. Ester, 1t. P. Kriegel, J. Sander, and X. Xu. A
density-ba~ed algorithm for discovering clusters in
large spatial databases with noise. In Proc. of the
ACM SIGKDD, 1996.
[6] F. Farnstrom, 3. Lewis, and C. Elkan. Scalability for
clustering .algorithms revisited. SIGKDD Explorations,
2(1):51-57, 2000.
[7] H. Frigui and M. Rhouma. A synchronization based
algorithm for discovering ellipsoidal clusters in large
datasets. In Proc. IEEE Conf. Data Mining, 2001.
[8] S. Guha, R. Rastogi, and K. Shim. CURE: An
efficient clustering algorithm for large data databases.
In Proc. of the ACM SIGMOD, 1998.
[9] A. Hinneburg and D. Keim. Clustering techniques for
large data sets: From the past to the future. In
Tutorial Notes for ACM SIGKDD, 1998.
[10] A. Hinneburg and D. Keim. An efficient approach to
clustering in large multimedia databases with noise. In
Proc. of the ACM SIGKDD, 1998.
[11] L. Kaufman and P. Rousseeuw. Finding Groups in
Data. John Wiley and Sons, 1989.
[12] L. Kanfman and P. J. Rousseeuw. Finding Groups in
Data: An Introduction to Cluster Analysis. Addison
Wesley, NEW York, 1990.
[13] R. Krishnapuram, H. Frigui, and O. Nasraoui. Fuzzy
and possibilistic shell clustering algorithms and their
application to boundary detection and surface
approximation I. IEEE Trans. FS, 3(1):29-43, 1995.
[14] R. T. Ng and J. Ham Efficient and effective clustering
methods tbr spatial data mining. In Proc. of the
VLDB, 1994.
[15] M. Rhouma and H. Frigui. Self-organization of a
population of coupled oscillators with application to
clustering. IEEE Trans. PAMI, 23(2), 2001.
[16] J. Rissanen. Stochastic Complexity in Statistical
Inquiry. World Scientific, 1989.
[17] R.O.Duda and P. E. Hart. Pattern Classification and
Scene Analysis. John Wiley and Sons, 1973.
[18] G. Sheikholeslami, S. Chatterjee, and A. Zhang.
Wavecluster: A multi-resolution clustering approach
for very large spatial databases. In Proc. VLDB, 1998.
[19] W. Wei, J. Yang, and R. Muntz. Sting: A statistical
infromation grid approach to spatial data mining. In
Proc. of the VLDB, Athens, Greece, 1997.
[20] X. Xiaowei, M. Ester, H. Kriegel, and J. Sander. A
distribution-based clustering algorithm for mining in
large spatial databases. In Proe. of the 14th Int. Conf.
on Data .Engineering, 1998.
[21] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH:
An efficient data clustering method for very large
databases. In Proc. of the ACM SIGMOD, 1996.




512

