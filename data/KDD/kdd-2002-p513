Scaling multi-class Support Vector Machines
using inter-class confusion

Shantanu Godbole
liT Bombay
shantanu @it.iitb.ac.in
Sunita Sarawagi
liT Bombay
sunita@ it.iitb.ac.in
Soumen Chakrabarti
liT Bombay
soumen @cse.iitb.ac.in



ABSTRACT
Support vector machines (SVMs) excel at two-class discrim-
inative learning problems. They often outperform genera-
tive classifiers, especially those that use inaccurate genera-
tive models, such as the naive Bayes (NB) classifier. On the
other hand, generative classifiers have no trouble in handling
an arbitrary number of classes efficiently, and NB classifiers
train much faster than SVMs owing to their extreme sim-
plicity. In contrast, SVMs handle multi-class problems by
learning redundant yes/no (one-vs-others) classifiers for each
class, further worsening the performance gap. We propose
a new technique for multi-way classification which exploits
the accuracy of SVMs and the speed of NB classifiers. We
first use a NB classifier to quickly compute a confusion ma-
trix, which is used to reduce the number and complexity of
the two-class SVMs that are built in the second stage. Dur-
ing testing, we first get the prediction of a NB classifier and
use that to selectively apply only a subset of the two-class
SVMs. On standard benchmarks, our algorithm is 3 to 6
times faster than SVMs and yet matches or even exceeds
their accuracy.


1.
INTRODUCTION
Support Vector Machines (SVMs) [14] are a kind of dis-
criminative classifier which have shown superb performance
for classifying text and other data. They are accurate, ro-
bust, and quick to apply to test instances. Inducing a lin-
ear SVM over training data {(:~,yi),i = 1,...,n},xi
E
R m, yi E {-1, 1} involves estimating a vector ~ and a scalar
b to maximize the distance of any training point from the
hyperplane defined by ~. ~ + b; this can be written as:

Minimize
~ w'w
(= ~ I I{ )
(1)

subject to
yi(~.~
+b) > 1
Vi=l ..... n

The distance of any training point from the optimized hy-
perplane (called the margin) will be at least 1/llwll. Notice
that there can be only two class labels. This holds for non-
linear SVMs as well.
The elegant theory behind the use of large-margin hyper-


Permission to make digital or hard copies of all or part of this work for
personal or classroomuse is granted without fee providedthat copies are
not made or distributedfor profitor commercialadvantageand that copies
bear this noticeand the full citationonthe firstpage. To copyotherwise,to
republish,to post onserversor to redistributeto lists,requirespriorspecific
permissionand/ora fee.
SIGKDD '02 Edmonton,Alberta,Canada
Copyright2002ACM 1-58113-567-X/02/0007...$5.00.
planes to separate two classes cannot be extended easily to
separate N mutually exclusive classes. A number of meth-
ods have been proposed for reducing a multi-class problem
to a collection of two-class problems and combining their
predictions in various ways [1, 12, 7]. The most popular
amongst these is the "one-vs-others" approach where, for
each of the N classes, we construct a one-vs-others SVM
which makes a yes/no judgment for that class alone. Given
a test instance, some of these N SVMs will say yes, and the
rest will say no. The winning SVM is the one which says
yes, and whose decision hyperplane is farthest from the test
instance amongst all competing SVMs.
The one-vs-others technique requires each of the N clas-
sifters to be built on the entire training data, causing each
document to be processed N times. Each test instance has
to be evaluated w.r.t, each SVM as well. Apart from the ef-
ficiency issue, it is unclear that a comparison of the discrim-
inant functions of different classification problems is mean-
ingful in any way, although this technique seems to work
reasonably well in practice.
Another technique is to construct SVMs between all pos-
sible pairs of classes [10]. During testing, each of the N(N-1)
2
classifiers votes for one class. The winning class is the one
with the largest number of accumulated votes. In this tech-
nique, even though the number of training instances per
classifier is limited, the number of classifiers is quadratic
in the number of classes and each document gets processed
(N-
1) times. A number of other proposed methods [5, 3,
1] fall somewhere in between these two methods. We will
discuss these in Section 4. The one-vs-others method is the
most widely used in practice and found to offer high accu-
racy [7]. We will use this approach when using multi-class
SVMs in our experiments.
Regardless of specific details, these ad-hoc techniques be-
come impractical with a large number of classes, especially
because SVMs, while accurate and quick to apply, are not
the fastest learners to train: on an n-instance problem the
time taken by recent, clever implementations ranges from
n 1'7 to n 2"1 [8, 4].
Together, these factors make it quite
difficult to scale up SVM-based systems to large Web direc-
tories like the Open Directory Project (http://dmoz. org/)
and Yahoo! (http://~nn~.yahoo.com/), which have tens of
thousands of classes and millions of documents.
Generative classifiers, in contrast, are essentially indepen-
dent of the number of classes as far as training time is con-
cerned. A popular generative classifier for text data is the
naive Bayes (NB) classifier.
NB classifiers trivially scale
with the number of classes as they process each document




513

only once independent of the number of classes. Also, NB
classifiers train much faster than SVMs owing to their ex-
treme simplicity. On the other hand, in terms of accuracy,
the linear SVM has decisively outperformed the NB clas-
sifter owing to the latter's high bias in assuming attribute
independence.
In this paper, we present a method that tries to achieve
the best of both worlds: scalability of NB classifiers w.r.t.
number of classes and accuracy of SVMs. In the first stage
we use the fast multi-class NB classifier to compute a con-
fusion matrix, which is used to reduce the number and com-
plexity of two-class SVMs that are built in the second stage
using the one-vs-others approach. During testing, we first
get the prediction of a NB classifier and use that to selec-
tively apply only a subset of the two-class SVMs, as indi-
cated by the confusion matrix. On standard benchmarks,
our algorithm is 3 to 6 times faster than multi-class SVMs,
and has superior scalability in terms of memory require-
ments and training set size. In terms of accuracy, the method
is 3% better than NB classifiers and comparable or superior
to SVMs.
Our proposed algorithm is very simple to understand and
requires negligible coding. It can be of substantial utility
while dealing with very large classifiers like those that would
be required for Web directories.


2.
OUR APPROACH
The genesis of our approach lies in the class relationships
derived from a confusion matrix, easily generated from a
fast classifier like NB. This can be obtained using a held-out
validation dataset.
d~$~,~ ..........................i'T
2~3 4 G e r e g,~o tl 12 13/4 Is le 1; 1~ 1~ 20
zlt.tMism
251i6il
3 32 1
1 2
1 2
00iO
0~0
0
0
0
0
0
~.~o~..~
..........19i77 0
I 6
0
0
I:0
0
0i0
0i!
2
2 0
0
0
1
i2~
1 O ~ 2
O f
f
9
0
t*.~J¢$:m~ ............... 2'T'0............................................................
:0:..~:::,.0
0!O
1
0
1 33 0
............................................ i3 2~3 24 3 0 t7 3 0
.....................
Ig!~,.~.'.~..............."'~'~'~"""~ ~'li~'0""'"'i'""O"
"6
0""0
0 i0 0
i0
2
0
1 IS 0
~.~.
O o ! 0 3 T~o
o
2 iTSfi"T
O
'2 o
..~p:~:x
...................
li
i2
1 0
1 248 0
2
2 ~i5
3
I
I
2
1
1 0
0
Id,:.#lJc$.mi~sl
0"T3'i
0
00"'~5" 0
'i 0i ~ " 0~ 0 ? 0 0
0 -I i
O
mcr/~t............. I 0
!.2 .L Q..3
0....~.0 3.i0 .!...i0 O
! 0
0
3 0
~.~.~:,k.e
O O:O
1 O 4
I
O
~6 1i2
O!J
2
1 0
0
1 O
.:~n.~:~i ....................o i
2
1
! 0 10 I
0 243123 7
33
3
0
0
0
0
~.~.p~.~-,~
o io
0"C 2I
0 ............~ S;2ei~q2i3 "i~
'o~ o
c~,rqE:~.~:~T~:h~.!e.....
I
1 0
2
1 0
0
7 i10 26018i9
1 0
O 0
0
..0. oi
. . . . . . . . . . . . . . . . . . . .
m.$m~nie$
1i0
I 0
I 5
2
O
O 7i,3
i32~
$
3
0
I O
0

~.~
o: i~ s ..................................0
I
1 O 0
1 20
272
2750
1
1
i


tllk#itics.~m.
Oi .......................0
0
9
'5 i
00
.......... 0
0"!0 !0
i~O
0
i 281 0
~:sp~:l'~/ ...............
0!1
0
0
0
1 0
0
0
2
010
1
1 O 0
0
3 0 291


Figure 1: 20-newsgroups
confusion matrix

For example, in Figure 1 we show an example of a con-
fusion matrix built on the 20-newsgroup dataset (details in
Section 3.1). The rows show actual classes and the columns
show predicted classes.
The matrix clearly shows that different classes have dif-
ferent degrees of confusion with other classes. Some classes,
like rec.sport.hockey, are well separated from the rest,
whereas others like comp. oe. ms-vindovs, m~sc are easily con-
fused with others. The mis-classifications of a class are usu-
ally limited to a small subset of classes. In fact, in most
cases, the rows and columns of a matrix can be rearranged
manually as shown in Figure 2 so as to'reveal clusters of
classes that confuse with each other. These appear as blocks
along the diagonal of the confusion matrix.
Not surpris-
ingly, in many cases, these clusters are formed of classes
whose names can be immediately recognized as forming nat-
ural hierarchies. The confusion matrix provides a domain-
independent method of deriving this relationship.
For automating this re-organization of classes into clusters
of similar classes, we use the technique used in [6] to auto-
matically generate topic hierarchies from a given fiat set of
classes. Each class is represented by a row in the confusion
matrix. For each class, it's respective row is converted to
a normalized N dimensional vector that denotes how much
the class confuses with other classes. We then use a distance
measure like the Euclidean L, or the KL-distance measure
to compute distance between the classes. These distances
are used to cluster the classes using a hierarchical agglomer-
ative clustering (HAC) algorithm. The output of HAC is a
dendrogram that we analyze to determine the clusters that
provide the maximum inter-class separation. The dendro-
gram is scanned bottom-up to find the distances at which
successive clusters get merged. We clip the dendrogram at
the point where the cluster merge distances begin increasing
sharply. The number of clusters left after clipping form the
clusters of our two-level hierarchy. For the 20-newsgroups
dataset, this method gives clusters very similar to those in
Figure 2.
Classname
t
2
5 lg
8
$
1t 17 '/2 i '/3 i 7 15 14 6 10 18i 20 g i 16 3

i~,,,~h~:.:
~'
O'O"OiO
O" fi"O'O"O
i~ 0i"2 '0
talk.re~.rnisc
230
..........................i0iO:~ ( 0"tiO '0"I '0":."(J:'22
t~.~s~
..... ~
o ~
o
o i ~ io o

t~:~,n$.~ ......
,~
o o o iO
o o o a o 1 o 3~
c~,gmeh~....................11 0 i~
o
~
~
oo"b
0T2iJ2



co~.~nd~.,........... ..!...i.! ............
2 ~ oi2
2 2
~.~
i~lO~
o ~ o 2 ~!!!~ff?~i~if~
~ifio~
~e~
..........ii iio .....0 o 'o ~
:~ 0 To"
~ ~
i
,~.n~.
,~ o n
2 o 3 ~ o+2+~
o 4 s
o+1
o




~.vypt
~
1~o
3
o
2
3 o
o
1
3 o
o
o
o
o
o

~ci:~,
.
~ja
........~.......9
0 o °o "2i~ ............i"o

Figure 2: 20-newsgroups re-organized confusion ma-
trix.

2.1
Hierarchical
Approach

We propose to exploit the clustering of classes to prune
the number and complexity of two-class classifiers needed
for multi-class SVMs. An obvious approach is to arrange
the clusters in a two-level tree hierarchy and train a classi-
fier at each internal node. If we restrict to NB classifiers,
Mitchell [11] has shown that if the same feature space were
used for all the classifiers and no smoothing is done, the ac-
curacy would be the
same
as
that of a fiat classifier. In prac-
tice, each classifier has to deal with a more easily separable
problem, and can use a independently optimized feature set;
this should lead to slight improvements in accuracy, apart
from the gain in training and testing speed. We propose
to use a combination of NB and SVM classifiers at the two
levels.
We first build a top-level classifier to discriminate amongst
the top-level clusters of labels, called the Level 1 (L1) clas-
sifter. This top-level classifier could be either a NB or SVM
classifier. Even with SVM, the training time will be smaller
since the number of classes is reduced, although each two-




514

Method
Accuracy (in %)
MCNB
MCSVM
NB-L1
SVM-L1
NB-L2 with NB-L1
NB-L2 with Perfect-L1
SVM-L2 with NB-L1
SVM-L2 with Perfect-L1
85.27
89.66
93.56
95.39
89.01
88.41
92.04
91.65

Table 1: Level-wise comparisons for 20newsgroups
data

class SVM will still need all documents.
At the second-
level (L2) we build multi-class SVMs within each cluster of
classes. The total number of SVMs at the second level will
be close to N but the number of classes per SVM is signifi-
cantly reduced.
Each L2 classifier can concentrate on a smaller set of
classes that confuse with each other. For a generative classi-
fier like NB, we expect this to result in better feature selec-
tion and thus enable finer distinctions amongst the confusing
classes [2]. For SVMs, the spread of the negative ('others')
class is reduced, which we expect will make separability eas-
ier and/or better.

2.1.1
Evaluation of the hierarchical approach
We evaluate the accuracy and training time for solving the
multi-class problem using a two-level hierarchy. We com-
pare four methods: Flat multi-class NB classifiers (MCNB),
flat multi-class SVMs (MCSVM) using the one-vs-others ap-
proach, NB classifiers at both the levels (L1 and L2) of
the hierarchy (Hier-NB) and NB classifier at L1 followed
by SVMs at L2 (Hier-SVM).


~t
89.66
"


"I u~
IIIIIIIIIII


:I, IIIllllllll@
IiI
~
141.1I
i ~
1
"
~,1
"

l R
'I
IMII
ra




~iB
~
I~'~B
Hi~'-~


Figure 3:
Accuracy of
Figure 4:
Training time
different
hierarchical
of different
hierarchical
methods for 20NG
methods for 20NG
We notice from Figures 3 and 4 that while the train-
ing times for Hier-NB and Hier-SVM are lower than for
MCSVM, the accuracy is also reduced.
The accuracy for
Hier-NB is even lower than that of MCNB. Hier-SVM has
lower accuracy than MCSVM though it is slightly better
than MCNB. Also, training time of Hier-SVM is less than
half that of MCSVM.
We show in Table 1, a comparison of accuracy of the two
levels separately for both NB and SVM classifiers. For L2,
we show two kinds of accuracy, first on documents that get
correctly classified to their correct group .by L1, and second
the absolute accuracy of L2 assuming a perfect L1 classifier.
As expected, all the L1 and L2 classifiers are individually
more accurate than the original flat classifier, as noted in [2]
and as we expected in Section 2.1. Even though NB-L2 has
an accuracy of 89.01%, combining with the NB-L1 accuracy
of 93.56% leaves us with a resultant accuracy of Hier-NB of
83.28%. Although NB-L1 and NB-L2 are both individually
better than MCNB (85.27%), we see that compounding of
classification errors leaves Hier-NB worse off than MCNB.
Similarly, SVM-L2 with a NB classifier at L1 (Hier-SVM)
has an accuracy of 92.04%. The accuracy of Hier-SVM still
drops down to 86.12% which is slightly better than MCNB,
but is worse than the MCSVM accuracy of 89.66%. If we
replace NB-L1 with SVM-L1 having 95.39% accuracy, the
overall accuracy for both Hier-NB and Hier-SVM improves
slightly, but the training time gets worse.
There is no conclusive comparison in [2, 9] which deci-
sively states whether a hierarchical classification scheme is
better than a flat one for NB classifiers.
These previous
studies have either restricted the number of features at each
node in the hierarchy or have tried to equalize the num-
ber across the flat and hierarchical schemes. It is not clear
whether by attempting to equalize the number of features,
one of the classification schemes is getting compromised.
The main reason for the low accuracy of the hierarchical
approaches is the compounding of errors at the two levels.
Even though the accuracy at both levels is higher than that
of a flat classifier, the product of their accuracies falls short
of the accuracy of a flat classifier. Increasing the levels be-
yond two is expected to worsen this compounding effect.
This led us to design a new algorithm GraphSVM, that at-
tempts to ensure that the first-stage classification will make
the overall process fast, but inaccuracy of the first-stage clas-
sifter will not jeopardize overall accuracy.

2.2 The GraphSVM algorithm
In this algorithm, we represent classconfusion in a more
general way using a graph, which may connect a classwith
any other class,instead of restrictingconfusion to a hierar-
chy of disjointgroups as in the previous approach.
As in the previous approach, we startwith the confusion
matrix obtained by a fastmulti-classNB classifierMi. For
each class i, we find the set of classes F(i) such that more
than a threshold (t) percentage of documents from each class
in F(i) gets mis-classified as class i. For example, for the
confusion matrix in Figure 1, we find that for the class alt.
atheism and with a threshold of 3%, F(alz.atheism) =
{talk. religion, misc, soc. religion, christian}.
Next, for each node i with non-empty F(i),we train a
multi-classclassifierM2 (i)to distinguishamongst the classes
in {i}UF(i). These classifiersare constructed using a more
accurate and possibly slower method likeSVMs.
During testing,we firstclassifya document d using Mi.
If the predicted class for d is i, we feed it to M2(i), if any,
and get a refined prediction j.
In the above example, when a test instance is predicted as
alc. atheism by Mi, we get the prediction refined by a one-
vs-others SVM, M2(i), between the classes air,atheism,
talk.religion.mist and see.religion.christian.
The
prediction of M2(i) is returned as the finalanswer for the
test instance.

2.2.1
Discussion of the algorithm
GraphSVM
partitions a classificationtask between NB
and SVMs such that SVMs are only invoked on small subsets
of classesthat get mis-classifiedby the NB classifiers.




515

We can claim that GraphSVM will not be worse than
MCNB provided that the training data is representative of
the test data. GraphSVM can choose not to use any sec-
ond stage SVM refinements in the rare case where the SVM
classifier is found to be worse than a NB classifier on a
validation dataset.
Compared to MCSVM, the main rea-
son GraphSVM may be worse is high positive values of the
threshold t. We can always decrease the threshold, at the
expense of increasing the training time, to match MCSVM
accuracy as shown in Section 3.5. In most cases, we expect
the strengths of NB and SVMs to combine to give perfor-
mance better than both individually.
The main property that we rely on for accuracy is that the
set of classes that a class confuses with, using a NB classifier,
should be the same for the training and test set. The relative
distribution of the confusion matrix is not required to remain
unchanged, provided entries that were previously below the
threshold t do not suddenly increase beyond it.
The benefit of GraphSVM will be greatest when the mis-
classifications of the first stage are spread across a small
number of classes. The worst case is when mis-classifications
of a class are uniformly distributed over many classes. In
this case, the algorithm will reduce to multi-class SVMs. In
most practical datasets that is rarely the case.


3.
EXPERIMENTAL EVALUATION
Text classification involves dealing with tens of thousands
of features and documents.
NB classifiers have proved to
be very fast and scalable and show a moderate accuracy,
whereas the SVM variants are the most accurate, but the
slowest to train.
Our aim is to scale up these expensive
SVMs using the proposed GraphSVM algorithm. Hence we
compare GraphSVM only with the fast NB classifiers and
the accurate SVM classifiers.
In this section we present an evaluation of the proposed
GraphSVM algorithm. We compare this algorithm to multi-
class NB (MCNB), multi-class SVMs (MCSVM) and the
hierarchical approach with a NB classifier at L1 and SVMs
at L2 (Hier-SVM). We compare the algorithm on accuracy,
training time and scalability w.r.t, size of the training set.

3.1
Datasets
20-newsgroups The 20-newsgroups (20NG) dataset 1 is
a collection of 18, 828 news wire articles from 20 Usenet
groups. The older version of the dataset had 1,000 articles
in each group, but the newer version we use is without du-
plicates and with most headers removed. This dataset is not
pre-processed into training and testing sets. We randomly
chose 70% of the documents for training and the remain-
ing 30% for testing. The corpus contained around 75,000
words. Features were selected using mutual information. All
words were stemmed using a Porter stemmer [13], all HTML
tags were skipped and all header fields except subject and
organization of the posted article were ignored.
Reuters-21578 The Reuters-21578 Text Categorization
Test collection2 is a standard text categorization bench-
mark. Itcontains 135 classes. We chose only those 60 classes
which have more than 10 training documents. This resulted
in 8819 training documents and 1887 testdocuments. XML

thttp :I/www. ai.mit. edu/'j renn±e/20_newsgroups/
2http://www.daviddlewis,com/resources/
test collect±ons/reuters21578/
tags were ignored and the words were stemmed with a Porter
stemmer [13]. We used the standard Mod-Apte train-test
split.
Training instances with multiple class assignments
were considered once in every assigned class. We ignored
multi-class test instances because we wanted to see if confu-
sion amongst classes can be resolved by using the proposed
algorithm.
All experiments were performed on a 1.4GHz P4 machine
with 512MB RAM, running Linux. Rainbow3 was used for
feature selection, text processing and experiments involv-
ing NB classifiers. SVMLight4 was used for all experiments
involving SVMs.

3.2
Overall comparison
In Figures 5 and 6 we show the accuracy and training time
for the four methods on the 20NG and Reuters datasets.




i ............................................................................................................N:INi·
ee.o6
so 72




I ~
~¢"
~"P~
o~
II ~
~
* ~ /


Figure 5: Accuracy comparison for all methods for
both datasets




.--
1 O0
7S.S




o
m



L
,,ON.
I
L--...,.r.
I

Figure 6: 'I~aining time comparison for all methods
for both datasets

In Figure 5~ for the 20NG dataset, we observe that the ac-
curacy of GraphSVM at 88.72% is slightly smaller than the
MCSVM accuracy of 89.66%. The accuracy of GraphSVM
is higher than the previous Hier-SVM accuracy of 86.12%.
For the Reuters dataset, GraphSVM has the highest accu-
racy of 94.86% while MCSVM and MCNB have accuracies
of 92.86% and 92.47% respectively.
From Figure 6 we observe that GraphSVM has the fastest
training time among the approaches involving SVMs, and
for both the datasets, is more than a factor of 3 faster than
MCSVM. As expected, MCSVM is the slowest to train.

3.3
Scalability with number of classes
We evaluated the training time of the different approaches
with increasing number of classes. We started with 5 ran-
domly picked classes, and added 5 randomly picked classes
at a time for both the datasets. In Figures 7 and 8 we ob-
serve that the gap between the training time of GraphSVM
and MCSVM increases as the number of classes is increased.

3http ://www. cs. cmu. edu/-mccallum/bov/
4http://svmlight.joach±ms,org/




516

250

"':" 20O

180

100
I ....
MCNB
1
/m
MCSVM
|
/




0
--
.
,
,

S
10
16
2O


Figure 7: Training time vs.
Number
of classes for
20NG

111o
............................................................................................................................................................




iS
10
16
20
26
30
31;
40
~
60
66
IlO


Figure 8: Training time vs. Number of classes for
Reuters

Figures 9 and 10 show that in all cases GraphSVM con-
tinues to maintain the high accuracy vis-a-vis MCSVM and
MCNB. For the 20NG dataset (Figure 9), GraphSVM main-
tains an accuracy of within 1% of MCSVM and for the
Reuters dataset (Figure 10) GraphSVM is on an average, 3%
better than MCSVM. We notice a large dip in Figure 10 for
MCNB and MCSVM. The Reuters dataset is highly skewed
in the distribution of instances per class. Figure 10 addition-
ally shows the total number of test instances over which the
micro-averaged accuracy values are reported. For 25 classes
there are only 166 test instances.
The 7% difference be~
tween GraphSVM and MCNB and MCSVM is due to only
11 additional instances correctly classified by GraphSVM.
Since these 25 classes are thinly populated, most of the mis~
classifications seen in the confusion matrix are larger than
the threshold and contribute to edges in the GraphSVM
algorithm.
These mis-classifications are corrected by the
more focused SVMs in the GraphSVM method. The graph
smoothens out after 30 classes when there are a larger num-
ber of instances and the accuracy of GraphSVM is consis-
tently better.




~_GMi.~hV:vM
......................... .

6
10
16
20


Figure 9: Accuracy vs. Number of classes for 20NG

3.4
ScalabUity with training set size
Training time: In Figure 11, the size of the training set
was varied from 10% to 70% of the whole data, while keeping
the relative train-test ratio constant at 70:30. We observe
imll.-:-~
11100
..:..
1,4oo
12100
1000




81D+
-I-200


15
10
t6
2O
211
:SO
311
40
~
I10
M



Figure 10:
Accuracy
vs.
Number
of classes for
Reuters

that the trainingtime of GraphSVM is nearly linearin the
training set sizes,while for multi-classSVMs the training
time increased super-linearlywith training set size. This
causes the gap between the two methods to become more
prominent for largerdatasets.
Accuracy: In Figure 11 we show the corresponding ac-
curacy values against varying percentages of training set
sizesfor20NG. We observe that as the accuracy of MCSVM
increaseswith increasingnumber of traininginstances and
GraphSVM closelytracks the increase and is always more
accurate than MCNB.
Maximum
memory:
In Figure 11 the percentage of
training documents is plotted against the maximum mem-
ory requiredto trainany SVM model in the GraphSVM and
MCSVM
approaches. In both cases,multiple one-vs-others
SVMs are learned,but the sizeand heterogeneityofthe neg-
ative ('others')classvarieslargelyleadingto differentmem-
ory requirements. In MCSVM,
thisnegative classcontains
the entiredataset apart from the positiveclass,whereas .it
is greatly pruned in the GraphSVM
approach. We notice
that GraphSVM requireslessthan one-fourth the memory
required by MCSVM.




| . . . . .
·
......
[....,o,.
r

o 1 ~__.~__.~._~.__.~.._....~-.---o
-o--
GraphSVM

10
ZO M ~
N
IlO 70



Figure 11: Training time, accuracy and maximum
model memory with varying training set sizes for
the 20NG dataset

3.5 Effect of the threshold parameter
An important parameter of GraphSVM is the threshold
used to decide what SVMs to create in the second stage.
In Table 2 we show the accuracy and training time for
differentvalues of the threshold (t).We see that a threshold
of 3% to 7% is appropriate for both these datasets because
of the base accuracy of the NB classifierchosen to get the
confusion matrix.




517

Method
Threshold
Training
Accuracy
t
time (secs)
in %
Reuters
MCSVM
GraphSVM
GraphSVM
GraphSVM
GraphSVM
0.07
0.05
0.03
0.01
158
36
41
46
65
92.86
94.91
94.86
94.86
95.33
20NO
MCSVM
235
89.66
GraphSVM
0.07
63
87.92
OraphSVM
0.05
73
88.72
GraphSVM
0.03
73
88.52

Table 2: Accuracy and Performance


If the threshold value is kept unnecessarily high, we will
hardly have any graph to construct, assuming the base clas-
sifter has a decent accuracy. On the other hand, even for
a moderately accurate base classifier, a very low threshold
(say 2% or less) will make a densely connected graph. In
that case, GraphSVM will approach MCSVM.
As seen in Sections 3.2 and 3.3, GraphSVM is more accu-
rate than MCSVM and MCNB for the Reuters dataset and
also requires less training time. The scalability results for
the Reuters dataset w.r.t, training set size are the same as
that for the 20NG dataset in Section 3.4, viz. GraphSVM is
highly scalable w.r.t, training time, accuracy and memory
requirements as compared to MCSVM. The detailed results
are omitted here for space constraints.


4.
RELATED WORK
A general framework,for solving multi-class problems us-
ing a collection of two-class problems is to associate it with
a coding matrix M where each class is a row and each two-
class classifier is a column [3, 1]. Each element ai~ is +1,-1
or 0, where +1 denotes the class i serving as a positive class
in classifier j, -1 denotes i serving as a negative class in j
and 0 denotes that class i does not participate in classifier
j. The coding matrix for one-vs-others will have N columns
with +1 on the diagonal and -1 in all other entries. For
Max-Wins [10], the coding matrix will have (~r) columns and
each column will have a single +i and -i and the rest of the
entries O. Error Correcting Output Codes (ECOC) classi-
tiers are proposed in [3] where the +ls and -ls are chosen in
such a way that the different rows are maximally separated.
During testing, the outcome of each classifier is treated as
a vector and compared with each row. The row to which
it is closest is returned as the predicted class. Ghani [5]
reports experiments with a number of ECOC classifiers to
solve large multi-class text classification problems using NB.
They report significant improvement in accuracy with the
ECOC method on the Industry section dataset. Rennie and
Ritkin [7] repeated similar experiment with Support Vec-
tor Machines as the base classifier. On the standard text
benchmarks, they found that for SVMs the various ECOC
classifiers did not provide any accuracy improvement over
the simple one-vs-others method.
Platt et al [12] present a modification of the testing proce-
dure of the Max-Wins algorithm which reduces the number
of kernel evaluations during testing. They arrange the var-
ious two-class classifiers in a DAG structure, that is used
to order the application of various two-class SVMs during
testing. This method does not affect the training time and
the accuracy is comparable.


5.
CONCLUSION
We have described GraphSVM, an effective framework for
extending discriminative classifiers like SVMs to handle data
with a large number of classes, accurately and efficiently.
GraphSVM estimates a measure of affinity between classes
which depends upon the severity of confusion between these
classes by a fast classifier like NB. This confusion indicates
a clustering of classes which is used to limit the number
and complexity of the SVMs that need to be trained for
multi-class categorization. GraphSVM beats the accuracy of
multi-class NB decisively even though it builds on a NB clas-
sifter. GraphSVM outperforms SVMs w.r.t, training time
and memory requirements. It matches or even exceeds the
accuracy of multi-class SVMs. GraphSVM is very simple to
understand and requires negligible coding, but it can be of
substantial utility while dealing with very large classifiers
with tens of thousands of classes and millions of instances.

Acknowledgments
This work was funded by a research grant from the Ministry of In-
formation Technology, India. We wish to thank Anand Janakira-
man, Ravindra Jaju and Amitabh Mehta for fruitful discussions.

6.
REFERENCES
[1] E. L. Allwein, R. E. Schapire, and Y. Singer. Reducing
multiclass to binary: A unifying approach for margin
classifiers. In 17th ICML, 2000.
[2] S. Chakrabarti, B. Dom, R. Agrawal, and P. Raghavan.
Scalable feature selection, classification and signature
generation for organizing large text databases into
hierarchical topic taxonomies. The VLDB Journal, 1998.
[3] T. G. Dietterieh and G. Bakiri. Solving multiclass learning
problems via ECOCs. JAIR, 2:263-286, 1995.
[4] S. Dumais, J. Platt, D. Heckerman, and M. Sahami.
Inductive learning algorithms and representations for text
categorization. In 7th CIKM, 1998.
[5] Rayid Ghani. Using error-correcting codes for text
classification. In 17th ICML, 2000.
[6] Shantanu Qodbole. Exploiting confusion matrices for
automatic generation of topic hierarchies and scaling up
multi-way classifiers. Technical Report, IIT Bombay, 2002.
http: //WW~,~.it.litb.ac.in/"shantanu/work/aps2002, pdf
[7] Ryan Rifkin and Jason D. M. R,ennie.Improving multi-class
text classification with the support vector machine, AI
Memo, AIM-2001-026, MIT, 2001.
[8] T. Joachims. A statistical learning model of text
classification for SVMs. In SIGIR ~001, volume 24, ACM.
[9] D. Koller and M. Sahami. Hierarchically classifying
documents using very few words. In 14th ICML, 1997.
[10] U. Kressel. Pairwise classification and support vector
machines. ]:n Advances in Kernel Methods: Support Vector
Learning, MIT Press, 1999.
[11] T. M. Mitchell. Conditions for the equivalence of
hierarchical and non-hierarchical Bayesian classifiers.
Technical note, 1998. Online at
hztp ://wl~1.cs. cmu.edu/"tom/hierproof,ps
[12] J. Platt, N. Cristianini, and J. Shawe-Taylor. Large margin
DAGs for multiclass classification. In Advances in NIPS 1P,
MIT Press, 2000.
[13] M. F. Porter. An algorithm for suffix stripping. Program,
14(3):130-137, 1980.
[14] V. N. Vapnik. The nature of statistical learning theory.
Springer Verlag, 1995.




518

