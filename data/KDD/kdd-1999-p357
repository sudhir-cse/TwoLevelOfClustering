Prediction
with
Local
Pat terns
using
Cross-Entropy


Heikki
Mannila
Dmitry
Pavlov
Microsoft
Research
University
of California
at Irvine
mannila@microsoft
.com
pavlovd@ics.uci.edu
Padhraic
Smyth
University
of California
at Irvine
smyth@ics.uci.edu


Abstract
Sets of local patterns in the forms of rules and co-occurrence
counts are produced
by many data mining
methods such
as association
rule algorithms.
While such patterns
can
yield useful insights
it is not obvious how to synthesize
local sparse information
into a coherent global predictive
model.
We study the use of a cross-entropy
approach to
combining
local patterns.
Each local pattern is viewed as a
constraint
on an appropriate
high-order
joint
distribution
of interest.
Typically,
a set of patterns
returned
by
a data mining
algorithm
under-constrains
the high-order
model.
The cross-entropy
criterion
is used to select a
specific distribution
in this constrained
family
relative
to
a prior.
We review the iterative-scaling
algorithm
which
is an iterative
technique
for hiding
a joint
distribution
given constraints.
We then illustrate
the application
of
this method to two specific problems.
The first problem is
combining
information
about frequent itemsets.
We show
that
the cross-entropy
approach
can be used for query
selectivity
estimation
for O/l
data sets. The results show
that we can accurately
answer a large class of queries using
just
a small set of aggregate
information.
The second
problem
involves sequence modeling using historical
rules,
with
an application
to protejn
sequences.
We conclude
that viewing
local patterns
as constraints
on a high-order
probability
model is a useful and principled
framework
for
prediction
based on large sets of mined patterns.


1
Introduction
Several data mining and rule induction methods provide
partial information
specified as conditional probabilities
(e.g., probabilistic
rules) or joint
probabilities
(e..g.,
frequent itemsets).
Such partial information
provides
useful insight
into the data, but it is not obvious
how to combine such pieces of knowledge for other
purposes.
We consider the discrete-valued
problem,
thus, we assume that a vector-valued
random variable
x =
(x1,22,...,
zd) takes values from some finite


permission to make digital or hard topics of all or part of this work fol
personal or classroom use is granted without
fee provided
that copies
are not made or distributed tbr profit or commercial advantage and that
copies hear this notice and the full citation on the tirst page. TO copy
otherwise, to republish, to post on servers or to redistribute to lists,
requires prior specific permission and!or a fee.
KDD-99
San Diego
CA
USA
Copyright ACM 1999 I-581 13-143-7/99/08...$5.00
alphabet.
We are interested
in estimating
the joint
distribution
P(z) = P(x~, ~2,. . ., zd), or some function
of P(X) such as a conditional
distribution,
given sets of
local rules containing
information
of the form P(z; =
a]zj = 6, zk = c) and/or P(zi
= a, zj = b,xk = c).
In general, if for example
each
zi can take m distinct
values, specification of P(r) will require O(md) different
entries in the joint probability
table.
It is clear that
estimating the full joint distribution
from data is often
impractical
given even moderate values of m and d,
because of the exponential
growth of the number of
different probabilities
which must be specified.
The primary focus of many current data mining algo-
rithms is to extract low-order information
in an efficient
manner.
For example, association rule algorithms
[l]
essentially compute all conditional
probabilities
of the
formp(ri
= 1 ] xj, = 1A.. .Axj, = 1) which are above a
given threshold and for which p(xj, = 1A +. .A xj, = 1),
the support,
is also above a prespecified
threshold.
These types of patterns are local to small sets of vari-
ables and specific values of these variables.
There are
numerous similar rule-finding
algorithms
that can effi-
ciently produce large lists of such local patterns. Typi-
cally the patterns are chosen based on the fact that they
differ from the expected norm, i.e., they are informative
in some sense relative to a uniform prior distribution
on

P(Xl,
22,.
. . , Xd).
We study the problem of how to combine such local
partial information
into a single global model which can
be used for prediction.
We retain the good properties of
both worlds: we get understandable
local patterns and
still are able to use them in prediction
or estimation
tasks.
While there are already some approaches to
combining
local patterns
for specific problems
like
classification
(e.g., [5]), we focus explicitly
here on a
probabilistic
framework.
We will not discuss how the
patterns
or rules can be found from the data since
there is a large body of techniques available to do this
already. Instead we focus on the problem of once given
the patterns, how can one combine them for prediction?
Combining
local patterns
to form a global model
for P(
Xl,
E2,.
. . ,
xd) is worthwhile
from
two different
perspectives. Firstly, from a data mining perspective, it
offers a framework for linking sets of local patterns with



357

the more well-established
world of global models, where
evaluation, prediction,
estimation, model selection, and
so forth, can be handled in a coherent and consistent
manner.
Secondly, from an applications
perspective,
the ability
to quickly
find local patterns
and then
use these to construct
a model for P(zi, ~2,. . . ,td)
can offer distinct
advantages in accuracy, efficiency,
and interpretability
compared to more conventional
approaches.

2
Problem
Statement
and the
Iterative
Scaling
Algorithm
In a data mining framework we will assume that our
patterns
and rules are statements
about frequency
counts in the data that can be represented as con-
straints.
For example, if we know that the number of
co-occurrences of (zj = 1, zk = 1) in the training
data
is njk, then we represent this as a constraint by writing
a constraint
equation of the form

c P(x)k(xli)
= dj.
(1)
5


where the ith
indicator
function
k(z]i)
yields
1 if
x satisfies the ith
constraint
and 0 otherwise
and
di denotes the fraction
of elements that satisfy this
constraint,
1 2 i 5 C, C being the total number of
such constraints.
In this case the indicator
function
is 1 when xj = 1 and zk = 1 and 0 otherwise,
so
that the left-hand
side of the above equation sums to
P(zj
= 1, Xk = 1). The right-hand
side di can be set
to njk/n,
where n is the total number of data points,
thus constraining
the probability
on the left to take
this maximum-likelihood
value. Let the total number
of such constraints
be C (this represents the number
of patterns
returned
by the data mining algorithm).
We can view the constraints
as requirements
that are
imposed on the otherwise
unknown joint
probability
distribution
P(X) .
Typically
this will underconstrain P(z), i.e., there is
an infinite set of probability
distributions
which satisfy
the constraints.
To choose a specific P(x) we invoke
the principle that the P(x) which is chosen from within
this set is the one closest to a specified prior r(x)
in
a cross-entropy
sense, i.e., PCE = arg minp CE(P, n),
where CE(P, x)
=
C, P(z) log $$
is the cross-
entropy between P and r. In this paper we will choose
r(z)
to be uniform,
and thus, minimizing
CE(P, T) is
equivalent
to maximizing
the entropy of P subject to
the given constraints.
Maximum
entropy has a long
history as a criterion for model selection that we will not
dwell on here (e.g., [4]): in a certain sense it picks the
distribution
which adds the least additional
information
beyond that already given.
Assume one of the constraints
in Equation
I is
k(x]O) = 1 for all x and do =' 1 to ensure that the
distribution
sums up to 1. The method of undetermined
Lagrangian
multipliers
can then be used to find PCE
satisfying
the constraints.
A slightly
surprising
(and
useful) theorem is that PCE can be expressed in the
following general functional
form:




i=l


where the ,LJ~,i =
O,... , C are positive
constants
satisfying for all i

c
p. c
n(x)k(x 1i) n ,~;(~`j) = di.
(2)
z
j=l


The general problem is thus reduced to the problem of
finding a set of pi from the set of Equations
2. It can
be shown that a solution to Equation
2 exists and is
unique if and only if the constraints
do not contradict
each other [3]. For example, if all the constraints
only
consist of relative frequency counts in the data (as is
the case in results presented below), then they will be
consistent automatically.
The generalized iterative
scaling algorithm
is well
known in the statistical
literature
as an iterative
tech-
nique which converges to the solution
PCE for prob-
lems of this general form (see, for instance, [3]). The
algorithm
can be used to obtain a numerical
solution
for the parameters satisfying
the given constraints.
A
high-level outline of the algorithm
is as follows.
b*-

1.
Choose an initial approximation
to P(x)
2.
While (Not all Constraints
are Satisfied)
For (i varying over all constraints)
Update ~0;
Update pi;
End;
EndWhile;
3.
Output
the constants
,ui

The update rules pi,t corresponding
to constraint
i at
iteration
t will be:

(3)

4
Pi,t+1
=
Pi,tSi,t,
(4


where Pt refers to our estimate of P at iteration
t.
This algorithm
is well-known
in statistics
(and related
applications
such as statistical
language
modeling),
however,
to our knowledge
this paper is the first
illustration
of its use in a data mining context.

3
Query
Selectivity
Estimation
In this section we present our first application,
query
selectivity
estimation
for O/l relations,
The method
actually
amounts to combining
association
rules for
prediction.
Suppose we are given a table r of OSand
1s; denote the schema (set of column headers) of the

358

table by R.
A conjunctive
query
Q over R is an
expression of the form A1 = bl A AZ = bz . . . A Ak =
bk, where k > 1, for each i we have Ai E R and
bi E {O,l}.
The size of the answer to the query Q
is the number of rows of r that satisfy Q, and the query
selectivity
is the fraction
of the rows that satisfy the
queries. Query selectivity
estimation
tries to find the
approximate
size of the answer to a conjunctive
query
without
actually
computing
the query.
This problem
has wide applications
in database query optimization,
and scores of methods have been developed for this task
[lo, 81.
A typical
solution
to the query selectivity
problem
is to maintain
for each attribute
Ai E R information
about the selectivity
c(Ai, bi) of the conditions Ai = b
forbi=O,l(
bo viously, for binary data only one number
per attribute
is needed). The selectivity c(Ai, bi) can be
viewed as a constraint on the margin of the variable Ai.
Assuming independence of attributes,
we can estimate
the selectivity
of query Q by nf=,
c(Ai, bi).
This
independence model is in fact the maximum
entropy
distribution
given only counts on individual
attributes,
i.e., it can be viewed as a special caseof the more general
approach presented below.
To use the cross-entropy
approach to give better
estimates we need additional
information
to constrain
the distribution.
The frequent
sets from association
rules [2] provide
a way of defining
useful marginal
information.
Given a set of attributes
X c R, the
frequency
f(X)
of X in r is defined as the fraction
of rows of r that have a 1 in all the columns of X:
f(X)
= I{t E r 1t[A] = 1 for all A E X)1/n,
where n
is the number of rows in table r. Given a threshold u,
the collection of a-frequent sets is the collection of all
subsets of R (and their frequencies) whose frequency is
at least 6.
We use the cross-entropy
approach to obtain im-
proved estimates for query selectivity
as follows.
Let
S be the collection of u-frequent sets for some given u.
Given a conjunctive
query A1 = bl A A2 = b2.. +A
Ak = bk, first find all frequent
sets X
E S such
that X C {Al, A-J,. . . ,Ak}, i.e., those frequent sets
which are subsets of the query attributes.
Using these
marginal counts, then build the maximum entropy dis-
tribution
for the state space consisting of all vectors
z = ($I,...
, zk) of length k, where each zi is either 0
or 1. After this has been done, answer the conjunctive
query is simple: we just find P(z) for the particular
z
corresponding
to the query Q.
The method is applicable to any type of queries, not
just conjunctive
queries. As we build an estimate of the
whole joint distribution
on the k attributes
occurring
in the query,
using this estimate
we can compute
approximate
answers to any query. The running time
per query of the method has the form O(Fk + IC2k),
where k is the number of variables in the query, F is the
size of the frequent set collection,
C is the number of
frequent sets that are included in the set of variables of
-
.
* .




: .
. *
.; .
.
*
.




-500 1
I
0
0.5
1
1.5
2
2.5
3
3.5
TRUE ANSWER
x 10'


Figure 1: Relative performance
of the independence
method and the maximum entropy method as a func-
tion of the true answer.


the query, and I is the number of iterations (typically
10
to 20 for the experiments below). The exponentiality
in
k makes the method impractical
for about k > 10, but
for queries with a small number of variables the method
is quite fast. Note that the running time is independent
of the size of the original data set.
For brevity,
we present results only on one data
set. The dataset contains information
about the buying
behavior of certain customers. It has 249 attributes and
32711 rows. The data are relatively
sparse, containing
98654 Is, and hence we would need about 130 kB to
represent the data (one byte for each one in the data set,
plus some overhead to indicate where each observation
starts). A textual representation
takes 335 kB.
Using the independence assumption requires that we
store 249 numbers, about 1 kB, i.e., the fraction of 1s
for each attribute.
For the maximum entropy method
we experimented
by computing
frequent sets using a
threshold
of g = 0.005 (selected to be small enough
to ensure that non-trivial
combinations of frequent sets
are generated).
This yielded 453 frequent sets for a
total of 702 constraints,
and the space needed for this
representation
of the dataset is about 4 kB. Thus the
maximum entropy method uses about 4 times as much
space as the pure independence assumption, but about
a fraction
of l/30 of the space needed for the whole
dataset.
The threshold of 0.005 means that we know
the size of each "1s only" query A1 = 1 A - esA Ak = 1,
provided the answer is at least 165.
We generated random queries with k = 4 and k = 5
conjuncts
by selecting attributes
Ai at random and
adding the conjunct Ai = 0 with probability
0.8 and the
conjunct Ai = 1 with probability
0.2. Due to this query
generation method, the queries tend to have either a
relatively
small or relatively
large answer.
The maximum entropy method produces consistently

359

very good estimates of the actual query size.
The
method performs significantly
better than the estima-
tion based only on the the frequencies of single at-
tributes.
For about 314 of the queries the query at-
tributes were such that maximum entropy method had
exactly the same information
as the independent
at-
tribute
method, and in those cases the methods pro-
duced, of course, the same answer.
In the remaining
25% of the cases the maximum
entropy method typi-
cally produced much better approximations.
Figure 1
shows for each of 1000 queries the difference between
the absolute errors for the maximum entropy method
and for the attribute
independence method.
The fig-
ure shows that
the maximum
entropy
method only
rarely produces an estimate that is worse than the esti-
mate produced by the attribute
independence method,
whereas for a large set of queries the use of the maxi-
mum entropy method gives significantly
better answers.
As mentioned
in the beginning
of this section, the
approach we have described can obviously be used to
combine association rules for prediction.
While some
work has been done on finding
predictive
association
rules (see, e.g., [7]), we are not aware of work on actually
combining association rules.
Given a set {Xi + A} of association rules, let k be the
number of distinct attributes
{Bi, . . . , &}
occurring in
the sets Xi.
We build the joint distribution
over the
2k different states. When we encounter an input row t
with values br, . . . , bk for the attributes
Bi, we use the
corresponding
entry in the joint distribution
to predict
the value of A. For brevity, we omit the details.

4
Modeling
Sequences
with
Probabilistic
Rules
In this
section
we apply
the methodology
of the
earlier sections to probabilistic
modeling of sequential
data.
Again we consider discrete-valued
variables with
alphabet C. For any symbol w occurring in the sequence
we define the L-history
of w, HL(w),
as the L symbols
preceding
w in the sequence.
For discrete-valued
sequences, there exist several data mining algorithms
which can extract sequential
rules of the general form "if
event A happens in HL(w)
then w will occur at position
t with
probability
p,"
or equivalently,
conditional
probability
statements of the form p(wlA
E HL(w))
=
p. Provided with a set of such probabilistic
rules from a
training data set, we can view such rules as constraints
on the unknown joint distribution
p(w, HL(w)).
Thus,
here we are interested in estimating the distributions
of
the form PL(w, HL(w))
or P(wlH~(w)).
As in the non-sequential
multivariate
case there are
many measures for defining interesting
rules and many
algorithms
for finding such sets from data.
We used
the average mutual
information
between A and w as
a simple measure to find and rank different
possible
rules (since we are primarily
interested here in how to
combine such patterns rather than how they are found).
We also limited our attention
to simple events of the
form "symbol v occurs k positions
before w" in the
sequence. The maximum
value of k is defined to be
the history length. For example, for history-length
1 we
would only be considering bigram terms p(w(t) jw(t-
1))
in our model.
The problem of estimating
p(w( HL(w))
given con-
straints in the form of rules described above is in its gen-
eral form equivalent to the problem discussed in Section
2 of choosing a specific distribution
from a constrained
family of distributions.
Once again one can invoke the
maximum entropy principle
to choose among distribu-
tions and use the iterative scaling algorithm to solve the
associated optimization
problem.
For sequential data,
the problem is actually
a little more complex in form
than the multivariate
case sketched in Section 2. We
use the same approach as used with
"trigger-models"
in natural
language modeling
[9]. For full details see
[6], where we also show that each iteration
of the iter-
ative scaling algorithm
scales roughly as NC where N
is the length of the training sequence and C is the total
number of constraints
(rules).
We applied this approach to modeling
of protein
sequence data.
The purpose of this experiment
was
to explore the utility
of probabilistic
rules for modeling
sequence structure
beyond simple bigram models. We
picked a well-known
set of 585 hemoglobin
protein
sequences (about
85,000 symbols in total)
available
on-line at http: //www . isb-sib.
ch/ and used various
combinations
of prior, bigram, and rule-based models
for modeling the conditional
distribution
of the current
symbol given its history. The size of the alphabet C is
20. We generated a lo-fold cross-validated
estimate of
the out-of-sample cross-entropy for each model, namely,
the sum of the negative
log probabilities
for each
observed symbol in the test sequence conditioned
on its
history.
The higher the probabilities
a model assigns
to observed symbols the lower (and better) the cross-
entropy score will be. A simple baseline score can be
defined as the cross-entropy of a model with a uniform
distribution
for p(w), namely log ICI = log(20) = 4.3219
bits. Thus, all decreases in reported cross-entropy
are
relative to this baseline uncertainty.
The upper panel of Table 2 shows the average relative
decrease in cross-entropy using only priors and a bigram
model.
As expected, bigrams do better than priors
alone on both the training
and test data. The two pairs
of columns in the bottom panel of Table 2 show the
results for combining rules with priors, and combining
rules with priors and bigrams, respectively,
where we
tried rules with different history lengths. In the "rules
with priors"
there were no single-lag
(bigram)
terms
in the model,
i.e., all rule events were at least 2
positions
back.
The rules clearly provide additional
information
beyond priors or bigrams.
The relative
decrease in entropy using rules is close to 1 bit and
almost 5 times higher than for the priors alone, and
2.5 times higher than for all bigrams
alone (upper

360

Priors
Priors and bigrams
Train
Test
Train
Test
5.94%
5.92%
13.05%
11.82%



History
Priors and
Priors, bigrams
length
rules
and rules
Train
Test
Train
Test
2
11.84%
10.66%
16.71%
14.55%
3
14.80%
13.28%
19.18%
16.86%
6
18.62%
16.64%
22.39%
19.80%
11
20.63%
18.14%
24.06%
21.12%
16
21.61%
18.60%
25.22%
21.54%
21
22.12%
18.67%
25.68%
21.65%


Table 1: Relative
percentage decrease in train
and
test cross-validated
cross-entropy for different models,
where ]C] = 20 and at most the top 6 rules with mutual
information
greater than 0.001 bit are retained for each
symbol w.


panel). Interestingly,
models based on priors and rules
only (without
bigrams, columns 2 and 3 of the lower
panel in table 3) consistently outperform the full bigram
model both in and out-of-sample.
For models with
rules, the performance of the model improves as the
history length increases, and the methodology
appears
relatively
robust to overfitting
on this data.
In [6] we report a variety of other experiments
of
this nature on this data set using rule-based probability
models.
The major
drawback
of the technique
is
the computational
complexity
of iterative
scaling; for
example, online modeling using the approach presented
here would be impractical.
Possible generalizations
are
numerous, such as allowing
a more general language
for rules with multiple
symbols, disjunctions,
and so
forth.
Parameters such as history lengths, numbers of
rules, bigrams, etc., could all be chosen automatically
in principle
using cross-validated
estimates of cross-
entropy.

5
Conclusions
Data mining algorithms are useful for efficiently finding
patterns in the form of conjunctive
expressions (rules)
with attached frequencies of occurrence from large data
sets. However, the problem of combining these patterns
into a coherent global model has been relatively
unex-
plored.
In this paper we proposed a straightforward
approach to this problem by viewing the local patterns
as constraints
on an unknown high-order distribution.
The iterative scaling procedure can then be used to find
the the distribution
that is closest in a cross-entropy
sense to a specified prior.
We illustrated
the applica-
tion of this idea on two different problems, query se-
lectivity
estimation
and sequence modeling.
In both
cases the method using local patterns provided signifi-
cant improvements over conventional
alternatives.
The
method we have proposed is applicable to various other
settings.

Acknowledgments
The work of DP and PS described in this paper was
supported by the National
Science Foundation
under
Grant IRI-9703120.

References
PI




PI




[31



PI


[51




F31




[71




PI

PI


WI
R. Agrawal, T. Imielinski,
and A. Swami.
Min-
ing association rules between sets of items in large
databases.
In P. Buneman
and S. Jajodia,
edi-
tors, Proceedings
of ACM
SIGMOD
Conference
on
Management
of Data
(SIGMOD'93),
pages 207 -
216, Washington,
D.C., USA, May 1993. ACM.

R. Agrawal, H. Mannila,
R. Srikant, H. Toivonen,
and A. I. Verkamo.
Fast discovery of association
rules.
In U. M. Fayyad, G. Piatetsky-Shapiro,
P. Smyth, and R. Uthurusamy,
editors, Advances
in Knowledge
Discovery
and Data
Mining,
pages
307 - 328. AAAI Press, Menlo Park, CA, 1996.

J. N. Darroch
and D. Ratcliff.
Generalized
iterative
scaling for log-linear
models.
Annals
of
Mathematical
Statistics,
43:1470-1480, 1972.

E. T. Jaynes.
Probability
Theory
- the Logic
of Science.
Physics, Washington
University,
St.
Louis, MO 63130, USA, 1994.

B. Liu, W. Hsu, and Y. Ma. Integrating
classifica-
tion and association rule mining. In Proceedings
of
the Fourth
International
Conference
on Knowledge
Discovery
and Data Mining,
pages 80-96, 1998.

H. Mannila,
D. Pavlov, and P. Smyth.
Predic-
tion with local patterns using cross-entropy. Tech-
nical Report UCI-ICS-TR-99-28,
Information
and
Computer Science, University
of California,
Irvine,
1999.

N. Megiddo and R. Srikant. Discovering predictive
association rules.
In Proc.
of the 4th hat'1 Con-
ference
on Knowledge
Discovery
in Databases
and
Data Mining,
1998.

R. Ramakrishnan.
Database
Management
Systems.
McGraw-Hill,
1997.

R. Rosenfeld.
A maximum
entropy approach to
adaptive statistical
language modelling.
Computer
Speech and Language,
10(3):187-228, July 1996.

J. D. Ullman.
Principles
of
Database
and
Knowledge-Base
Systems, volume I. Computer Sci-
ence Press, Rockville, MD, USA, 1988.


361

