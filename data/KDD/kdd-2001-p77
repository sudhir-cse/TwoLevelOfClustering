Proximal Support Vector Machine Classifiers

Glenn Fung and Olvi L. Mangasarian
ComputerSciencesDepartment
University of Wisconsin
1210West DaytonStreet
Madison, Wl 53706
(gfung,olvi)@cs.wisc.edu



ABSTRACT
Instead of a standard support vector machine (SVM) that
classifies points by assigning them to one of two disjoint half-
spaces, points are classified by assigning them to the closest
of two parallel planes (in input or feature space) that are
pushed apart as far as possible.
This formulation, which
can also be interpreted as regularized least squares and con-
sidered in the much more general context of regularized net-
works [8, 9], leads to an extremely fast and simple algorithm
for generating a linear or nonlinear classifier that merely re-
quires the solution of a single system of linear equations. In
contrast, standard SVMs solve a quadratic or a linear pro-
gram that require considerably longer computational time.
Computational results on publicly available datasets indi-
cate that the proposed proximal SVM classifier has compa-
rable test set correctness to that of standard SVM classifiers,
but with considerably faster computational time that can be
an order of magnitude faster. The linear proximal SVM can
easily handle large datasets as indicated by the classifica-
tion of a 2 million point 10-attribute set in 20.8 seconds.
All computational results ave based on 6 lines of MATLAB
code.


Keywords
data classification,
tions
support vector machines, linear equa-




1.
INTRODUCTION

Standard support vector machines (SVMs) [36, 6, 3, 5,
20], which are powerful tools for data classification, classify
points by assigning them to one of two disjoint halfspaces.
These halfspaces are either in the original input space of
the problem for linear classifiers, or in a higher dimensional
feature space for nonlinear classifiers [36, 6, 20]. Such stan-
dard SVMs require the solution of either a quadratic or a
linear program which require specialized codes such as [7].
In contrast we propose here a proximal SVM (PSVM) which


Permission to make digital o1hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or dis|ributed for profit or commercial advantage and that
copies bear this notice and the lhll citation on the first page. To copy
otherwise, to republish, to post on serve~,'sor to redistribute to lists,
requires prior specific permission and/or a fee.
KDD 01 San Francisco CA USA
Copyright ACM 2001 1-58113-391-x/01/08...$5.00
classifies points depending on proximity to one of two paral-
lel planes that are pushed as far apart as possible. In fact our
geometrically motivated proximal formulation has been con-
sidered in the much more general context of regularization
networks [8, 9]. These results, which give extensive theoret-
ical and statistical justification for the proximal approach,
do not contain the extensive computational implementation
and results given here. Furthermore, our specific formula-
tion leads to a strongly convex objective function which is
not always the case in [8, 9]. Strong convexity plays a key
role in the simple proximal code provided here as well the
very fast computational times obtained. Obtaining a linear
or nonlinear PSVM classifier requires nothing more sophis-
ticated than solving a single system of linear equations. Ef-
ficient and fast linear equation solvers are freely available [1]
or are part of standard commercial packages such as MAT-
LAB [26], and can solve large systems very fast.
We briefly summarize the contents of the paper now. In
Section 2 we introduce the proximal linear support vector
machine, give the Linear Proximal Algorithm 2.1 and an ex-
plicit expression for the leave-one-out-correctness in terms of
problem data (16). In Section 3 we introduce the proximal
kernel-based nonlinear support vector machine, the corre-
sponding nonlinear classifier (28) and the Nonlinear Proxi-
mal Algorithm 3.1. Section 3 contains many numerical test-
ing results for both the linear and nonlinear classifiers based
on an extremely simple MATLAB [26] code of 6 lines for
both the linear and nonlinear PSVM. The results surpass
all other Mgorithms compared to in speed and give very
comparable testing set correctness.
A word about our notation and background material. All
vectors will be column vectors unless transposed to a row
vector by a prime superscript '.
For a vector x in the n-
dimensional real space R ~, the step function step(x) is de-
fined as step(x)i -- 1 if x~ > 0 else step(x)i = 0 if x~ _~ 0
for i -- 1,... , n. The scalar (inner) product of two vectors
x and y in the n-dimensional real space R n will be denoted
by x'y and the 2-norm of x will be denoted by [[x[]. For a
matrix A E Rm×~,Ai is the ith row of A which is a row
vector in R ~, while A.j is the jth column of A. A column
vector of ones of arbitrary dimension will be denoted by e.
For A E R m×n and B E R n×k, the kernel K(A,B)
maps
R m×~ x R "~×k into R mxk. In particular, if x and y are col-
umn vectors in R ~ then, K(x', y) is a real number, K(x', A')
is a row vector in R "~ and K(A, A') is an m × m matrix. The
base of the natural logarithm will be denoted by 6. We will
make use of the following Gaussian kernel [36, 6, 20] that is




77

frequently used in the SVM literature:

(K(A,B))~j
=
g-u[]AJ-B'jI[2, i = 1 . . . . . .,m, j = 1
,k,
(1)


where A E Rm×~, B E /~×k and/~ is a positive constant.
The identity matrix of arbitrary dimension will be denoted
by I.


2.
THE LINEAR PROXIMAL
SUPPORT
VEC-

TOR MACHINE

We consider the problem, depicted in Figure 1, of classify-
ing m points in the n-dimensionalreal space R~, represented
by the m × n matrix A, according to membership of each
point A~ in the class A+ or A- as specified by a given m x m
diagonal matrix D with plus ones or minus ones along its
diagonal. For this problem, the standard support vector ma-
chine with a linear kernel [35, 6] is given by the following
quadratic program with parameter ~ > 0:

1
I
min
v,e~y + -~w w
(w,-f,y)ERn+l+ m

s.t.
D(Aw-e'7)+y
>_ e
(2)
y
_> 0.

As depicted in Figure 1, w is the normal to the bounding
planes:

x ~w =
"7+ 1
x'w
=
7-1,
(3)

that bound most of the sets A+ and A- respectively. The
constant ~ determines their location relative to the origin.
When the two classes are strictly linearly separable, that is
when the error variable y = 0 in (2) (which is not the case
shown in Figure 1), the plane x'w = ~ + 1 bounds all of the
class A+ points, while the plane x'w = 7 - 1 bounds all of
the class A- points as follows:

A~w >_ ~ + 1,
for
Di~ = 1
Aiw < "~-~ 1,
for
Dii = -1.
(4)

Consequently, the plane:

· 'w = %
(5)

midway between the bounding planes'(3), is a separating
plane that separates A+ from A- completely if y --- 0, else
only approximately as depicted in Figure 1. The quadratic
term in (2), which is twice the reciprocal of the square of
2 between the two bounding planes
the 2-norm distance

of (3) (see Figure 1), maximizes this distance, often called
the "margin". Maximizing the margin enhances the gener-
alization capability of a support vector machine [35, 6]. If
the classes are linearly inseparable, which is the case shown
in Figure 1, then the two planes bound the two classes with
a "soft margin" (i.e. bound approximately with some error)
determined by the nonnegative error variable y, that is:

Aiw
+
y~ _> ~+1,
forD~i=l
Aiw
-
Yi ~
~-1,
forDii=-l.
(6)

The 1-norm of the error variable y is minimized parametri-
cally with weight u in (2) resulting in an approximate sepa--
rating plane (5) as depicted in Figure 1. This plane acts as
a linear classifier as follows:




!0,
thenxEA+,
xtw-~
0, thenxEA-,

0, thenxEA+orxEA-.
(7)




Our point of departure is similar to that of [23, 24], where
the optimization problem (2) is replaced by the following
problem:




vl y2
min
~Jl ]l +½(w'w+~ 2)
(~,~,~)eR~+'+~
(8)
s.t.
D(Aw-
e?) +y
>
e




Note that no explicit nonnegativity constraint is needed on
y, because if any component yl is negative then the objec-
tive function can be decreased by setting that y~ = 0 while
still satisfying the corresponding inequality constraint. Note
further that the 2-norm of the error vector y is minimized
instead of the 1-norm, and the margin between the bound-
ing planes is maximized with respect to both orientation w
and relative location to the origin % Extensive computa-
tional experience, as in [22, 23, 24, 18, 17} indicates that
this formulation is just as good as the classical formulation
(2) with some added advantages such as strong convexity of
the objective function. Our key idea in this present paper is
to make a very simple, but very fundamental change in the
formulation (8), namely replace the inequality constraint by
an equality as follows:




min
1
2
½(wtw + 72)
~lhylI
+
(~,~,~)en-+~+~
(9)
s.t.
D(Aw-
e~/) +y
=
e




This modification, even though very simple, changes the na-
ture of optimization problem significantly. In fact it turns
out that we can write an explicit exact solution to the prob-
lem in terms of the problem data as we show below, whereas
it is impossible to do that in the previous formulations be-
cause of their combinatorial nature. Geometrically the for-
mulation (9) is depicted in Figure 2 which can be inter-
preted as follows. The planes x~w - "~--- +1 are not bound-
ing planes anymore, but can be thought of as "proximal"
planes, around which the points of each class are clustered
and which are pushed as far apart as possible by the term
(w'w + ~/2) in the objective function which is nothing other
than the reciprocal of the 2-norm distance squared between
the two planes in the (w,-y) space of R ~+1.




78

· ,,
,x~w=7+l


%',,,\0
x
Xx
A+
o o \ o'. \x
x
o
o oh
',, \x
x
OOOo~,.,~
"
x
x
A-
o"X\",x\
x
oo°o %'%o


Margin= ~"
\'',,,\
w
· (
'ktj'r"

Separating Plane: x'w = 7


Figure 1:
The Standard
Support
Vector Machine
Classifier in the w-space of _Rn: The approximately
bounding planes of equation (3) with a soft (i.e. with
2
and the plane of equation
some error) margin II-~H'
(5) approximately separating A+ from A-.


,,
~L)xtw
- ~/= +1

\x
x '",,
o~,x
\:Oo°x XX A+
o
~o
',,
x
\.
x
· °O~ox
',,x
~
x
3\
o
o o
o
0 '"~



II[~]fl
\
(~../

Separating Plane: x%' - 7 = 0


Figure 2:
The Proximal
Support
Vector Machine
Classifier in the (w,3')-space of R~+I:
The planes
x~w -3'
= +1 around
which points of the sets A+
and A- cluster and which are pushed apart by the
optimization problem (9).


We note that our formulation (9) can be also interpreted
as a regularized least squares solution [34] of the system of
linear equations D(Aw - e3") = e, that is finding an ap-
proximate solution (w, 3') with least 2-norm. Similarly the
standard SVM formulation (2) can be interpreted, by using
linear programming perturbation theory [21], as a least 2-
norm approximate solution to the system of linear inequal-
ities D(Aw - e3") > e.
Neither of these interpretations,
however, is based on the idea of maximizing the margin,
the distance between the parallel planes (3), which is a key
feature of support vector machines [36, 6, 20].
The Karush-Kuhn-~cker (KKT) necessary and sufficient
optimality conditions [19, p.
112] for our equality con-
strained problem (9) are obtained by setting equal to zero
the gradients with respect to (w, 3",y, u) of the Lagrangian:




(10)

Here, u E R m is the Lagrange multiplier associated with the
equality constraint of (9). Setting the gradients of L equal
to zero gives the following KKT optimality conditions:


w-
XDu
=
0

3"+e'Du
=
0
(11)
vy- u
=
0
D(Aw-e3")+y-e
=
0

The first three equations of (11) give the following expres-
sions for the original problem variables (w, % y) in terms of
the Lagrange multiplier u:
t
U
w= A'Du,
3"=-e
Du,
y= -.
(12)
v

Substituting these expressions in the last equality of (11)
allows us to obtain an explicit expression for u in terms of
the problem data A and D as follows:

u = (I + D(AA' + ee')D)-'e = (I + gH,)_le,
(13)

where H is defined as:

H=D[A
-e I.
(14)

Having u from (13), the explicit solution (w,%y) to our
problem (9) is given by (12). Because the solution (13) for
u entails the inversion of a possibly massive m × m matrix,
we make immediate use of the Sherman-Morrison-Woodbury
formula [14, p. 51] for matrix inversion, as was done in [23,
10, 24], which results in:

u = v(I - H( I + H'H)-IH')e.
(15)

This expression, as well as another simple expression (29)
for [~] below, involve the inversion of a much smaller di-
mensional matrix of order (n + 1) x (n + 1) and completely
solves the classification problem. For concreteness we ex-
plicitly state our very simple algorithm.

ALGORITHM 2.1. Linear Proximal SVM Given m data
points in R n represented by the m x n matrix A and a di-
agonal matrix D of =hl labels denoting the class of each row
of A, we generate the linear classifier (7) as follows:

(i) Define H by (14) where e is an m x 1 vector of ones
and compute u by (15) for some positive v. Typically
is chosen by means of a tuning (validating) set.

5i) Determine (w,'y) from (12).

(iii) Classify a new x by using (7).

For standard SVMs, support vectors consist of all data
points which are the complement of the data points that
can be dropped from the problem without changing the sep-
arating plane (5) [36, 20]. Thus, for the standard SVM for-
mulation (2), support vectors correspond to data points for



79

which the Lagrange multipliers are nonzero because, solving
(2) with these data points only will give the same answer as
solving it with the entire dataset. In our proximal formu-
lation (9) however, the Lagrange multipliers u are merely a
multiple of the error vector y: u = vy as given by (12). Con-
sequently, because all components of y are typically nonzero
since none of the data points usually lie on the proximal
planes x'w = ±1, the concept of support vectors needs to
be modified as follows. Because (w, 7) E R~+1 are given as
linear functions of y by (11), it follows by the basis theo-
rem for linear equations [13, Theorem 2.11][25, Lemma 2.1],
applied to the last equality of (11) for a fixed value of the
error vector y, that at most n + 1 linearly independent data
points are needed to determine the basic nonzero compo-
nents of (w, V) E Rn+l. Guided by this fact that only a
small number of data points can characterize any specific
(w, ~,), we define the concept of e-support vectors as those
data points Ai for which error vector yi is less than e in
absolute value. We typically pick e small enough such that
about 1% of the data are e-support vectors. Re-solving our
proximal SVM problem (9) with these data points and a
adjusted (typically upwards) by a tuning set gives test set
correctness that is essentially identical to that obtained by
using the entire dataset.
We note that with explicit expressions (w, 7, y, u) in terms
of problem data given by (12) and (15), we are able to get
also an explicit expression for the leave-one-out-correctness
looc [32], that is the fraction of correctly classified data
points if each point in turn is left out of the PSVM formula-
tion (9) and then is classified by the classifier (7). Omitting
some algebra, we have the following leave-one-out-correctness:


looc = e'step(h)
- - ,
(16)
m

where the "step" function is defined in the Introduction, and
for i = 1,... ,m:


hi - DiHHi'D~u~ = DiHHC Di(I - Hi( I + Hi'Hi)-lHi')e.
l]
lJ
(17)

Here, H is defined by (14), Hi denotes row i of H, while H ~
denotes H with row Hi removed from H, and ui is defined
by (15) with H replaced by H i. Similarly, D~ denotes row i
of D.
We extend now some of the above results to nonlinear
proximal support vector machines.


3.
NONLINEAR
PROXIMAL
SUPPORT
VEC-

TOR MACHINES

To obtain our nonlinear proximal classifier we modify our
equality constrained optimization problem (9) as in [20, 18]
by replacing the primal variables w by its dual equivalent
w = A'Du from (12) to obtain:

min
//1 y221[[I + ½(U'U+~'/2)
(~,,,y)eR-~+l+m
(18)
s.t.
D( AAt Du - e'~) + y
=
e,

where the objective function has also been modified to mini-
mize weighted 2-norm sums of the problem variables (u, % y).
If we now replace the linear kernel AA t by a nonlinear kernel
K(A, A') as defined in the Introduction, we obtain:

min
5[t [[ +
vl
y 2
½(u,u+~2)
(u,~,y)eR-~+l+m
(19)
s.t.
D(K(A,A')Du-ev)+y
=
e.

Using the shorthand notation:

K := K(A, A'),
(20)

the Lagrangian for (19) can be written similarly to (10) as:


-~],1[u],[2-v'(D(KDu-e~/)+y-e).
L(u,z,y,v)
= ~llyll ~ +

(21)

Here, v E RTM is the Lagrange multiplier associated with
the equality constraint of (19). Setting the gradients of this
Lagrangian with respect to (u, % y, v) equal to zero gives the
following KKT optimality conditions:

u- DK'Dv
=
0
+ e'Dv
=
0
uy - v
=
0
(22)

D(KDu-
e'~) +y
=
e.

The first three equations of (22) give the following expres-
sions for (u, 3', Y) in terms of the Lagrange multiplier v:

I
V
u= DK'Dv,
"~= -e Dv, y= -.
(23)
12


Substituting these expressions in the last equality of (22)
gives an explicit expression for v in terms of the problem
data A and D as follows:

v = (I + D(KK' + ee')D)-le = (I + GG,))_le '
(24)


where G is defined as:

O=D[K
-e I.
(25)

Note the similarity between G above and H as defined in
(14). This similarity allows us to obtain G from the expres-
sion (14) for H by replacing A by K in (14). This can be
taken advantage of in the MATLAB code 4.1 of Algorithm
2.1 which is written for the linear classifier (7). Thus, to gen-
erate a nonlinear classifier by Algorithm 3.1 merely replace
A by K in the algorithm.
Having the solution v from (24), the solution (u,% y) to
our problem (19) is given by (23). Unlike the situation with
linear kernels, the Sherman-Morrison-Woodbury formula is
useless here because the kernel matrix K = K(A, A t) is a
square m x m matrix, so the inversion must take place in
a potentially high-dimensional Rm. However, the reduced
kernel techniques of [171 can be utilized to reduce the m x
m dimensionality of the kernel K = K(A, A r) to a much
smaller m x r~ dimensionality of a rectangular kernel K =
K(A,4'), where ~ is as small as 1% of m and A is an
x n random submatrix of of A. Such reduced kernels not
only make most large problems tractable, but they also often
lead to improved generalization by avoiding data overfitting.
The effectiveness of these reduced kernels is demonstrated
by means of a numerical test problem in the next section of
the paper.
The nonlinear separating surface corresponding to the ker-
nel K(A, A') [20, Equation (8.1)] and can be deduced from




80

the linear separating surface (5) and w = A'Du from (12)
as follows:

x'w - "7= x' A' Du - "7= O.
(26)

If we replace x'A' by the corresponding kernel expression
K(x', A'), and substitute from (23) for u and 7: u = DK'Dv
and "7 = -e'Dv we obtain the nonlinear separating surface:

K(x', A')Du
-
"7
=
K(x', A')DDK(A, A')'Dv + e'Dv
=
(K(x', A')K(A, A')' + e')Dv = O.
(27)

The corresponding nonlinear classifier to this nonlinear sep-
arating surface is then:



(K(x', A')K(A, A')' + e')Dv
!0,
thenx6A+,

0, then x 6 A-,

0, thenxEA+orxEA-.

(28)

We now give an explicit statement of our nonlinear classifier
algorithm.

ALGORITHM 3.1. Nonlinear Proximal SVM Given m
data points in R n represented by the m x n matrix A and a
diagonal matrix D of:h1 labels denoting the class of each row
of A, we generate the nonlinear classifier (28) as follows:

(i) Choose a kernel/unction K(A, A'), typically the Gaus-
sian kernel (1).

5i) Define G by (25) where K = K(A,A') and e is an
m × 1 vector of ones.
Compute v by (24) for some
positive ~. (Typically ~, is chosen by means of a tuning
set.)

(iii) The nonlinear surface (27) with the computed v con-
stitutes the nonlinear classifier (28) for classifying a
new point x.

The nonlinear classifier (28), which is a direct generalization
of the linear classifier (7), works quite effectively as indicated
by the numerical examples presented in the next section.


4.
NUMERICAL IMPLEMENTATION
AND COM-

PARISONS

Most of our computations were performed on the Univer-
sity of Wisconsin Data Mining Institute "locopl" machine,
which utilizes a 400 Mhz Pentium II and allows a maximum
of 2 Gigabytes of memory for each process. This computer
runs on Windows NT server 4.0, with MATLAB 6 installed.
Even though "locopl" is a multiprocessor machine, only one
processor was used for all the experiments since MATLAB
is a single threaded application and does not distribute any
load across processors [26]. Our algorithms require the so-
lution of a single square system of linear equations of the
size of the number of input attributes n in the linear case,
and of the size of the number of data points m in the non-
linear case. When using a rectangular kernel [18], the size
of the problem can be reduced from m to k with k << m
for the nonlinear case. Because of the simplicity of our algo-
rithm, we give below the actual MATLAB implementation
that was used in our experiments and which consists of 6
lines of native MATLAB code:
--4




--6




--8 8
I
t




I
I
[
I
I
I
I
J
--6
--4
--2
0
2
4
6
8




Figure 3: The spiral dataset consisting of 97 black
points and 97 white points intertwined as two spirals
in 2-dimensional space. PSVM with a Gaussian ker-
nel generated a sharp nonlinear spiral-shaped sepa-
rating surface.

CODE 4.1. PSVM MATLAB Code


function
[w,gamma]
= psvm(A,D,nu)
~, PSVM:linear
and nonlinear
classification
Z INPUT:
A, D, nu.
OUTPUT:
w, gamma
Y, [w, gamma]
= psvm(A,D,nu);
fro,n] =size (A) ;e=ones (m, i) ;H=D* [A -e] ;
r=sum(H) ' ; Y,r=H'*e;
r=(speye(n+l)/nu+H'*H)\r;
~,solvs (I/nu+H'*H)r=H'*e
u=nu* (I- (H,r)) ; s=D*u;
w= (s'*A) ';
Zw=A'*D*u
gamma=-sum (s) ;
Y,gamma=-e '*D*u




Note that the command line in the MATLAB code above:
r= (speye (n+1)/nu+H' *H) ~ computes directly the factor (~+
H'H)-IH'e of (15). This is much more economical and sta-
ble than computing the inverse z
,
-1
(~ + H H)
explicitly then
multiplying it by H'e. The calculations H'e and A's involve
the transpose of typically large matrices which can be time
consuming. Instead, we calculate r=sum(H) ' and w=(s'*A) '
respectively, the transposes of these vectors.
We further note that the MATLAB code above not only
works for a linear classifier, but also for a nonlinear classifier
as well. In the nonlinear case, the matrix K(A, A') is used
as input instead of A, and the pair (~,'7), where ~t = Du, is
returned instead of (w, '7). The nonlinear separating surface
is then given by (27) as:

K(x, A')~ - "7= O.

Rectangular kernels [17] can also be handled by this code.
The input then is the rectangular matrix K(A, A') , where
6 Rre×k, k << m and the given output is the pair (~2,'7)
with fi 6 R k and ~ =/9~, where b and ~ are the D and u
associated with the reduced matrix .4.
A final note regarding a further simplification of PSVM.
If we substitute the expression (15) for u in (12), we obtain
after some algebra the following simple expression for w and




81

3" in terms of the problem data:
[:]'
= (~ + E'E)-IE'De,
(29)


where E = DH and hence H = DE. Thus:

E=DH=[A
-e],
and H=DE=D[A
-el.
(30)

This direct explicit solution of our PSVM problem (9) can
be written as the following single line of MATLAB code,
which also does not perform the explicit matrix inversion
I
t
--1
(~ + E E)
, and is slightly faster than the above MATLAB
code:

r=(I/nu+E'*E) \ (diag(D) '*E) ' ;w=r(1 :n) ;gamma=r (n+i) ;
(31)

Here, according to MATLAB commands, diag(D) is an rn × 1
vector generated from the diagonal of the matrix D. Com-
putational testing results using this one-line MATLAB code
(31) are slightly better than those obtained with Code 4.1
and are the ones reported in the tables below. We comment
further that the solution (29) can also be obtained directly
from (9) by using the equality constraint to eliminate y from
the problem and solving the resulting unconstrained mini-
mization problem in the variables w and 3"by setting to zero
the gradients with respect to w and 3'- We turn now to our
computations.
The datasets used for our numerical tests were the follow-
ing:

· Seven publicly available datasets from the UCI Ma-
chine Learning Repository [28]: WPBC, Ionosphere,
Cleveland Heart, Pima Indians, BUPA Liver, Mush-
room, Tic-Tac-Toe.

· The Census dataset is a version of the US Census Bu-
reau "Adult" dataset, which is publicly available from
the Silicon Graphics website [4].

· The Galaxy Dim dataset used in galaxy discrimination
with neural networks from [30]

· Two large datasets (2 million points and 10 attributes)
created using David Musicant's NDC Data Generator
[29].

· The Spiral dataset proposed by Alexis Wieland of the
MITRE Corporation and available from the CMU Ar-
tificial Intelligence Repository [37].

We outline our computational results now in five groups
as follows.

1. Table 1: Comparison of seven different meth-
ods on the Adult dataset
In this experiment we
compared the performance of seven different methods
for linear classification on different sized versions of
the Adult dataset. Reported results on the SOR [22],
SMO [311and SVMl'ght [161are from [221. Results for
LSVM [24] results were computed here using "locopl',
whereas SSVM [18] and RaP [2] are from [18]. The
SMO experiments were run on a 266 MHz Pentium II
processor under Windows NT 4 using Microsoft's Vi-
sual C++ 5.0 compiler. The SOR experiments were
run on a 200 MHz Pentium Pro with 64 megabytes
of RAM, also under Windows NT 4 and using Visual
C++ 5.0. The SVM light experiments were run on the
same hardware as that for SOR, but under the So-
laris 5.6 operating system.
Bold type indicates the
best result and a dash (-) indicates that the results
were not available from [22]. Although the timing com-
parisons are approximate because of the different ma-
chines used, they do indicate that PSVM has a distinct
edge in speed, e.g. solving the largest problem in 7.4
seconds, which is much faster than any other method.
Times and ten-fold testing correctness are shown in
Table 1. Times are for the ten-folds.

2. Table 4: Comparative performances of LSVM
[24] and PSVM on a large dataset

Two large datasets consisting of 2 million points and
10 attributes were created using the NDC Data Gen-
erator [29]. One of them is called NDC-easy because it
is highly linearly separable (around 90%). The other
one is called NDC-hard since it has linear separability
of around 70%. As is shown in Table 4 the linear clas-
sifters obtained using both methods performed almost
identically. Despite the 2 million size of the datasets,
PSVM solved the problems in about 20 seconds each
compared to LSVM's times of over 650 seconds. In
contrast, SVMl~gh~ [16] failed on this problem [24].

3. Table 3:
Comparison of PSVM~ SSVM and
LSVM and SVM light, using a Linear Classifier

In this experiment we compared four methods: PSVM,
SSVM, LSVM and SVM light on seven publicly avail-
able datasets from UCI Machine Learning Repository
[28] and [30]. As shown in Table 3, the correctness of
the four methods were v~ry similar but the execution
time including ten-fold cross validation for PSVM was
smaller by as much as one order of magnitude or more
than the other three methods tested.
Since LSVM,
SSVM and PSVM are all based on similar formula-
tions of the classification problem the same value of v
was used for all of them. For SVM light the trade-off
between trading error and margin is represented by a
parameter C. The value of C was chosen by tuning.
A paired t-test [27] at 95% confidence level was per-
formed to compare the performance of PSVM and the
other algorithms tested. The p-values obtained show
that there is no significant difference between PSVM
and the other methods tested.

4. Figure 3: PSVM on the Spiral Dataset

We used a Gaussian kernel in order to classify the spi-
ral dataset. This dataset consisting of 194 black and
white points intertwined in the shape of a spiral is a
synthetic dataset [37]. However, it apparently is a diffi-
cult test case for data mining algorithms and is known
to give neural networks severe problems [15]. In con-
trast, a sharp separation was obtained using PSVM as
can be seen in Figure 3.

5. Table 2: Nonlinear Classifier Comparison using
PSVM, SSVM and LSVM

For this experiment we chose four datasets from the
UCI Machine Learning Repository for which it is known
that a nonlinear classifier performs significantly better




82

that a linear classifier. We used PSVM, SSVM and
LSVM in order to find a Gaussian-kernel-based non-
linear classifier to classify the data.
In all datasets
tested, the three methods performed similarly as far
as ten-fold cross validation is concerned. However, ex-
ecution time of PSVM ~vas much smaller than that
of other two methods. Note that for the mushroom
dataset that consists of m = 8124 points with n -- 22
attributes each, the square 8124 x 8124 kernel matrix
does not fit into memory. In order to address this prob-
lem, we used a rectangular kernel with A E R215xs12a
instead, as described in [17]. In general, our algorithm
performed particularly well with a rectangular kernel
since the system solved is of size k × k, with k << m
and where k is the much smaller number of rows of J].
In contrast with a full square kernel matrix the system
solved is of size m x m. A paired t-test [27] at 95%
confidence level was performed to compare the perfor-
mance of PSVM and the other algorithms tested. The
p-values obtained show that there is no significant dif-
ference between PSVM and the other methods tested
as far as ten-fold testing correctness is concerned.


5,
CONCLUSION AND FUTURE WORK

We have proposed an extremely simple procedure for gen-
erating linear and nonlinear classifiers based on proximity to
one of two parallel planes that are pushed as far apart as pos-
sible. This procedure, a proximal support vector machine
(PSVM), requires nothing more sophisticated than solving
a simple nonsingular system of linear equations, for either a
linear or nonlinear classifier. In contrast, standard support
vector machine classifiers require a more costly solution of a
linear or quadratic program. For a linear classifier, all that
is needed by PSVM is the inversion of a small matrix of the
order of the input space dimension, typically of the order of
100 or less, even if there are millions of data points to clas-
sify. For a nonlinear classifier, a linear system of equations
of the order of the number of data points needs to be solved.
This allows us to easily classify datasets with as many as a
few thousand of points. For larger datasets, data selection
and reduction methods such as [11, 17, 12] can be utilized as
indicated by some of our numerical results and will be the
subject of future work. Our computational results demon-
strate that PSVM classifiers obtain test set correctness sta-
tistically comparable to that of standard of SVM classifiers
at a fraction of the time, sometimes an order of magnitude
less.
Another avenue for future research is that of incremen-
tal classification for large datasets. This appears particu-
larly promising in view of the very simple explicit solutions
(15) and (24) for the linear and nonlinear PSVM classifiers
that can be updated incrementally as new data points come
streaming in.
To sum up, the principal contribution of this work, is a
very efficient classifier that requires no specialized software.
PSVM can be easily incorporated into all sorts of data min-
ing applications that require a fast, simple and effective clas-
sifter.


Acknowledgements

The research described in this Data Mining Institute Report
00-02, February 2001, was supported by National Science
Foundation Grants CCR-9729842 and CDA-9623632, by Air
Force Office of Scientific Research Grant F49620-00-1-0085
and by the Microsoft Corporation. We are grateful to Pro-
fessor C.-J. Lin of National Taiwan University who pointed
out reference [33], upon reading the original version of this
paper. Least squares are also used in [33] to construct an
SVM, but with the explicit requirement of Mercer's positive
definiteness condition [35], which is not needed here. Fur-
thermore, the objective function of the quadratic program
of [33] is not strongly convex like ours. This important fea-
ture of PSVM influences its speed as evidenced by the many
numerical comparisons given here but not in [33].


6.
REFERENCES
[1] E. Anderson, Z. Bai, C. Bischof, S. Blackford,
J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum,
S. Hammarling, A. McKenney, and D. Sorensen.
LAPACK User's Guide. SIAM, Philadelphia,
Pennsylvania, third edition, 1999.
http://www.netlib.org/lapack/.
[2] K. P. Bennett and O. L. Mangasarian. Robust linear
programming discrimination of two linearly
inseparable sets. Optimization Methods and Software,
1:23-34, 1992.
[3] P. S. Bradley and O. L. Mangasarian. Massive data
discrimination via linear support vector machines.
Optimization Methods and Software, 13:1-10, 2000.
ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-
03.ps.
[4] US Census Bureau. Adult dataset. Publicly available
from: www.sgi.com/Technology/mlc/db/.
[5] C. J. C. Burges. A tutorial on support vector
machines for pattern recognition. Data Mining and
Knowledge Discovery, 2(2):121-167, 1998.
[6] V. Cherkassky and F. Mulier. Learning from Data -
Concepts, Theory and Methods. John Wiley & Sons,
New York, 1998.
[7] CPLEX Optimization Inc., Incline Village, Nevada.
Using the CPLEX(TM) Linear Optimizer and
CPLEX(TM) Mixed Integer Optimizer (Version 2.0),
1992.
[8] T. Evgeniou, M. Pontil, and T. Poggio. Regularization
networks and support vector machines. Advances in
Computational Mathematics, 13:1-50, 2000.
[9] T. Evgeniou, M. Pontil, and T. Poggio. Regularization
networks and support vector machines. In A. Smola,
P. Bartlett, B. Sch51kopf, and D. Schuurmans, editors,
Advances in Large Margin Classifiers, pages 171-203,
Cambridge, MA, 2000. MIT Press.
[10] M. C. Ferris and T. S. Munson. Interior point methods
for massive support vector machines. Technical Report
00-05, Computer Sciences Department, University of
Wisconsin, Madison, Wisconsin, May 2000.
ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/00-05.ps.
[11] G. Fung and O. L. Mangasarian. Data selection for
support vector machine classification. In
R. Ramakrishnan and S. Stolfo, editors, Proceedings
KDD2000: Knowledge Discovery and Data Mining,
August 20-23, 2000, Boston, MA, pages 64-70, New
York, 2000. Asscociation for Computing Machinery.
ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/00-02.ps.
[12] G. Fung, O. L. Mangasarian, and A. Smola. Minimal




83

kernel classifiers. Technical Report 00-08, Data Mining
Institute, Computer Sciences Department, University
of Wisconsin, Madison, Wisconsin, November 2000.
ftp:/ / ftp.cs,wisc.edu/pub/ dmi/ tech-reports/ O0-O8.ps.
[13] D. Gale. The Theory of Linear Economic Models.
McGraw-Hill Book Company, New York, 1960.
[14] G. H. Golub and C. F. Van Loan. Matrix
Computations. The John Hopkins University Press,
Baltimore, Maryland, 3rd edition, 1996.
[15] J. Gracke, M. Griebel, and M. Thess. Data mining
with sparse grids. Technical report, Institut f/ir
Angrwandte Mathematik, Universita£t Bonn, Bonn,
Germany, 2000. http://wissrech.iam.uni-
bonn.de/research/projects/garcke/sparsemining.html.
[16] T. Joachims. Making large-scale support vector
machine learning practical. In B. SchSlkopf, C. J. C.
Burges, and A. J. Smola, editors, Advances in Kernel
Methods - Support Vector Learning, pages 169-184.
MIT Press, 1999.
[17] Y.-J. Lee and O. L. Mangasarian. RSVM: Reduced
support vector machines. Technical Report 00-07,
Data Mining Institute, Computer Sciences
Department, University of Wisconsin, Madison,
Wisconsin, July 2000. Proceedings of the First SIAM
International Conference on Data Mining, Chicago,
April 5-7, 2001, CD-ROM.
ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/00-07.ps.
[18] Yuh-Jye Lee and O. L. Mangasarian. SSVM: A smooth
support vector machine. Technical Report 99-03, Data
Mining Institute, Computer Sciences Department,
University of Wisconsin, Madison, Wisconsin,
September 1999. Computational Optimization and
Applications 20(1), October 2001, to appear.
ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/99-03.ps.
[19] O. L. Mangasarian. Nonlinear Programming. SIAM,
Philadelphia, PA, 1994.
[20] O. L. Mangasarian. Generalized support vector
machines. In A. Smola, P. Bartlett, B. SchSlkopf, and
D. Schuurmans, editors, Advances in Large Margin
Classifiers, pages 135:146, Cambridge, MA, 2000.
MIT Press. ftp://ftp.cs.wisc.edu/math-prog/tech-
reports/98-14.ps.
[21] O. L. Mangasarian and R. R. Meyer. Nonlinear
perturbation of linear programs. SIAM Journal on
Control and Optimization, 17(6):745-752, November
1979.
[22] O. L. Mangasarian and D. R. Musicant. Successive
overrelaxation for support vector machines. IEEE
Transactions on Neural Networks, 10:1032-1037, 1999.
ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-
18.ps.
[23] O. L. Mangasarian and D. R. Musicant. Active
support vector machine classification. Technical
Report 00-04, Data Mining Institute, Computer
Sciences Department, University of Wisconsin,
Madison, Wisconsin, April 2000. Machine Learning, to
appear.
ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/OO-O4.ps.
[24] O. L. Mangasarian and D. R. Musicant. Lagrangian
support vector machines. Technical Report 00-06,
Data Mining Institute, Computer Sciences
Department, University of Wisconsin, Madison,
Wisconsin, June 2000. Journal of Machine Learning
Research, to appear.
ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/00-06.ps.
[25] O. L. Mangasarian and T.-H. Shiau. Lipschitz
continuity of solutions of linear inequalities, programs
and complementarity problems. SIAM Journal on
Control and Optimization, 25(3):583-595, May 1987.
[26] MATLAB. User's Guide. The MathWorks, Inc.,
Natick, MA 01760, 1994-2001.
http://www.mathworks.com.
[27] T. M. Mitchell. Machine Learning. McGraw-Hill,
Boston, 1997.
[28] P. M. Murphy and D. W. Aha. UCI repository of
machine learning databases, 1992.
www.ics.uci.edu/~mlearn/MLRepository.html.
[29] D. R. Musicant. NDC: normally distributed clustered
datasets, 1998.
www.cs.wisc.edu/~-.musicant/data/ndc/.
[30] S. Odewahn, E. Stockwell, R. Pennington,
R. Humphreys, and W. Zumach. Automated
star/galaxy discrimination with neural networks.
Astronomical Journal, 103(1):318-331, 1992.
[31] J. Platt. Sequential minimal optimization: A fast
algorithm for training support vec¢or machines. In
B. SchSlkopf, C. J. C. Burges, and A. J. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning, pages 185-208. MIT Press, 1999.
http://www.research.microsoft.com/~jplatt/smo.html.
[32] A. Smola, P. L. Bartlett, B. SchSlkopf, and
J. Schiirmann (editors). Advances in Large Margin
Classifiers. MIT Press, Cambridge, MA, 2000.
[33] J. A. K. Suykens and J. Vandewalle. Least squares
support vector machine classifiers. Neural Processing
Letters, 9(3):293-300, 1999.
[34] A. N. Tikhonov and V. Y. Arsenin. Solutions of
Ill-Posed Problems. John Wiley ~: Sons, New York,
1977.
[35] V. N. Vapnik. The Nature of Statistical Learning
Theory. Springer, New York, 1995.
[36] V. N. Vapnik. The Nature of Statistical Learning
Theory. Springer, New York, second edition, 2000.
[37] Alexis Wieland. Twin spiral dataset, http://www-
cgi.cs.cmu.edu/afs/cs.cmu.edu/project/ai-
repository/ai/areas/neural/bench/cmu/0.html.




84

Table 1: Testing set correctness
and running
times on the larger Adult
dataset
obtained
by seven different methods
using a linear classifier. Timing
compar-
isons are approximate
because
of the
different
machines
used,
but
they
do
indicate that
PSVM
has a distinct
edge, e.g.
solving the largest problem
in
7.4 seconds, much faster than
any other
method.
Best results
are shown
in
bold.



Dataset size

(Training, Testing)

n--no,
of attributes
PSVM I LSVM [ SSVM I
i
i
i

(1605, 30957)
84.00
84.27
84.27
n = 123
0.3
3.3
1.9
(2265, 30297)
84.13
84.68
84.57
n -- 123
0.5
5.0
2.8
(3185, 29377)
84.25
84.55
84.63
n --- 123
0.7
8.1
3.9
(4781, 27781)
84.35
84.55
84.55
n ~- 123
1.2
8.1
6.0
(6414, 26148)
84.49
84.68
84.60
n = 123
1.6
18.8
8.1
(11221, 21341)
84.48
84.84
84.79
n = 123
2.5
38.9
14.1
(16101, 16461)
84.78
85.01
84.96
n -- 123
3.7
60.5
21.5
(22697, 9865)
85.16
85.35
85.35
n --- 123
5.2
92.0
29.0
(32562, 16282)
84.56
85.05
85.02
n -- 123
7.4
140.9
44.5
Testing Correctness %
Running Time Sec.

Method

SOR

84.06
0.3
84.24
1.2
84.23
1.4
84.28
1.6
84.30
4.1
84.37
18.8
84.62
24.8
85.06
31.3
84.96
83.9
I SMO ] SVM '~gnt ] RLP

84.25
78.68
0.4
5.4
9.9
84.43
77.19
0.9
10.8
19.12
84.40
77.83
1.8
21.0
80.1
84.47
79.15
3.6
43,2
88.6
84.43
71.85
5.5
87.6
218.8
84.68
60.00
17.0
306.6
449.2
84.83
72.52
35.3
667.2
632.6
85.17
77.43
85.7
1425.6
991.9
85.05
83.25
163.6
2184.0
1561.1



Table 2: PSVM,
SSVM
and LSVM
training
and ten-fold testing cor-
rectness
and
running
times
using
a nonlinear
classifier.
Execution
times include ten-fold training.
Same
value of v was used in all the
methods.
Best results are in bold.

Data Set
PSVM
SSVM
LSVM
m × n
Train
Train
Train
Test
Test
Test
Time (Sec.)
Time (Sec.)
Time (Sec.)
p-value *
p-value*
Ionosphere
96.5%
97.0 %
97.0 %
351 x 34
95.2%
95.8 %
95.8%
4.60
25.25
14.58
0.71
0.71
BUPA Liver
345 x 6



Tic-Tac-Toe
958 x 9
75.7%
73.6%
4.34

98.0%
98.4%
74.95
75.8%
73.7"%
20.65
0.97
98.0%
98.4%
395.30
I
75.8%
73.7%
30.75
0.97
98.2%
94.7%
350.64
1
Mushroom **
88.0%
89.0%
87.6
8124 x 22
88.0%
88.8%
87.8
35.50
307.66
503.74
0.09
0.79


* Paired t-test p-values are calculated for each method relative to PSVM for
ten-fold test correctness.
** A Rectangular kernel [18] of the size 8124 x 215 was used here instead of the
square 8124 x 8124 kernel which does not fit into memory.




85

Table 3: PSVM,
SSVM, LSVM and SVM ugm training and ten-fold
testing correctness and running times using a linear classifier. Execu-
tion times include ten-fold training.
Same value of u was used in all
the methods. The value of the parameter C in SVM ught was chosen by
tuning. Best results are in bold.

Data Set
PSVM
SSVM
LSVM
SVMught
m x n
Train
Train
Train
Train
Test
Test
Test
Test
Time (Sec.)
Time (Sec.)
Time (Sec.)
Time (Sec.)
p-value *
p-value*
p-vMue*
WPBC (60 mo.)
110 x 32



Ionosphere
351 x 34



Cleveland Heart
297 x 13



Pima Indians
768 x 8



BUPA Liver
345 × 6



Galaxy Dim
4192 x 14
70.8%
68.5%
0.02

90.7%
87.3%
0.17


87.0%
85.9%
0.01


77.9%
77.5%
0.02


70.1%
69.4%
0.02


93.7%
93.5%
0.34
70.8%
68.5%
0.17
1
94.3 %
88.7 %
1.23
0.55
87.3%
86.2%
0.7
0.91
78.2%
77.6%
0.78
0.95
70.2%
70.0%
0.78
0.84
95.0%
95.0%
5.21
4 x 10-4
70.8%
68.5%
0.53
1
94.4 %
88.7 %
1.40
0.55
87.3%
86.2%
0.78
0.91
78.2%
77.6%
2.18
0.95
70.2%
70.0%
2.18
0.72
95.0%
95.0%
21.56
4
x
10-4
62.7%
62.7%
3.85
0.30
91.4 %
88.0 %
2.19
0.71
87.7%
86.5 %
1.44
0.80
77.0 %
76.4 %
37.00
0.59
70.6%
69.5%
6.65
0.96
94.2 %
94.1 ~o
28.33
0.14
Mushroom
81.0%
81.7%
81.7%
81.5%
8124 x 22
81.0%
81.5%
81.5%
81.5%
1.15
11.73
61.62
145.59
0.49
0.49
0.48'


* Paired t-test p-values are calculated for each method relative to PSVM for ten-fold test correctness.




Table 4: LSVM and PSVM performance on two, 2 million point NDC datasets
with 10-attributes. A linear classifier with parameter
v = 0.1 was used in both
methods on the same locopl machine. SVM light failed to solve this problem.


Method
Dataset


LSVM ] NDC-easy
PSVM
NDC-easy

LSVM
PSVM
Training
Correctness %

90.86
91.23
90.80
91.13

NDC-hard
69.80
69.44
NDC-hard
69.84
69.52
Testing
Time
Correctness %
(CPU) sec.

658.5
20.8

655.6
20.6




85

