Efficient Elastic Burst Detection in Data Streams





Yunyue Zhu
Department of Computer Science
Courant Institute of Mathematical Sciences
New York University, New York, NY, 10012
yunyue@cs.nyu.edu
Dennis Shasha
Department of Computer Science
Courant Institute of Mathematical Sciences
New York University, New York, NY, 10012
shasha@cs.nyu.edu



ABSTRACT
Burst detection is the activity of finding abnormal aggre-
gates in data streams. Such aggregates are based on sliding
windows over data streams. In some applications, we want
to monitor many sliding window sizes simultaneously and to
report those windows with aggregates significantly different
from other periods. We will present a general data struc-
ture for detecting interesting aggregates over such elastic
windows in near linear time. We present applications of the
algorithm for detecting Gamma Ray Bursts in large-scale
astrophysical data. Detection of periods with high volumes
of trading activities and high stock price volatility is also
demonstrated using real time Trade and Quote (TAQ) data
from the New York Stock Exchange (NYSE). Our algorithm
beats the direct computation approach by several orders of
magnitude.


Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications--
Data Mining


Keywords
data stream, elastic burst


1. INTRODUCTION
Consider the following application that motivates this re-
search. An astronomical telescope, Milagro[1] was built in
New Mexico by a group of prestigious astrophysicists from
the Los Alamos National Laboratory and many universities.
This telescope is actually an array of light-sensitive detectors
covering a large pool of water about the size of a football
field. It is used to constantly observe high-energy photons
from the universe. When many photons observed, the sci-
entists assert the existence of a Gamma Ray Burst. The
scientists hope to discover primordial black holes or com-
pletely new phenomena by the detection of Gamma Ray


Work supported in part by U.S. NSF grants IIS-9988636,
MCB-0209754 and N2010-0115586.




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGKDD '03, August 24-27, 2003, Washington, DC, USA.
Copyright 2003 ACM 1-58113-737-0/03/0008...$5.00.
Bursts. The occurrences of Gamma Ray Bursts are highly
variable, flaring on timescale of minutes to days. Once such
a burst happens, it should be reported immediately. Other
telescopes could then point to that portion of sky to confirm
the new astrophysical event. The data rate of the observa-
tion is extremely high. Hundreds of photons can be recorded
within a second from a tiny spot in the sky[8, 9].

There are also many applications in data stream mining and
monitoring when people are interested in discovering time
intervals with unusually high numbers of events. For exam-
ple:


· In telecommunication, a network anomaly might be in-
dicated if the number of packages lost within a certain
time period exceeds some threshold.

· In finance, stocks with unusually high trading vol-
umes would attract the notice of the traders (or regu-
lators). Also stocks with unusually high price fluctua-
tions within a short time period provide more opportu-
nity of speculation. Therefore they would be watched
more closely.


Intuitively, given an aggregate function F (such as sum or
count), the problem of interest is to discover subsequences
s of a time series stream such that F(s) >> F(s ) for most
subsequences s of size |s|. In the case of burst detection,
the aggregate is sum. If we know the duration of the time
interval, we can maintain the sum over sliding windows of
a known window size and sound an alarm when the moving
sum is above a threshold. Unfortunately, in many cases, we
cannot predict the length of the burst period. In fact, dis-
covering that length is part of the problem to be solved. In
the above example of Gamma Ray Burst detection, a burst
of photons associated with a special event might last for a
few milliseconds, a few hours, or even a few days. There are
different thresholds associated with different durations. A
burst of 10 events within 1 second could be very interesting.
At the same time, a burst that lasts longer but with lesser
density of events, say 50 events within 10 seconds, could be
of interest too.

Suppose that we want to detect bursts for a time series of size
n and we are interested in all the n sliding window sizes. A
brute-force search has to examine all the sliding window sizes
and starting positions. Because there are O(n2) windows,
the lower bound of the time complexity is O(n2). This is




336

very slow for many applications. Fortunately, because we
are interested only in those few windows that experience
bursts, it is possible to design a nearly linear time algorithm.
In this paper we present a burst detection algorithm with
time complexity approximately proportional to the size of
the input plus the size of the output, i.e. the number of
windows with bursts.


1.1 Problem Statement
There are two categories of time series data stream moni-
toring: point monitoring and aggregate monitoring. In point
monitoring, the latest data item in the stream is of interest.
When the latest item falls in some predefined domain, an
alarm would be sounded. For example, a stock trader who
places a limited sell order on Enron informs the stock price
stream monitor to raise an alarm (or automatically sell the
stock) once the price of stock fall under $10 to avoid further
losses. Since only the latest data in the stream need to be
considered, point monitoring can be implemented without
much effort.

Aggregate monitoring is much more challenging. Aggregates
of time series are computed based on certain time intervals
(windows). There are three well-known window models that
are the subjects of many research projects [10, 11, 12, 22].


1. Landmark windows: Aggregates are computed based
on the values between a specific time point called the
landmark and the present. For example, the average
stock price of IBM from Jan 1st, 2003 to today is based
on a landmark window.

2. Sliding windows: In a sliding window model, aggre-
gates are computed based on the last w values in the
data stream. The size of a sliding window w is prede-
fined. For example, the running maximum stock price
of IBM during the previous 5 days is based on sliding
windows.

3. Damped window: In a damped window model the
weights of data decrease exponentially into the past.
For example, the damping moving average avgnew af-
ter a new data item x is inserted can be updated as
follows:

avgnew = avgold  p + x  (1 - p),0 < p < 1


The sliding window model is the most widely used in data
stream monitoring. Motivated by the Gamma Ray example,
we have generalized this to the elastic window model. In
an elastic window model, the user needs to specify only the
range of the sliding window sizes, the aggregate function and
alarms domains, and will be notified of all those window sizes
in the range with aggregates falling in the corresponding
alarm domains.

Here we give the formal definition of the problem of moni-
toring data stream over elastic windows.


Problem 1. For a time series x1,x2,...,xn, given a set
of window sizes w1,w2,...,wm, an aggregate function F and
threshold associated with each window size, f(wj),j = 1,2,...,m,
monitoring elastics window aggregates of the time series is
to find all the subsequences of all the window sizes such that
the aggregate applied to the subsequences cross their window
sizes' thresholds, i.e.

i  1..n,j  1..m,s.t. F(x[i .. i+wj-1])  f(wj)


The threshold above can be estimated from the historical
data or the model of the time series. Elastic burst detec-
tion is a special case of monitoring data streams on elastic
windows. In elastic burst detection, the alarm domain is
[f(wj),). Note that it is also possible for the alarm do-
main to be (-,f(wj)].

1.2 Our Contributions
The contributions of the paper are as follows.


· We introduce the concept of monitoring data streams
on elastic windows and show several important appli-
cations of this model.

· We design an innovated data structure, called the Shifted
Wavelet Tree, for efficient elastic burst monitoring.
This data structure is applied to general aggregate
monitoring and burst detection in higher dimensions.

· We apply our algorithm to real world data including
the Milagro Gamma Ray data stream, NYSE real time
tick data and text data. Our method is up to several
magnitudes faster than a direct computation, which
means that a multi-day computation can be done in a
matter of minutes.

2. DATA STRUCTURE AND ALGORITHM
In this section, we first give some background on the wavelet
data structure. In section 2.2 we discuss the Shifted Wavelet
Tree and the algorithm for efficient elastic burst detection
in an offline setting. This is extended to a streaming algo-
rithm in section 2.3. Our algorithm will also be generalized
to other problems in data stream monitoring over elastic
windows in section 2.4 and to higher dimensions in section
2.5.

2.1 Wavelet Data Structure
In wavelet analysis, the wavelet coefficients of a time series
are maintained in a hierarchical structure. Let us consider
the simplest wavelet, the Haar wavelet. For simplicity of
notation, suppose that the size of time series n is a power
of two. This would not affect the generality of the results.
The original time series makes up of level 0 in a wavelet tree.
The pair wise (normalized) averages and differences of the
adjacent data items at level 0 produce the wavelet coeffi-
cients at level 1. The process is repeated for the averages at
level i to get the averages and differences at level i + 1 un-
til there is only one average and difference at the top level.
Table 1 shows the process in computing the Haar wavelet
decomposition. The Haar wavelet coefficients include the
average in the highest level and the differences in each level.
From these wavelet coefficients, the original time series can
be constructed without loss of information. Usually a few
wavelet coefficients can represent the trend of the time se-
ries, and they are selected as a compact representation of
the original time series.




337

Table 1: Haar Wavelet decomposition
Level 3
a1+a2+a3+a4+a5+a6+a7+a8
2

2
(a1+a2+a3+a4)-(a5+a6+a7+a8)
2

2
Level 2
a1+a2+a3+a4
2
a5+a6+a7+a8
2
(a1+a2)-(a3+a4)
2
(a5+a6)-(a7+a8)
2
Level 1
a1+a2

2
a3+a4

2
a5+a6

2
a7+a8

2
a1-a2

2
a3-a4

2
a5-a6

2
a7-a8

2
Level 0
a1
a2
a3
a4
a5
a6
a7
a8




Level 3



Level 2



Level 1
Level 4
Level 5




Level 0
Level 4




Level 1
Level 2
Level 5




Level 3




Level 0




Figure 1: (a)Wavelet Tree (left) and (b)Shifted Wavelet Tree(right)


The wavelet coefficients above can also be viewed as the
aggregates of the time series at different time intervals. Fig-
ure 1-a shows the time interval hierarchy in the Haar wavelet
decomposition. At level i, there are n2-
i
consecutive win-
dows with size 2i. All the windows at the same level are
disjoint. The aggregates that the Haar wavelet maintains
are the (normalized) averages and differences. In our dis-
cussion of burst detection, the aggregate of interest is the
sum. Obviously, such a wavelet tree can be constructed in
O(n) time.

The first few top levels of a wavelet tree yield concise multi-
resolution information of the time series. This gives the
wavelet lots of applications. However, for our purpose of
burst detection, such a data structure has a serious problem.
Because the windows at the same level are non-overlapping,
a window of arbitrary start position and arbitrary size might
not be included in any window in the wavelet tree, except
the window at the highest level that includes everything.
For example, the window consisting of three time points in
the middle, (n/2 - 1,n/2,n/2 + 1), is not contained in any
window in the wavelet tree except the largest one. This
makes wavelets inconvenient for the discovery of properties
of arbitrary subsequences.



2.2 Shifted Wavelet Tree
In a shifted wavelet tree (SWT) (figure 1-b), the adjacent
windows of the same level are half overlapping. In figure 1,
we can see that the size of a SWT is approximately double
that of a wavelet tree, because at each level, there is an addi-
tional "line" of windows. These additional windows provide
valuable overlapping information for the time series. They
can be maintained either explicitly or implicitly. If we keep
only the aggregates for a traditional wavelet data structure,
the aggregates of the overlapping windows at level i can be
computed from the aggregates at level i - 1 of the wavelet
data structure.
Given :
x[1..n],n=2a
Return:
shifted wavelet tree SWT[1..a][1..]

b=x;
FOR i = 1 TO a //remember a = log2 n
//merge consecutive windows and form
//level i of the shifted wavelet tree
FOR j = 1 TO size(b)-1 STEP 2
SWT[i][j]=b[j]+b[j+1];
ENDFOR
//downsample, retain a non-overlapping cover
FOR j = 1 TO size(SWT[i])/2
b[j]=SWT[i][2*j-1];
ENDFOR
ENDFOR


Figure 2: Algorithm to construct shifted wavelet
tree



To build a SWT, we start from the original time series and
compute the pair wise aggregate (sum) for each two consec-
utive data items. This will produce the aggregates at level
1. A downsampling on this level will produce the input for
the higher level in the SWT. Downsampling is simply sam-
pling every second item in the series of aggregates. In figure
1-b, downsampling will retain the upper consecutive non-
overlapping windows at each level. This process is repeated
until we reach the level where a single window includes every
data point. Figure 2 gives a pseudo-code to build a SWT.
Like regular wavelet trees, the SWT can also be constructed
in O(n) time.

For a subsequence starting and ending at arbitrary positions,
there is always a window in the SWT that tightly includes
the subsequence as figure 3 shows and the following lemma
proves.




338

Figure 3: Examples of the windows that include subsequences in the shifted wavelet tree


Lemma 1. Given a time series of length n and its shifted
wavelet tree, any subsequence of length w,w  2i is included
in one of the windows at level i + 1 of the shifted wavelet
tree.


Proof. The windows at level i+1 of the shifted wavelet
tree are:

[(j-1)2i+1 .. (j+1)2i],j = 1,2,...,
n
2i
- 1.

A subsequence with length 2i starting from an arbitrary po-
sition c will be included in at least one of the above windows,
because

[c .. c+2i-1]  [(j-1)2i+1 .. (j+1)2i],j =
c - 1
2i
+ 1.

Any subsequence with length w,w  2i is included in some
subsequence(s) with length 2i, and therefore is included in
one of the windows at level i+1. We say that windows with
size w,2i
-1
< w  2i, are monitored by level i + 1 of the
SWT.


Because for time series of non-negative numbers the aggre-
gate sum is monotonically increasing, the sum of the time
series within a sliding window of any size is bounded by the
sum of its including window in the shifted wavelet tree. This
fact can be used as a filter to eliminate those subsequences
whose sums are far below their thresholds.

Figure 4 gives the pseudo-code for spotting potential sub-
sequences of size w,w  2i, with sums above its threshold
f(w). The algorithm will search for burst in two stages.
First, the potential burst is detected at the level i+1 in the
SWT, which corresponds to the subsequence x[(j -1)2i +
1 .. (j+1)2i]. In the second stage, those subsequences of size
2i within x[(j-1)2i+1 .. (j+1)2i] with sum less than f(w)
are further eliminated. The moving sums of sliding window
size 2i can be reused for burst detection of other window
size w = w,w  2i. The detailed search of burst on the
surviving subsequences is then performed. A detailed search
in a subsequence is to compute the moving sums with win-
dow size w in the subsequence directly and to verify if these
moving sums cross the burst threshold.

In the spirit of the original work of [2] that uses lower bound
technique for fast time series similarity search, we have the
Given :
time series x[1..n], n = 2a,
shifted wavelet tree SWT[1..a][1..],
window size w, threshold f(w)
Return:
Subsequences of x with burst

i = log2 w ;
FOR j = 1 TO size(SWT[i+1])
IF (SWT[i+1][j]>f[w])
//possible burst in subsequence x[(j-1)2i+1..(j+1)2i],
//first we compute the moving sums with
//window size 2i within this subsequence.
FOR c = (j - 1)2i + 1 TO j2i
y=sum(x[c .. c+2i-1]);
IF y>f[w]
detailed search in x[c .. c-1+2i]
ENDIF
ENDFOR
ENDIF
ENDFOR



Figure 4: Algorithm to search for a burst


following lemma that guarantees the correctness of our al-
gorithm.


Lemma 2. The above algorithm can guarantee no false
negatives in elastic burst detection from a time series of non-
negative numbers.


Proof. From lemma 1, any subsequence of length w,w 
2i is contained within a window in the SWT:

[c .. c + w - 1]  [c .. c + 2i - 1]  [(j-1)2i+1 .. (j+1)2i]

Because the sum of the time series of non-negative numbers
is monotonic increasing, we have

(x[c..c+w-1])
(x[c..c+2i-1])
(x[(j-1)2i+1..(j+1)2i]).

By eliminating sequences with lengths larger than w but
with sums less than f(w), we do not introduce false nega-
tives because

(x[(j-1)2i+1 .. (j+1)2i]) < f(w) 
(x[c .. c+w-1]) < f(w).




339

In most applications, the algorithm will perform detailed
search seldom and then usually only when there is a burst
of interest. For example, suppose that the moving sum of
a time series is a random variable from a normal distribu-
tion. Let the sum within sliding window w be So(w) and its
expectation be Se(w), We assume that

So(w) - Se(w)
Se(w)
 Norm(0,1).

We set the threshold of burst f(w) for window size w such
that the probability that the observed sums exceed the thresh-
old is less that p, i.e., Pr(So(w)  f(w))  p. Let (x) be
the normal cumulative distribution function, we have

f(w) = Se(w)(1 - -
1
(p)).

Because our algorithm monitors the burst based on windows
with size W = Tw,1  T < 2, the detailed search will
always report real bursts. Actually our algorithm performs
a detailed search whenever there are more than f(w) events
in a window of W. Therefore the rate of detailed search
pf is higher than the rate of true alarms. Suppose that
Se(W) = TSe(w), we have

So(W) - Se(W)
Se(W)
 Norm(0,1).



pf = Pr(So(W)  f(w)) = Pr
So(W) - Se(W)
Se(W)

f(w) - Se(W)
Se(W)

=  -
f(w) - Se(W)
Se(W)
=  1 -
f(w)
TSe(w)

=  1 -
1 - -
1
(p)
T


The rate of detailed search is very small for small p, the
primary case of interest. For example, let p = 10-
6
,T = 1.5,
pf is 0.006. In this model, the upper bound of false alarm
rate is guaranteed.

The time for a detailed search in a subsequence of size W is
O(W). The total time for all detailed searches is linear in
the number of false alarms and true alarms(the output size
k). The number of false alarm depends on the data and the
setting of thresholds, and it is approximately proportional to
the output size k. So the total time for detailed searches is
bounded by O(k). To build the SWT takes time O(n), thus
the total time complexity of our algorithm is approximately
O(n+k), which is linear in the total of the input and output
sizes.


2.3 Streaming Algorithm
The SWT data structure in the previous section can also
be used to support a streaming algorithm for elastic burst
detection. Suppose that the set of window sizes in the elastic
window model are 2L < w1 < w2 < ... < wm  2U. For
simplicity of explanation, assume that new data becomes
available at every time unit.

Without the use of SWT, a naive implementation of elastic
burst detection has to maintain the m sums over the sliding
windows. When a new data item becomes available, for each
sliding window, the new data item is added to the sum and
the corresponding expiring data item of the sliding window
is subtracted from the sum. The running sums are then
checked against the monitoring thresholds. This takes time
O(m) for each insertion of new data. The response time is
one time unit if enough computing resources are available.

By comparison, the streaming algorithms based on the SWT
data structure will be much more efficient. For the set of
window sizes 2L < w1 < w2 < ... < wm  2U, we need
to maintained the levels from L + 2 to U + 1 of the SWT
that monitor those windows. There are two methods that
provide tradeoffs between throughput and response time.


· Online Algorithm:The online algorithm will have a
response time of one time unit. In the SWT data
structure, each data item is covered by two windows in
each level. Whenever a new data item becomes avail-
able, we will update those 2(U - L) aggregates of the
windows in the SWT immediately. Associated with
each level, there is a minimum threshold. For level
i, the minimum threshold i is the min of the thresh-
olds of all the windows monitored by level i, that is,
i = minf(wj),2i
-2
< wj  2i
-1
. If the sum in the
most recently completed window at level i exceeds i, it
is possible one of the windows monitored by level i ex-
ceeds its threshold. We will perform a detailed search
on those time intervals. Otherwise, the data stream
monitor awaits insertions into the data stream. This
online algorithm provides a response time of one time
unit, and each insertion of the data stream requires
2(U - L) updates plus possible detailed searching.
· Batch Algorithm: The batch algorithm will be lazy
in updating the SWT. Remember that the aggregates
at level i can be computed from the aggregates at level
i - 1. If we maintain aggregates at an extra level of
consecutive windows with size 2L
+1
, the aggregates
at levels from L + 2 to U + 1 can be computed in
batch. The aggregate in the most recently completed
window of the extra level is updated every time unit.
An aggregate of a window at the upper levels in the
SWT will not be computed until all the data in that
window are available. Once an aggregate at a certain
upper level is updated, we also check alarms for time
intervals monitored by that level. A batch algorithm
gives higher throughput though longer response time
(with guaranteed bound close to the window size whose
threshold was exceeded) than an online algorithm as
the following lemmas state.


Lemma 3. The amortized processing time per insertion
into the data stream for a batch algorithm is 2.


Proof. At level i,L + 2  i  U + 1, of the SWT, ev-
ery 2i-
1
time units there is a window in which all the data
are available. The aggregates at that window can be com-
puted in time O(1) based on the aggregates at level i - 1.
Therefore the amortized update time for level i is
1
2i
-1
. The
total amortized update time for all levels (including the ex-
tra level) is


1 +
U+1



i=L+2
1
2i-
1
 2.




340

Figure 5: (a)Wavelet Tree 2D(left) and (b)Shifted
Wavelet Tree 2D(right)


Lemma 4. The burst activity of a window with size w will
be reported with a delay less than 2
log2 w
.


Proof. A window with size w,2i
-1
< w  2i, is moni-
tored by level i + 1 of the SWT. The aggregates of windows
at level i + 1 are updated every 2i time units. When the
aggregates of windows at level i + 1 are updated, the burst
activity of window with size w can be checked. So the re-
sponse time is less than 2i = 2
log2 w
.

2.4 Other Aggregates
It should be clear that in addition to sum, the monitoring
of many other aggregates based on elastic windows could
benefit from our data structure, as long as the following
conditions holds.


1. The aggregate F is monotonically increasing or de-
creasing with respect to the window, i.e., if window
[a1..b1] is contained in window [a2..b2], then F(x[a1..b1]) 
F(x[a2..b2]) (or F(x[a1..b1])  F(x[a2..b2])) always holds.

2. The alarm domain is one sided, that is, [threshold,)
for monotonic increasing aggregates and (-,threshold]
for monotonic decreasing aggregates.


The most important and widely used aggregates are all mono-
tonic: Max, Count are monotonically increasing and Min is
monotonically decreasing. Another monotonic aggregate is
Spread. Spread measures the volatility or surprising level of
time series. Spread of a time series x is

Spread(x) = Max(x) - Min(x).

Spread is monotonically increasing. The spread within a
small time interval is less than or equal to that within a
larger time interval. A large spread within a small time
interval is of interest in many applications in data stream
because it indicates that the time series has experienced
large movement.

2.5 Extension to Two Dimensions
The one-dimensional shifted wavelet tree for time series can
naturally be extended to higher dimensions, such as spa-
tial dimensions. In this section we consider the problem
0
5
10
15
20
25
30
35
40
45




1913
1915
1917
1919
1922
1924
1934
1937
1945
1947
1949
1951
1953
1955
1957
1959
1961
1963
1965
1967
1969
1971
1973
1975
1977
1979
1981
1983
1985
1987
1990
1992
1995
1997
1999
2002
Japan




0
10
20
30
40
50
60




1913
1915
1917
1919
1922
1924
1934
1937
1945
1947
1949
1951
1953
1955
1957
1959
1961
1963
1965
1967
1969
1971
1973
1975
1977
1979
1981
1983
1985
1987
1990
1992
1995
1997
1999
2002
Russia




0
5
10
15
20
25
30




1913
1915
1917
1919
1922
1924
1934
1937
1945
1947
1949
1951
1953
1955
1957
1959
1961
1963
1965
1967
1969
1971
1973
1975
1977
1979
1981
1983
1985
1987
1990
1992
1995
1997
1999
2002
Iraq




Figure 6: Bursts of the number of times that coun-
tries were mentioned in the presidential speech of
the state of the union


of discovering elastic spatial bursts using a two-dimensional
shifted wavelet tree. Given a fixed image of scattering dots,
we want to find the regions of the image with unexpectedly
high density of dots. In an image of the sky with many dots
representing the stars, such regions might indicate galax-
ies or supernovas. The problem is to report the positions
of spatial sliding windows (rectangle regions) having differ-
ent sizes, within which the density exceeds some predefined
threshold.

The two-dimensional shifted wavelet tree is based on the
two-dimensional wavelet structure. The basic wavelet struc-
ture separates a two-dimensional space into a hierarchy of
windows as shown in figure 5-a (similar to quadtree[18]). Ag-
gregate information will be computed recursively based on
those windows to get a compact representation of the data.
Our two-dimensional shifted wavelet tree will extend the
wavelet tree in a similar fashion as in the one-dimensional
case. This is demonstrated in figure 5-b. At the same level of
the wavelet tree, in addition to the group of disjoint windows
that are the same as in the wavelet tree, there are another
three groups of disjoint windows. One group of windows
offsets the original group in the horizontal direction, one in
the vertical direction and the third one in both directions.

Any square spatial sliding window with size w×w is included
in one window of the two-dimensional SWT. The size of such
a window is at most 2w×2w. Using the techniques of section
2.2, burst detection based on the SWT-2D can report all the
high density regions efficiently.


3. EMPIRICAL RESULTS
Our empirical study will first demonstrate the desirability
of elastic burst detection for some applications. We also
study the performance of our algorithm by comparing our
algorithm with the brute force searching algorithm in section
3.2.


3.1 Effectiveness Study



341

Sliding window size = 0.1 second




0
1
2
3
4
5




0
500
1000
1500
2000
2500
3000
Time (second)
N
um
berofE
vents
Sliding window size = 1 second




0
1
2
3
4
5
6
7
8




0
500
1000
1500
2000
2500
3000
Time (second)
N
um
berofE
vents
Sliding window size = 10 seconds




0
2
4
6
8
10
12
14
16
18
20
22




0
500
1000
1500
2000
2500
3000
Time (second)
N
um
berofE
vents




Figure 7: Bursts in Gamma Ray data for different sliding window size




Figure 8: Bursts in population distribution data for different spatial sliding window sizes


As an emotive example, we monitor bursts of interest in
countries from the presidential State of the Union addresses
from 1913 to 2003. The same example was used by Klein-
berg[16] to show the bursty structure of text streams. In
figure 6 we show the number of times that some countries
were mentioned in the speeches. There are clearly bursts
of interest in certain countries. An interesting observation
here is that these bursts have different durations, varying
from years to decades.

The rationale behind elastic burst detection is that a pre-
defined sliding window size for data stream aggregate mon-
itoring is insufficient in many applications. The same data
aggregated in different time scales will give very different
pictures as we can see in figure 7. In figure 7 we show the
moving sums of the number of events for about an hour's
worth of Gamma Ray data. The sizes of sliding windows
are 0.1, 1 and 10 seconds respectively. For better visualiza-
tion, we only show those positions with bursts. Naturally,
bursts at small time scales that are extremely high will pro-
duce bursts in larger time scales too. More interestingly,
bursts at large time scales are not necessarily reflected at
smaller time scales, because those bursts at large time scales
might be composed of many consecutive "bumps". Bumps
are those positions where the numbers of events are high but
not high enough to be "bursts". Therefore, by looking at
different time scales at the same time, elastic burst detection
will give more insight into the data stream.

We also show in figure 8 an example of spatial elastic bursts.
We use the 1990 census data of the population in the con-
tinental United State. The population in the map are ag-
gregated in a grid of 0.2 × 0.2 in Latitude/Longitude. We
compute the total population within sliding spatial windows
with sizes 1 × 1, 2 × 2 and 5 × 5. Those regions with
population above the 98 percentile in different scales are
highlighted. We can see that the different sizes of sliding
windows give the distribution of high population regions at
different scales.

3.2 Performance Study
Our experiments were performed on a 1.5GHz Pentium 4
PC with 512 MB of main memory running Windows 2000.
We tested the algorithm with two different types of data
sets:


· The Gamma Ray data set : This data set includes
12 hours of data from a small region of the sky, where
Gamma Ray bursts were actually reported during that
time. The data are time series of the number of pho-
tons observed (events) every 0.1 second. There are to-
tally 19,015 events in this time series of length 432,000.

· The NYSE TAQ Stock data set : This data set includes
four years of tick-by-tick trading activities of the IBM
stock between July 1st, 1998 and July 1st, 2002. There
are 5,331,145 trading records (ticks) in total. Each
record contains trading time (precise to the second),
trading price and trading volume.


In the following experiments, we set the thresholds of differ-
ent window sizes as follows. We use the first few hours of
Gamma Ray data and the first year of Stock data as train-
ing data respectively. For a window of size w, we compute
the aggregates on the training data with sliding window of
size w. This gives another time series y. The thresholds are
set to be f(w) = avg(y) + std(y), where avg(y) and std(y)
are the average and standard deviation respectively. The
factor of threshold  is set to 8. The list of window sizes is
5,10,...,5  Nw time units, where Nw is the number of win-
dows. Nw varies from 5 to 50. The time unit is 0.1 seconds
for the Gamma Ray data and 1 minute for the stock data.




342

Processing time vs. Number of Windows




0
10000
20000
30000
40000
50000
60000
70000
80000




0
10
20
30
40
50

Number of Windows
P
rocessing
tim
e
(m
s)




SWT Algorithm

Direct Algorithm




Figure 9: The processing time of elastic burst de-
tection on Gamma Ray data for different number of
windows



First we compare the wall clock processing time of elastic
burst detection from the Gamma Ray data in figure 9. Our
algorithm based on the SWT data structure is more than ten
times faster than the direct algorithm. The advantage of us-
ing our data structure becomes more obvious as we examine
more window sizes. The processing time of our algorithm
is output-dependent. This is confirmed in figure 10, where
we examine the relationship between the processing time
using our algorithm and the number of alarms. Naturally
the number of alarms increases as we examine more window
sizes. We also observed that the processing time follows the
number of alarms well. Recall that the processing time of the
SWT algorithm includes two parts: building the SWT and
the detailed searching of those potential portions of burst.
Building the SWT takes only 200 milliseconds for the data
set, which is negligible when compared to the time to do
the detailed search. Also for demonstration purposes, we
intentionally, to our disadvantage, set the thresholds lower
and therefore got many more alarms than what physicists
are interested in. If the alarms are scarce, as is the case
for Gamma Ray burst detection, our algorithm will be even
faster. In figure 11 we fix the number of windows to be 25
and change the factor of threshold . The larger  is, the
higher the thresholds are, and therefore the fewer alarms
will be sounded. Because our algorithm is dependent on the
output sizes, the higher the thresholds are, the faster the al-
gorithm runs. In contrast, the processing time of the direct
algorithm does not change accordingly.

For the next experiments, we test the elastic burst detection
algorithm on the IBM Stock trading volume data. Figure
12 shows that our algorithm is up to 100 times faster than a
brute force method. We also zoom in to show the processing
time for different output sizes in figure 13.

In addition to elastic burst detection, our SWT data struc-
ture works for other elastic aggregates monitoring too. In
the following experiments, we search for big spreads on the
IBM Stock data. Figure 14 and 15 confirms the performance
advantages of our algorithm. Note that for the aggregates
of Min and Max, and thus Spread, there is no known de-
terministic algorithm to update the aggregates over sliding
windows incrementally in constant time. The filtering prop-
erty of SWT data structure gains more by avoiding unnec-
essary detailed searching. So in this case our algorithm is
up to 1,000 times faster than the direct method, reflecting
Processing time vs. Number of Alarms




0
1000
2000
3000
4000
5000
6000




5
10
15
20
25
30
35
40
45
50

Number of Windows
Tim
e
(m
s)




0
2000
4000
6000
8000
10000
12000




N
um
berofA
larm
s




Processing time

Alarms




Figure 10: The processing time of elastic burst de-
tection on Gamma Ray data for different output
sizes


Processing time vs. Thresholds




0
5000
10000
15000
20000
25000
30000
35000
40000




4
5
6
7
8
9
10
11
12

Factor of Threshold
P
rocessing
tim
e
(m
s)




SWT Algorithm

Direct Algorithm




Figure 11: The processing time of elastic burst de-
tection on Gamma Ray data for different thresholds


Processing time vs. Number of Windows




0
10
20
30
40
50
60
70
80




0
10
20
30
40
50

Number of Windows
P
rocessing
tim
e
(seconds)

SWT Algorithm

Direct Algorithm




Figure 12: The processing time of elastic burst de-
tection on Stock data for different number of win-
dows


Processing time vs. Number of Alarms




0
200
400
600
800
1000
1200
1400
1600




5
10
15
20
25
30
35
40
45
50

Number of Windows
Tim
e
(m
s)




0
500
1000
1500
2000
2500
3000
3500




N
um
berofA
larm
s




Processing time

Alarms




Figure 13: The processing time of elastic burst de-
tection on Stock data for different output sizes




343

Processing time vs. Number of Windows




1
10
100
1000
10000
100000
1000000




0
10
20
30
40
50

Number of Windows
P
rocessing
tim
e
(m
s)




SWT Algorithm

Direct Algorithm




Figure 14: The processing time of elastic spread de-
tection on Stock data for different number of win-
dows


Processing time vs. Number of Alarms




0
50
100
150
200
250
300
350
400
450




5
10
15
20
25
30
35
40
45
50

Number of Windows
Tim
e
(m
s)




0
50
100
150
200
250
300
350
400




N
um
berofA
larm
s




Processing time

Alarms




Figure 15: The processing time of elastic spread de-
tection on Stock data for different output sizes



the advantage of a near linear algorithm as compared with
a quadratic one.


4. RELATED WORK
There is much recent interest in data stream mining and
monitoring. An excellent survey of models and issues in
data stream can be found in [3]. The sliding window is recog-
nized as an important model for data stream. Based on the
sliding window model, previous research studies the compu-
tation of different aggregates of data stream, for example,
correlated aggregates [11], count and other aggregates[7],
frequent itemsets and clusters[10], and correlation[22]. The
work [13] studies the problem of learning models from time-
changing streams without explicitly applying the sliding win-
dow model. The Aurora project[4] considers the systems as-
pect of monitoring data streams. Also the algorithm issues
in time series stream statistics monitoring are addressed in
StatStream[22]. In this paper we extend the sliding win-
dow model to the elastic sliding window model, making the
choice of sliding window size more automatically.

Wavelets are heavily used in the context of data manage-
ment and data mining, including selectivity estimation[17],
approximate query processing[20, 5], dimensionality reduc-
tion[6] and streaming data analysis[12]. However, its use
in elastic burst detection is innovative. We achieve efficient
detection of subsequences with burst in a time series by fil-
tering lots of subsequences that are unlikely to have burst.
This is an extension to the well-known lower bound tech-
nique in similarity search from time series[2].
Data mining on bursty behavior has attracted more atten-
tion recently. Wang et al. [21] study fast algorithms using
self-similarity to model bursty time series. Such models can
generate more realistic time series streams for testing data
mining algorithm. Kleinberg[16] also discusses the problem
of burst detection in data streams. The focus of his work
is in modeling and extracting structure from text streams.
Our work is different in that we focus on the algorithmic
issue of counting over different sliding windows.

We have extended the data structure for burst detection to
high-spread detection in time series. Spread measures the
surprising level of time series. There is also work in finding
surprising patterns in time series data. However the defi-
nition of surprise is application dependent and it is up to
the domain experts to choose the appropriate one for their
application. Jagadish et al.[14] use optimal histograms to
mine deviants in time series. In their work deviants are
defined as points with values that differ greatly from that
of surrounding points. Shahabi et al.[19] also use wavelet-
based structure(TSA-Tree) to find both trends and surprises
in large time series dataset, where surprises are defined as
large difference between two consecutive averages of a time
series. In very recent work, Keogh et al.[15] propose an al-
gorithm based on suffix tree structures to find surprising
patterns in time series database. They try to learn a model
from the previously observed time series and declare sur-
prising for those patterns with small chance of occurrence.
By comparison, an advantage of our definition of surprise
based on spread is that it is simple, intuitive and scalable to
massive and streaming data.


5. CONCLUSIONS AND FUTURE WORK
This paper introduces the concept of monitoring data streams
based on an elastic window model and demonstrates the de-
sirability of the new model. The beauty of the model is that
the sliding window size is left for the system to discover
in data stream monitoring. We also propose a novel data
structure for efficient detection of elastic bursts and other
aggregates. Experiments on real data sets show that our
algorithm is faster than a brute force algorithm by several
orders of magnitude. We are currently collaborating with
physicists to deploy our algorithm for online Gamma Ray
burst detection. A robust way of setting the thresholds of
burst for different window sizes is a topic for future work.
Also the problem of monitoring of non-monotonic aggregates
is an interesting future work.


6. ACKNOWLEDGMENTS
We are grateful to Prof. Allen I. Mincer of the Milagro
Project, for giving us the preliminary tutorial of astrophysics.
We also thank the Milagro collaboration for making the
Gamma Ray data available to us.


7. REFERENCES
[1] http://www.lanl.gov/milagro/, 2003.

[2] R. Agrawal, C. Faloutsos, and A. N. Swami. Efficient
Similarity Search In Sequence Databases. In
D. Lomet, editor, Proceedings of the 4th International
Conference of Foundations of Data Organization and
Algorithms (FODO), pages 69­84, Chicago, Illinois,
1993. Springer Verlag.




344

[3] B. Babcock, S. Babu, M. Datar, R. Motwani, and
J. Widom. Models and issues in data stream systems.
In Proceedings of the Twenty-first ACM
SIGACT-SIGMOD-SIGART Symposium on Principles
of Database Systems, June 3-5, Madison, Wisconsin,
USA, pages 55­68. ACM, 2002.

[4] D. Carney, U. Cetintemel, M. Cherniack, C. Convey,
S. Lee, G. Seidman, M. Stonebraker, N. Tatbul, and
S. B. Zdonik. Monitoring streams - a new class of data
management applications. In VLDB 2002,Proceedings
of 28th International Conference on Very Large Data
Bases, August 20-23, 2002, Hong Kong, China, 2002.

[5] K. Chakrabarti, M. N. Garofalakis, R. Rastogi, and
K. Shim. Approximate query processing using
wavelets. In VLDB 2000, Proceedings of 26th
International Conference on Very Large Data Bases,
September 10-14, 2000, Cairo, Egypt, pages 111­122,
2000.

[6] K.-P. Chan and A. W.-C. Fu. Efficient time series
matching by wavelets. In Proceedings of the 15th
International Conference on Data Engineering,
Sydney, Australia, pages 126­133, 1999.

[7] M. Datar, A. Gionis, P. Indyk, and R. Motwani.
Maintaining stream statistics over sliding windows. In
Proceedings of the Thirteenth Annual ACM-SIAM
Symposium on Discrete Algorithms, January 6-8,
2002, San Francisco, CA, USA. ACM/SIAM, 2002,
pages 635­644, 2002.

[8] R. Atkins et. al. (The Milagro Collaboration).
Evidence for TeV emission from GRB 970417a. In
Ap.J. Lett. 533, L119, 2000.

[9] A. J. Smith for the Milagro Collaboration. A search
for bursts of tev gamma rays with milagro. In
Proceedings of the 27th International Cosmic Ray
Conference(ICRC 2001), 07-15 August 2001,
Hamburg, Germany, 2001.

[10] V. Ganti, J. Gehrke, and R. Ramakrishnan. Demon:
Data evolution and monitoring. In Proceedings of the
16th International Conference on Data Engineering,
San Diego, California, 2000.

[11] J. Gehrke, F. Korn, and D. Srivastava. On computing
correlated aggregates over continual data streams. In
Proc. ACM SIGMOD International Conf. on
Management of Data, 2001.

[12] A. C. Gilbert, Y. Kotidis, S. Muthukrishnan, and
M. Strauss. Surfing wavelets on streams: One-pass
summaries for approximate aggregate queries. In
VLDB 2001, pages 79­88. Morgan Kaufmann, 2001.

[13] G. Hulten, L. Spencer, and P. Domingos. Mining
time-changing data streams. In Proceedings of the
seventh ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 97­106.
ACM Press, 2001.

[14] H. V. Jagadish, N. Koudas, and S. Muthukrishnan.
Mining deviants in a time series database. In M. P.
Atkinson, M. E. Orlowska, P. Valduriez, S. B. Zdonik,
and M. L. Brodie, editors, VLDB'99, Proceedings of
25th International Conference on Very Large Data
Bases, September 7-10, 1999, Edinburgh, Scotland,UK,
pages 102­113. Morgan Kaufmann, 1999.

[15] E. Keogh, S. Lonardi, and B. Y. Chiu. Finding
surprising patterns in a time series database in linear
time and space. In the 8th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining,July 23 - 26, 2002. Edmonton, Alberta,
Canada, pages 550­556, 2002.

[16] J. Kleinberg. Bursty and hierarchical structure in
streams. In the 8th ACM SIGKDD International
Conference on Knowledge Discovery and Data
Mining,July 23 - 26, 2002. Edmonton, Alberta,
Canada, pages 91­101, 2002.

[17] Y. Matias, J. S. Vitter, and M. Wang. Wavelet-based
histograms for selectivity estimation. In L. M. Haas
and A. Tiwary, editors, SIGMOD 1998, Proceedings
ACM SIGMOD International Conference on
Management of Data, June 2-4, 1998, Seattle,
Washington, USA, pages 448­459, 1998.

[18] H. Samet. The quadtree and related hierarchical data
structures. ACM Computing Surveys, 16(2):187­260,
1984.

[19] C. Shahabi, X. Tian, and W. Zhao. Tsa-tree: A
wavelet-based approach to improve the efficiency of
multi-level surprise and trend queries on time-series
data. In 12th International Conference on Scientific
and Statistical Database Management
(SSDBM'00),July 26 - 28, 2000,Berlin, Germany,
pages 55­68, 2000.

[20] J. S. Vitter and M. Wang. Approximate computation
of multidimensional aggregates of sparse data using
wavelets. In SIGMOD 1999, Proceedings ACM
SIGMOD International Conference on Management of
Data, June 1-3, 1999, Philadephia, Pennsylvania,
USA, pages 193­204, 1999.

[21] M. Wang, T. M. Madhyastha, N. H. Chan,
S. Papadimitriou, and C. Faloutsos. Data mining
meets performance evaluation: Fast algorithms for
modeling bursty traffic. In ICDE 2002, 18th
International Conference on Data Engineering,
February 26-March 1, 2002, San Jose, California,
2002.

[22] Y. Zhu and D. Shasha. Statstream: Statistical
monitoring of thousands of data streams in real time.
In VLDB 2002,Proceedings of 28th International
Conference on Very Large Data Bases, August 20-23,
2002, Hong Kong, China, pages 358­369, 2002.




345

