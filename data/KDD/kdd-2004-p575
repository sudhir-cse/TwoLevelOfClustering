Discovering Additive Structure in Black Box Functions


Giles Hooker
Department of Statistics
Stanford University
Stanford, CA, USA
gilesh@stanford.edu



ABSTRACT
Many automated learning procedures lack interpretabil-
ity, operating effectively as a black box: providing a predic-
tion tool but no explanation of the underlying dynamics that
drive it. A common approach to interpretation is to plot the
dependence of a learned function on one or two predictors.
We present a method that seeks not to display the behav-
ior of a function, but to evaluate the importance of non-
additive interactions within any set of variables. Should the
function be close to a sum of low dimensional components,
these components can be viewed and even modeled paramet-
rically. Alternatively, the work here provides an indication
of where intrinsically high-dimensional behavior takes place.
The calculations used in this paper correspond closely
with the functional ANOVA decomposition; a well-developed
construction in Statistics. In particular, the proposed score
of interaction importance measures the loss associated with
the projection of the prediction function onto a space of ad-
ditive models. The algorithm runs in linear time and we
present displays of the output as a graphical model of the
function for interpretation purposes.


Categories and Subject Descriptors
G.3 [Probability and Statistics]: Nonparametric statis-
tics; I.5.2 [Pattern Recognition]: Design Methodology--
Feature Evaluation and Selection; I.6.5 [Simulation and
Modeling]: Model Development--Modeling Methodologies


General Terms
Algorithms, Measurement, Design, Verification


Keywords
Visualization, Diagnostics, Functional ANOVA, Additive
models, Graphical models, Interpretation, Feature Selection




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
1. INTRODUCTION
Many procedures in Data Mining produce prediction func-
tions that act effectively as a "black box": predicting a cer-
tain quantity given particular inputs without providing in-
sight into the underlying dynamics that would allow a better
understanding of the system. This is particularly the case
for neural networks, support vector machines and other ker-
nel methods, and also for many ensemble methods such as
boosting and bagging. Indeed, trees ­ the most obviously
interpretable Machine Learning models ­ have proved to be
highly unstable (c.f. [2]) and using them in ensemble meth-
ods destroys that interpretability.
This paper initiates a computationally feasible investiga-
tion into the structure of such black boxes. In particular,
we are interested in measuring the importance of variables
in determining the output of the function and in finding
underlying additive, or approximately additive, interactions
between subsets of variables. Doing this will allow us to
decompose a response function into additive parts1. If each
of these components is low dimensional, then they may be
individually viewed, interpreted, and possibly even mod-
eled parametrically. Additionally, this allows us to produce
graphical models of function interactions to better see the
important components in the structure of a potential func-
tional ANOVA decomposition.
The diagnostic tools presented here also have a limit-
ing function. It is common, when attempting to interpret
black box predictions, to produce a matrix of bivariate plots,
each describing aggregate behavior on two of the predictors.
These are often complemented with plots of conditional vari-
ance: how much functional variation is not being accounted
for at each value of those two predictors. [13] and [9] ad-
vocate this approach, and an example is given in Figure
1. If the function in question is additive up to first order
interactions, then such a plot-matrix exactly captures its
dynamics: one only needs to sum up the values of the plots,
a relatively simple cognitive procedure. Plots of bivariate
conditional variances may indicate where something intrin-
sically higher-dimensional is going on, but they do not indi-
cate what variables to look for as additional interactions, or
indeed how many variables might be included in this inter-
action. The work that we present here solves this problem.
Most of the approaches mentioned above also involve an
evaluation of the function based on a uniform distribution.

1
It is important to emphasize the distinction here between
additivity of functional effects and independence of the un-
derlying predictors. That we can write F(x,y) = f(x)+g(y)
does not make x and y independent.



575
Research Track Poster

This introduces the problem of extrapolation. Prediction
functions are rarely learned on data that is even close to
uniform, so that Monte Carlo evaluation on uniformly sam-
pled data will inevitably lead to evaluating the function at
points of extrapolation. Indeed, such a uniform sample may
contain more points of extrapolation than points close to
the learning sample. The approach presented below miti-
gates this problem.


2. THE FUNCTIONAL ANOVA
Visualization procedures for the dynamics of a function or
model often rely on plotting univariate or bivariate effects.
A standard definition of these is found in the functional
ANOVA decomposition, given for example in [11]. This con-
struction has a long history going back to 1948 in [7]. This
section will provide an introductory summary.
Let F(x) : R
k
 R be square integrable, with

x = (x1,...,xk)

For u  {1,...,k} we denote by xu the subset of variables
whose indexes are in u. Similarly x-u will indicate the vari-
ables with indexes not in u. We can write f(x) uniquely
as

F(x) =

u{1,...,k}
fu(x)


with fu depending only on xu and defined by


fu(x) =
x-u
F(x) -
vu
fv(x) dx-u


In more concrete terms, F is represented as a constant (u =
), plus terms in one variable (u = {i}), plus terms in two
variables and so forth. Each term is calculated as the pro-
jection of F onto a particular subset of the predictors, tak-
ing out the lower-order effects which have already been ac-
counted for.
It can be shown that the fu are orthogonal and that the
functional variance 2(f) = f2dx may be decomposed as

2(F) =

u{1,...,k}
2u(fu).
(1)


Note that this definition can be generalized trivially to any
product measure without compromising orthogonality. The
Functional ANOVA decomposition was used in [9] and [13]
as the basis for a representation of functional behavior using
bivariate plots. Throughout the following, we will assume
that F is scaled to give 2(F) = 1.


3. PARTIAL DEPENDENCE PLOTS
The interest in this work is to understand functions learned
on data. Since most of the data sets are far from uniform, or
any product measure, creating a Monte Carlo sample from
one of these will distort our picture of the dynamics of the
function; placing greater emphasis on areas of low proba-
bility mass. [5] defines the partial dependence of a function
F(x) on xl to be

Fu(xu) = Ex-u [F(x)] =
F(xu,x-u)p-u(x-u)dx-u

where p-u(x-u) is the marginal distribution of x-u. [5]
notes that taking the marginal distribution rather than a
12
14
16
18
20
22
22.05
22.06
22.07
22.08
22.09




age
PDP
on
age




age
ptratio




0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0




30
40
50
60
70




0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0




age
ptratio




0
20
40
60
80
100
5
10
15




ptratio
PDP
on
ptratio




Figure 1: A typical matrix of partial dependence
plots given for a neural network trained on the
Boston Housing Data. The partial dependence on
"age" (top left), and that on "ptratio" (bottom
right). A contour plot of the partial dependence on
the pair (top right) and a filled contour plot of the
variance of the function given "age" and "ptratio"
(bottom left).


conditional distribution given xu preserves additive struc-
ture in F in the sense that the partial dependence recovers
the components of an additive function up to a constant.
This comes at the expense of distorting the underlying prob-
ability space. This distortion, however, is less severe than
using a uniform distribution, or than taking the product
of marginal densities in all dimensions in the sense that
the Kullback-Leibler distance of the underlying distribution
with respect to the original data distribution is smaller.
[5] provides an efficient algorithm for producing plots of
these functions for ensembles of trees. A data-driven ap-
proximation can be produced for any black box function F
by calculating


^
Fu(xu) =
1
N
N



i=1
F(xu,xi,
-u
)
(2)


where {xi}N
i=1
is the sample used to learn F.
As an example, we trained a 13-26-1 neural network on
the Boston Housing Data [6] using the default settings in
the R package, nnet[12]. A typical display might look some-
thing like Figure 1: a matrix of plots containing the partial
dependence of a prediction function on individual variables
on the diagonal. The upper diagonal elements then contain
contours of the partial dependence of the function on pairs
of variables. Below the diagonal is the variance of the func-
tion at each value of a pair of variables. We have presented
the two variables "age" and "ptratio", but more would typ-
ically be given. A constant variance would indicate that
the function has no interaction between the pair of variables
specified and the other predictors.


4. TESTS OF ANOVA STRUCTURES
Throughout this section we will assume that the predictor
variables are drawn from a product distribution in order



576
Research Track Poster

to retain the interpretational properties of the Functional
ANOVA. In §5 we will give a data-driven approximation
that is similar to partial dependence plots (2).
We say that a function f has ANOVA structure described
by a collection U of subsets of {1,...,k} if fu(x) = 0 for
all u that are proper supersets of some element of U. Ex-
pressing the subsets of {1,...,k} as a lattice space ordered
by the subset operator, this is equivalent to U being a
least upper bound on the set of elements u of the lattice
with non-zero 2u(fu). In concrete terms, a function of the
form f1(x1) + f2(x2,x3) would be said to have structure
{{1},{2,3}}, describing the terms that are necessary to re-
cover the function.
The L2 projection of F onto the set of functions with
ANOVA structure described by U is given by:


GU(x) =
|U|



i=0
(-1)|U
|-i


viU
E-vF(x)
(3)


where |U| is the cardinality of U, iU represents the col-
lection of i-way intersections among the elements of U, and
E-vF(x) represents expectation conditioned on xv. The
natural measure of goodness of fit for this projection is there-
fore:

E(F(x) - GU(x))2

with the expectation taken over the underlying product mea-
sure.
Consider a 3 dimensional function, f(x1,x2,x3), with un-
derlying uniform measure. Let us project this function onto
the set of functions with ANOVA structure {{1},{2}}: ad-
ditive in x1 and x2 and constant in x3. The projection is
given by:


E(f|x1) + E(F|x2) - E(f) =
f(x1,x2,x3)dx2dx3


+
f(x1,x2,x3)dx1dx3


-
f(x1,x2,x3)dx1dx2dx3

= f1(x1) + f2(x2) + f0

the corresponding two first effects plus the mean. In general
(3) is equal to the sum of the effects indexed by subsets of
the elements of U.
Generally, we are interested in the significance of an in-
teraction u, indexed by a subset of {1,...,k}. Formally,
we are asking: v  U : u  v: Is there a non-zero func-
tional ANOVA component v that contains u? To measure
this, we project onto
(vu)
fv(xv): the set of all functions
with no interaction in u. The equivalent ANOVA structure
is U = {{1,...,k}\{i}}i
u
. This is an upper bound for the
set of ANOVA structures not containing u. In this case (3)
simplifies to:

Gu(x) =
vu
(-1)k
-|v|-1
EvF(x).
(4)


Here the quantity of interest, E(F(x)-Gu(x))2, corresponds,
in functional ANOVA terms, to the measure

¯
2u =
vu
2v(fv).
(5)
In the three dimensional example above, ¯
21 measures the er-
ror associated with approximating f(x1,x2,x3) with a func-
tion of the form g(x2,x3), leaving out x1. We could test the
interaction {x2,x3} by projecting onto the set of functions
of the form g1(x1,x2) + g2(x1,x3).
This measure is of interest in the statistical quadrature
literature [10] although it differs from the measures of sub-
set importance given by [14]. It can be viewed as the L2
cost of excluding the interaction u from the model and will
alternatively be labeled the L2 Cost of Exclusion or L2CoE.


5. EMPIRICAL ESTIMATES
In order to estimate the quantities (5) empirically we will
perform a simple Monte Carlo integration using our training
data, in the vein of §3. Here,


^
E-vF(x) =
1
N
N



i=1
F(xv,xi,
-v
)


is the empirical partial dependence function of F on v. We
use this to construct the empirical projection
^
GU(x) from
(3) and measure

1
N
N



i=1
(F(xi) -
^
GU(xi))2.
(6)


Although we assumed that the data are drawn from a prod-
uct distribution in §4, this is not strictly necessary. The
inequalities below will still hold using partial dependence
operators. Particularly, a function which exactly fits a given
ANOVA structure will be exactly recoverable with these es-
timates. However, we are no longer producing the L2 pro-
jection of the prediction function under a known measure
onto a space of functions defined by a given ANOVA struc-
ture. In this sense, the exact interpretation of the empirical
L2CoE is less clear.
Note that there is a variance associated with the estimate
of both the Gu(xi) and the outer sum of (6). The com-
putational cost of (6) is O(N2). However, if for each i we
estimate
^
GU(xi) using a randomly drawn subsample of size
N1, we can express

^
GU(xi) = GU(xi) +
i

where
i
has mean zero and variance
1
N1
E(F(x)-GU(x))2.
This gives


E
1
N
N



i=1
(F(xi) -
^
GU(xi))2 = 1 +
1
N1
E(F(x) - GU(x))2


with a variance of O
1
N
+
1
NN1
. If we can set N1 = 1

to provide an O(N) estimation scheme without sacrificing
variance. At this point, the estimate of ¯
2u becomes


^
¯
2u =
1
2N
N



i=1


F
(xi) -
|U|



i=0
(-1)|U
|-i


viU
F(xi,v,xr
(i),-v
)



2




where r(i) is an integer randomly chosen in {1,...,N}.
Some algebra shows that if ¯
2u = 0 (the function does not
have an interaction in u) then the estimate also returns zero.
This estimate is very similar to the Monte Carlo estimates
employed by [10]; although that paper only examines a uni-
form distribution.



577
Research Track Poster

Figure 2: An example of the difference in the
VIN graph for functions of the form f(x1,x2,x3) and
g1(x4,x5) + g2(x4,x6) + g3(x5,x6).



6. GRAPHICAL DISPLAYS
In order to make use of the cost measures developed above
as a diagnostic tool, it is helpful to have a graphical represen-
tation of the ANOVA structure of a function. In a learned-
function context, very rarely will the importance score for
any particular interaction be identically zero. Therefore, a
representation that conveys which interactions are deemed
significant, as well as the overall importance of interactions,
is needed.
For first-order interactions we can represent the set of sig-
nificant interactions as edges in a graph that uses predictor
variables as nodes. In this case, the graph has a similar inter-
pretation to a Bayesian Network. That is: F is additive in xi
and xj given the collection of variables xu if any path from
xi to xj crosses xu. In fact, a Bayesian Network represents
exactly this structure for the log density of the variables.
An equivalent functional interpretation is that F is additive
in xi and xj if for any fixed xu, F(xi,xj;xu) = f(xi)+g(xj)
for some functions f and g. This implies the statement


F(xi,xj,xu)dxu = f(xi) + g(xj).


We have labeled such a representation a Variable Interaction
Network (VIN).
The inverse of the L2CoE measure can be taken as a dis-
tance between nodes, so that the graph representation may
be incorporated into a multi-dimensional scaling routine.
The package XGvis [3] has been used to produce the graphs
in Figures 2, 3, and 4, although the nodes have been posi-
tioned by hand. We have found that while dynamic views
of the scaled graphs ­ having a plot rotate with a three di-
mensional representation ­ are informative about interaction
strengths, static two-dimensional plots have not provided a
good representation of interaction strengths.
A representation limited to edges in a graph with vari-
ables as nodes still does not distinguish higher-order inter-
actions, except in so far as they must appear as cliques in
the graph: a fact that will appear below. We will there-
fore represent these higher interactions by a "cartwheel" to
distinguish them from a set of additive first order terms.
The VIN network we now produce can be interpreted as a
hypergraph, with cartwheels representing multi-dimensional
edges. We demonstrate this difference in Figure 2.
As an example, we consider the function

F(x) = x1x2

2x3 - sin-1(x4) + log(x3 + x5)

-
x9
x10
x7
x8
- x2x7.
(7)
Figure 3: Variable Interaction Network for the func-
tion (7) showing non-additive structure.


The true VIN components for F are

U = {{1,2,3},{2,7},{4},{3,5},{7,8,9,10}}
(8)

which induces the plot in Figure 3. Here edges {1,2}, {1,3}
and {2,3} would normally form a clique if only first-order
interactions were considered. A similar clique occurs for
{x7,x8,x9,x10}.
Note that these cartwheels do not alter the graphical in-
terpretation if we allow as paths any route through an "in-
tersection".
These plots may also be thought of as a graphical version
of the representation for hierarchical log-linear models given
in [4]. There, a log-linear model is specified on categorical
data with the highest order interaction terms listed as in (8).
In that case, the absence of an interaction represents inde-
pendence between categorical variables instead of additivity
of a response in either categorical or continuous predictors.


7. THE VIN ALGORITHM
While the evaluation of the strength of a particular inter-
action can be made in O(N) functional evaluations, there are
still 2k interactions to be evaluated. Denoting by |u| the size
of an interaction, the complexity of evaluation also scales as
2|u
|
. This becomes prohibitive as k and |u| increases. How-
ever, for many functions, the strength of interactions drops
off quickly with their size, enabling a very aggressive search
strategy.
We will make use of the following monotonicity property:

u  v  ¯
2u  ¯
2v.

That this holds is clear from equation (5). This allows us
to observe that a d-way interaction can only be considered
significant if all its (d - 1)-way interactions are significant2.
Thus we can begin by considering main effects - removing
one variable - giving a measure of variable importance. We
then proceed to first order interactions whose components
are all in the significant list. Second order interactions are
only considered if all the first order interactions that they
contain are included, and so forth. The algorithm here bears
strong resemblance to the Apriori algorithm for association
rules [1]. Both of these rely on a monotonicity property to
dramatically reduce the search space as we increase the com-
plexity of interactions (or item-sets) that we are considering.
Suppose we have a threshold
 0, and we wish to find
u : ¯
2u  . The following algorithm provides a least upper
bound for this set.

2
We will define significance by whether its L2CoE exceeds
some threshold .



578
Research Track Poster

i = 1
U = 
Loop:
K = {u  1,...,n : |u| = i,v  u,v  U,|v| = i - 1}
For Each u  K:
Calculate PF (u)
If PF (u) 
U  U  u
U  U \ {v  u}
End If
End For
i  i + 1
End Loop (K =  or i > D)

So long as the function in question does not exhibit very
high-dimensional behavior, this algorithm will examine only
a very small subset of interactions, which themselves will
be of low order. We believe that it is unlikely that we will
find functions that are intrinsically high dimensional. [11]
explores many apparently high dimensional functions and
finds that many are very close to additive. In the event of
interactions exceeding some given order (say 6), the algo-
rithm can be curtailed as an indication that interpretation
will become very difficult.


8. UPPER BOUNDS ON LATTICE SPACES
In §7, we produced a search algorithm designed to find
u : ¯
2u  . The natural measure to concern ourselves with
is the overall error resulting from a choice of interaction
terms. We would like to find a minimal U to give

E(F(x) - GU(x))2 <

for GU defined in 3: a representation that explains almost
all of the function. Here, minimality is taken to be a least
upper bound on a lattice. Instead of the set of elements with
non-zero score 2u(fu) given in §4, we are interested in an
upper bound U that gives3



u U
2u(fu)  1 - .
(9)


Unfortunately, this problem is ill-defined and many such U
may exist.

8.1 Breadth and Depth Searches
In order to specify the problem, importance can be given
either to having low-order interactions, or a small number of
variables. For the former, a breadth-first search can be per-
formed, including all low-order interactions until the fitting
requirement (9) is met. In this case we successively include
the term with highest L2CoE among those candidates of
lowest order. In doing this we maintain the hierarchical re-
quirement that possible candidates must already have all of
their subsets included.
This algorithm is given formally in the pseudo-code below.

S = 0
U = 
Loop:
K1 = {u  1,...,n : v  u,|v| = i}
K2 = {u  K1 : |u| = minv
K1
|v|}
For Each u  K2:

3
u
U is here taken to indicate v  U : u  v.
Calculate PF (u)
v  argmaxu
K
PF(u)
U  U  v
S  S + 2(v)
End Loop S > 1 -

We can estimate

^
2u(u) =
vu
(-1)|u
|-|v|
^
¯
2(v).


The effect of this is to place a penalty on the maximum size
of an interaction, only including higher-order terms after all
the lower-order have been entered. Here we are looking for
a graph that minimizes the size of the greatest "cartwheel"
in favor of many lower-order edges.
The alternative is a depth-first search: including the term
with highest L2CoE among those candidates of highest or-
der. The pseudo-code is identical to that above, replacing
the definition of K2 with

K2 = {u  K : |u| = max
vK
|v|}.

This will do the exact converse to the breadth-first search
and include a new predictor variable only after all interac-
tions in the current set of predictors have been allowed. The
graph from this is expected to have large "cartwheels" but
a smaller number of nodes.

8.2 Diagnostics and Greatest Lower Bounds
Given that the problem (9) is poorly specified, we believe
that the algorithm as originally stated provides a reasonable
set of interactions. The interactions resulting from it can be
interpreted as a greatest lower bound on the sets U that
satisfy (9).

Theorem 8.1. The collection V = {u : ¯
2u  } repre-
sents a lattice greatest lower bound on collections U that
satisfy (9).

Proof. Suppose that ¯
2u < , then the collection U =
{v : (v  u)} has


v U
2u(fu) = 1 -
wV
2w(fw)


> 1 - .

Conversely, for ¯
2u  , any U with u
U has


v U
2u(fu) = 1 -
wV
2w(fw)


 1 - .



This is satisfying in providing a collection of subsets that
must be included to provide a good fit. Further, it aids
interpretation significantly by providing a smaller collection
of interactions.
There remains the question of choosing . In §9 we have
employed a criteria of explaining 99% of the variance of
the function on the empirical distribution of the training
data. An alternative is to consider an ordered plot of scores
for main effects - these typically die off exponentially - and
choose a cutoff manually using a natural break in the scores.
This cutoff could be employed throughout, or rechosen at
every interaction size. It may well be advantageous to be
more or less aggressive in choosing how many terms to in-
clude when the relative scores are seen.



579
Research Track Poster

9. EXAMPLE: BOSTON HOUSING DATA
NEURAL NETWORK
We examine the ANOVA structure of the 13-16-1 neural
network from §3. Here we picked = 0.7 which corresponds
to 1% of the over-all variance of the function, calculated from
the predictions given at the original data. At this cut-off, we
include all but three variables: "chas," "indus" and "zn."
The remaining variables all have interactions with "lstat"
apart from "b." These are very similar findings to those of
[13]. There is only one second-order interaction, in "age"-
"tax"-"b". This gives us the final VIN plot in Figure 4. An
additive model fit using these interactions and cubic splines
improved test error by 12% indicating that this structure is
already sufficiently flexible to model the data well.
From a diagnostic point of view, Figure 4 indicates that
this prediction function can be well represented by a sum of
low dimensional terms. This means that, plots of bivariate
representations of the function do provide us with a close-
to-comprehensive account of functional behavior. However,
care should be taken when examining interactions between
the "age", "tax" and "b" predictors.




Figure 4: Variable Interaction Network for a Neural
Network trained on the Boston Housing Data, with
cutoff = 0.7.



10. CONCLUSIONS
The work presented here can be viewed as a lattice search
for a greatest lower bound on the set of hierarchical func-
tional ANOVA components that can be used to represent
a learned predictor function. This allows us to represent
the complexity of a function in terms of the size of its non-
additive interaction components, providing not only an in-
dication of the intrinsic dimensionality of the system, but
which predictors interact in a non-additive manner.
This algorithm employs the monotonicity of the L2CoE
measure to provide an efficient search through this lattice.
Additional sampling theory allows us to do this in O(N)
function evaluations. We have developed a graphical dis-
play to make the results more accessible to the user and
demonstrated that it has good interpretational properties.
The empirical estimators of which we used are tied di-
rectly to the estimation of partial dependence plots. These
have been chosen as providing a data-driven method which
distorts the distribution of predictor variables less than the
uniform distribution normally associated with the functional
ANOVA. Results in [8] suggest that it is possible to estimate
L2CoE measures without requiring any predictor variables
to be independent. Where the underlying distribution is
not an issue or is unknown, estimates based on a uniform
distribution can be employed. The algorithms presented in
this paper are compatible with any measure of interaction
importance for which monotonicity holds.

Acknowledgements
The author would like to thank Jerome Friedman and Art
Owen for helpful discussion.


11. REFERENCES
[1] R. Agrawal and R. Srikant. Fast algorithms for mining
association rules. In Proceedings of the 20th
International Conference on Very Large Databases,
1994.
[2] L. Breiman. Bagging predictors. Mach. Learn.,
24(2):123­140, 1996.
[3] A. Buja, D. F. Swayne, M. L. Littman, N. Dean, and
H. Hofmann. Xgvis: Interactive data visualization
with multidimensional scaling, 2001.
http://www.research.att.com/areas/stat/xgobi/index.html.
[4] S. E. Feinberg. The Analysis of Cross-Classified
Categorical Data. MIT Press, 1980.
[5] J. H. Friedman. Greedy function approximation: A
gradient boosting machine. Annals of Statistics,
29(5):1189­1232, 2001.
[6] D. Harrison and D. L. Rubinfeld. Hedonic prices and
the demand for clean air. Journal of Environmental
Economics and Management, 5:81­102, 1978.
[7] W. Hoeffding. A class of statistics with asymptotically
normal distributions. Annals of Mathematical
Statistics, 19:293­325, 1948.
[8] G. Hooker. Black box diagnostics and the problem of
extrapolation: Extending the functional anova.
Technical report, Stanford University, 2004.
[9] T. Jiang and A. B. Owen. Quasi-regression with
shrinkage. Math. Comput. Simul., 62(3-6):231­241,
2003.
[10] R. Liu and A. B. Owen. Estimating mean
dimensionality. Technical report, Stanford University,
2003.
[11] A. B. Owen. The dimension distribution and
quadrature test functions. Statistica Sinica, 13(1),
2003.
[12] R-project. http://www.r-project.org/.
[13] C. Roosen. Visualization and Exploration of
High-Dimensional Functions Using the Functional
Anova Decomposition. PhD thesis, Stanford
University, 1995.
[14] I. M. Sobol. Global sensitivity indices for nonlinear
mathematical models and their monte carlo estimates.
Mathematics and Computers in Simulation,
5:271­280, 2001.




580
Research Track Poster

