Activity
Monitoring:
Noticing
interesting
changes
in behavior

Tom Fawcett
Bell Atlantic
Science and Technology
500 Westchester Ave
White Plains, New York 10604
tfawcett@acm.org



Abstract
We introduce
a problem class which we term activity
moni-
toring. Such problems involve monitoring
the behavior
of a
large population
of entities for interesting
events requiring
action.
We present a framework
within
which each of the
individual
problems
has a natural
expression,
as well as a
methodology
for evaluating
performance
of activity
moni-
toring techniques.
We show that two superficially
different
tasks, news story monitoring
and intrusion
detection,
can
be expressed naturally
within the framework,
and show that
key differences in solution methods can be compared.


1
Introduction
In this paper we introduce
a problem class which we
term activity
monitoring.
Such problems typically
in-
volve monitoring
the behavior of a large population
of
entities for interesting
events requiring
action.
Exam-
ples include the tasks of fraud detection, computer in-
trusion detection, network performance monitoring,
cri-
sis monitoring,
some forms of fault detection, and news
story monitoring.
These applications
may differ greatly
in representation.
For example, the data streams be-
ing monitored may be streams of numbers, mixed-type
feature vectors, or free-text documents. However, prob-
lems in this class share significant
characteristics
that
differentiate
them from other KDD problems.
The goal of activity
monitoring
is to issue alarms ac-
curately and in a timely fashion. Standard KDD tech-
niques such as classification,
regression and time series
analysis are useful as solution
components,
but these
techniques
do not completely
address the goal.
Al-
though activity
monitoring
applications
have received
much attention
individually,
to our knowledge
they
have never been generalized and analyzed in a common
framework.
We present a framework within which these

Permission
to make digital
or hard copies ofall
or part ofthis
work fol
,,ersonal
or classroom
use is granted without
fee
proded thatcopies
are not made or distributed
for profit
or commercial
advantage
and that
copies hear this notice and the full citation
on the lirst page. TO COPY
ot)ler\visc,
to republish,
to post on scrvcrs or to redisrribute
to lists,
requires prior specific
permission
and/or a fee.
KDD-99
San Diego
CIA USA
Copyright
ACM
1999 l-581 13-143-7/99/08...$5.00
Foster Provost
Bell Atlantic
Science and Technology
500 Westchester Ave
White Plains, New York 10604
provost@acm.org



tasks have a natural expression.
This framework cod-
ifies similarities
of the tasks and highlights
significant
differences.
We begin by discussing briefly cellular phone fraud
detection as a domain to illustrate
some of the issues
in activity
monitoring.
We define the problem formally
and present an evaluation
methodology.
Then we ex-
plain some important
differences between types of ac-
tivity
monitoring
tasks. The resulting framework com-
prises the problem definition,
the evaluation methodol-
ogy, and the categorization
of activity
monitoring
tech-
niques. We believe this framework is a significant
con-
tribution
because it helps to understand activity
moni-
toring better in general. To demonstrate this, we show
that it aids in understanding
how methods from fraud
detection can apply directly
to the seemingly different
problem of creating stock market alerts based on news
stories, yet the same methods do not seem to work well
on the problem of computer intrusion
detection, which
seems closely related to fraud detection.


2
Cellular
phone
fraud
detection

Cellular
phone fraud
detection
is a typical
activity
monitoring
problem.
The task is to scan a large set
of accounts, examining
the calling behavior
of each,
and to issue an alarm when an account appears to have
been defrauded.
This can be done in many ways, such
as profiling
users' behavior,
looking for known fraud
patterns, etc. The goal is to identify fraudulent
activity
as soon as possible without
creating too many false
alarms [7].
Calling activity
may be represented in various ways,
but is usually described with call records.
Each call
record is a vector of features, both continuous
(e.g.,
CALLDURATION)
and discrete
(e.g., CALLING-CITY).
However, there is no inherent primitive
representation
in this domain.
Calls can be aggregated by time,
for example into call-hours
or call-days.
Though
it
is not obvious,
they can also be broken up into a
finer grainsize:
more detailed intra-call
information
is
available,
such as the delays between button
presses,




53

switch hand-offs during each call, etc.
The task is to monitor all accounts continuously
and
to identify
fraudulent
activity
as soon as possible.
In
one intervention
scenario, when an alarm is issued, a
human fraud analyst must check the account carefully
and decide what to do. Therefore, it is important
that
the system not create too many false alarms.
The
precise definitions
of "as soon as possible"
and "too
many false alarms" vary with the amount of fraud, the
size of the workforce, and other factors.
In evaluating solutions to this problem, several issues
arise:

Granularity:
Because of the different
possible
problem representations,
it is difficult
to evaluate
and compare different
solutions.
A method that
classifies individual
calls cannot be compared easily
to one that
classifies account-days
or individual
segments of calls. Both the false alarm rate and the
true alarm rate must be normalized before solutions
can be compared.

Multiple
alarms:
Alarms do not have equal value.
For example,
a monitor
might
generate several
alarms in a row for a given defrauded account. The
first alarm is important,
but subsequent alarms oc-
curring closely thereafter contribute
virtually
noth-
ing. Evaluation
should take into account the dimin-
ished importance
of multiple
alarms.

Benefit
of
timely
alarms:
Fraud
should
be
detected as soon as possible and evaluation
should
reflect this. The benefit of an earlier alarm can often
be quantified.
In cellular
phone fraud detection,
the cost of delaying an alarm may be measured in
real time, the number of fraudulent
calls missed,
the amount of uncompensated
airtime or the total
expense incurred by the fraud.

These characteristics
are common among activity
monitoring
domains.
Perhaps surprisingly,
most prior
work in such domains ignores one or more of these is-
sues, even though they are exhibited in the correspond-
ing application.
Section 5 argues that standard evalu-
ation metrics such as classification
accuracy, cost, and
even ROC analysis are insufficient
for evaluating activ-
ity monitoring
systems. In the next section we intro-
duce a formalism and a framework that can accommo-
date these complexities.

3
Activity
Monitoring
We define the general problem as follows.
Let 27 be a
set of data streams, where each data stream Di E V is
an ordered set of data items {di,i, . . . , di,k;} (ki need
not equal lcj, for i # j-that
is, the length of the
episodes need not be equal). We will be liberal in what
are considered data-numeric
measurements,
vectors
of symbolic
features,
text,
etc.
Each data stream
represents information
about
a single entity
in the
population
being monitored,
for example, customers,
users, network equipment, nuclear reactor components,
or even potential
investment
opportunities.
For the
rest of this paper we only refer to one data stream
at a time, so for simplicity
we omit the subscript
i,
and refer instead to a data stream D and its elements
{dj},j
= l,...
,Ic.
The dj's are information
related to the entity
be-
ing monitored,
and comprise the evidence upon which
alarms are to be issued. The data may be measurements
taken directly from the entity, such as the temperature
of an engine part at a point in time. They may be de-
scriptions of the entity's behavior, such as transactions
billed to an account. They may also be indirect descrip-
tions of the behavior from a third party, such as news
stories about a particular
company. The dj's are totally
ordered in time, so j > lc H time(dj)
> time(dk).
The
dj's are not necessarily evenly spaced in time.
Activity
monitoring
is the task of analyzing
the
data streams in order to detect the occurrence
of
interesting
behavior,
which
we refer to as positive
activity.
Episodes of positive
activity
need not be
similar to each other, nor do is the non-positive
activity
in one data stream necessarily similar to that of any
other.
Let r be a point in time denoting the onset of
positive activity.
For this paper, we assume that each
D contains at most one r, and that if there is a period
of positive
activity,
it is at the end of D.
Although
real data streams may contain
multiple
periods
of
positive
activity,
we will see that this assumption
is
not overly
restrictive.
For the data stream D, r
designates the beginning
of a contiguous
subsequence
D, = (dPl, . . . ,dP,)
such that time(di)
2 7 ti
di E
D,.
Because the positive activity
is necessarily at the
end of D, p,
= k. Note that the positive activity
is
separate from the existence of the activity's
evidence:
D, may be the empty sequence. The goal of an activity
monitor
is to give an indication
that the sequence is
exhibiting
positive activity;
such an indication
is called
an alarm. Formally, an alarm cr represents the point in
time when it is issued.
Figure 1 illustrates
a data stream with two alarms.
A monitor
may issue multiple
alarms for any D. For
completeness,
a D that contains no positive
activity
implies r = co; if a monitor
never alarms on a D, we
consider cr = co.


3.1
Costs
and
benefits

There
is a crucial
difference
between
this problem
formulation
and those used in previous work. The goal
in activity
monitoring
is not to identify
completely
all
the positive activity,
nor to classify each dj as positive




54

Normal activity
Positive activity




Time
I
I
I
CXl
z
a2



7 = onset of positive activity
al = false alarm
a2 = hit


Figure 1: A data stream D with alarms (~1 and crz.



or negative. Rather, the goal is to identify
in a timely
fashion
that positive
activity
has begun.
Alarming
earlier may be more beneficial, but after a first alarm, a
second alarm on the same sequence may add no value.
To accommodate these considerations,
we define two
task-dependent
functions:
a score function
s and a
false alarm function
f.
Let so(r,a,
H, 0) be a score
function
which returns the value (benefit) of an alarm
cr on a given subsequence, with respect to a given r.
The variable H is a sequence of the alarms that have
already occurred on the sequence. We can now define
the positive activity
in terms of s: it is the subsequence
of D for which s.o(r, time(dj),
(),D)
> 0.
A symmetric
function
f~ can be defined for false
alarm penalties.
Let f~(a, H, 0) be the cost of a false
alarm on sequence D, with H being the sequence of
false alarms already issued.
We make several simplifications
to these functions for
presentation.
Within
a given domain we can eliminate
the D subscript, although it is important
to remember
that the score function depends on the domain and on
the particular
sequence of activity.
For this paper we
will assume that for most domains the first alarm is
significant
but subsequent alarms add no value, so the
score function returns a non-zero value only if H is the
empty sequence. Thus we remove H from s and assume
an implicit
clause within
s:

s(~,cx,H,D)
= OifH
# ()

Similarly,
for this paper we assume that the cost of a
false alarm is constant and does not depend on history,
so f reduces to a variable representing this cost.
The goal of activity
monitoring
is two-fold.
A tech-
nique should maximize the expected value of s(r, CY,D)
for any given alarm, while minimizing
the number of
false alarms.
Adopting
standard
terminology,
Q is
a false alarm if (I: < I-; (Y is a hit if cr > r and
~(7, (Y,D) > 0. If r < 00 and there does not exist
an (Y such that r 5 (Y < oo then the interesting
period
was missed.
It is important
to understand
that the concept of
"true negative" is not well defined in activity
monitor-
ing. In practice, one can create definitions
based on the
particular
representation
used, but in principle
there
are infinitely
many possible "true negatives," because of
the continuity
of time. This observation has been made
before, for example in classification
for visual pattern
recognition
[2], and we will return to it when we discuss
the evaluation framework.

3.2
Cellular
phone
fraud
detection
revisited
The
problem
formulation
may be clarified
by an
example from cellular phone fraud detection [7].l Each
D E D represents a customer account, comprising
a
sequence of cellular phone calls (the dj).
Each dj is a
time-stamped
vector of features representing details of
the call. "Positive activity"
is cellular cloning fraud.
In this domain there are several realistic
ways to
define a score function s(r, Q, D) to measure the benefit
of alarming at cr. One possibility
is to count the number
of fraudulent
calls that would have been made had the
fraud not been caught. These are the calls after (Y:


s(r,o, D) = 1{di E D 1callstart
2 o} 1

A more elaborate score function
calculates the cost
of the airtime of these fraudulent
calls:


S(T,Q, D) = aircost x
c
airtime
d;E{cED 1callstart(c)>a}

where callstart
is the start time of a given call, and
air-cost is the (constant)
cost of a unit of call airtime.
Our previous work assumed a monetary
cost of $.40
per minute for airtime, and a fixed cost of $5 for a false
alarm, so aircost = 0.4 and f = 5;
Our assumption
of at most one 7 per episode is
not problematic
in practice.
If a customer account is
defrauded twice, the sequence can be decomposed into
two single-r sequences without
affecting the evaluation.
More generally, as long as the value of s depends on only
one T, the subsequence can be decomposed into separate
episodes for the purpose of evaluation.
There may be
cases where s would combine value from multiple
r's,
but we are aware of no non-contrived
examples.

3.3
Evaluating
activity
monitoring
performance
We have argued
elsewhere
[12, 131 that
Receiver
Operating
Characteristic
(ROC)
analysis,
a method

`We did not use this framework
in our prior
work,
which we
criticize
later along with other prior work.

False positive
rate
OO1
0.2
0.4
0.6
0.3
1I


False alarm rate


(4
(b)
(cl


Figure 2: (a) AMOC curve of a random classifier, (b) ROC curves of three classifiers, (c) AMOC curves of the same
three classifiers.


from signal detection theory, is the appropriate
metric
for analyzing
classifiers
under
imprecision.
ROC
analysis
depicts
the tradeoff
between
true
positive
classifications and false positive ones, which is similar to
the goal of activity
monitoring.
For activity
monitoring
we use ROC analysis with two minor modifications,
which for this paper we call AMOC
(Activity
Monitor
Operating
Characteristic).

On the X-axis,
ROC curves plot the false-positive
rate,
meaning
here the percentage
of the negative
examples classified as positive.
Since we have no fixed
notion
of a "negative
example,"
we define the false-
alarm rate to be the number of false alarms, normalized
by the distance metric
used in s, for example,
the
expected number of false alarms per unit time.
This
is analogous to the FROC technique
found in visual
pattern
recognition
[2], which also assumes no fixed
notion of a negative example so false alarms per unit
area are measured. Normalizing
false-alarm rate allows
us to compare methods
that
cannot
be compared
under standard ROC analysis.
For example, in fraud
detection
a transaction
classifier and an account-day
classifier have different notions of negative example, so
their standard false-positive
rates cannot be compared
directly.
However, their normalized
false-alarm
rates
can, because each produces a certain number of false
alarms per hour.
The second modification
to ROC
analysis is that on the Y-axis, rather than plotting
the
true positive rate, AMOCs plot the expected value of
47, a, 0).
AMOCs retain some of the advantages of ROC curves
[12] but are tailored to activity
monitoring.
An example
highlights
the differences.
Consider a fraud detection
problem where fraud must be caught within
five hours
of its onset.
The score function
is binary, where the
benefit of fraud detection is 1 only if detection is within
five hours:


S(T,&D)
=
1 if O<a-r15
0
otherwise

The false alarm rate is normalized per hour.
Figure 2a illustrates
why even when simplified
there
is a fundamental
difference between AMOC
and ROC
curves.
The figure
shows an AMOC
curve for a
hypothetical
fraud detector. How good is the detection?
In fact, this curve shows the performance of classifiers
that alarm randomly with different frequencies (0.1 per
hour, 0.2 per hour, etc.). Unlike ROC curves, for which
the diagonal y = 2 depicts the performance of random
guessing, in AMOCs
performance
depends upon the
definition
of s. Even with a simple binary
definition
of s, an AMOC is not necessarily the same as an ROC.
Figures 2b and c show why the AMOC
formulation
is important
for evaluating
activity
monitoring.
The
figures show ROC and AMOC curves for three classifiers
on an activity
monitoring
domain. The ROC curves in
b show that classifier 3 dominates the other two in ROC
space [12], so its instance classification
performance
is
generally superior.
From this we might conclude that
it is unconditionally
best. However, the AMOC
curves
in c show that classifier 2 performs better for activity
monitoring
if maximal
detection
score is much more
important
than false alarm rate.
Figure 3 shows an algorithm for generating an AMOC
curve from a probabilistic
classifier.
As with
ROC
curves,
to generate AMOCs
it is not necessary to
specify thresholds on the classifier's continuous output.
Given
an assignment
of probabilities
to examples,
the examples are sorted decreasing by their assigned
probabilities.
Then the sorted list is traversed, updating




56

Given:
Alarms:
Set of alarm tuples
(p,a,i) where:
p: probability
calculated
by the classifier
a: alarm
i: index of D;
Output:
R: Set of points on AMOC.

S = 0; /*
Current
cumulative
score */
F = 0; /* False alarm rate */
R = {(`A 011;
sort Alarms
in decreasing
order by p values;
for
((p,a, i) E Alarms) do
if (a < 7;) then
/* a is a false alarm
*/
F=F+f;
else
/* a is in activity
period
*/
if (Hi is undefined)
then
/*
First
alarm
*/
S = S + S(C, a, D);
H; = a;
else if (a < H;) then
/* Earlier
than previous
first
alarm
*/
S = S - s(q, H;, D);
S = S + s(q, a, D);
H; = cr;
end if
end if
end if
Add point
(F, S) to R;
end for;
Stota1 = s;
Ftotal = F;
/* Scale F and S values to [O,l]
*/
for
((F,S)
E R) do
F = FIFtotal;
s
=
SIStota1;
end for;




Figure 3: Algorithm
for generating
an AMOC
curve
from a set of alarms


the score and the false-alarm rate after each example.
Each update produces a point on the AMOC curve; this
point represents the performance of the classifier that
would result from placing a threshold
just below the
corresponding
probability.
One list traversal generates
points for all thresholds.
Because we have made a
simplifying
assumption that any true alarms after the
first have no contribution
to the score, we need deal
only with one other alarm (Hi) as history.
This keeps
the algorithm's
time complexity
to O(n).

3.4
Key differences
between
methods
There are many similarities
between activity monitoring
problems.
A framework
for a problem
class also
should aid in understanding
intraclass
differences; for
example, which differences are superficial and which are
fundamental.
We now focus on one distinction:
the
nature of the modeling and monitoring
process. There
are two high-level classes of methods:


l
A profiling
method
builds
a model
of normal
(non-positive)
activity
without
reference to positive
activity.
A system can then use profiles to alarm on
activity
that deviates significantly
from normal.

l
A discriminating
method builds a model of inter-
esting (positive)
activity,
usually with reference to
normal activity.
This model is then used to scan for
positive activity.

Similar distinctions
have been made regarding meth-
ods for simple classification
[14]. Here the distinction
is based on a fundamental
asymmetry of activity
mon-
itoring problems: normal activity
is common and posi-
tive activity
is rare. Therefore, methods other than su-
pervised learning algorithms can play a significant
role.
For example, modeling normal activity
with a regres-
sion curve or a probability
distribution
may be useful
for judging typicality
of subsequent activity.
An orthogonal
distinction
separates methods that
consider activity
to be uniform
over all Di from those
that model each individual
Di.
Adding
this second
distinction
yields four distinct
categories of methods:

1.




2.




3.




4.
Uniform profiling
builds a profile of normal activity,
assuming it is uniform across all Di.
For example,
in a network surveillance task where only errors are
reported,
a general profile might be a stream of
zeros, or a mean and standard deviation
of errors
over the population.

Individual
profiling
builds a profile of the normal
activity
of each Di.
For example,
one type of
intrusion
detection
creates a profile of each user's
normal activity,
and then looks for deviations
from
the profile.

Uniform
discriminating
builds a general model to
distinguish
positive
from uniform
normal activity.
For example, creating investment
alerts from com-
pany news bulletins requires forming a model of the
type of news that indicates an interesting investment
opportunity.

Individual
discriminating
builds a model to distin-
guish positive activity
from the normal activity
of
each Di.
For example, examples of intrusions
may
be compared with each user's normal
activity
to
build individualized
scanning models.

We have considered the uniform/individual
distinc-
tion only with respect to normal activity.
For discrimi-
nating models, this distinction
may be made with re-
spect to the positive
activity
as well.
For example,
in monitoring
device behaviors, particular
devices may
have idiosyncratic
failure modes that must be modeled.
However, such individualized
positive
activity
is un-
usual; positive activity
typically
either is too scarce to
be modeled individually,
or simply is not related to any
particular
data stream.



57

For some tasks, discriminating
models may be suf-
ficient.
In intrusion
detection,
misuse detection is a
simple example: "defining intrusions ahead of time and
watching
for their occurrence"
[9, p.263. As another
example, consider monitoring
news stories for interest-
ing investment
opportunities.
Noticing
that today's
news text differs from prior news text is not particu-
larly promising,
but a discriminating
model of "useful
investment news" could be quite valuable.
Are there cases where profiling
would be more use-
ful than discriminating?
Detecting non-malicious
intru-
sions is one example. For this task a model of positive
activity
may not exist, so discriminating
may be im-
possible.
An instance of profiling,
anomaly detection
"observes the activity
of subjects and generates profiles
for them that represent their behavior"
[9, p.161. Profil-
ing is much more useful than discriminating
in the bur-
geoning application
of news story topic detection and
tracking [l]. When positive activity
is simply the intro-
duction of new news topics, discriminating
via modeling
positive activity
may be of little use.
Profiling
and discriminating
are two disjoint
classes
of techniques, but in practice they often are interwoven.
For example, the DC-l system [7] builds fraud detectors
automatically
by first using normal and positive activity
to construct
dimensions along which profiling
will be
effective. Then it creates profiling
monitors that model
each account's normal activity
with respect to a single
dimension.
Finally
it uses the outputs of the profiling
monitors
(indicating
how far from normal the current
activity
is) as inputs
to a discriminator
that
was
trained to distinguish
positive deviations
from normal
deviations.
DC-l uses a particular
type of discriminating
method
we call change detection, which is based on a model of
transitions
that occur from normal to positive activity
in each individual
account.
To be useful, a change
model must be able to refer to a profile of normal
activity
for each account.
Change detection
may
be effective when there is little
commonality
within
positive
activity,
but significant
differences
between
normal and positive
activity.
For example, in fraud
detection,
there may be no times of day or locations
that indicate
fraud, but for a given account changes
from its typical time of day or location
usage may be
very reliable indicators.
Change detection
capitalizes
(in the modeling)
on the temporal
nature of activity
monitoring
problems.
Technically,
uniform and individual
modeling are two
ends of a spectrum of groupings of data streams.
In
many applications,
Di's can be clustered
into useful
groups such that profiling
or discriminating
may be
made more effective.
For example,
for phone-fraud
detection,
customers often can be grouped usefully as
high-volume
users, nine-to-five
users, emergency-only
users, etc. This paper does not address the problem of
grouping data streams.


4
Empirical
Demonstration
We have claimed that this framework
allows activity
monitoring
tasks to be viewed as a single type of prob-
lem, and that the distinctions
that arise help to under-
stand this type of problem.
We now provide a demon-
stration that the framework facilitates understanding
of
activity
monitoring
across different domains.
First we describe the task of creating
investment-
related news alerts, which is superficially
very different
from fraud detection,
but similar as an activity
moni-
toring task. Once representational
issues are addressed,
we can apply a system designed for fraud detection to
this seemingly different problem. We then use the con-
cepts of the framework to explain why this is the case.
Secondly, we consider a task of computer intrusion
detection.
Though
it is superficially
similar to fraud
detection,
the framework
suggests that
within
the
class they are fundamentally
different.
We show that,
as would
be predicted,
the fraud
detection
system
unmodified
does not perform particularly
well, but that
a simple modification,
suggested by the framework,
performs better.
The goal here is not to advance the state of the art in
either news monitoring
or intrusion
detection.
Rather,
these demonstrations
serve to illustrate
the value of
viewing activity
monitoring
as a problem class.

4.1
An
overview
of the
DC-l
system
Before discussing the two domains, we review briefly the
DC-l
system which will be used in the experiments.
DC-l
was designed previously
to create detectors for
cellular phone fraud by mining
data on the behavior
of cellular
phone accounts.
It is described in detail
elsewhere [7]; only its salient aspects will be covered
here, cast in the activity
monitoring
framework.
DC-l
constructs
a fraud detector
in three stages.
First
it generates rules that
distinguish
fraudulent
calls from legitimate
ones.
This can be done using
either uniform
discriminating
modeling
(e.g., learning
a classifier from positive and negativecalls)
or change
modeling (e.g., learning rules to distinguish
fraudulent
changes in behavior within individual
accounts). These
rules are then used to create profiling
monitors.
Each
monitor
models the behavior
of each account
with
respect to one rule and, when monitoring,
describes how
far the account's activity
is from its typical
behavior.
Finally, DC-l weights the monitor outputs to maximize
the effectiveness of the resulting fraud detector.
To do
so, the outputs of the monitors are provided as features
to a standard learning program along with the desired
output
(an account-day's
correct class: fraud or non-
fraud).
The result is a weighted sum of the outputs of



58

fourth
compare cd]
income
quarter
liscal
earnings
per
diluted
fiscal
quarter
ended
expenses
months ended
today
reported
consensus
quarter
earnings
year
ended
repurchase
lower
than
research
[and]
development
fourth-quarter
first
call
9 Cl2341
below analyst
for
quarter
shortfall
said [it]
expects
same period
revenues
increase
over
per share




Table 1: Change indicators from news stories preceding stock spikes. Text in brackets was removed in lexical analysis.



the monitors, along with a threshold on the sum.
In use, the monitors
view each day's calls from a
given account, and each monitor
generates a number
indicating
how unusual that account-day looks for the
account.
The numeric outputs from the monitors
are
treated as evidence and are combined by the detector.
When the detector has enough evidence of fraudulent
activity
on an account, based on the indications
of the
monitors, it generates an alarm.

4.2
News Story
Monitoring
The goal of this task is to scan news stories associated
with a large number of companies and to issue alarms
on specific companies when their stocks are about to
exhibit positive activity.
We collected stories and stock
prices for approximately
6000 companies over a three
month period.
Each story was tagged with a list of
companies to which it pertained; each company's stories
constituted
a different data stream, and an "interesting
event"
(positive
activity)
was defined to be a 10%
change in the company's stock price (a price "spike")
in either direction.
The activity
monitoring
goal is to
minimize
the number of false alarms and to maximize
the number of correctly predicted price spikes.
This domain seems very different from fraud detec-
tion. The data items are represented as free text rather
than as predefined feature vectors of numeric and dis-
crete attributes.
The news stories are not as clearly
associated with positive activity
as are cellular phone
calls, which are carefully tagged by a cellular company's
billing
system.
Also, the alarms of this domain are
fundamentally
"opportunistic"
rather than failure- or
crisis-driven.
A researcher attacking this problem would
likely see it as very dissimilar
to fraud detection,
and
would probably would seek an information
retrieval so-
lution, using precision and recall as evaluation metrics.
However,
generating
stock alerts is easily cast as
an activity
monitoring
problem.
Each news story
constitutes
a dj of its associated company (D).
We
heuristically
associate with
a price spike any story
appearing from midnight
the day before the spike up
until lo:30 AM of the day of the spike.2 With the prior

2The actual
heuristics
used are more complex
than
this,
in
order to filter
secondary
stories reporting
on the stock's activity.
This does not affect the definition
of 7.
midnight
as r, the scoring function s for an alarm (Ycan
be defined as:
-
S(T,cy,D)
1 if 0 5 rr
r 5 34.5 hours
=
0
otherwise

The news story domain is similar to fraud detection
with respect to building discriminating
models. Specifi-
cally, "positive"
news stories can be differentiated
from
"normal"
stories using DC-l's
data mining.
Because
news story text is superficially
quite different from cel-
lular call records, the representation
had to be adjusted
before DC-l could be applied. Each story was lexically
analyzed and reduced to its constituent
words.
The
words were stemmed, then a stop list was applied to re-
move common noise words. The final representation for
a story was a set of its processed words and bi-grams.
The reduced stories were each labeled as positive or neg-
ative using the heuristics discussed above. DC-l then
learned indicators
of positive
activity
(discriminating
models) from these stories. Some specific indicators are
shown in Table 1. These indicators
were used to form
a DC-l monitor (as described above).
Figure 4 shows the performance of different activity
monitors
applied
to this domain.
Each curve was
generated by lo-fold cross-validation.
Random
shows
the performance
of monitors that alarm randomly
on
stories with varying probabilities
(each probability
gives
one curve point).
DC-l
shows the performance of the
DC-l monitor.
Consider this problem in terms of the distinctions
made in Section 3.4.
Because of the large number
of features
(approximately
lo5 words and bi-grams
appeared more than once), and because the news is
always changing,
we would
expect little
behavioral
consistency at the word level. Consequently, we would
predict
that
profiling
would add little
compared to
discriminating;
specifically,
that the profiling
done by
DC-l adds little to the activity
monitoring
performance
on this task.
The curve labeled
DC-l
without
profiling
demonstrates
this:
the performance indeed
decreases little
when profiling
is turned off, at which
point DC-l just scans using the learned classifier.

4.3
Intrusion
Detection
Intrusion
detection
is a field of computer
security
concerned with
detecting
attacks on computers
and



59

0 l
I
0
0.2
0.4
0.6
0.6
1
False alarm rate


Figure 4: Performance of several methods for news story
monitoring



computer networks [8, 91. Within
intrusion
detection,
anomaly
detection
systems characterize
behavior
of
individual
users and issue alarms of intrusions,
based
on anomalies in behavior.
Since traces of actual computer
intrusions
are rare
and difficult
to obtain, we chose a variant of this task
common in computer intrusion
research [6, 10, 151. In
this variant, the goal is to predict, based on commands
typed, when the user of a given account is not the actual
legitimate
user. With such a task, typically
one user
at a time is chosen to be the "legitimate"
user and
one to be the "intruder."
The task then becomes one
of characterizing
the behavior
of each user.
For this
domain we used a dataset of Unix commands taken from
about 8000 login sessions collected from a population
of
77 users.3
Intrusion
detection
is easily cast as activity
moni-
toring.
Each user constitutes
a D with an associated
stream of sessions dj . Each session is a set of Unix com-
mands. False positives were normalized per session. We
consider an alarm effective only if it occurs within
the
first five intrusion sessions, so the s function was defined
as:

S(T,cqD)
1
=
if 0 5 1{di E D, 1time(di)
5 a}1 < 5
0
otherwise

Change modeling can be used to discover differences
between command sets used by various users. Table 2
shows some of the change indicators
extracted
from
the Davison-Hirsh
dataset. Although
these commands
may seem mundane, some indicate common distinctions
among Unix users: emacs vs vi, exit
vs logout,
and

3These data were provided
by Davison and Hirsh [5] who used
them in a study on command
sequence prediction.
ol
-
t
8
a
0
0.2
0.4
0.6
0.6
1

False alarm rate

Figure 5: Performance of several methods for intrusion
detection


gs
kill
:Ip
emacs
a.out
pro j ect6.
out
exit
Chat-Client
gee
finger
more

Table 2: Indicators
from change-modeling
Unix users.



text processing
(gs) vs code development,
(gee and
a.out).
Figure 5 shows the performance of several classifiers.
Each curve was generated by lo-fold
cross-validation.
Random
represents the effect of issuing random alarms
with
varying
probabilities
(as above).
DC-l
generated
indicators
such as those above to generate features for
distinguishing
users, and used these indicators
to form
its profiling
monitor.
As shown in Figure 5, DC-l
detects intrusions
better than random.
However, given our experience with the effectiveness
of DC-l for fraud detection,
its performance on intru-
sion detection was disappointing.
We believe that the
reasons for the performance
are clarified
by viewing
the two problems within the activity
monitoring
frame-
work. In cellular fraud detection,
for which DC-l
was
designed, discriminating
positive activity
is an impor-
tant component
because fraudulent
behavior
often is
quite different from legitimate
behavior.
Superficially,
detection intrusions
is very similar to detecting cellular
fraud.
However, note that there is no distinguishable
intruder
behavior because any user can fill the role of
an intruder
with respect to some other user. Discrimi-
native modeling is impracticable
because there is no dis-
tinct positive activity.
Profiling
should be much more
important.
To (partially)
test this hypothesis we created a pro-
filer based on the x2 statistical
test, which models each



60

user's normal
behavior
as a probability
distribution
across the commands they use. As predicted,
the x2
technique outperformed
the DC-l
technique.
A com-
parison of this simple profiler to existing intrusion
de-
tection methods might be revealing.

5
Prior
work
Much work in KDD has involved monitoring
activity for
unusual behavior; for example, fraud detection [3, 4, 71,
intrusion
detection [6, 10, 111,and network monitoring
[16, 171. Indeed it is this wealth of prior work on
closely related problems that makes the formulation
of
a general class worthwhile.
It is important
to examine
how activity
monitoring
problems have been framed
in prior
work, in order to compare them with
the
framework we propose.
Much prior work frames activity
monitoring
as a
classification
problem
in which
the effectiveness
of
alarms
is evaluated
based on simple
classification
accuracy or error rate.
However, such measures are
particularly
inappropriate
for problems such as activity
monitoring
where one class is rare and where the cost of
a miss and the cost of a false alarm are not equal [13].
Other related work addresses unequal costs by fram-
ing the task as a cost-sensitive
classification
problem
[3, 71. Unfortunately,
in activity
monitoring
domains it
is usually difficult
to specify costs precisely.
More im-
portantly,
cost-sensitive classification
relies on a speci-
fication of class priors, which can be even more difficult
to determine precisely. In fact, the work cited uses pri-
ors known to be inaccurate.
It is difficult
to generalize
from such results without special studies weakening the
basic, framing assumption of knowledge of class priors.
Some prior work relies on classification
frameworks
that make explicit
the tradeoff between hits and false
alarms, including
frameworks
that use ROC curves,
precision/recall
curves, and others [6, lo]. If done well,
this addresses the problem of imprecision in knowledge
of costs and of class distributions.
However,
all these classification-based
frameworks
share a basic flaw, which our prior
work on fraud
detection admitted:

In this work we have dealt with differing costs
of false positive and false negative errors. How-
ever, we have still glossed over some complexity.
For a given account, the only false negative fraud
days that incur cost to the company are those
prior to the first true positive alarm. After the
fraud is detected, it is terminated.
Thus, our
analysis overestimates the costs ... [7, p. 3081

The flaw is that they ignore the fundamental
sequential
nature of the problem and, thereby, the goal of timely
classification.
We have concentrated
on classification-
based frameworks because they are the most prevalent;
corresponding
simplifications
are found
in activity
monitoring
work rooted in other disciplines.
Weiss and Hirsh [17] created an activity
monitoring
framework for their study of activity
preceding failures
in a telecommunications
network.
Their
framework
models the sequential nature of the problem as time-
stamped data streams, defines the goal as identifying
a "rare event ," notes that there is a window of the
data stream where (in our terminology)
s(r,cr,D)
> 0,
and looks at the tradeoff between hits and false alarms.
Their
framework
can be expressed within
ours by
appropriate
definitions
of r and s(r,cr,D),
but ours
applies more generally.
In particular,
their framework
requires that the event to be identified
be one of the
dj.
Our framework
allows the event initiating
the
activity
to be separate from the data; for example,
some action of a company triggers news agencies to
report on the action.
More fundamentally,
Weiss and
Hirsh require that the activity
precede the event in
question. While this is appropriate for their application
(network
performance
degrades before a failure),
we
found it hard to extend in general (predictive
activity
does not precede a case of fraud).
We prefer to
recast such problems:
r marks the beginning
of the
period
prior
to the failure
for which detection
will
be useful,
and activity
contained
therein
would be
considered positive
in the application.
For example,
there may be an underlying
occurrence that
causes
(and therefore precedes) the performance degradation;
the failure
itself, just a further
manifestation
of the
underlying
cause, may be a key factor in the definition
of s. In other cases, such as those described by Weiss
and Hirsh, r can be defined as a fixed offset prior to
the hard failure,
representing
"the maximum
amount
of time prior to the target event for which a prediction
is considered correct" [17, p. 3591.
An important
benefit of an activity monitoring
frame-
work is to allow fundamentally
different approaches to
be compared. For example, many fraud detection meth-
ods focus on individual
transactions
[7, 31, and eval-
uation typically
measures classifications
of individual
transactions.
Other methods [7] aggregate transactions
into account-days
and measure differences in activity
between them. A classification-based
framework makes
performance on different representations
(transactions
versus account days) difficult
to compare.
However,
they can be compared within
the activity
monitoring
framework.
Activity
monitoring
tries to identify
where in a se-
quence an interesting
change in behavior has occurred.
This differs from the typical goals of time series anal-
ysis, such as predicting
the next value of a series or
characterizing
the functional
form of the series. HOW-
ever, we are not claiming that any of these methods is
inappropriate
as part of the solution to a activity
mon-



61

itoring problem.
Indeed the opposite is true: all these
methods, standard time-series prediction,
HMMs, clas-
sification
models, and others, are candidate tools for
solving an activity
monitoring
problem.

6
Conclusions
Activity
monitoring
is worth
studying
as a general
KDD problem.
Explicitly
differentiating
this class of
problems should benefit KDD both scientifically
and
practically.
We believe it is an ideal problem
for
KDD because it requires contributions
from (at least)
the three main constituent
subfields:
statistics,
for
expertise on time-series problems; databases, because
of the tremendous
amount of data typically
involved;
and machine learning, because of the representational
complexity.
We hope this paper has laid valuable
groundwork.

7
Acknowledgments
We thank Tom Dietterich
for his encouraging
words
regarding generalizing our work on fraud detection.
We
thank Brian Davison for providing
the Unix command
data used in the computer intrusion
experiments.

References
[l] ALLAN, J., PAPKA, R., AND LAVRENKO,V. On-
line new event detection and tracking.
In Proceed-
ings of the 2lst ACM-SIGIR
International
Confer-
ence on Research and Development in Information
Retrieval
(August 1998).

[2] BURL, M., ASKER, L., SMYTH, P., FAYYAD, U.,
PERONA, P., AUBELE, J., AND CRUMPLER, L.
Learning to recognize volcanoes on Venus. Machine
Learning 30, 2/3 (1998), 165-194.

[3] CHAN, P., ANDSTOLFO,S. Toward scalable learn-
ing with non-uniform
class and cost distributions:
A case study in credit card fraud detection.
In
KDD-98
(1998), Agrawal, Stolorz, and Piatetsky-
Shapiro, Eds., AAAI
Press, pp. 164-168.

[4] Cox, K., EICK,~., WILLS, G., ANDBRACHMAN,
R. Visual data mining: Recognizing telephone call-
ing fraud. Data Mining
and Knowledge Discovery
1, 2 (1997), 225-231.

[5] DAVISON, B. D., AND HIRSH, H.
Predicting
sequences of user actions. In Predicting the Future:
AI Approaches
to Time Series Problems
(July
1998), AAAI
Press, pp. 5-12. WS-98-07.

[6] DUMOUCHEL, W., AND SCHONLAU,M.
A fast
computer intrusion
detection
algorithm
based on
hypothesis
testing of command transition
proba-
bilities.
In KDD-98
(1998), Agrawal, Stolorz, and
Piatetsky-Shapiro,
Eds., AAAI Press, pp. 189-193.
[7] FAWCETT,T., AND PROVOST,F. Adaptive fraud
detection.
Data Mining
and Knowledge Discovery
1, 3 (1997), 291-316.

PI FRANK,
J.
Machine
learning
and intrustion
detection:
Current
and future
directions.
In
National
Computer Security Conference (October
1994), vol. 1, pp. 22-33.

[9] KUMAR, S.
A Pattern
Matching
Approach
to
Misuse Intrusion
Detection.
PhD thesis, Purdue
University,
Department
of Computer
Sciences,
August 1995.

[lo] LANE, T.,
AND BRODLEY, C.
Approaches
to online
learning
and concept
drift
for user
identification
in computer
security.
In KDD-98
(1998), Agrawal,
Stolorz, and Piatetsky-Shapiro,
Eds., AAAI
Press, pp. 259-263.

[ll]
LEE, W., STOLFO, S., AND MOK, K.
Mining
audit data to build intrusion
detection models. In
KDD-98
(1998), Agrawal, Stolorz, and Piatetsky-
Shapiro, Eds., AAAI
Press, pp. 66-72.

[12] PROVOST,F., AND FAWCETT, T.
Analysis and
visualization
of classifier performance:
Compari-
son under imprecise class and cost distributions.
In Proceedings of KDD-97
(1997), AAAI
Press,
pp. 43-48.

[13] PROVOST,F., FAWCETT, T., AND KOHAVI, R.
The case against accuracy estimation
for compar-
ing induction
algorithms.
In ICML-98
(1998),
AAAI
Press.
Available:
http://www.croftj.
net/-fawcett/papers/ICML98-final.ps.gz.

[14] RUBINSTEIN,Y. D., AND HASTIE, T.
Discrimi-
native vs informative
learning. In KDD-97
(1997),
AAAI
Press, pp. 49-60.

[15] RYAN, J.,
LIN,
M.-J.,
AND MIIKKULAINEN,
R.
Intrusion
detection
with
neural
networks.
In AI Approaches
to Fraud Detection
and Risk
Management (1997), Fawcett, Haimowitz,
Provost,
and Stolfo, Eds., AAAI
Press.

[16] SASISEKHARAN,R., SESHADRO,V., AND WEISS,
S. M.
Proactive
network
maintenance
using
machine learning. In KDD-94
(1994), pp. 453-461.

[17] WEISS, G., AND HIRSH, H. Learning to predict
rare events in event sequences. In KDD-98
(1998),
Agrawal,
Stolorz,
and Piatetsky-Shapiro,
Eds.,
AAAI
Press, pp. 359-363.




62

