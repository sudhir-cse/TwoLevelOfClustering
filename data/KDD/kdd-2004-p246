Machine Learning for Online Query Relaxation


Ion Muslea
SRI International
333 Ravenswood
Menlo Park, CA 94025
ion.muslea@sri.com



ABSTRACT
In this paper we provide a fast, data-driven solution to the
failing query problem: given a query that returns an empty
answer, how can one relax the query's constraints so that
it returns a non-empty set of tuples? We introduce a novel
algorithm, loqr, which is designed to relax queries that are
in the disjunctive normal form and contain a mixture of dis-
crete and continuous attributes. loqr discovers the implicit
relationships that exist among the various domain attributes
and then uses this knowledge to relax the constraints from
the failing query.
In a first step, loqr uses a small, randomly-chosen sub-
set of the target database to learn a set of decision rules
that predict whether an attribute's value satisfies the con-
straints in the failing query; this query-driven operation is
performed online for each failing query. In the second step,
loqr uses nearest-neighbor techniques to find the learned
rule that is the most similar to the failing query; then it
uses the attributes' values from this rule to relax the fail-
ing query's constraints. Our experiments on six application
domains show that loqr is both robust and fast: it suc-
cessfully relaxes more than 95% of the failing queries, and
it takes under a second for processing queries that consist
of up to 20 attributes (larger queries of up to 93 attributes
are processed in several seconds).


Categories and Subject Descriptors
I.2.6 [Artificial intelligence]: Learning


General Terms
Algorithms, Experimentation


Keywords
online query relaxation, failing query, rule learning, nearest
neighbor, Web-based information sources




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
1. INTRODUCTION
The proliferation of online databases lead to an unprece-
dented wealth of information that is accessible via Web-
based interfaces. Unfortunately, exploiting Web-based infor-
mation sources is non-trivial because the user has only indi-
rect access to the data: one cannot browse the whole target
database, but rather only the tuples that satisfy her queries.
In this scenario, a common problem for the casual user is
coping with failing queries, which do not return any answer.
Manually relaxing failing queries is a frustrating, tedious,
time-consuming task because, in the worst case, one must
consider an exponential number of possible relaxations (i.e.,
various combinations of values for each possible subset of
attributes); furthermore, these difficulties are compounded
by the fact that over-relaxing the query (i.e., weakening the
constraints so that there are many tuples that satisfy them)
may lead to prohibitive costs in terms of bandwidth or fees
to be paid per returned tuple. Researchers have proposed
automated approaches to query relaxation [12, 9, 7], but
the existing algorithms have a major limitation: the knowl-
edge used in the query relaxation process is acquired offline,
independently of the failing query to be relaxed.
We introduce here an online, query-guided algorithm for
relaxing failing queries that are in the disjunctive normal
form. Our novel algorithm, loqr1, uses a small, randomly-
chosen subset D of the target database2 to discover implicit
relationships among the various domain attributes; then it
uses this extracted domain knowledge to relax the failing
query.
Given the small dataset D and a failing query, loqr pro-
ceeds as follows. In the first step, it uses D to learn decision
rules that predict the value of each attribute from those of
the other ones. This learning step is on-line and query-
guided: for each attribute Attr in the failing query, loqr
uses the other attributes' values to predict whether the value
of Attr satisfies the query's constraints on it. Then, in a sec-
ond step, loqr uses nearest-neighbor techniques to find the
learned rule that is the most similar to the failing query.
Finally, loqr relaxes the query's constraints by using the
attribute values from this "most similar" rule.
For example, consider an illustrative laptop purchasing
scenario in which the query

Q: Price  $2,000
Display  17
Weight  3 lbs

1
loqr stands for Learning for Online Query Relaxation
2
As we will show in the empirical validation, as long as
the examples in D are representative of those in the target
database T D , D and T D can be - in fact - disjoint.



246
Research Track Paper

fails because large-screen laptops weigh more than three
pounds. In the first step, loqr uses a small dataset of laptop
configurations to learn decision rules such as

R :
Price  $2,300
Cpu < 2.1 GHz
Display  13
 Weight  3lbs

The right-hand side (i.e., the consequent) of this rule con-
sists of a constraint from the failing query, while the left-
hand side specifies the conditions under which this con-
straint is satisfied in the dataset D. After learning such
rules for each constraint in the query, loqr uses nearest-
neighbor techniques to find the learned rule that is the most
similar to the failing query (for the time being, let us assume
that the rule R is this "most similar" rule).
To relax the failing query, loqr "fuses" the constraints on
the attributes that appear in both Q and R. In our example,
the relaxed query

QR :
Price  $2,300
Weight  3 lbs

is obtained as follows: R's constraint on CPU is ignored
because CPU does not appear in Q. Q's constraint on the
screen size is dropped because it conflicts with the one in
R (Display cannot be simultaneously smaller that 13" and
larger than 17"). Finally, Q's price limit is increased to
the value in R, which is less constraining than the original
amount of $2, 000. Note that this price increase is crucial for
ensuring that QR does not fail: as long as the constraints in
QR, which are a subset of R's, are at most as tight as those
in R, QR is guaranteed to match at least the tuples covered
by R (i.e., the examples from which R was learned).
In summary, the main contribution of this paper is a
novel, data-driven approach to query relaxation. Our on-
line, query-guided algorithm uses a small dataset of domain
examples to extract implicit domain knowledge that is then
used to relax the failing query. The rest of the paper is
organized as follows: after discussing the related work, we
present a detailed illustrative example. Then we describe
the loqr algorithm and discuss its empirical validation.

2.
RELATED WORK
Query modification has long been studied in both the fields
of databases and information retrieval [15, 18, 10, 13, 17,
4, 19, 2, 5, 3]. More recently, with the advent of XML,
researchers have proposed approaches for approximate pat-
tern matching [25], answer ranking [11], and XML-oriented
query relaxation [1, 16, 20].
co-op [18] is the first system to address the problem of
empty answers (i.e., failing queries). co-op is based on the
idea of identifying the failing query's erroneous presupposi-
tions, which are best introduced by example. Consider the
query ``get all people who make less than $15K and work in
the R & D department at the Audio, Inc.
company''. Note
that this query may fail for two different reasons: either no-
body in Audio, Inc.'s R & D department makes less than $15K
or the company does not have an R & D department. The for-
mer is a genuine null answer, while the latter is a fake empty
answer that is due to the erroneous presuppositions that the
company has an R & D department. co-op, which focuses on
finding the erroneous presuppositions, transforms the origi-
nal query into an intermediate, graph-oriented language in
which the connected sub-graphs represent the query's pre-
suppositions; if the original query fails, then co-op tests
each presupposition against the database by converting the
subgraphs into sub-queries.
The flex system [23] can be seen as generalizing the ideas
in co-op. flex reaches a high tolerance to incorrect queries
by iteratively interpreting the query at lower levels of cor-
rectness. flex is also cooperative in the sense that, for any
failing query, it provides either an explanation for the failure
or some assistance for turning the query into a non-failing
one. Given a failing query, flex generates a set of more gen-
eral queries that allow it to determine whether the query's
failure is genuine (in which case it suggest similar, but non-
failing queries) or fake (in which case it detects the user's
erroneous presuppositions).
The main drawback of systems such as flex is their high
computational cost, which comes from computing and test-
ing a large number of presupposition (to identify the sig-
nificant presupposition, a large number of queries must be
evaluated on the entire database). In order to keep the run-
ning time acceptable, Motro [22] suggests heuristics for con-
straining the search. A related approach is proposed by
Gaasterland [12], who controls the query relaxation process
by using heuristics based on semantic query-optimization
techniques. Finally, on the theoretical side, Godfrey [14]
proves complexity results for finding all/some minimal fail-
ing and maximal succeeding sub-queries: finding all of them
is np-hard, while finding a subset takes polynomial time.
The CoBase system [8, 6, 9, 7] is the closest approach
to our loqr algorithm. Central to the CoBase approach is
the concept of Type Abstraction Hierarchies (tahs), which
synthesize the database schema and tuples into a compact,
abstract form. In order to relax a failing query, CoBase
uses three types of tah-based operators: generalization, spe-
cialization, and association; these operations correspond to
moving up, down, or between the hierarchies, respectively.
CoBase automatically generates the tahs by clustering all
the tuples in the database [7, 21].
CoBase is similar to loqr in the sense that it uses machine
learning techniques for relaxing the queries. However, the
two approaches are radically different: in CoBase, the cpu-
intensive clustering is performed only once, in an off-line
manner, on all the tuples in the database. CoBase then uses
this resulting tah to relax all failing queries. In contrast,
loqr customizes the learned decision rules to each failing
query by applying - online - the c4.5 learner [24] to a small,
randomly-chosen subset of the database.

3.
THE INTUITION
Let us consider again the illustrative laptop purchasing
domain, in which the query

Q0 :
Price  $2,000
CPU  2.5 GHz

Display  17
Weight  3 lbs
HDD  60GB

fails because of two independent reasons:

- laptops that have large screens (i.e., Display  17 ) weigh
more than three pounds;

- fast laptops with large hard disks (CPU  2.5GHz HDD 
60GB) cost more than $2,000.

In order to relax Q0, loqr proceeds in three steps: first, it
learns decision rules that express the implicit relationships
among the various domain attributes; then it uses nearest-
neighbor techniques to identify the learned rule that is most



247
Research Track Paper

Brand
Price
CPU
HDD
Weight
Screen
Sony
$2899
3.0 GHz
40 GB
3.9 lbs
18"
Dell
$1999
1.6 GHz
80 GB
3.6 lbs
12"
...
...
...
...
...
...

Table 1: The original dataset D.



similar to the failing query; finally, it uses the attribute val-
ues from this most-similar learned rule to relax the con-
straints from the failing query.

3.1
Step 1: Extracting domain knowledge
In this step, loqr uses the small, randomly-chosen subset
D of the target database to discover knowledge that can be
used for query relaxation. loqr begins by considering Q0's
constraints independently of each other: for each constraint
in Q0 (e.g., CPU  2.5 GHz), loqr uses D to find patterns
that predict whether this constraint is satisfied. Intuitively,
this corresponds to finding the "typical values" of the other
attributes in the examples in which CPU  2.5 GHz.
For example, consider the dataset D that consists of the
laptop configurations from Table 1. In order to predict, for
any laptop configuration, whether CPU  2.5 GHz is satis-
fied, loqr uses D to create the additional dataset D1 shown
in Table 2. Note that each example in D is duplicated in
D1, except for the value of its CPU attribute. The original
CPU attribute is replaced in a binary one (values YES or
NO) that indicates whether CPU  2.5 GHz is satisfied
by the original example from D. The new, binary CPU
attribute is designated as the class attribute of D1.
loqr extracts the potentially useful domain knowledge by
applying the c4.5 learner to the dataset D1, thus learning
a set of decision rules such as

R1 :
Price  $2,900
Display  18
Weight  4 lbs

 IsSatisfied(CPU  2.5GHz) == YES
R2 :
Price  $3,500
 IsSatisfied(CPU  2.5GHz) == YES

R3 :
Price  $2,000
HDD  60
Weight  4 lbs

 IsSatisfied(CPU  2.5GHz) == NO

Such rules can be used for query relaxation because they
describe sufficient conditions for satisfying a particular con-
straint from the failing query. In our example, the rules
above exploit the values of the other domain attributes to
predict whether CPU  2.5 GHz is satisfied.
Besides D1, loqr also creates four other datasets D2 -
D5, which correspond to the constraints imposed by Q0 to
the other domain attributes (i.e., Price, HDD, Weight, and
Display). Each of these additional datasets is used to learn
decision rules that predict whether Q0's constraints on the
corresponding attributes are satisfied.
Note that this learning process takes place online, for each
individual query; furthermore, the process is also query-
guided in the sense that each of the datasets D1 - D5 is
created at runtime by using the failing query's constraints.
This online, query-guided nature of the process is the key
feature that distinguishes loqr from existing approaches.

3.2
Step 2: Finding the "most useful" rule
At this point, we must emphasize that the rule R1, R2,
and R3 that were learned above can be seen as the existentially-
quantified statements. For example, R1 can be interpreted
Brand
Price
CPU
HDD
Weight
Screen
Sony
$2899
YES
40 GB
3.9 lbs
18"
Dell
$1999
NO
80 GB
3.6 lbs
12"
...
...
...
...
...
...


Table 2: The newly created dataset D1.



as the statement "there are some examples in D that satisfy
the condition

Q1 :
Price  $2,900
Display  18

Weight  4 lbs
CPU  2.5 GHz

Consequently, if we apply Q1 to D, Q1 is guaranteed not to
fail because it certainly matches the examples covered by R1
(i.e., the ones from which R1 was learned). Furthermore, as
D is a subset of the target database, it also follows that Q1
is guaranteed not to fail on the target database.
In this second step, loqr converts all the learned rules
into the corresponding existential statements. Then it iden-
tifies the existential statement that is the "most useful" for
relaxing the failing query (i.e., the one that is the most sim-
ilar to Q0).
This "most similar" statement is found by
nearest-neighbor techniques. For example, the statement
Q1 above is more similar to Q0 than

Q2 :
Price  $3,000
Display  18

Weight  4 lbs
CPU  2.5 GHz

because Q1 and Q2 differ only on their constraint on Price,
and Q0's Price  $2, 000 is more similar (i.e., closer in
value) to Q1's Price  $2, 900 than to Q2's Price  $3, 000.
Likewise, Q1 is more similar to Q0 than

Q3 :
Brand == "Sony
CPU  2.5 GHz

which shares only the CPU constraint with the failing query.

3.3
Step 3: Relaxing the failing query
For convenience, let us assume that of all learned state-
ments from the datasets D1 -D5, Q1 is the one most similar
to Q0. Then loqr creates a relaxed query Qr that con-
tains only constraints on attributes that appear both in Q0
and Q1; for each of these constraints, Qr uses the less con-
straining value of those in Q0 and Q1. In our example, the
resulting relaxed query is

Qr :
Price  $2,900
CPU  2.5 GHz

Display  17
Weight  4 lbs

which is obtained by dropping the original constraint on the
hard disk (since it appears only in Q0), keeping the con-
straint on CPU unchanged since (Q0 and Q1 have identical
constraints on CPU), and setting the values in the con-
straints on Price, Display, and Weight to the least con-
straining ones (i.e., the values from Q1, Q0, and Q1, respec-
tively).
The approach above has two advantages. First, as Q1 is
the statement the most similar to Q0, loqr makes minimal
changes to the original failing query. Second, as the con-
straints in Qr are a subset of those in Q1, and they are at
most as tight as those in Q1 (some of them may use the looser
values from Q0), it follows that all examples that satisfy Q1
also satisfy Qr. In turn, this implies that Qr is guaranteed



248
Research Track Paper

not to fail on the target dataset because Q1 satisfies at least
the examples covered by R1.3
Let us now briefly consider a more complex example of
query relaxation. Suppose that instead of Q1, the existential
statement that is the most similar to Q0 is

Q4 :
Price  $2,500
Display  18

Weight  2.5 lbs
CPU  2.5 GHz

Note that for both Price and Weight, the constraints in Q0
and Q4 use the "opposite" operators  and  (e.g.,  in
Q0 and  in Q4, or vice versa). When relaxing Q0 based on
the constraints in Q4, loqr creates the relaxed query

Qr : CPU  2.5 GHz
Display  17
Weight  [2.5,3]

in which

- the Weight is constrained to the values that are com-
mon to both Q0 and Q4 (i.e., Weight  2.5 lbs and
Weight  3 lbs implies Weight  [2.5,3]);

- the constraint on Price is dropped because there are no
values of this attribute that can simultaneously satisfy
the corresponding constraints from Q0 and Q4 (i.e.,
Price  $2,000 and Price  $2,500);

- the other constraints are obtained exactly in the same way
as they were computed for Qr.

3.4
Beyond the illustrative example
At this point we must make two important comments.
First, the failing query Q0 is just a conjunction of sev-
eral constraints on the various attributes. In order to relax
queries that are in disjunctive normal form (i.e., disjunctions
of conjunctions similar to Q0 - Q4), loqr simply considers
the conjunctions independently of each other and applies the
three steps above to each individual conjunction.
Second, the query-guided learning process above creates
a dataset Di for each attribute that is constrained in the
failing query (e.g., the five datasets D1 - D5 for the query
Q0). This idea generalizes in a straightforward manner for
the scenario in which the user is allowed to specify "hard
constraints" that should not be relaxed under any circum-
stances: for each example in D, the entire set of hard con-
straints is replaced by a single binary attribute that specifies
whether or not all the hard constraints are simultaneously
satisfied in that particular example. This is an additional
benefit of our online, query-guided approach to query relax-
ation, and it is not shared by other approaches.

4.
THE LOQR ALGORITHM
In this section we present a formal description of the loqr
algorithm. We begin by briefly describing the syntax of the
input queries, after which we discuss the algorithm itself.

4.1
The query syntax
loqr takes as input queries in the disjunctive normal form
(dnf). Consequently, a failing query Q0 consists of a dis-
junction of conjunctions of the form Q0 = C1
C2
... Cn.
In turn, each Ck is a conjunction of constraints imposed on
(a subset of) the domain attributes:
3
Note that this guarantee holds only if D is a subset of the
target database. If these two datasets are disjoint (see our
experimental setup), QR may fail, even though it is highly
unlikely to do so.
Given:
- a failing query Q in Disjunctive Normal Form
- a small, randomly-chosen subset D of the target database

RelaxedQuery = 
FOR EACH of Q's failing conjunctions Ck DO
- Step 1: Rules = ExtractDomainKnowledge(Ck, D)
- Step 2: Refiner = FindMostSimilar(Ck, Rules)
- Step 3: RelaxedConjunction = Refine(Ck, Refiner)
- add RelaxedConjunction to RelaxedQuery



Figure 1: loqr successively relaxes each conjunction
independently of the other ones.


Ck = Constr(Ai1)
Constr(Ai2)
...
Constr(Aik).

When the context is ambiguous, the notation ConstrCk(Aj)
is used to denote the constraint imposed by the conjunction
Ck on the attribute Aj.
Each constraint consists of a domain attribute, an op-
erator, and one or several constants. For the discrete at-
tributes, loqr accepts constraints of the type =, =, , or 
(e.g., Color = black or Manufacturer  {Sony,HP }). For
the continuous attributes, the constraints use the inequality
operators , <, , or > (e.g., Price < 2000).
For a dnf query to fail, each of its conjunctions Ck must
fail (i.e., the query consists of failing conjunctions only);
conversely, by successfully relaxing any of its failing con-
junctions, one turns a failing query into a non-failing one.
Based on this observation, loqr successively relaxes the fail-
ing conjunctions independently of each other (see Figure
1); consequently, without any loss of generality, we focus
here on the three steps used to relax a failing conjunction
Ck: extracting the implicit domain knowledge (expressed as
learned decision rules), finding the decision rule that is the
most similar to Ck, and using this decision rule to actually
relax Ck.

4.2
Step 1: Extracting domain knowledge
In this first step, loqr uses a subset of the target database
to uncover the implicit relationships that hold among the
domain attributes. This is done by the following strategy:
for each attribute Aj that appears in the failing conjunction
Ck, loqr uses the values of the other domain attributes to
predict whether Aj's value satisfies ConstrCk(Aj).
As shown in Figure 2, loqr starts with a randomly-chosen
subset D of the target database and creates one additional
dataset Dj for each attribute Aj that is constrained in Ck.
Each dataset Dj is a copy of D that differs from the original
only by the values of Aj: for each example in Dj, if the
original value of Aj satisfies ConstrCk(Aj), loqr sets Aj
to yes; otherwise Aj is set to no. For each Dj, the binary
attribute Aj is designated as the class attribute.
After creating these additional datasets, loqr applies c4.5-
rules [24] to each of them, thus learning decision rules that,
for each attribute Aj in Ck, predict whether Aj satisfies
ConstrCk(Aj). In other words, these learned decision rules
represent patterns that use the values of some domain at-
tributes to predict whether a particular constraint in Ck is
satisfied.

4.3
Step 2: Finding the "refiner statement"
After the learning step above, loqr converts each learned
decision rule into the equivalent existentially-quantified state-



249
Research Track Paper

ExtractDomainKnowledge (conjunction Ck, dataset D)
- Rules = 
FOR EACH attribute Aj that appears in Ck DO
- create the following binary classification dataset Dj:
- FOR EACH example ex  D DO
- make a copy ex of ex
- IF ex .Aj satisfies ConstrCk(Aj)
THEN set ex .Aj to "yes"
ELSE set ex .Aj to "no"
- add ex to Dj
- designate Aj as the (binary) class attribute of Dj
- apply the c4.5-rules algorithm to Dj
- add these learned rules to Rules
- return Rules


Figure 2: Step 1: query-guided extraction of domain
knowledge expressed as decision rules.


ment. This is done by simply replacing "" by " " in each
rule (remember than any decision rule "a b c  d" can
be interpreted as the existential statement "there are exam-
ples in D such that a b c d").
Note that the resulting existential statements have the
same syntax as the failing conjunctions; i.e., they both rep-
resent a conjunction of constraints on the domain attributes.
Consequently, loqr can detect the existential statement
that is the most similar to Ck by performing an attribute-
wise comparison between the constraints in Ck and those in
each of the learned statements (i.e., loqr finds Ck's "nearest
neighbor" among the existential statements).
In order to evaluate the similarity between a conjunction
Ck and a statement S, loqr use the function



dist(Ck,S) =
AjCk AjS
wj × Dist(Ck,S,Aj)



in which wj denotes the user-provided (relative) weight of
the attribute Aj. Dist(Ck,S,Aj) is a measure of the sim-
ilarity between the constraints imposed on Aj by Ck and
S; it has the range [0,1] (the smaller the value, the more
similar the constraints) and is defined as follows:

- if the attribute Aj does not appear in both Ck and S,
then Dist(Ck,Si,Aj) = 1; i.e., Ck and S are highly
dissimilar with respect to Aj.

- if
Aj
takes
discrete
values,
then


Dist(Ck,Si,Aj) =
0
if ConstrCk(Aj)
ConstrS(Aj) = 
1
otherwise

In other words, Ck and S are highly similar with re-
spect to Aj if there is at least a value of Aj that is
valid for both ConstrCk(Aj) and ConstrS(Aj).

- if
Aj
takes
continuous
values,
then

Dist(Ck,Si,Aj) =
|V alue(ConstrCk(Aj))-V alue(ConstrS(Aj))|
MaxAj -MinAj
where V alue() returns the constraint's numeric value,
while MaxAj and MinAj are the largest and the small-
est value of Aj in D, respectively.
Intuitively, the
smaller the relative difference between the values in
the two constraints, the more similar the constraints.
Refine ( conjunction Ck, statement S)
- CRelax = 
FOR EACH attribute Aj that appears in both Ck and S DO
IF Aj is discrete THEN
- CRelax = CRelax (ConstrCk(Aj)
ConstrS(Aj))
ELSE /*------------­ Aj is continuous ----------­*/
IF ConstrCk(Aj) and ConstrS(Aj) are of "same type" THEN
- CRelax = CRelax Least(ConstrCk(Aj),ConstrS(Aj))
ELSE
- CRelax = CRelax (ConstrCk(Aj)
ConstrS(Aj))
- return CRelax


Figure 3: Step 3: relaxing a failing conjunction.


4.4
Step 3: Refining the failing conjunction
After finding the statement S that is the most similar to
the failing conjunction Ck, loqr uses the domain knowledge
synthesized by S to relax the constraints in Ck. As shown
in Figure 3, the relaxation works as follows:

- the relaxed conjunction CRelax includes only constraints
on attributes that are present in both Ck and S;

- if Aj is discrete, CRelax constrains its values to the ones
common to both ConstrCk(Aj) and ConstrS(Aj). If
V alues(ConstrCk(Aj))
V alues(ConstrS(Aj)) = ,
then CRelax does not impose any constraint on Aj.

- if Aj is continuous, there are two possible scenarios:

1. if ConstrCk(Aj) and ConstrS(Aj) use the same type
of inequality (e.g., one of > or ), then Cj con-
tains the least constraining4 of ConstrCk(Aj) and
ConstrS(Aj). For example, if the constraints are
Price < $1,000 and Price  $799, then CRelax con-
tains the former.

2. if ConstrCk(Aj) and ConstrS(Aj) use different types
of inequalities (e.g., one of them uses > or ,
while the other one uses < or ), then CRelax con-
tains the intersection of the two constraints. For
example, if the constraints are Price < $1, 000 and
Price  $799, then CRelax contains Price  [799,
1000); if the constraints are Price  $1, 000 and
Price < $799, their intersection is empty, which
means that CRelax imposes no constraint on Price.


5.
EXPERIMENTAL RESULTS
In this section we begin with a brief overview of the five
algorithms to be evaluated, followed by the description of
the datasets, the experimental setup, and the actual results.

5.1
The Algorithms
We empirically compare the performance of loqr with
that of the following four algorithms: loqr-50, loqr-90, s-
nn, and r-nn. The first two are variants of loqr, while the
other ones represent two baselines.
The baselines work as follows: for each failing conjunction
Ck, they use the distance measure from Section 4.3 to find
the example Ex  D that is the most similar to Ck. Then
4
For each continuous attribute, loqr requires the user to
specify whether larger or smaller values are more desirable.
In our laptop scenario, the larger the hard disk, the better;
conversely, the smaller the Price the better, too.



250
Research Track Paper

they use Ex to create a conjunction Ck that has the same
constraints as Ck, except that the original attribute values
are replaced by the corresponding values from Ex.
The
difference between s-nn and r-nn is that the former simply
returns Ck as the relaxed query, while the latter uses Ck to
relax Ck as explained in Section 4.4.
loqr-50 and loqr-90 represent variants of loqr that il-
lustrate the trade-offs between the following strategies: "gen-
erate over-relaxed queries that are highly unlikely to fail, but
return a (relatively) large number of tuples" vs "create under-
relaxed queries that return fewer tuples, but are more likely to
fail". Both algorithms assume that the user designates one
of the attributes as being the most relevant (ideally, the con-
straint on this attribute should not be relaxed at all).
For each failing conjunction Ck, loqr-50 and loqr-90
run loqr, which computes the relaxed conjunction CRelax.
After the user selects an attribute Aj from CRelax as the
most relevant, the algorithms

- apply QR to the dataset D and obtain the set CT of com-
pliant tuples, which consists of the examples covered
by the decision rule used to relax the failing query.

- determine the set V of all values taken by the attribute Aj
over the dataset CT .

- replace the value in ConstrCk(Aj) by "the most constrain-
ing" 50- or 90- percentile value, respectively.5 For ex-
ample, if ConstrCk(Aj) is Price < $2000, loqr-90 re-
places $2000 by a value v  V such that exactly 90%
of the values in V are worse (i.e., smaller) than v.
Similarly, if ConstrCk(Aj) is CPU > 2.5 GHz, loqr-90
replaces 2.5 GHz by a value v  V such that exactly
90% of the values in V are larger then v.

For all five algorithms above, we used equal weights (wj =
1) in the formula for dist(Ck,S), which measures the simi-
larity between two conjunctive formulas (see Section 4.3).

5.2
The Datasets and the Setup
We evaluate the algorithms above on six different datasets.
The first one, laptops, is the original motivation for this
work. It consists of 1257 laptop configurations extracted
from yahoo.com; each laptop is described by five numeric
attributes: price, cpu speed, ram, hdd space, and weight.
The other five domains are taken from the UC Irvine repos-
itory: breast cancer Wisconsin (bcw), low resolution spec-
trometer (lrs), Pima Indians diabetes (pima), water treat-
ment plant (water), and waveform data generator (wave).
In order to evaluate the performance of the five algorithms
above, we proceed as follows. Given a failing query Q and
a dataset D, each algorithm uses D to generate a relaxed
query QR. In order to estimate its adequacy, QR is then
evaluated on a test set that consists of all examples in the
target database except the ones in D. We have chosen to
keep D and the test set disjoint (which may lead to the
relaxed query failing on the test set) because in our moti-
vating Web-based domains the owner of a database may be
unwilling to provide the dataset D. By keeping D and the
test set disjoint, we can simulate (up to a certain level) the

5
Obviously, this strategy applies only to the continuous at-
tributes. For the discrete ones, the user is asked to select a
correspondingly small subset of the most desirable values.
scenario in which the relaxation algorithm exploits an alter-
native information source that is somewhat representative
of the data in the target database.
For each of the six domains, we have seven distinct failing
queries. We also consider various sizes of the dataset D:
50, 100, 150, . . . , 350 examples; for each of these sizes, we
create 100 arbitrary instances of D, together with the 100
corresponding test sets. For each size of D and each of the
seven failing queries, each query relaxation algorithms is run
100 times (once for each instance of D); consequently, the
results reported here are the average of these 700 runs.

5.3
The Results
In our experiments, we focus on two performance mea-
sures:

- robustness: what percentage of the failing queries are suc-
cessfully relaxed (i.e., they don't fail anymore)?

- coverage: what percentage of the examples in the test set
satisfy the relaxed query?

Figures 4 and 5 show the robustness and coverage results on
the six evaluation domains. In terms of robustness, loqr
obtains by far the best results: independently of the size of
D, on all six domains loqr's robustness is above 90%; in
fact, most of the robustness results are close or above 99%
(i.e., about 99% of the queries are successfully relaxed).
In contrast, the two baselines, s-nn and r-nn, display ex-
tremely poor robustness: on lrs, pima, water, and wave
their robustness is below 10%. The baselines' best results
are obtained on small-sized D (i.e., Size(D) = 50), where
the scarcity of the training data makes it unlikely to find a
domain example that is highly-similar to the failing query;
in turn this leads to an over-relaxation of the query, which
improves the robustness. However, as Size(D) increases, the
performance of the two baselines degrades rapidly.
The second measure of interest, coverage, must be consid-
ered in conjunction with the robustness results: even though
we are interested in low-coverage results6, a low-coverage,
non-robust algorithm is of little practical importance. Con-
sequently, the low-coverage results of the two baselines (see
Figure 5) must be put in perspective: after all, the vast
majority of the queries relaxed by these algorithms still fail.
loqr scores less spectacularly in terms of coverage: when
learning from datasets of 350 examples, its coverage on the
six domains is between 2% and 19%. However, by trading-
off robustness for coverage, loqr-90 obtains excellent overall
results: when using 350 training examples, on all domains
but bcw, loqr-90 reaches robustness levels between 69%
and 98%, while also keeping the coverage under 5%.
Last but not least, we are also interested in the amount
of time spent relaxing a query: given that each query is
processed online, it is crucial that loqr quickly relaxes the
incoming queries. In Table 3, we show the cpu time (in
seconds) that is spent refining the queries in the six applica-
tion domains. Our results show that loqr is extremely fast:
6
Our motivation comes from Web-based information
sources, for which high-coverage queries may be unaccept-
able because of (1) the database's owners unwillingness to
return large chunks of the data; (2) the bandwidth problems
associated with transmitting huge datasets over the Inter-
net; (3) the fee that one may have to pay for each returned
tuple. Consequently, we are interested in query relaxations
that return only a few, highly-relevant tuples.



251
Research Track Paper

40
50
60
70
80
90
100




0
50
100
150
200
250
300
350
robustnes
(%)




nmb. exs in D
LAPTOPS




LOQR
LOQR-50
LOQR-90
s-NN
r-NN




40
50
60
70
80
90
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
BCW



LOQR
LOQR-50
LOQR-90
s-NN
r-NN




0
10
20
30
40
50
60
70
80
90
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
WATER



LOQR
LOQR-50
LOQR-90
s-NN
r-NN




0
20
40
60
80
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
LRS



LOQR
LOQR-50
LOQR-90
s-NN
r-NN




0
10
20
30
40
50
60
70
80
90
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
PIMA




LOQR
LOQR-50
LOQR-90
s-NN
r-NN


0
10
20
30
40
50
60
70
80
90
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
WAVE




LOQR
LOQR-50
LOQR-90
s-NN
r-NN




Figure 4: Robustness: what percentage of the relaxed queries are not failing?


queries that consist of at most 21 attributes are processed
under 1.40 seconds; for queries that consist of 93 attributes,
it may take up to 30 seconds. To put things into perspective,
a human cannot even read and comprehend a 93-attribute
query in this amount of time; in fact, it is highly unlikely
that humans are even willing to write 93-attribute queries.
Note that the running time is influenced both by the size
of the dataset D and the number of attributes in the query.
The former relationship is straightforward: the larger the
dataset D, the longer it takes to learn the decision rules.
The latter is more subtle: as loqr creates a new dataset
for each attribute in the query, it follows that the more at-
tributes in the query, the longer it takes to process the query.
However, not all constraints are equally time consuming; for
example, if an attribute A is constrained to take a value that
is out of the range of values encountered in D, then it is su-
perfluous to learn from the corresponding dataset, which
consists solely of "negative examples" (the constraint on A
is not satisfied in any of the examples from D).


6.
DISCUSSION
Before concluding this paper, we must discuss two im-
portant design choices that heavily influence loqr's perfor-
mance: the online and the query-driven nature of the learn-
ing process. The former refers to the fact that the learning
step is performed at run-time, for each failing query. The
latter specifies that the learning task is designed as a binary
classification problem in which the class attribute represents
the boolean value "does Aj's value satisfy the constraints
imposed onto it by the failing query?"

6.1
Online vs offline learning
In order to illustrate the advantages of our online ap-
proach, we re-use the experimental setup above to compare



252
Research Track Paper

0
5
10
15
20
25
30
35
40
45
50




0
50
100
150
200
250
300
350
coverage
(%)




nmb. exs in D
LAPTOPS


LOQR
LOQR-50
LOQR-90
s-NN
r-NN




0
1
2
3
4
5
6
7
8




0
50
100
150
200
250
300
350
coverage
(%)




nmb. exs in D
BCW


LOQR
LOQR-50
LOQR-90
s-NN
r-NN




0
5
10
15
20
25
30
35
40




0
50
100
150
200
250
300
350
coverage
(%)




nmb. exs in D
WATER


LOQR
LOQR-50
LOQR-90
s-NN
r-NN




0
10
20
30
40
50
60
70




0
50
100
150
200
250
300
350
coverage
(%)




nmb. exs in D
LRS


LOQR
LOQR-50
LOQR-90
s-NN
r-NN




0
5
10
15
20
25
30




0
50
100
150
200
250
300
350
coverage
(%)




nmb. exs in D
PIMA


LOQR
LOQR-50
LOQR-90
s-NN
r-NN




0
10
20
30
40
50
60




0
50
100
150
200
250
300
350
coverage
(%)




nmb. exs in D
wave


LOQR
LOQR-50
LOQR-90
s-NN
r-NN




Figure 5: Coverage: what percentage of the examples in the test set match the relaxed query?


Dataset
Attributes
cpu time (seconds per query)
per query
|D| = 50
|D| = 350
laptops
5
0.06
0.15
pima
8
0.15
0.47
bcw
10
0.14
0.37
wave
21
0.46
1.39
water
38
0.87
4.40
lrs
93
4.32
30.71

Table 3: Running times for loqr, averaged over the
runs on each of the 100 instances of D that are cre-
ated for 50 and 350 examples, respectively.



loqr with an offline variant, off-k. The only difference be-
tween the two algorithms is that off-k performs the learn-
ing step only once, independently of the constraints that
appear in the failing queries. Similarly to loqr, off-k also
tries to predict an attribute's value from those of the other
attributes; however, because it does not have access to the
constraints in the failing query, off-k proceeds as follows:
for discrete attributes, it learns to predict each discrete value
from the values of the other attributes; for continuous at-
tributes, it discretizes the attribute's range of values in D so
that it obtains a number of k intervals that have equal size.
In this empirical comparison, we use two offline versions
(i.e., k = 2 and k = 3) for both loqr and loqr-90. Figures
6 and 7 show the robustness results7 for loqr and loqr-90,
respectively (off-2 and off-3 denote the two offline ver-
sions of each algorithm). The graphs show that both loqr
and loqr-90 clearly outperform their offline variants,8 thus

7
Because of space limitations, we do not show the coverage
results. However, they can be summarized as follows: on all
six domains, the coverage of the online and offline algorithms
are extremely close. On water and lrs, both loqr and
loqr-90 outperform their offline counterparts, while on bcw
and wave the performance is virtually the same.
8
Figures 6 and 7 also show that off-3 does not always out-
demonstrating the superiority of the online, query-guided
approach.

6.2
Query-driven learning
As we have already seen, guiding the learning process by
the constraints that appear in the failing query leads to dra-
matic robustness improvements. However, the query-driven
approach used by loqr is by no means the only possible
approach. In fact, we can distinguish four main scenarios:

- no constraints: this approach corresponds to offline learn-
ing, in which none of the query constraints are used to
guide the learning process.

- class-attribute constraints: this is the approach used by
loqr, in which we create a dataset D for each attribute
in the query. Each such dataset uses exactly one of the
failing query's constraints to guide the learning; more
precisely, one of the constrained attributes becomes
the designated class-attribute that takes two discrete
values (i.e., does/does-not satisfy the constraint).

- set of hard constraints: this scenario represents a straight-
forward generalization of the previous one. If the user
specifies a subset of M of the failing query's constraints
that must be satisfied (i.e., hard constraints), one can
then replace the corresponding M attributes by a sin-
gle discrete one that represents the conjunction of the
M hard constraints and then apply loqr.

- all constraints: this final scenario correspond to simultane-
ously replacing the original values of all the attributes

perform off-2. This is because the discretization of the con-
tinuous attributes is made independently of the values from
the failing query: as the discretization takes place offline, the
values that appear in query's constraints may lay anywhere
within a discretized interval. Consequently, the "purity" of
the discretized class that includes the query value may vary
wildly (e.g., almost all values in that interval may or may
not satisfy the constraint from the query), which - in turn -
dramatically affects the quality of the relaxed query.



253
Research Track Paper

75
80
85
90
95
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
LAPTOPS




online
off-2C
off-3C
95
95.5
96
96.5
97
97.5
98
98.5
99
99.5
100




0
50
100 150 200 250 300 350
robustness
(%)




nmb. exs in D
BC_WISC




online
off-2C
off-3C
75
80
85
90
95
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
water




online
off-2C
off-3C




86
88
90
92
94
96
98
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
LRS




online
off-2C
off-3C
75
80
85
90
95
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
PIMA




online
off-2C
off-3C
70
75
80
85
90
95
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
wave




online
off-2C
off-3C




Figure 6: Online vs Offline: robustness results for loqr.


in the query with the corresponding boolean value (i.e.,
does/does-not satisfy the constraints in the query).

In this paper we have analyzed the first two of the sce-
narios above. The third one represents a straightforward
extension of loqr that was not addressed here because of
space limitations. Finally, the last scenario has both advan-
tages and disadvantages over loqr. On one hand, by si-
multaneously replacing the values of all attributes with the
corresponding boolean values, the "all constraints" scenario
creates a single dataset D instead that one per attribute;
in turn this leads to a considerable gain in terms of pro-
cessing speed. On the other hand, in the "all constraints"
scenario there is only one way to relax a query, namely by
dropping constraints. In contrast, loqr permits both con-
straint dropping and constraint relaxation (i.e., replacing
the original value by a less constraining one), thus providing
a significantly more flexible solution to the query relaxation
problem.


7.
CONCLUSIONS & FUTURE WORK
In this paper we have introduced a novel, data-driven ap-
proach to query relaxation. Our algorithm, loqr, performs
online, query-driven learning from a small subset of the tar-
get database. The learned information is then used to relax
the constraints in the failing query. We have shown empiri-
cally that loqr is a fast algorithm that successfully relaxes
the vast majority of the failing queries.
We intend to continue our work on query relaxation along
several directions. First, we plan to extend our data-driven
approach by also exploiting user preferences that are learned
as the system is in use.
Second, we are interested in a
query visualization algorithm that would allow a user to
explore the trade-offs between the various possible query re-
laxations. Finally, we plan to integrate the query visualiza-
tion module in a mixed initiative system in which the user
interacts with the query relaxation algorithm by expressing
various preferences over the domain attributes.
8. ACKNOWLEDGMENTS
This material is based upon work supported by the De-
fense Advanced Research Projects Agency (DARPA), through
the Department of the Interior, NBC, Acquisition Services
Division, under Contract No. NBCHD030010.
We would like to thank Melinda Gervasio, Karen Myers,
Maria Muslea, and Tomas Uribe for their helpful comments
on this paper. We also thank Steven Minton and Fetch Tech-
nologies, Inc. for providing us with the Laptops dataset.


9. REFERENCES
[1] S. Amer-Yahia, S. Cho, and D. Srivastava. Tree
pattern relaxation. In International Conference on
Extending Database Technology EDBT, 2002.
[2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern
Information Retrieval. Addison-Wesley, 1999.
[3] K. Chakrabarti, M. Ortega, S. Mehrotra, and
K. Porkaew. Evaluating refined queries in top-k
retrieval systems. IEEE Transactions on Knowledge
and Data Engineering, 15(5), 2003.
[4] S. Chaudhuri. Generalization and a framework for
query modification. In Proceedings of the Sixth
International Conference on Data Engineering,
February 5-9, 1990, Los Angeles, California, USA,
pages 138­145. IEEE Computer Society, 1990.
[5] S. Chaudhuri and L. Gravano. Evaluating top-k
selection queries. In Proceedings of the Conference on
Very Large Databases, pages 397­410, 1999.
[6] W. Chu, Q. Chen, and A. Huang. Query answering
via cooperative data inference. Journal of Intelligent
Information Systems, 3(1):57­87, 1994.
[7] W. Chu, K. Chiang, C.-C. Hsu, and H. Yau. An
error-based conceptual clustering method for
providing approximate query answers.
Communications of acm, 39(12):216­230, 1996.
[8] W. Chu, R. C. Lee, and Q. Chen. Using type
interfaces and induced rules to provide intentional



254
Research Track Paper

65
70
75
80
85
90
95




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
LAPTOPS




on--90
off-2C
off-3C
40
45
50
55
60
65
70




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
BC_WISC




on--90
off-2C
off-3C
65
70
75
80
85
90
95




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
water




on--90
off-2C
off-3C




84
86
88
90
92
94
96
98
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
LRS




on--90
off-2C
off-3C
45
50
55
60
65
70
75
80
85
90
95




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
PIMA




on--90
off-2C
off-3C
70
75
80
85
90
95
100




0
50
100
150
200
250
300
350
robustness
(%)




nmb. exs in D
wave




on--90
off-2C
off-3C




Figure 7: Online vs Offline: robustness results for loqr and loqr-90.


answers. In Proceedings of the Seventh International
Conference on Data Engineering, 1991.
[9] W. Chu, H. Yang, K. Chiang, M. Minock, G. Chow,
and C. Larson. Cobase: A scalable and extensible
cooperative information system. Journal of
Intelligence Information Systems, 6(2/3):223­59, 1996.
[10] F. Corella, S. J. Kaplan, G. Wiederhold, and L. Yesil.
Cooperative responses to boolean queries. In
Proceedings of the 1st International Conference on
Data Engineering, pages 77­85, 1984.
[11] N. Fuhr and K. Grosjohann. XIRQL: A query
language for information retrieval in XML documents.
In Research and Development in Information
Retrieval, pages 172­180, 2001.
[12] T. Gaasterland. Cooperative answering through
controlled query relaxation. IEEE Expert, 12(5):48­59,
1997.
[13] A. Gal. Cooperative responses in deductive databases.
PhD thesis, Department of Computer Science,
University of Maryland, College Park, 1988.
[14] P. Godfrey. Minimization in cooperative response to
failing database queries. International Journal of
Cooperative Information Systems, 6(2):95­149, 1997.
[15] J. M. Janas. Towards more informative user interfaces.
In A. L. Furtado and H. L. Morgan, editors, Fifth
International Conference on Very Large Data Bases,
October 3-5, 1979, Rio de Janeiro, Brazil, Proceedings,
pages 17­23. IEEE Computer Society, 1979.
[16] Y. Kanza and Y. Sagiv. Flexible queries over
semistructured data. In Proceedings of the 20th ACM
SIGMOD-SIGACT-SIGART symposium on Principles
of database systems, pages 40­51, 2002.
[17] M. Kao, N. Cercone, and W.-S. Luk. Providing
quality responses with natural language interfaces:
The null value problem. IEEE Transactions on
Software Engineering, 14(7):959­984, 1988.
[18] S. Kaplan. Cooperative aspects of database
interactions. Artificial Intelligence, 19(2):165­87, 1982.
[19] D. A. Keim and H. Kriegel. VisDB: Database
exploration using multidimensional visualization.
Computer Graphics and Applications, 1994.
[20] D. Lee. Query Relaxation for XML Model. PhD thesis,
Department of Computer Science, University of
California Los Angeles, 2002.
[21] M. Merzbacher and W. Chu. Pattern-based clustering
for database attribute values. In Proceedings of AAAI
Workshop on Knowledge Discovery in Databases, 1993.
[22] A. Motro. seave: a mechanism for verifying user
presupositions in query system. acm Transactions on
Information Systems, 4(4):312­330, 1986.
[23] A. Motro. Flex: A tolerant and cooperative user
interface databases. IEEE Transactions on Knowledge
and Data Engineering, 2(2):231­246, 1990.
[24] R. Quinlan. C4.5: programs for machine learning.
Morgan Kaufmann Publishers, 1993.
[25] A. Theobald and G. Weikum. Adding relevance to
XML. Lecture Notes in Computer Science,
1997:105­131, 2001.




255
Research Track Paper

