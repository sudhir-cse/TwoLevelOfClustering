Adversarial Classification


Nilesh Dalvi
Pedro Domingos
Mausam
Sumit Sanghai
Deepak Verma
Department of Computer Science and Engineering
University of Washington, Seattle
Seattle, WA 98195-2350, U.S.A.
{nilesh,pedrod,mausam,sanghai,deepak}@cs.washington.edu




ABSTRACT
Essentially all data mining algorithms assume that the data-
generating process is independent of the data miner's activ-
ities.
However, in many domains, including spam detec-
tion, intrusion detection, fraud detection, surveillance and
counter-terrorism, this is far from the case: the data is ac-
tively manipulated by an adversary seeking to make the clas-
sifier produce false negatives. In these domains, the perfor-
mance of a classifier can degrade rapidly after it is deployed,
as the adversary learns to defeat it. Currently the only so-
lution to this is repeated, manual, ad hoc reconstruction of
the classifier. In this paper we develop a formal framework
and algorithms for this problem. We view classification as a
game between the classifier and the adversary, and produce a
classifier that is optimal given the adversary's optimal strat-
egy. Experiments in a spam detection domain show that this
approach can greatly outperform a classifier learned in the
standard way, and (within the parameters of the problem)
automatically adapt the classifier to the adversary's evolving
manipulations.


Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications--
data mining; I.2.6 [Artificial Intelligence]: Learning--
concept learning, induction, parameter learning; I.5.1 [Patt-
ern Recognition]: Models--statistical; I.5.2 [Pattern Re-
cognition]: Design Methodology--classifier design and eval-
uation, feature evaluation and selection; G.3 [Mathematics
of Computing]: Probability and Statistics--multivariate
statistics


General Terms
Algorithms


Keywords
Cost-sensitive learning, game theory, naive Bayes, spam de-
tection, integer linear programming



Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00.
1. INTRODUCTION
Many major applications of KDD share a characteristic
that has so far received little attention from the research
community: the presence of an adversary actively manip-
ulating the data to defeat the data miner.
In these do-
mains, deployment of a KDD system causes the data to
change so as to make the system ineffective. For example,
in the domain of email spam detection, standard classifiers
like naive Bayes were initially quite successful (e.g., [23]).
Unfortunately, spammers soon learned to fool them by in-
serting "non-spam" words into emails, breaking up "spam"
ones with spurious punctuation, etc. Once spam filters were
modified to detect these tricks, spammers started using new
ones [4]. Effectively, spammers and data miners are engaged
in a never-ending game where data miners continually come
up with new ways to detect spam, and spammers continually
come up with new ways to avoid detection.
Similar arms races are found in many other domains: com-
puter intrusion detection, where new attacks circumvent the
defenses put in place against old ones [17]; fraud detection,
where perpetrators learn to avoid the actions that previously
gave them away [5, 25]; counter-terrorism, where terrorists
disguise their identity and activities in ever-shifting ways
[11]; aerial surveillance, where targets are camouflaged with
increasing sophistication [22]; comparison shopping, where
merchants continually change their Web sites to avoid wrap-
ping by shopbots [3]; file sharing, where media companies
try to detect and frustrate illegal copying, and users find
ways to circumvent the obstacles [14]; Web search, where
webmasters manipulate pages and links to inflate their rank-
ings, and search engines reengineer their ranking functions
to deflate them back again [9, 16]; etc.
In many of these domains, researchers have noted the pres-
ence of adaptive adversaries and the need to take them into
account (e.g., [4, 5, 11]), but to our knowledge no system-
atic approach for this has so far been developed. The result
is that the performance of deployed KDD systems in ad-
versarial domains can degrade rapidly over time, and much
human effort and cost is incurred in repeatedly bringing the
systems back up to the desired performance level. This pa-
per proposes a first step towards automating this process.
While complete automation will never be possible, we be-
lieve our approach and its future extensions have the poten-
tial to significantly improve the speed and cost-effectiveness
of keeping KDD systems up to date with their adversaries.
Notice that adversarial problems cannot simply be solved
by learners that account for concept drift (e.g., [10]): while
these learners allow the data-generating process to change




99
Research Track Paper

over time, they do not allow this change to be a function of
the classifier itself.
We first formalize the problem as a game between a cost-
sensitive classifier and a cost-sensitive adversary (Section 2).
Focusing on the naive Bayes classifier (Section 3), we de-
scribe the optimal strategy for the adversary against a stan-
dard (adversary-unaware) classifier (Section 4), and the op-
timal strategy for a classifier playing against this strategy
(Section 5). We provide efficient algorithms for computing
or approximating these strategies. Experiments in a spam
detection domain illustrate the sometimes very large util-
ity gains that an adversary-aware classifier can yield, and
its ability to co-evolve with the adversary (Section 6). We
conclude with a discussion of future research directions (Sec-
tion 7).


2. PROBLEM DEFINITION
Consider a vector variable X = (X1, . . . , Xi, . . . , Xn), wh-
ere Xi is the ith feature or attribute, and let the instance
space X be the set of possible values of X. An instance x
is a vector where feature Xi has the value xi. Instances can
belong to one of two classes: positive (malicious) or negative
(innocent). Innocent instances are generated i.i.d. (indepen-
dent and identically distributed) from a distribution P (X|-),
and malicious ones likewise from P (X|+). The global dis-
tribution is thus P (X) = P (-)P (X|-) + P (+)P (X|+). Let
the training set S and test set T be two sets of (x, y) pairs,
where x is generated according to P (X) and y is the "true"
class of x. We define adversarial classification as a game
between two players: Classifier, which attempts to learn
from S a function yC = C(x) that will correctly predict the
classes of instances in T , and Adversary, which attempts
to make Classifier classify positive instances in T as neg-
ative by modifying those instances from x to x
= A(x).
(Adversary cannot modify negative instances, and thus
A(x) = x for all x  -.) Classifier is characterized by a
set of cost/utility parameters (see Table 1 for a summary of
the notation used in this paper):

1. Vi: Cost of measuring Xi. Depending on their costs,
Classifier may choose not to measure some features.

2. UC(yC, y): Utility of classifying as yC an instance with
true class y. Typically, UC(+, -) < 0 and UC(-, +) < 0,
denoting the cost of misclassifying an instance (costs
being negative utilities), and UC(+, +) > 0, UC(-, -)
> 0.

Adversary has a corresponding set of parameters:

1. Wi(xi, xi) : Cost of changing the ith feature from xi
to xi.
Wi(xi, xi) = 0 for all xi.
We will also use
W (x, x ) to represent the cost of changing an instance
x to x (which is simply the sum of the costs of all the
individual feature changes made).

2. UA(yC, y): Utility accrued by Adversary when Clas-
sifier classifies as yC an instance of class y. Typically,
UA(-, +) > 0, UA(+, +) < 0 and UA(-, -) = UA(+, -) =
0, and we will assume this henceforth.

The goal of Classifier is to build a classifier C that will
maximize its expected utility, taking into account that in-
stances may have been modified by Adversary:
UC =
(x,y)XY
P (x, y) 
UC(
C(A(x)),y)-
XiXC (x)
Vi

(1)


where Y = {+, -} and XC(x)  {X1, . . . , Xn} is the set of
features measured by C, possibly dependent on x. We call
C the optimal strategy of Classifier.
The goal of Adversary is to find a feature change strat-
egy A that will maximize its own expected utility:


UA =
(x,y)XY
P (x, y) [UA(C(A(x)), y) - W (x, A(x))] (2)


We call A the optimal strategy of Adversary. Notice that
Adversary will not change instances if the cost of doing
so exceeds the utility of fooling Classifier. For example,
a spammer will not modify his emails to the point where
they no longer help sell his product. In practice, UC and UA
are estimated by averages over T : UC = (1/|T |)
(x,y)T
[UC(C(A(x)), y) -
XiXC (x)
Vi], etc.
Given two players, the actions available to each, and the
payoffs from each combination of actions, classical game the-
ory is concerned with finding a combination of strategies
such that neither player can gain by unilaterally changing
its strategy. This combination is known as a Nash equilib-
rium [7]. In our case, the actions are classifiers C and feature
change strategies A, and the payoffs are UC and UA. As the
following theorem shows, some realizations of the adversar-
ial classification game always have a Nash equilibrium.


Theorem 2.1. Consider a classification game with a bi-
nary cost model for Adversary, i.e., given a pair of in-
stances x and x , Adversary can either change x to x (in-
curring a unit cost) or it cannot (the cost is infinite). This
game always has a Nash equilibrium, which can be found in
time polynomial in the number of instances.


We omit the proof due to lack of space. Unfortunately,
the calculation of the Nash equilibrium requires complete
and perfect knowledge of the probabilities of all the in-
stances, which in practice Adversary and Classifier will
not have. Computing Nash equilibria will generally be in-
tractable. The chief difficulty is that even in finite domains
the number of available actions is doubly exponential in the
number of features n. The best known algorithms for find-
ing Nash equilibria in general (nonzero) sum games have
worst-case exponential time in the number of actions, mak-
ing them triply exponential in our case.
Even using the
more general notion of correlated equilibria, for which poly-
nomial algorithms exist, the computational cost is still dou-
bly exponential. Recent years have seen substantial work on
computationally tractable approaches to game theory, but
they focus mainly on scaling up with the number of play-
ers, not the number of actions [12].
Further, equilibrium
strategies, either mixed or pure, assume optimal play on the
part of the opponent, which is highly unrealistic in our case.
When this assumption is not met, standard game theory
gives no guidance on how to play.
(This, and computa-
tional intractability, have significantly limited its practical
use.) We thus leave the general existence and form of Nash
or other equilibria in adversarial classification as an open



100
Research Track Paper

Symbol
Meaning
X = (X1, X2, . . . , Xn)
Instance.
P (x)
Probability distribution of untainted data.
Xi
ith feature (attribute).
X,Xi
Domain of X and Xi, respectively.
x , xi
An instance and the ith attribute of that instance.
S,T
Training and test set.
yC = C(x)
The Classifier function.
xA = A(x)
The Adversary transformation.
Vi
Cost of measuring Xi.
UC(yC, y)
Utility for Classifier of classifying as yC an instance of class y.
Wi(xi, xi), W (x, x )
Cost of changing the ith feature from xi to xi and instance x to x , respectively.
UA(yC, y)
Utility accrued by Adversary when Classifier classifies as yC an instance of class y.
XC(x)
Set of features measured by C.
LOC(xi)
Log-odds or "contribution" of ith attribute to naive Bayes classifier
ln
P (Xi=xi|+)
P (Xi=Xi|-)
.

gap(x)
gap(x) > 0  NB classifies x as positive
LOC(x) -
UC (-,-)-UC (+,-)
UC (+,+)-UC (-,+)
.
UA
Adversary's utility gain from successfully camouflaging a positive instance (UA(-,+)-UA(+,+)).
LOi,x
i
"Gain" towards making x negative by changing ith feature to xi (LOC(xi) - LOC(xi)).
MCC(x)
Nearest instance (costwise) to x which Naive Bayes classifies as negative.
x[i
=xi]
An instance identical to x except that ith attribute is changed to xi  Xi.
PA(x)
Probability distribution after Adversary has modified the data.

Table 1: Summary of the notation used in this paper.


question, and propose instead to start from a set of assump-
tions that more closely resembles the way adversarial clas-
sification takes place in practice: Classifier initially oper-
ates assuming the data is untainted (i.e., A(x) = x for all
x); Adversary then deploys an optimal plan A(x) against
this classifier; Classifier in turn deploys an optimal classi-
fier C(A(x)) against this adversary, etc. This approach has
some commonality with evolutionary game theory [26], but
the latter makes a number of assumptions that are inappro-
priate in our case (infinite population of players repeatedly
matched at random, symmetric payoff matrices, players hav-
ing offspring proportional to average payoff, etc.).
In this paper, we focus mainly on the single-shot version of
the adversarial classification game: one move by each of the
players. We touch only briefly on the repeated version of the
game, where players continue to make moves indefinitely.
A number of learning approaches to repeated games have
been proposed [6], but these are also intractable in large ac-
tion spaces. Other learning approaches focus on games with
sequential states (e.g., [15]), while classification is stateless.
We make the assumption, standard in game theory, that
all parameters of both players are known to each other. Al-
though this is unlikely to be the case in practice, it is gener-
ally plausible that each player will be able to make a rough
guess of the other's (and, indeed, its own) parameters. Clas-
sification with imprecisely known costs and other parame-
ters has been well studied in KDD (e.g., [20]), and extending
this to the adversarial case is an important item for future
work.


3. COST-SENSITIVE LEARNING
In this paper, we will focus on naive Bayes as the classi-
fier to be made adversary-aware [2]. Naive Bayes is attrac-
tive because of its simplicity, efficiency, and excellent perfor-
mance in a wide range of applications, including adversarial
ones like spam detection [23].
Naive Bayes estimates the
probability that an instance x belongs to class y as



P (y|x) =
P (y)
P (x) P
(x|y) =
P (y)
P (x)
n



i=1
P (xi|y)
(3)


and predicts the class with highest P (y|x). The denomina-
tor P (x) is independent of the class, and can be ignored.
P (x|y) =
n
i=1
P (xi|y) is the naive Bayes assumption. The
relevant probabilities are learned simply by counting the cor-
responding occurrences in the training set S.
We begin by extending naive Bayes to incorporate the
measurement costs Vi and classification utilities UC(yC, y)
defined in the previous section, and to maximize the ex-
pected utility UC (Equation 1). For now, we assume that no
adversary is present (i.e., A(x) = x for all x). We remove
this restriction in the next sections.
Cost-sensitive learning has been the object of substantial
study in the KDD literature [1, 27]. Given a classification
utility matrix UC(yC, y), the Bayes optimal prediction for
an instance x is the class yC that maximizes the conditional
utility U(yC|x):


U(yC|x) =
yY
P (y|x)UC(yC, y)
(4)


This is simply Equation 1 conditioned on a particular x, and
ignoring the adversary and measurement costs Vi. In naive
Bayes, P (y|x) is computed using Equation 3.
Measurement costs are incorporated into the choice of
which subset of features to measure, XC {X1, . . . , Xn}. In-
tuitively, we want to measure feature Xi only if this improves
the expected utility UC by more than Vi. Since a feature's
effect on UC will in general depend on what other features
are being measured, finding the optimal XC requires a po-
tentially exponential search. In practice, XC can be found
using standard feature selection algorithms with UC as the
evaluation function. We use greedy forward selection ([13])



101
Research Track Paper

in our experiments. (Feature selection can also be carried
out online, but we do not pursue that approach here.)

4. ADVERSARY STRATEGY
In this section, we formalize the notion of an optimal strat-
egy for Adversary. We model it as a constrained optimiza-
tion problem, which can be formulated as an integer linear
program. We then propose a pseudo-linear time solution to
the integer LP, based on dynamic programming. We make
the following assumptions.

Assumption 1. Complete Information: Both Clas-
sifier and Adversary know all the relevant parameters:
Vi, UC, Wi, UA and the naive Bayes model learned by Clas-
sifier on S (including XC, P (y), and P (xi|y) for each fea-
ture and class).

Assumption 2. Adversary assumes that Classifier is
unaware of its presence (i.e., Adversary assumes that C(x)
is the naive Bayes model described in the previous section).

To defeat Classifier, Adversary needs only to modify
features in XC, since the others are not measured.
From
Equation 3:


log
P (+|x)
P (-|x)
= log
P (+)
P (-)
+
xiXC
log
P (xi|+)
P (xi|-)
(5)


For brevity, we will use the notation LOC(x) = log
P (+|x)
P (-|x)
and LOC(xi) = log
P (xi|+)
P (xi|-)
, where LO is short for "log odds."
Naive Bayes classifies an instance x as positive if the ex-
pected utility of doing so exceeds that of classifying it as
negative, i.e., if UC(+, +)P (+|x) + UC(+, -)P (-|x) > UC(-, +)
P (+|x) + UC(-, -)P (-|x), or


P (+|x)
P (-|x)
>
UC(-, -) - UC(+, -)
UC(+, +) - UC(-, +)
(6)

Let the log of the right hand side be LT (UC) (log thresh-
old). Then naive Bayes classifies instance x as positive if
LOC(x) > LT (UC), or equivalently if gap(x) > 0, where
gap(x) = LOC(x) - LT (UC). If the instance is classified as
negative, Adversary does not need to do anything. Let us
assume, then, that x is classified as positive, i.e., gap(x) > 0.
The objective of Adversary is to make some set of feature
changes to x that will cause it to be classified as negative,
while incurring the minimum possible cost. This causes Ad-
versary to gain a utility of UA = UA(-, +) - UA(+, +).
Thus Adversary will transform x as long as the total cost
incurred is less than UA and not otherwise.
We formulate the problem of finding an optimal strategy
for Adversary as an integer linear program. Recall that
Xi is the domain of Xi. For xi  Xi, let i,x
i
be an integer
(binary) variable which takes the value one if the feature Xi
is changed from xi to xi, and zero otherwise. Let the new
data item thus obtained be x . The cost of transforming x
to x is W (x, x ) = Wi(xi, xi), and the resulting change in
log odds is LOC(x ) - LOC(x) = LOC(xi) - LOC(xi). Define
LOi,x
i
= LOC(xi) - LOC(xi). This is the gain in Adver-
sary's objective of making the instance negative. Note that
LOi,xi = 0; this represents the case where Xi has not been
changed. To transform x so that the new instance is classi-
fied as negative, Adversary needs to change the values of
some features such that the sum of their gains (decrease in
log odds) is more that gap(x).
Thus, to find the minimum cost changes required to trans-
form this instance into a negative instance, we need to solve
the following integer (binary) linear program:


min 

XiXC
xiXi
Wi(xi, xi)i,x
i



s.t.




XiXC
xiXi
LOi,x
i
i,x
i
 gap(x)


i,x
i
 {0,1},
xiXi
i,x
i
 1



The binary i,x
i
values encode which features are changed to
which values. The optimizing equation minimizes the cost
incurred in this transformation. The first constraint makes
sure that the new instance will be classified as negative. The
second constraint encodes the requirement that a feature
can only have a single value in an instance. We will call the
transformed instance obtained by solving this integer linear
program the minimum cost camouflage (MCC) of x.
In
other words, MCC(x) is the nearest instance (costwise) to
x which naive Bayes classifies as negative.
After solving this integer LP, Adversary transforms the
instance only if the minimum cost obtained is less than UA.
Therefore, letting NB(x) be the naive Bayes class prediction
for x,



A(x) =
MCC(x)
if NB(x) = +, W (x, MCC(x)) < UA
x
otherwise
(7)
The above integer (binary) LP problem is NP-hard, as the
0-1 knapsack problem can be reduced to it [8]. However, a
pseudo-linear time algorithm can be obtained by discretiz-
ing LOC, which allows dynamic programming to be used.
Although the algorithm is approximate, it can compute the
solution to arbitrary precision.
The procedure is shown in Algorithm 1. Function Find-
MCC(i, w) computes the minimum cost needed to change
the log odds of x by w using only the first i features. It re-
turns the pair (MinCost, MinList) where MinCost is the
minimum cost and MinList is a list of feature-value pairs
denoting the changes to be made to x. (In each pair, i is
the feature index and xi is the value it should be changed
to.) To obtain the optimal adversary strategy, we need to
compute FindMCC(n, W ), where the integer W is gap(x)
after discretization and n is the number of features in XC.
Note that LOi,x
i
is now a (non-negative) integer in the
discretized log odds space.
The algorithm can be efficiently implemented using top-
down recursion with memoization (so that no recursive call
is computed more than once). Note that although the fea-
tures can be considered in any order, some orderings may
find solutions faster than the others. If, in the discretized
space, instance x requires a gap of W to be filled by the
transformation, then the algorithm runs in time at most
O(W
i
|Xi|) (since the for loop in FindMCC is called at
most W times and each time it takes O(
i
|Xi|) time).
Hence it is pseudo-linear in the number of features. Pseudo-



102
Research Track Paper

linearity may be expensive for large values of W or for cases
where features have large domains.
We now present two
pruning rules, one for use in the first situation, and one for
the second.

Algorithm 1 FindMCC(i,w)
if w  0 then
return (0, {})
end if
if i = 0 then
return (,Undefined)
end if
MinCost  
MinList  Undefined
for xi  Xi do
if LOi,x
i
 0 then
(CurCost, CurList)FindMCC(i - 1, w - LOi,x
i
)
CurCost  CurCost + Wi(xi, xi).
CurList  CurList + (i, xi).
if CurCost < MinCost then
MinCost  CurCost
MinList  CurList
end if
end if
end for
return (MinCost, MinList)



Algorithm 2 A(x)
W  gap(x) (discretized).
(MinCost, MinList) FindMCC(n, W )
if NB(x) = + and MinCost < UA then
newx  x
for all (i, xi)  MinList do
newxi  xi
end for
return newx
else
return x
end if



Lemma 4.1. If


max
i,xi
LOi,x
i
Wi(xi, xi)
<
gap(x)
UA

then A(x) = x.

This lemma is easy to prove and can be used to detect
the instances for which MinCost > UA. Instances which
are positive by very large gap(x) values can thus be pruned
early on, and we need to run the algorithm only for more
reasonable values of gap(x).
Our second pruning strategy can be employed in situa-
tions where the cost metric is sufficiently coarsely discretized.
We globally sort all the (i, xi) tuples in increasing order of
Wi(xi, xi)  0. For identical values of Wi(xi, xi), we use
decreasing order of LOi,x
i
as the secondary key. For a
particular i, Wi(xi, xi) combination, over all i, we can re-
move all but the first entry in the list. This is valid because,
if the Xi is changed in the optimal solution, then taking the
value xi with the highest LOi,x
i
will also yield the optimal
solution. We can prune even further by only considering the
first k tuples in each W such that
i-1
j=1
LOj,x
j
> gap(x)

and
k
j=1
LOj,x
j
< gap(x). It is easy to see that this
pruning does not affect the optimal solution.
Thus, if the feature-changing costs Wi are sufficiently co-
arsely discretized, we will never need to consider more than
a few tuples for each integer value of Wi. Our algorithm will
thus run efficiently even when the domains of features are
large.

5. CLASSIFIER STRATEGY
We now describe how Classifier can adapt to the adver-
sary strategy described in the previous section. We derive
the optimal C(x) taking into account A(x), and give an ef-
ficient algorithm for computing it. We make the following
additional assumptions.

Assumption 3.
Classifier assumes that Adversary
uses its optimal strategy to modify test instances (Algorithm 2).

Assumption 4. The training set S used for learning the
initial naive Bayes classifier is not tampered with by Ad-
versary (i.e., S is drawn from the real distribution of ad-
versarial and non-adversarial data).

Assumption 5. Xi  X , Wi(xi, xi) is a semi-metric,
i.e., it has the following properties:

1. Wi(xi, xi)  0 and the equality holds iff xi = xi

2. Wi(xi, xi )  Wi(xi, xi) + Wi(xi, xi )

The above also implies that W (x, x )  W (x, x )+W (x , x ).
The triangular inequality for cost holds in most real do-
mains. This is because to change a feature from xi to xi
the adversary always has the option of changing it via xi,
i.e., with xi as an intermediate value.
The goal of Classifier, as in Section 3, is to predict for
each instance x
the class that maximizes its conditional
utility (Equation 4). The difference is that now we want to
take into account the fact that Adversary has tampered
with the data. Of all the probabilities used by Classifier
(Equation 3), the only one that is changed by Adversary
is P (x |+); P (+), P (-) and P (x |-) remain unaltered. Let
PA(x |+) be the post-adversary version of P (x |+). Then


PA(x |+) =
xX
P (x|+)PA(x |x, +)
(8)


In other words, the probability of observing an instance x is
the probability that the adversary generates some instance
x and then modifies it into x , summed over all x. Since
PA(x |x, +) = 1 if A(x) = x and PA(x |x, +) = 0 otherwise,


PA(x |+) =
xXA(x )
P (x|+)
(9)


where XA(x ) = {x : x
= A(x)}.
There are two cases
where Adversary will leave an instance x untampered (i.e.,
A(x) = x): when naive Bayes predicts it is negative, since
then no action is necessary, and when there is no transfor-
mation of x whose cost is lower than the utility gained by
making it appear negative. Thus



103
Research Track Paper

PA(x |+) =

x
XA(x )
P (x|+)

+ I(x )P (x |+)
(10)


where XA(x ) = XA(x ) \ {x }, I(x ) = 1 if NB(x ) = -
or W (x , MCC(x ))  UA, and I(x ) = 0 otherwise (see
Equation 7 and Algorithm 2). The untampered probabilities
P (x |+) are estimated using the naive Bayes model (Equa-
tion 3): P (x |+) =
XiXC
P (Xi = xi|+).1
The optimal
adversary-aware classification algorithm C(x ) is shown be-
low, with
^
P () used to denote the probability P () estimated
from the training data S.
^
PA(x |+) is given by Equation 10
using the empirical estimates of P (x |+). The second term
in Equation 10, I(x )P (x |+), is easy to compute given calls
to NB(x ) and Algorithm 1 to determine if x has a feasi-
ble camouflage. The remainder of this section is devoted to
efficiently computing the first term,
xXA(x )
P (x|+).


Algorithm 3 C(x )

P -x 
^
P (-)
i
^
P (Xi = xi|-)
P +x 
^
P (+)
^
PA(x |+)
U(+|x )  P +x UC(+, +) + P -x UC(-, +)
U(-|x )  P +x UC(-, +) + P -x UC(-, -)
if U(+|x ) > U(-|x ) then
return +
else
return -
end if

One solution is to iterate through all possible positive ex-
amples and check if x is their minimum cost camouflage.
This is, of course, not feasible. We now study some theo-
retical properties of the MCC function which will later be
used to prune this search. Recall that if NB(x) =- then
gap(x) < 0 and vice versa. We define x[i
=xi]
as a data in-
stance which is identical to x except that its ith feature is
changed to xi.

Lemma 5.1. Let xA be any positive instance and let x =
MCC(xA). Then, i,

(xA)i = xi  gap(x ) + LOC((xA)i) - LOC(xi) > 0
Proof. Let x
= x[i
=(xA)i]
. This implies that W (xA, x )
< W (xA, x ), since x
differs from xA on one less feature
than x . Also gap(x ) = gap(x ) + LOC((xA)i) - LOC(xi).
Since x is MCC(xA) and W (xA, x ) < W (xA, x ), NB(x )
must be +, and therefore gap(x ) > 0, proving the result.


Given a negative instance x , for each feature we compute
all values v that satisfy Lemma 5.1. To compute XA(x ),
we only need to take combinations of these feature-value
pairs and check if x is their MCC. This can substantially
reduce the number of positive instances in our search space.
The search space can still potentially contain an exponential
number of instances.
However, after we employ the next
theorem, we obtain a fast algorithm for estimating the set
XA(x ).
1
Notice
that the optimal
feature
subset
XC for
the
adversary-aware
classifier
may
be
different
from
the
adversary-unaware one, but can be found using the same
methods (see Section 3).
Theorem 5.2. Let xA be a positive instance such that
x
= MCC(xA).
Let D be the set of features that are
changed in xA to obtain x .
Let E be a non-trivial subset
of D, and let xA be an instance that matches x for all fea-
tures in E and xA for all others, i.e., (xA)i = xi if Xi  E,
(xA)i = (xA)i otherwise. Then x = MCC(xA).
Proof. By contradiction. Suppose x
= MCC(xA) and
x
= x . Then W (xA, x ) < W (xA, x ). Also, since E  D,
by definition of W (x, y) we have W (xA, x ) = W (xA, xA) +
W (xA, x ). So by the triangle inequality

W (xA, x )
 W(xA,xA) + W(xA,x )
<
W (xA, xA) + W (xA, x )
=
W (xA, x )

Thus W (xA, x ) < W (xA, x ), which gives a contradiction,
since then x = MCC(xA). This completes the proof.

The above theorem implies that if xA is a positive instance
such that x = MCC(xA) then x cannot be the MCC of any
other instance xA such that the changed features from xA
to x form a superset of the changed features from xA to
x . We now use the following result to obtain bounds on
XA(x ).

Corollary 5.3. Let F V be the set of feature-value pairs
that satisfy Lemma 5.1. Let GV  F V be such that (i, xi) 
GV if x[i
=xi]
 XA(x ). Then for every xA  XA(x ), the set
of feature-value pairs where xA and x differ form a subset
of GV .

From the above corollary, after we compute GV , we only
need to consider the combinations of feature-value pairs that
are in GV and change those in the observed instance x .
Theorem 5.2 also implies that performing single changes
from GV returns instances in XA(x ).
This gives us the
following bounds on
xXA(x )
P (x|+).

Theorem 5.4. Let x be any instance and let GV be the
set defined in Corollary 5.3. Let G = {i|xi(i, xi)  GV }
and let X G
i
= {xi  Xi|(i, xi)  GV }. Then


(i,xi)GV
P (x[i
=xi]
|+) 
xXA(x )
P (x|+) 




i
G

1
+
xiX G
i
P (x[i
=xi]
|+)
P (x |+)


- 1


Proof. The proof of the first inequality follows directly
from Theorem 5.2, which states that changing any single
feature of x to any value in GV returns an instance from
XA(x ). To prove the second inequality, we observe that the
expression on the right side, when expanded, gives the sum
of probabilities of all possible changes in x due to the set
GV , and XA(x ) is a subset of those instances.

Given these bounds, we can classify a test instance as fol-
lows. If plugging the lower bound into Algorithm 3 gives
U(+|x ) > U(-|x ), then the instance can be safely classi-
fied as positive. Similarly, if using the upper bound gives
U(+|x ) < U(-|x ), then the instance is negative.
If GV
is large, so is the lower bound on PA(XA(x )|+). If GV is
small, we can do an exhaustive search over the subsets of GV
and check if each of the items considered belongs to XA(x ).
In our experiments we find that using the lower bound for
making predictions works well in practice.



104
Research Track Paper

6. EXPERIMENTS
We implemented an adversarial classifier system for the
spam filtering domain. Spam is an attractive testbed for
our methods because of its practical importance, its rapidly
evolving adversarial nature, the wide availability of data (in
contrast to many other adversarial domains), the fact that
naive Bayes is the de facto standard classifier in this area,
and its richness as a challenge problem for KDD [4]. One
disadvantage of spam as a testbed is that feature measure-
ment costs are generally negligible, leaving this part of our
framework untested. (In contrast, in a domain like counter-
terrorism feature measurements are a major issue, often
requiring large numbers of personnel and expensive equip-
ment, raising privacy issues, and imposing costs on millions
of individuals and transactions.)
We used the following two datasets in our experiments:

Ling-Spam [24]: This corpus contains the legitimate dis-
cussions on a linguistics mailing list and the spam
mails received by the list. There are 2412 non-spam
messages and 481 spam ones. Thus, around 16.6% of
the corpus is spam.

Email-Data [19]: This corpus consists of texts from 1431
emails, with 642 non-spam messages (conferences (370)
and jobs (272)) and 789 spam ones.

Each of these datasets was divided into ten parts for ten-
fold cross-validation.
We defined three scenarios, as de-
scribed below, and applied our implementation of naive Bayes
(NB) and the adversary-aware classifier (AC) to each. We
used ifile [21] for preprocessing emails.

6.1 Scenarios
The three spam filtering scenarios that we implemented
differ in how the email is represented for the classifier, how
the adversary can modify the features, and at what cost.

Add Words (AW): This the simplest scenario. The bino-
mial model of text for classification is used [18]: there
is one Boolean feature per word, denoting whether or
not the word is present in the email. The only way
to modify the email is by adding words which are not
already present, and each word added incurs unit cost.
This is akin to saying that the original mail has content
that the spammer is not willing to change, and thus
the spammer only adds unnecessary words to fool the
spam detector. In this model, Adversary's strategy
reduces to a greedy search where it adds words in de-
creasing order of LOC.

Add Length (AL): This model is very similar to AW, ex-
cept that the cost of adding a word is proportional to
the number of characters in it. This corresponds to
a hypothetical situation where Adversary needs to
pay a certain amount per bit of mail transmitted, and
wants to minimize the number of characters sent.

Synonym (SYN): Generally, spammers want to avoid de-
tection while preserving the semantic content of their
messages. Thus, in this scenario we consider the case
where Adversary changes the mail by replacing the
existing words by other semantically similar words.
For example, a spammer attempting to sell a product
would like to send emails claiming it to be cheap, but
(+, +)
(+, -)
(-, +)
(-, -)
UA
0
0
20
0
UC
1
-10/-100/-1000
-1
1

Table 2: Utility matrices for Adversary and Classi-
fier used in the experiments.


without the use of words like "free," "sale", etc. This
is because the naive Bayes classifier uses the presence
or absence of specific words with high LOC to classify
emails, independent of their actual meaning.
Given
the above intent, we define this scenario as follows.
We use the multinomial model of text [18]: an email
is viewed a sequence of word positions, with one fea-
ture per position, and the domain of each feature is
the set of words in the vocabulary. In this case, the
number of times a word occurs in an email is impor-
tant. However, the word order is disregarded (i.e., the
probability of word occurrence is assumed to be inde-
pendent of location). For each word, we obtain a list
of synonyms from the WordNet lexical database [28].
A word in an email can then be changed only to one
of its synonyms, at unit cost.

It is easy to see that the costs used in all scenarios are
metrics, so we can apply Lemma 5.1 and Theorem 5.2.
The UA classification utility matrix for Adversary we
used is such that whenever a spam email is classified as
non-spam the adversary receives a utility of 20, and all other
entries are 0. Thus, in the SYN and AW scenarios 20 word
replacements/additions are allowed. In the AL scenario, the
cost of adding a character is set to 0.1, and as a result 200
character additions are allowed.
For Classifier, we ran the experiments with three dif-
ferent utility matrices (UC). All matrices had a utility of
1 for correctly classifying an email and a penalty (negative
utility) of 1 for incorrectly classifying a spam email as non-
spam. The penalty for incorrectly classifying a non-spam
email as spam was set to 10 in one matrix, 100 in another,
and 1000 in the third. This reflects the fact that, in spam
filtering, the critical and dominant cost is that of false posi-
tives: letting a single spam email get through to the user is
a relatively insignificant event, but filtering out a non-spam
email is highly undesirable (and potentially disastrous). The
different UC(+, -) values correspond to the different values of
the  parameter in Sakkis et al [24]. Table 2 summarizes the
utility parameters used in the experiments.

6.2 Results
The results of running the various algorithms on the Ling-
Spam and Email-data datasets are shown in Figures 1 and
2 respectively.
The figures show the average utilities ob-
tained (with a maximum value of 1.0) by naive Bayes and
the adversary-aware classifier under the different scenarios
and different UC matrices.
The utility of naive Bayes on
the original, untampered data ("PLAIN") is represented by
the black bar on the left. The remaining black bars rep-
resent the performance of naive Bayes on tainted data in
the three scenarios, and the white bars the performance of
the corresponding adversary-aware classifier.
We observe
that Adversary significantly degrades the performance of
naive Bayes in all three scenarios and with all three Classi-
fier utility matrices. This effect is more pronounced in the
Email-Data set because it has a higher percentage of spam



105
Research Track Paper

PL AW SYN AL
-0.2
0
0.2
0.4
0.6
0.8
1
U
C
(+,-)=-10




Scenarios
Avg.
Utility




NB
AC

PL AW SYN AL
-0.2
0
0.2
0.4
0.6
0.8
1
U
C
(+,-)=-100




Scenarios
Avg.
Utility




NB
AC

PL AW SYN AL
-0.2
0
0.2
0.4
0.6
0.8
1
U
C
(+,-)=-1000




Scenarios
Avg.
Utility




NB
AC




Figure 1: Utility results on the Ling-Spam dataset
for different values of UC(+, -).




PL AW SYN AL
-0.2
0
0.2
0.4
0.6
0.8
1
U
C
(+,-)=-10




Scenarios
Avg.
Utility




NB
AC

PL AW SYN AL
-0.2
0
0.2
0.4
0.6
0.8
1
U
C
(+,-)=-100




Scenarios
Avg.
Utility




NB
AC
PL AW SYN AL
-0.2
0
0.2
0.4
0.6
0.8
1
U
C
(+,-)=-1000




Scenarios
Avg.
Utility




NB
AC




Figure 2: Utility results on the Email-Data set for
different values of UC(+, -).


emails than Ling-Spam. For naive Bayes on Email-Data,
the cost of misclassifying spam emails exceeds the utility of
the correct predictions, causing the overall utility to be neg-
ative. In contrast, Classifier was able to correctly identify
a large percentage of the spam emails in all cases, and its
accuracy on non-spam emails was also quite high.
To help in interpreting these results, we report the num-
bers of false negatives and false positives for the Ling-Spam
dataset in Table 3. We observe that as the misclassifica-
tion penalty for non-spam increases, fewer non-spam emails
are classified incorrectly, but naturally more spam emails
are misclassified as non-spam. Notice that the adversarial
classifier never produces false positives (except for the SYN
scenario with UC(+, -) = 10). As a result, its average utility
stays approximately constant even when UC(+, -) changes
by two orders of magnitude. An interesting observation is
that Adversary's manipulations can actually help Classi-
fier to reduce the number of false positives. This is because
Adversary is unlikely to send a spam mail unaltered, and
as a result many non-spam emails which were previously
UC(+, -)
10
100
1000
Classifier
FN
FP
FN
FP
FN
FP
NB-PLAIN
94
2
124
1
165
1
NB-AW
481
2
481
1
481
1
AC-AW
93
0
123
0
164
0
NB-AL
477
2
477
1
477
1
AC-AL
94
0
124
0
165
0
NB-SYN
408
2
413
1
414
1
AC-SYN
164
1
196
0
229
0

Table 3:
False positives and false negatives for
naive Bayes and the adversary-aware classifier on
the Ling-Spam dataset. The total number of posi-
tives in this dataset is 481, and the total number of
negatives is 2412.


classified as positive are now classified as negative.
We also compared the running times of our algorithms
for the three scenarios, for both the adversary and classi-
fier strategies.
For both AW and SYN models, the aver-
age running times were less than 5 ms per email. For AL,
the average running time of the classifier strategy was less
than 5 ms per mail while the running time of the adversary
strategy was around 500 ms per email. The adversary run-
ning time for AW was small because one can use a simple
greedy algorithm to implement the adversary strategy. In
the SYN model, the search space is small because there are
few synonyms per word. Hence the time taken by both algo-
rithms is small. However, in the AL model, when the LOC of
emails was high (> 50), the adversary took longer. On the
other hand, the adversarial classifier, after using the pruning
strategies, had to consider very few instances and these had
small LOC. Hence its running time was quite small. From
the experiments we can conclude that in practice we can
use the pruning strategies for Adversary and Classifier
to reduce the search space and time without compromising
accuracy.
To simulate the effects of non-adversarial concept drift (a
reality in spam and many other adversarial domains), we
also tried classifying the mails in the Email-data set using
NB and AC trained on the Ling-Spam data.
As the fre-
quencies of spam and non-spam mails are different in the
two datasets, we ran the classifiers without considering the
class priors. For both algorithms, the results obtained were
only marginally worse than the results obtained by training
on the Email-data set itself, demonstrating the robustness
of the algorithms.

6.3 Repeated Game
In Sections 4 and 5 we discussed one round of the game
that goes on between Adversary and Classifier. It con-
sists of one ply of Adversary in which it finds the best
strategy to fool Classifier and then one ply of Classifier
to adapt to it. Both parties can continue playing this game.
However, Classifier is no longer using a simple naive Bayes
algorithm. In these experiments, we make the simplifying
assumption that Adversary continues to model the classi-
fier as Naive Bayes, and uses the techniques that we have
developed for Naive Bayes.
At the end of each round, Adversary learns a Naive
Bayes classifier based on the outputs of the actual classi-
fier that Classifier is using in that round.
We denote
the classifier used by Classifier in round i - 1 by Ci
-1
.



106
Research Track Paper

Let NBi
-1
be the classifier that Adversary learns from it.
Then Ai(x) is defined as the optimal adversary strategy (as
in Algorithm 2) to fool NBi
-1
instead of the original NB
learned on S. The data coming from Adversary in round
i is Ai applied to the original test data to produce Ti, i.e.,
Ti = Ai(T ). Classifier uses Algorithm 3 based on NBi
-1
to classify Ti, i.e., Yi = Ci(Ti). The key insight is that a
new naive Bayes model NBi can be trained on (Ti, Yi) and
that can serve as the "Classifier" for the next round. We
compared the performance of NBi with that of Ci and found
them to be very similar, justifying our assumption, as Ad-
versary is not reacting to a "crippled" Classifier but to
one which performs almost as well as the "optimal" Clas-
sifier. This procedure can then repeated for an arbitrary
number of rounds.
Figure 3 shows the results of this experiment on the Ling-
Spam dataset for the AW scenario. The X-axis is round i
of the game, and the Y-axis is the average utility obtained
by Yi (the ith adversary-aware classifier). The graphs also
show the average utility obtained by NBi
-1
(Ti), to demon-
strate the effect of using an adversary-aware classifier at each
round.
In all rounds of the game, Classifier using the adversary-
aware strategy performs significantly better than the plain
naive Bayes. As expected, the difference is highest when the
penalty for misclassifying non-spam is 1000. Furthermore,
in this scenario Classifier and Adversary never reach an
equilibrium, and utility alternates between two values. This
is surprising at first glance, but a closer examination eluci-
dates the reason. In the AW scenario, Adversary can only
add words. So the only way of tampering with an instance
is to add "good" words with very low (negative) LOC (based
on NBi
-1
in the ith round). Let the top few "good" words
be GWi
-1
. These would have a high frequency of occur-
rence in spam mails of Ti. When NBi is learned on (Ti, Yi)
these words no longer have a low LOC and hence are not in
GWi. Thus, Ai
+1
ignores these words, making them have
a high LOC in NBi
+1
and be in GWi
+1
! This phenomenon
causes LOC values for a word to oscillate, giving rise to the
periodic average utility in Fig. 3.




-0.5
0
0.5
1




0
2
4
6
8
10
12
14
Avg.
Utility




Round
AC(10)
AC(100)
AC(1000)
NB(10)
NB(100)
NB(1000)




Figure 3: Utility of naive Bayes and the adversarial
classifier for a repeated game in the AW scenario
and Ling-Spam dataset. The number in parentheses
is UC(+,-).
7. FUTURE WORK
This paper is only the first step in a potentially very rich
research direction. The next steps include:

Repeated games. In reality, Adversary and Classifier
never cease to evolve against each other. Thus, we need
to find the optimal strategy A for Adversary taking
into account that an adversarial classifier C(A(x)) is be-
ing used, then find the optimal strategy for Classifier
taking A into account, and so on indefinitely. To what
extent this can be done analytically is a key open ques-
tion.

Theory. We would like to answer questions like: What are
the most general conditions under which adversarial clas-
sification problems have Nash or correlated equilibria? If
so, what form do they take, and are there cases where
they can be computed efficiently? Under what condi-
tions do repeated games converge to these equilibria?
Etc.

Incomplete information. When Classifier and Adver-
sary do not know each other's parameters, and Adver-
sary does not know the exact form of the classifier, addi-
tional learning needs to occur, and the optimal strategies
need to be made robust to imprecise knowledge.

Approximately optimal strategies.
When finding the
optimal strategy is too computationally expensive, ap-
proximate solutions and weaker notions of optimality be-
come necessary. Also, real-world adversaries will often
act suboptimally, and it would be good to take this into
account.

Generalization to other classifiers. We would like to ex-
tend the ideas in this paper to classifiers like decision
trees, nearest neighbor, support vector machines, etc.

Interaction with humans. Because adversaries are resour-
ceful and unpredictable, adversarial classifiers will al-
ways require regular human intervention. The goal is
to make this as easy and productive as possible. For ex-
ample, extending the framework to allow new features
to be added at each round of the game could be a good
way to combine human and automatic refinement of the
classifier.

Multiple adversaries. Classification games are often played
against more than one adversary at a time (e.g., mul-
tiple spammers, intruders, fraud perpetrators, terrorist
groups, etc.). Handling this case is a natural but non-
trivial extension of our framework.

Variants of the problem. Our problem definition does not
fit all classification games, but it could be extended ap-
propriately. For example, adversaries may produce inno-
cent as well as malicious instances, they may deliberately
seek to make the classifier produce false positives, detec-
tion of some malicious instances may deter them from
producing more, etc.

Other domains and tasks. We would like to apply ad-
versarial classifiers to computer intrusion detection, fraud
detection, face recognition, etc., and to develop adver-
sarial extensions to related data mining tasks (e.g., ad-
versarial ranking for search engines).



107
Research Track Paper

8. CONCLUSION
In domains ranging from spam detection to counter-terror-
ism, classifiers have to contend with adversaries manipulat-
ing the data to produce false negatives. This paper formal-
izes the problem and extends the naive Bayes classifier to
optimally detect and reclassify tainted instances, by taking
into account the adversary's optimal feature-changing strat-
egy. When applied to spam detection in a variety of sce-
narios, this approach consistently outperforms the standard
naive Bayes, sometimes by a large margin. Research in this
direction has the potential to produce KDD systems that
are more robust to adversary manipulations and require less
human intervention to keep up with them.


ACKNOWLEDGMENTS
We are grateful to Daniel Lowd, Foster Provost and Ted Sen-
ator for their insightful comments on a draft of this paper.
This research was partly supported by a Sloan Fellowship
awarded to the second author.


9. REFERENCES

[1] P. Domingos. MetaCost: A general method for making
classifiers cost-sensitive. In Proceedings of the Fifth
ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages
155­164, San Diego, CA, 1999. ACM Press.
[2] P. Domingos and M. Pazzani. On the optimality of the
simple Bayesian classifier under zero-one loss. Machine
Learning, 29:103­130, 1997.
[3] R. B. Doorenbos, O. Etzioni, and D. S. Weld. A
scalable comparison-shopping agent for the
World-Wide Web. In Proceedings of the First
International Conference on Autonomous Agents,
pages 39­48, Marina del Rey, CA, 1997. ACM Press.
[4] T. Fawcett. "In vivo" spam filtering: A challenge
problem for KDD. SIGKDD Explorations,
5(2):140­148, 2003.
[5] T. Fawcett and F. Provost. Adaptive fraud detection.
Data Mining and Knowledge Discovery, 1(3):291­316,
1997.
[6] D. Fudenberg and D. Levine. The Theory of Learning
in Games. MIT Press, Cambridge, MA, 1999.
[7] D. Fudenberg and J. Tirole. Game Theory. MIT
Press, Cambridge, MA, 1991.
[8] M. R. Garey and D. S. Johnson. Computers and
Intractability. Freeman, New York, NY, 1979.
[9] L. Guernsey. Retailers rise in Google rankings as rivals
cry foul. New York Times, November 20, 2003.
[10] G. Hulten, L. Spencer, and P. Domingos. Mining
time-changing data streams. In Proceedings of the
Seventh ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 97­106,
San Francisco, CA, 2001. ACM Press.
[11] D. Jensen, M. Rattigan, and H. Blau. Information
awareness: A prospective technical assessment. In
Proceedings of the Ninth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pages 378­387, Washington, DC, 2003. ACM Press.
[12] M. Kearns. Computational game theory. Tutorial,
Department of Computer and Information Sciences,
University of Pennsylvania, Philadelphia, PA, 2002.
http://www.cis.upenn.edu/ mkearns/nips02tutorial/.
[13] R. Kohavi and G. John. Wrappers for feature subset
selection. Artificial Intelligence, 97(1-2):273­324, 1997.
[14] B. Krebs. Online piracy spurs high-tech arms race.
Washington Post, June 26, 2003.
[15] M. L. Littman. Markov games as a framework for
multi-agent reinforcement learning. In Proceedings of
the Eleventh International Conference on Machine
Learning, pages 157­163, New Brunswick, NJ, 1994.
Morgan Kaufmann.
[16] B. Lloyd. Been gazumped by Google? Trying to make
sense of the "Florida" update. Search Engine Guide,
November 25, 2003.
[17] M. V. Mahoney and P. K. Chan. Learning
nonstationary models of normal network traffic for
detecting novel attacks. In Proceedings of the Eighth
ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages
376­385, Edmonton, Canada, 2002. ACM Press.
[18] A. McCallum and K. Nigam. A comparison of event
models for Naive Bayes text classification. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Categorization, Madison, WI, 1998. AAAI Press.
[19] F. Nielsen. Email data, 2003.
http://www.imm.dtu.dk/ rem/datasets/.
[20] F. Provost and T. Fawcett. Robust classification for
imprecise environments. Machine Learning,
42:203­231, 2001.
[21] J. Rennie. Ifile spam classifier, 2003.
http://www.nongnu.org/ifile/.
[22] P. Robertson and J. M. Brady. Adaptive image
analysis for aerial surveillance. IEEE Intelligent
Systems, 14(3):30­36, 1999.
[23] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz.
A Bayesian approach to filtering junk e-mail. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Categorization, Madison, WI, 1998. AAAI Press.
[24] G. Sakkis, I. Androutsopoulos, G. Paliouras,
V. Karkaletsis, C.D. Spyropoulos, and
P. Stamatopoulos. A memory-based approach to
anti-spam filtering for mailing lists. In Information
Retrieval, volume 6, pages 49­73. Kluwer, 2003.
[25] T. Senator. Ongoing management and application of
discovered knowledge in a large regulatory
organization: A case study of the use and impact of
NASD regulation's advanced detection system (ADS).
In Proceedings of the Sixth ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, pages 44­53, Boston, MA, 2000. ACM
Press.
[26] J. M. Smith. Evolution and the Theory of Games.
Cambridge University Press, Cambridge, UK, 1982.
[27] P. Turney. Cost-sensitive learning bibliography. Online
bibliography, NRC Institute for Information
Technology, Ottawa, Canada, 1998.
http://members.rogers.com/peter.turney/bibliograph-
ies/cost-sensitive.html.
[28] WordNet 2.0: A lexical database for the English
language, 2003.
http://www.cogsci.princeton.edu/wn/.




108
Research Track Paper

